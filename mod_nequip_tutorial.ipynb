{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mod-nequip-tutorial",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabriele16/nequip/blob/main/mod_nequip_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFpAi8g9XmUU"
      },
      "source": [
        "# Molecular Dynamics with NequIP \n",
        "\n",
        "### Authors: Simon Batzner, Albert Musaelian, Lixin Sun, Anders Johansson, Boris Kozinsky"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMnK_PtDZ32t"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/nequip3.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6YiV6ShQWB2"
      },
      "source": [
        "## What is this? \n",
        "\n",
        "This is a Colab tutorial for NequIP, software for building extremely accurate Machine Learning Interatomic Potentials. The ideas are described in the paper below. We have released an open-source software with the goal of building NequIP potentials with a few simple commands, at the Github link below. This tutorial serves as a simple introduction into the NequIP code. \n",
        "\n",
        "The goal of NequIP is to be as simple as possible. You will never have to write a single line of Python, but instead you can train a network with one single command and you will be ready to run MD with it in LAMMPS or ASE. \n",
        "\n",
        "Paper: https://www.nature.com/articles/s41467-022-29939-5\n",
        "\n",
        "Code: https://github.com/mir-group/nequip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8gImqa_N_e1"
      },
      "source": [
        "## Contents\n",
        "\n",
        "This tutorial will teach you how to:\n",
        "\n",
        "* Train a model \n",
        "* Deploy the model intro production\n",
        "* Run MD with it in LAMMPS\n",
        "\n",
        "We will do all this in this Colab, including LAMMPS. Training + inference will take only about 10 minutes. Before you get started, however, you will have to compile LAMMPS which takes approximately 5 minutes. Once we have installed NequIP + LAMMPS, we're ready to get started. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYSB405TdZWF"
      },
      "source": [
        "## Before you begin 🛑\n",
        "\n",
        "1. Save a copy of this colab in your own drive \n",
        "2. Run the first two code cells below to install NequIP and the Molecular Dynamics code LAMMPS \n",
        "\n",
        "## Now you're ready to get started :) ✅"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.10"
      ],
      "metadata": {
        "id": "dnUlOqq8978r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d31d184-db97-452f-9b6f-a8bc58a714a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.10\n",
            "  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:42tcmalloc: large alloc 1147494400 bytes == 0x300c000 @  0x7fc64e02d615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 19 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10) (4.1.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPqnt-SAXyvL"
      },
      "source": [
        "### Turn on GPU\n",
        "\n",
        "Make sure Runtime --> Change runtime type is set to GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XTzJ0cj-jmS"
      },
      "source": [
        "%%capture\n",
        "# install wandb\n",
        "!pip install wandb\n",
        "# install nequip\n",
        "!git clone --depth 1 \"https://github.com/mir-group/nequip.git\"\n",
        "!pip install nequip/\n",
        "# fix colab imports\n",
        "import site\n",
        "site.main()\n",
        "# set to allow anonymous WandB\n",
        "import os\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "import numpy as np\n",
        "import torch \n",
        "from ase.io import read, write\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0j3j9gVp0C7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59bf5fb1-db24-4018-e0dd-ab81d61c6322"
      },
      "source": [
        "# compile lammps\n",
        "!git clone -b \"stable_29Sep2021_update2\" --depth 1 \"https://github.com/lammps/lammps.git\"\n",
        "!git clone https://github.com/mir-group/pair_nequip\n",
        "!cd pair_nequip && ./patch_lammps.sh /content/lammps/\n",
        "!cp /content/pair_nequip/*.cpp /content/lammps/src/\n",
        "!cp /content/pair_nequip/*.h /content/lammps/src/\n",
        "! sed -i 's/CMAKE_CXX_STANDARD 11/CMAKE_CXX_STANDARD 14/g'  /content/lammps/cmake/CMakeLists.txt\n",
        "!pip install mkl mkl-include\n",
        "!cd lammps && mkdir -p build && cd build && cmake ../cmake -DCMAKE_PREFIX_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` && make -j4"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lammps'...\n",
            "remote: Enumerating objects: 11732, done.\u001b[K\n",
            "remote: Counting objects: 100% (11732/11732), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8609/8609), done.\u001b[K\n",
            "remote: Total 11732 (delta 3932), reused 6311 (delta 2924), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (11732/11732), 110.00 MiB | 17.29 MiB/s, done.\n",
            "Resolving deltas: 100% (3932/3932), done.\n",
            "Note: checking out '7586adbb6a61254125992709ef2fda9134cfca6c'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "Checking out files: 100% (11058/11058), done.\n",
            "Cloning into 'pair_nequip'...\n",
            "remote: Enumerating objects: 418, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 418 (delta 79), reused 85 (delta 65), pack-reused 302\u001b[K\n",
            "Receiving objects: 100% (418/418), 427.14 KiB | 15.25 MiB/s, done.\n",
            "Resolving deltas: 100% (208/208), done.\n",
            "Copying files...\n",
            "Updating CMakeLists.txt...\n",
            "Done!\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.7/dist-packages (2019.0)\n",
            "Collecting mkl-include\n",
            "  Downloading mkl_include-2022.1.0-py2.py3-none-manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: intel-openmp in /usr/local/lib/python3.7/dist-packages (from mkl) (2022.1.0)\n",
            "Installing collected packages: mkl-include\n",
            "Successfully installed mkl-include-2022.1.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.17.1\") \n",
            "-- Appending /usr/local/cuda/lib64/stubs to CMAKE_LIBRARY_PATH: /usr/local/cuda/lib64/stubs\n",
            "-- Running check for auto-generated files from make-based build system\n",
            "-- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so (found version \"3.1\") \n",
            "-- Found MPI: TRUE (found version \"3.1\")  \n",
            "-- Looking for C++ include omp.h\n",
            "-- Looking for C++ include omp.h - found\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
            "-- Found JPEG: /usr/lib/x86_64-linux-gnu/libjpeg.so (found version \"80\") \n",
            "-- Found PNG: /usr/lib/x86_64-linux-gnu/libpng.so (found version \"1.6.34\") \n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
            "-- Found GZIP: /bin/gzip  \n",
            "-- Found FFMPEG: /usr/bin/ffmpeg  \n",
            "-- Looking for C++ include cmath\n",
            "-- Looking for C++ include cmath - found\n",
            "-- Generating style headers...\n",
            "-- Generating package headers...\n",
            "-- Generating lmpinstalledpkgs.h...\n",
            "-- Could NOT find ClangFormat (missing: ClangFormat_EXECUTABLE) (Required is at least version \"8.0\")\n",
            "-- The following tools and libraries have been found and configured:\n",
            " * Git\n",
            " * MPI\n",
            " * OpenMP\n",
            " * JPEG\n",
            " * PNG\n",
            " * ZLIB\n",
            "\n",
            "-- <<< Build configuration >>>\n",
            "   Operating System: Linux Ubuntu 18.04\n",
            "   Build type:       RelWithDebInfo\n",
            "   Install path:     /root/.local\n",
            "   Generator:        Unix Makefiles using /usr/bin/make\n",
            "-- Enabled packages: <None>\n",
            "-- <<< Compilers and Flags: >>>\n",
            "-- C++ Compiler:     /usr/bin/c++\n",
            "      Type:          GNU\n",
            "      Version:       7.5.0\n",
            "      C++ Flags:     -O2 -g -DNDEBUG\n",
            "      Defines:       LAMMPS_SMALLBIG;LAMMPS_MEMALIGN=64;LAMMPS_OMP_COMPAT=3;LAMMPS_JPEG;LAMMPS_PNG;LAMMPS_GZIP;LAMMPS_FFMPEG\n",
            "-- <<< Linker flags: >>>\n",
            "-- Executable name:  lmp\n",
            "-- Static library flags:    \n",
            "-- <<< MPI flags >>>\n",
            "-- MPI_defines:      MPICH_SKIP_MPICXX;OMPI_SKIP_MPICXX;_MPICC_H\n",
            "-- MPI includes:     /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include;/usr/lib/x86_64-linux-gnu/openmpi/include\n",
            "-- MPI libraries:    /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so;\n",
            "-- Looking for C++ include pthread.h\n",
            "-- Looking for C++ include pthread.h - found\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
            "-- Looking for pthread_create in pthreads\n",
            "-- Looking for pthread_create in pthreads - not found\n",
            "-- Looking for pthread_create in pthread\n",
            "-- Looking for pthread_create in pthread - found\n",
            "-- Found Threads: TRUE  \n",
            "-- Found CUDA: /usr/local/cuda (found version \"11.1\") \n",
            "-- Caffe2: CUDA detected: 11.1\n",
            "-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\n",
            "-- Caffe2: CUDA toolkit directory: /usr/local/cuda\n",
            "-- Caffe2: Header version is: 11.1\n",
            "-- Found CUDNN: /usr/lib/x86_64-linux-gnu/libcudnn.so  \n",
            "-- Found cuDNN: v7.6.5  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)\n",
            "-- /usr/local/cuda/lib64/libnvrtc.so shorthash is 3a20f2b6\n",
            "-- Autodetected CUDA architecture(s):  6.0\n",
            "-- Added CUDA NVCC flags for: -gencode;arch=compute_60,code=sm_60\n",
            "\u001b[33mCMake Warning at /usr/local/lib/python3.7/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\n",
            "  static library kineto_LIBRARY-NOTFOUND not found.\n",
            "Call Stack (most recent call first):\n",
            "  /usr/local/lib/python3.7/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:127 (append_torchlib_if_found)\n",
            "  CMakeLists.txt:922 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch: /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so  \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/lammps/build\n",
            "-- Git Directory: /content/lammps/.git\n",
            "[  0%] \u001b[34m\u001b[1mGenerating includes/lammps/atom.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/variable.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/angle.h\u001b[0m\n",
            "[  1%] Built target atom.h\n",
            "[  1%] Built target angle.h\n",
            "[  1%] Built target variable.h\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/bond.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/citeme.h\u001b[0m\n",
            "[  1%] Built target bond.h\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/compute.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/comm.h\u001b[0m\n",
            "[  2%] Built target compute.h\n",
            "[  2%] Built target citeme.h\n",
            "[  2%] Built target comm.h\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/dihedral.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/domain.h\u001b[0m\n",
            "[  2%] Built target dihedral.h\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/error.h\u001b[0m\n",
            "[  2%] Built target domain.h\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/force.h\u001b[0m\n",
            "[  2%] Built target force.h\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/fix.h\u001b[0m\n",
            "[  2%] Built target error.h\n",
            "[  2%] Built target fix.h\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/improper.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/group.h\u001b[0m\n",
            "[  3%] Built target improper.h\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/input.h\u001b[0m\n",
            "[  3%] Built target group.h\n",
            "[  3%] Built target input.h\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/info.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/kspace.h\u001b[0m\n",
            "[  4%] Built target info.h\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lammps.h\u001b[0m\n",
            "[  4%] Built target kspace.h\n",
            "[  4%] Built target lammps.h\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lattice.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/library.h\u001b[0m\n",
            "[  5%] Built target lattice.h\n",
            "[  5%] Built target library.h\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/lmppython.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/memory.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/lmptype.h\u001b[0m\n",
            "[  6%] Built target lmppython.h\n",
            "[  6%] Built target memory.h\n",
            "[  6%] Built target lmptype.h\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/neighbor.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/modify.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/neigh_list.h\u001b[0m\n",
            "[  6%] Built target neighbor.h\n",
            "[  6%] Built target modify.h\n",
            "[  6%] Built target neigh_list.h\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/pair.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/output.h\u001b[0m\n",
            "[  7%] Built target pair.h\n",
            "[  7%] Built target output.h\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/pointers.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/region.h\u001b[0m\n",
            "[  7%] Built target pointers.h\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/timer.h\u001b[0m\n",
            "[  8%] Built target region.h\n",
            "[  8%] Built target timer.h\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/universe.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/update.h\u001b[0m\n",
            "[  8%] Built target universe.h\n",
            "[  8%] Built target update.h\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/utils.h\u001b[0m\n",
            "[  8%] Built target utils.h\n",
            "-- Generating lmpgitversion.h...\n",
            "[  8%] Built target gitversion\n",
            "[  8%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_zero.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_hybrid.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_deprecated.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/arg_info.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_map.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_atomic.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_body.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_charge.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_ellipsoid.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_hybrid.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_line.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_sphere.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_tri.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/balance.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/body.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_deprecated.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_hybrid.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_zero.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/change_box.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/citeme.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_brick.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_tiled.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_aggregate_atom.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle_local.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angmom_chunk.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond_local.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centro_atom.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centroid_stress_atom.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_atom.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_spread_atom.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cluster_atom.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cna_atom.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com_chunk.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_coord_atom.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_deprecated.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral_local.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole_chunk.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_displace_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_fragment_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_global_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_group_group.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration_chunk.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_heat_flux.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper_local.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_inertia_chunk.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke_atom.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd_chunk.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_omega_chunk.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_orientorder_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair_local.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe_atom.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pressure.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_atom.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_chunk.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_local.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_rdf.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_chunk.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_region.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_slice.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_stress_atom.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_chunk.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_com.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_deform.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_partial.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_profile.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_ramp.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_region.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_sphere.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_torque_chunk.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vacf.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_atoms.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vcm_chunk.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_bonds.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_box.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_atoms.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_bonds.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/deprecated.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_deprecated.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_hybrid.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_zero.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/displace_atoms.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/domain.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_atom.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_cfg.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_custom.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_deprecated.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_image.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_local.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_movie.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_xyz.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/error.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/finish.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_adapt.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_addforce.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_atom.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_chunk.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_correlate.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo_weight.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_time.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_aveforce.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_balance.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_box_relax.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deform.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deposit.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deprecated.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dt_reset.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dummy.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_efield.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_enforce2d.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_evaporate.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_external.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_gravity.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_group.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_heat.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_halt.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_indent.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_langevin.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_lineforce.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_minimize.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_momentum.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_move.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_neigh_history.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh_sphere.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph_sphere.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt_sphere.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_limit.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_noforce.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_sphere.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sllod.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sphere.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_planeforce.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_press_berendsen.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_print.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_property_atom.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_read_restart.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_recenter.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_respa.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_restrain.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_setforce.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_chunk.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_self.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_force.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_state.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_berendsen.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_rescale.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_thermal_conductivity.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_vector.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_viscous.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_harmonic.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj1043.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj126.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj93.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_morse.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_reflect.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_region.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_format.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_os.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/force.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/gridcomm.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/group.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/hashlittle.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/image.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_group.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_neigh.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_store.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_time.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_var.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_deprecated.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_hybrid.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_zero.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/info.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/input.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/integrate.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/irregular.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace_deprecated.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lammps.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lattice.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/library.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lmppython.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_eigen.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_extra.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_special.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/memory.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_cg.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_fire.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_fire_old.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_hftn.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_linesearch.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_quickmin.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_sd.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/minimize.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/modify.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/molecule.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_page.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_pool_chunk.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_multi.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_standard.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_list.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_request.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neighbor.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_copy.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin_atomonly.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin_ghost.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_multi.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_multi_old.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_nsq.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_nsq_ghost.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_atomonly_newton.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newtoff.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newtoff_ghost.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newton.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newton_tri.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newtoff.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newton.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newton_tri.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newtoff.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newton.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newton_tri.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newtoff.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newtoff_ghost.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newton.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newtoff.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newton.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newton_tri.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_nsq_newtoff.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_nsq_newton.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newtoff.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newton.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newton_tri.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newtoff.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newton.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newton_tri.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newtoff.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newton.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newton_tri.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_nsq_newtoff.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_nsq_newton.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_halffull_newtoff.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_halffull_newton.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_respa.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on_oneside.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_bin_2d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_bin_3d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_ghost_bin_2d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_ghost_bin_3d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_2d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_3d.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_old_2d.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_old_3d.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_2d.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_2d_tri.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_3d.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_3d_tri.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_2d.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_2d_tri.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_3d.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_3d_tri.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_2d.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_2d_tri.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_3d.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_3d_tri.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_all.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_partial.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_template.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_all.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_partial.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_template.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_all.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_partial.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_all.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_template.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_partial.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_template.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/output.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_born.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck_coul_cut.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_cut.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_debye.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_dsf.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_wolf.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_deprecated.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_overlay.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_scaled.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut_coul_cut.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_expand.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_morse.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_nequip.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_soft.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_yukawa.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_table.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zbl.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zero.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/potential_file_reader.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/procmap.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_mars.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_park.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rcb.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_dump.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_data.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_restart.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_native.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_xyz.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_block.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cone.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cylinder.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_deprecated.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_intersect.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_plane.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_prism.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_sphere.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_union.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/replicate.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rerun.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atom_ids.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_mol_ids.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/respa.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/run.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/set.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/special.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/table_file_reader.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tabular_function.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/text_file_reader.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/thermo.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/timer.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tokenizer.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/universe.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/update.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/utils.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/variable.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/lammps/src/variable.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint LAMMPS_NS::Variable::next(int, char**)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/lammps/src/variable.cpp:714:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "         \u001b[01;35m\u001b[Kfread(buf,1,64,fp)\u001b[m\u001b[K;\n",
            "         \u001b[01;35m\u001b[K~~~~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/velocity.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/verlet.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_coeff.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_data.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_dump.cpp.o\u001b[0m\n",
            "[100%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_restart.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX static library liblammps.a\u001b[0m\n",
            "[100%] Built target lammps\n",
            "[100%] \u001b[32mBuilding CXX object CMakeFiles/lmp.dir/content/lammps/src/main.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable lmp\u001b[0m\n",
            "[100%] Built target lmp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HDmxkn3z8_m"
      },
      "source": [
        "## 3 Steps: \n",
        "* Train: using a data set, train the neural network 🧠 \n",
        "* Deploy: convert the Python-based model into a stand-alone potential file for fast execution ⚡\n",
        "* Run: run Molecular Dynamics, Monte Carlo, Structural Minimization, ...  with it in LAMMPS 🏃"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OD71eeDz7dA"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/all.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62aEgq6QYFIn"
      },
      "source": [
        "### Train a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELdBzH_8z4_2"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/train.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX0QKkAauSZO"
      },
      "source": [
        "This tutorial is set up to use `wandb` in anonymous mode; when you use NequIP yourself you will be presented with a login prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KuOIippfVfd"
      },
      "source": [
        "Here, we will train a NequIP potential on the following system\n",
        "\n",
        "* Toluene\n",
        "* sampled at T=500K from AIMD\n",
        "* at CCSD(T) accuracy (gold standard of quantum chemistry)\n",
        "* Using 100 training configurations\n",
        "* The units of the reference data are in kcal/mol and A. If you're more familiar with eV, remember 1 kcal/mol is chemical accuracy and is approximately 43 meV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q_GyQfC0npt"
      },
      "source": [
        "Start a training run: this will print output to our console, but it is usually more convenient to view the results in a web interface called Weights and Biases. Click the link next to the rocket emoji to watch the run in the WandB interface 🚀 \n",
        "\n",
        "In WandB, watch the followingkeys:\n",
        "\n",
        "* Plot 1: validation_all_f_mae, training_all_f_mae\n",
        "* Plot 2: validation_e/N_mae, training_e/N_mae\n",
        "\n",
        "These are the validation/training error in all force components and the validation/training error in the potential energy, normalized by the number of atoms, respectively. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc-i-KbA_2ly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d8567d0-d155-4adb-fe08-e94b8d60cbeb"
      },
      "source": [
        "!rm -rf ./results\n",
        "!nequip-train nequip/configs/example.yaml"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220710_095935-csw38ouy\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexample-run-toluene\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/anony-moose-350753/toluene-example?apiKey=346134a58e9d74974f32b0ca7cfa6410207b04dc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/anony-moose-350753/toluene-example/runs/csw38ouy?apiKey=346134a58e9d74974f32b0ca7cfa6410207b04dc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Do NOT share these links with anyone. They can be used to claim your runs.\n",
            "Torch device: cuda\n",
            "Downloading http://quantum-machine.org/gdml/data/npz/toluene_ccsd_t.zip\n",
            "Processing dataset...\n",
            "Loaded data: Batch(batch=[15000], cell=[1000, 3, 3], edge_cell_shift=[154352, 3], edge_index=[2, 154352], forces=[15000, 3], pbc=[1000, 3], pos=[15000, 3], ptr=[1001], total_energy=[1000, 1])\n",
            "Cached processed data to disk\n",
            "Done!\n",
            "Successfully loaded the data set of type NpzDataset(1000)...\n",
            "Replace string dataset_forces_rms to 30.621034622192383\n",
            "Replace string dataset_per_atom_total_energy_mean to -11319.556640625\n",
            "Atomic outputs are scaled by: [H, C: 30.621035], shifted by [H, C: -11319.556641].\n",
            "Replace string dataset_forces_rms to 30.621034622192383\n",
            "Initially outputs are globally scaled by: 30.621034622192383, total_energy are globally shifted by None.\n",
            "Successfully built the network...\n",
            "Number of weights: 154200\n",
            "! Starting training ...\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      0     5        0.989        0.976       0.0129         22.1         30.2         14.8         30.5         22.7         19.5         39.1         29.3         51.8         3.46\n",
            "\n",
            "\n",
            "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Initial Validation          0    3.867    0.005        0.993       0.0125         1.01         22.3         30.5         14.6           31         22.8         19.2         39.7         29.4           51          3.4\n",
            "Wall time: 3.8677697430000535\n",
            "! Best model        0    1.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      1    10        0.433        0.421       0.0119         15.4         19.9         10.6         20.8         15.7         13.2         25.5         19.3           50         3.33\n",
            "      1    20        0.229        0.214       0.0154         10.4         14.2            7         14.3         10.7         8.74         18.5         13.6         57.1          3.8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      1     5        0.269        0.268      0.00101         11.7         15.9         7.37         16.7           12         9.07         21.1         15.1         13.5        0.902\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               1    6.276    0.005        0.586       0.0181        0.604         16.4         23.4         10.4         23.2         16.8         14.4         30.6         22.5         54.7         3.65\n",
            "! Validation          1    6.276    0.005        0.231       0.0012        0.232         10.8         14.7          6.9         15.2           11         8.68         19.4           14         15.3         1.02\n",
            "Wall time: 6.27642894600001\n",
            "! Best model        1    0.232\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      2    10        0.184        0.173       0.0107         9.64         12.7         7.26         12.4         9.81          9.1         15.9         12.5         47.3         3.15\n",
            "      2    20        0.115        0.115     4.91e-05         8.15         10.4         5.95         10.6          8.3         7.38           13         10.2         2.47        0.165\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      2     5        0.139        0.139      0.00012         8.61         11.4         5.77         11.9         8.82          7.2         14.8           11         4.29        0.286\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               2    7.973    0.005        0.169       0.0127        0.182         9.36         12.6         6.29         12.9         9.58         7.99         16.4         12.2         38.6         2.57\n",
            "! Validation          2    7.973    0.005        0.122     7.91e-05        0.122         7.98         10.7         5.34           11         8.17         6.85         13.8         10.3         3.19        0.213\n",
            "Wall time: 7.973746426000048\n",
            "! Best model        2    0.122\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      3    10        0.103        0.102      0.00139         6.89         9.76         4.41         9.72         7.06         5.58           13         9.28           17         1.14\n",
            "      3    20        0.105        0.105     9.87e-05         7.38         9.92         4.62         10.5         7.58         6.27         12.9         9.58         4.02        0.268\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      3     5       0.0996       0.0995      4.9e-05         7.25         9.66         4.78         10.1         7.42         5.94         12.6         9.29         2.63        0.176\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               3    9.648    0.005        0.105     0.000845        0.106          7.3         9.92         4.87         10.1         7.47         6.25         12.9         9.57         11.2        0.746\n",
            "! Validation          3    9.648    0.005       0.0894     3.95e-05       0.0895         6.79         9.16         4.46         9.46         6.96         5.71         11.9         8.82         2.34        0.156\n",
            "Wall time: 9.648477155000023\n",
            "! Best model        3    0.089\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      4    10       0.0728       0.0728     4.93e-05         6.24         8.26         4.45          8.3         6.37         5.67         10.5         8.07         2.89        0.193\n",
            "      4    20       0.0673       0.0672     7.39e-05         5.96         7.94         4.08         8.11          6.1         5.13         10.2         7.69         3.46        0.231\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      4     5       0.0784       0.0783     3.73e-05         6.38         8.57         4.44         8.61         6.52         5.56           11          8.3         2.39        0.159\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               4   11.318    0.005       0.0755     0.000134       0.0756         6.24         8.41         4.33         8.42         6.38         5.52         10.8         8.17         4.51        0.301\n",
            "! Validation          4   11.318    0.005        0.073     3.55e-05        0.073         6.15         8.27          4.3         8.27         6.28         5.47         10.6         8.04         2.21        0.147\n",
            "Wall time: 11.31841935\n",
            "! Best model        4    0.073\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      5    10       0.0515       0.0514     0.000102         5.13         6.94         3.97         6.46         5.22         4.99         8.65         6.82         4.44        0.296\n",
            "      5    20       0.0407       0.0406     9.01e-05         4.43         6.17         3.05         6.01         4.53         4.17         7.86         6.01         4.07        0.272\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      5     5       0.0608       0.0607     3.23e-05         5.62         7.55         3.97         7.51         5.74         5.06         9.63         7.35         2.08        0.139\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               5   12.991    0.005       0.0566     0.000105       0.0567          5.4         7.28         3.85         7.17         5.51         4.96         9.25          7.1         4.17        0.278\n",
            "! Validation          5   12.991    0.005       0.0571     2.97e-05       0.0571         5.44         7.32         3.88         7.22         5.55         5.02         9.27         7.14         1.96         0.13\n",
            "Wall time: 12.991930861000014\n",
            "! Best model        5    0.057\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      6    10       0.0419       0.0418      5.7e-05         4.68         6.26         3.43         6.11         4.77         4.38         7.88         6.13         2.94        0.196\n",
            "      6    20       0.0246       0.0246     3.02e-05         3.45          4.8         2.79         4.21          3.5         3.67         5.83         4.75         2.39        0.159\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      6     5       0.0428       0.0428     4.37e-05         4.75         6.33         3.59         6.08         4.84          4.7         7.79         6.25         2.41        0.161\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               6   14.651    0.005       0.0394     7.48e-05       0.0395         4.55         6.08         3.45         5.79         4.62          4.5         7.49         5.99         3.15         0.21\n",
            "! Validation          6   14.651    0.005       0.0399     3.65e-05       0.0399         4.62         6.12         3.57         5.83          4.7         4.69         7.42         6.05         2.21        0.147\n",
            "Wall time: 14.652003796000031\n",
            "! Best model        6    0.040\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      7    10       0.0334       0.0333     7.65e-05         4.17         5.59         3.32         5.13         4.23         4.21         6.83         5.52         3.62        0.242\n",
            "      7    20       0.0193       0.0193     7.35e-06         3.25         4.25         2.62         3.98          3.3         3.33          5.1         4.22         0.95       0.0633\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      7     5       0.0295       0.0295      2.7e-05         3.98         5.26          3.3         4.75         4.03         4.37         6.12         5.24         1.81        0.121\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               7   16.338    0.005       0.0269     0.000219       0.0271         3.82         5.02         3.21         4.51         3.86          4.2         5.83         5.01         5.58        0.372\n",
            "! Validation          7   16.338    0.005       0.0269     2.68e-05       0.0269         3.83         5.02         3.25         4.49         3.87         4.34          5.7         5.02         1.84        0.122\n",
            "Wall time: 16.33872654700008\n",
            "! Best model        7    0.027\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      8    10       0.0205       0.0203     0.000141         3.27         4.36         2.69         3.94         3.31         3.52         5.17         4.34         5.17        0.345\n",
            "      8    20       0.0161        0.016     0.000105         3.01         3.88         2.39         3.72         3.06         2.97         4.71         3.84         4.52        0.301\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      8     5       0.0238       0.0238     1.81e-05         3.58         4.72         3.08         4.15         3.61          4.1         5.34         4.72         1.46       0.0975\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               8   17.994    0.005       0.0214     0.000107       0.0215          3.4         4.48          2.9         3.97         3.43          3.8         5.15         4.47         3.86        0.258\n",
            "! Validation          8   17.994    0.005       0.0218     2.08e-05       0.0218         3.41         4.52            3         3.88         3.44         4.08         4.97         4.53         1.61        0.107\n",
            "Wall time: 17.99429343500003\n",
            "! Best model        8    0.022\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      9    10       0.0164       0.0159     0.000478         2.88         3.86         2.59          3.2          2.9         3.34         4.37         3.86         9.99        0.666\n",
            "      9    20       0.0136       0.0133     0.000284         2.67         3.54         2.29         3.11          2.7         2.93         4.12         3.53         7.62        0.508\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      9     5        0.021        0.021     1.45e-05         3.37         4.44         2.93         3.86          3.4         3.91         4.97         4.44         1.31       0.0872\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               9   19.658    0.005       0.0183     0.000217       0.0185         3.14         4.14         2.71         3.63         3.17         3.58          4.7         4.14         5.51        0.367\n",
            "! Validation          9   19.658    0.005       0.0193     1.81e-05       0.0193         3.18         4.25         2.82          3.6         3.21         3.89         4.63         4.26         1.49       0.0993\n",
            "Wall time: 19.658535013000005\n",
            "! Best model        9    0.019\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     10    10       0.0208       0.0208     4.96e-05         3.15         4.41         2.89         3.45         3.17         3.95         4.89         4.42         3.11        0.207\n",
            "     10    20       0.0253        0.025     0.000382          3.3         4.84          2.8         3.86         3.33         4.21         5.46         4.84          8.9        0.594\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     10     5        0.019        0.019     7.81e-06          3.2         4.22         2.81         3.63         3.22         3.78         4.68         4.23         1.04       0.0694\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              10   21.367    0.005       0.0165     6.75e-05       0.0166         2.99         3.94          2.6         3.42         3.01         3.44         4.44         3.94         3.07        0.205\n",
            "! Validation         10   21.367    0.005       0.0175     1.42e-05       0.0175         3.03         4.05         2.69          3.4         3.05         3.75         4.36         4.06          1.4       0.0931\n",
            "Wall time: 21.36772535800003\n",
            "! Best model       10    0.017\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     11    10       0.0157       0.0155     0.000168         2.96         3.81         2.81         3.13         2.97         3.71         3.93         3.82          5.8        0.387\n",
            "     11    20       0.0153       0.0152      5.3e-06         3.01         3.78         2.54         3.56         3.05         3.19         4.36         3.77         1.01       0.0671\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     11     5       0.0179       0.0179     1.32e-05         3.09         4.09         2.73         3.49         3.11         3.68         4.52          4.1         1.26       0.0838\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              11   23.035    0.005        0.017     9.23e-05       0.0171         3.05         3.99         2.64         3.53         3.08         3.44         4.54         3.99         3.31        0.221\n",
            "! Validation         11   23.035    0.005       0.0164     1.65e-05       0.0164         2.93         3.92         2.61          3.3         2.95         3.64         4.22         3.93         1.42       0.0946\n",
            "Wall time: 23.035597583000026\n",
            "! Best model       11    0.016\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     12    10        0.014        0.014      8.6e-06          2.8         3.62         2.57         3.06         2.81         3.38         3.87         3.63         1.28       0.0854\n",
            "     12    20       0.0108       0.0108     5.96e-06         2.53         3.18         2.22         2.88         2.55         2.79         3.58         3.18        0.978       0.0652\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     12     5       0.0167       0.0166      1.2e-05         2.98         3.95         2.66         3.35         3.01         3.57         4.34         3.96         1.19       0.0796\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              12   24.716    0.005       0.0139     3.74e-05       0.0139         2.75         3.61          2.4         3.14         2.77         3.16         4.06         3.61          2.4         0.16\n",
            "! Validation         12   24.716    0.005       0.0154     1.53e-05       0.0154         2.84          3.8         2.55         3.18         2.86         3.55         4.06         3.81         1.37       0.0916\n",
            "Wall time: 24.716790650999997\n",
            "! Best model       12    0.015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     13    10       0.0106       0.0106     2.57e-05         2.31         3.15         2.03         2.63         2.33         2.93         3.38         3.16         2.04        0.136\n",
            "     13    20      0.00973      0.00964     9.57e-05         2.38         3.01         2.13         2.68          2.4         2.78         3.24         3.01         4.33        0.289\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     13     5       0.0158       0.0158     1.05e-05          2.9         3.84          2.6         3.23         2.92          3.5         4.21         3.85         1.15       0.0766\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              13   26.397    0.005        0.013     4.81e-05       0.0131         2.66         3.49         2.33         3.04         2.69         3.09          3.9          3.5         2.57        0.171\n",
            "! Validation         13   26.397    0.005       0.0146     1.42e-05       0.0146         2.76          3.7         2.49         3.08         2.78         3.48         3.94         3.71         1.33        0.089\n",
            "Wall time: 26.39760405700008\n",
            "! Best model       13    0.015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     14    10        0.015       0.0149     7.09e-06         2.88         3.74         2.35         3.48         2.91         2.98         4.46         3.72        0.772       0.0515\n",
            "     14    20       0.0107       0.0107     2.08e-05         2.43         3.16         2.32         2.56         2.44         3.01         3.32         3.17         1.82        0.121\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     14     5       0.0148       0.0148        1e-05         2.81         3.72         2.54         3.12         2.83         3.41         4.05         3.73         1.13       0.0751\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              14   28.080    0.005       0.0126     4.25e-05       0.0127         2.63         3.44          2.3         3.01         2.66         3.03         3.86         3.44         2.39        0.159\n",
            "! Validation         14   28.080    0.005       0.0139     1.36e-05       0.0139          2.7         3.61         2.43            3         2.72          3.4         3.83         3.61         1.31       0.0873\n",
            "Wall time: 28.081466369000054\n",
            "! Best model       14    0.014\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     15    10       0.0106       0.0106     6.01e-06         2.47         3.15         2.07         2.93          2.5         2.71         3.59         3.15        0.772       0.0515\n",
            "     15    20      0.00925      0.00922     2.28e-05          2.2         2.94         2.04         2.37         2.21         2.77         3.12         2.95         2.03        0.135\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     15     5        0.014        0.014     7.85e-06         2.74         3.62         2.48         3.03         2.75         3.32         3.94         3.63        0.995       0.0664\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              15   29.740    0.005       0.0119        3e-05       0.0119         2.54         3.33         2.21         2.92         2.57         2.92         3.75         3.34         2.04        0.136\n",
            "! Validation         15   29.740    0.005       0.0132      1.2e-05       0.0132         2.63         3.51         2.38         2.91         2.65         3.32         3.72         3.52         1.23        0.082\n",
            "Wall time: 29.74057647500001\n",
            "! Best model       15    0.013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     16    10       0.0159       0.0157     0.000134         2.72         3.84         2.43         3.05         2.74          3.4         4.28         3.84         5.23        0.349\n",
            "     16    20        0.011        0.011     4.59e-05         2.39         3.21         1.89         2.96         2.42         2.62         3.77         3.19         3.08        0.205\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     16     5       0.0132       0.0132     8.03e-06         2.65         3.51         2.39         2.94         2.67         3.23         3.81         3.52         1.02       0.0677\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              16   31.420    0.005       0.0114     0.000122       0.0115         2.49         3.27         2.17         2.86         2.52         2.87         3.68         3.27         4.09        0.273\n",
            "! Validation         16   31.420    0.005       0.0126     1.16e-05       0.0126         2.57         3.43         2.33         2.85         2.59         3.24         3.64         3.44         1.21       0.0806\n",
            "Wall time: 31.42131119999999\n",
            "! Best model       16    0.013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     17    10      0.00882      0.00881     1.31e-05         2.22         2.87         2.06         2.41         2.24         2.62         3.14         2.88         1.38       0.0921\n",
            "     17    20       0.0107       0.0107     9.27e-06         2.44         3.17         2.12         2.82         2.47         2.74         3.59         3.17         1.07       0.0715\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     17     5       0.0124       0.0124     7.92e-06         2.58         3.41         2.33         2.87          2.6         3.14          3.7         3.42         1.02       0.0679\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              17   33.079    0.005      0.00991     3.38e-05      0.00994         2.34         3.05         2.07         2.64         2.35         2.73         3.38         3.05         2.07        0.138\n",
            "! Validation         17   33.079    0.005        0.012     1.13e-05        0.012         2.51         3.35         2.27         2.79         2.53         3.16         3.55         3.36         1.19       0.0792\n",
            "Wall time: 33.08011634700006\n",
            "! Best model       17    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     18    10       0.0103       0.0103     8.33e-06          2.4         3.11         1.87         3.02         2.44         2.47         3.71         3.09         1.05       0.0702\n",
            "     18    20       0.0105       0.0104     4.49e-05         2.38         3.13         2.18         2.61          2.4         2.91         3.36         3.14         2.93        0.195\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     18     5       0.0116       0.0116     6.61e-06         2.49          3.3         2.25         2.77         2.51         3.04         3.57         3.31        0.905       0.0603\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              18   34.744    0.005      0.00966     5.46e-05      0.00971          2.3         3.01         2.01         2.65         2.33         2.65         3.38         3.01         2.83        0.188\n",
            "! Validation         18   34.744    0.005       0.0113     1.01e-05       0.0114         2.45         3.26         2.21         2.72         2.47         3.08         3.46         3.27         1.15       0.0768\n",
            "Wall time: 34.744798629\n",
            "! Best model       18    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     19    10      0.00757       0.0075     6.61e-05         2.03         2.65         1.83         2.25         2.04          2.5         2.82         2.66         3.66        0.244\n",
            "     19    20      0.00893      0.00889     3.61e-05         2.16         2.89         1.81         2.56         2.18         2.39         3.37         2.88         2.41        0.161\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     19     5       0.0109       0.0109     6.76e-06         2.41          3.2         2.17         2.68         2.43         2.95         3.47         3.21        0.902       0.0601\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              19   36.643    0.005      0.00914     9.14e-05      0.00923         2.22         2.93         1.94         2.55         2.24         2.56          3.3         2.93         3.81        0.254\n",
            "! Validation         19   36.643    0.005       0.0108      9.8e-06       0.0108         2.39         3.18         2.16         2.66         2.41            3         3.38         3.19         1.12       0.0744\n",
            "Wall time: 36.643684747000066\n",
            "! Best model       19    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     20    10      0.00574      0.00563      0.00011         1.75          2.3         1.37         2.18         1.78         1.83         2.74         2.28         4.75        0.316\n",
            "     20    20       0.0102       0.0102     2.46e-05         2.49         3.09         2.41         2.57         2.49         2.96         3.23          3.1         2.05        0.137\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     20     5       0.0104       0.0104     6.41e-06         2.34         3.12         2.08         2.62         2.35         2.85         3.39         3.12        0.853       0.0569\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              20   38.445    0.005      0.00923     5.54e-05      0.00929         2.24         2.94         1.92          2.6         2.26         2.53         3.35         2.94         2.74        0.183\n",
            "! Validation         20   38.445    0.005       0.0103      9.4e-06       0.0103         2.33         3.11         2.09         2.61         2.35         2.91         3.31         3.11         1.11       0.0737\n",
            "Wall time: 38.445394959000055\n",
            "! Best model       20    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     21    10        0.005        0.005     6.05e-06         1.58         2.16         1.29         1.91          1.6         1.82          2.5         2.16        0.922       0.0615\n",
            "     21    20         0.01         0.01        2e-05          2.2         3.07         1.98         2.44         2.21         2.88         3.26         3.07         1.76        0.117\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     21     5       0.0098       0.0098     6.28e-06         2.27         3.03         2.01         2.57         2.29         2.76         3.31         3.04         0.85       0.0567\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              21   40.150    0.005      0.00821     4.01e-05      0.00825          2.1         2.77         1.82         2.41         2.12         2.43         3.12         2.78         2.37        0.158\n",
            "! Validation         21   40.150    0.005      0.00978     9.08e-06      0.00979         2.27         3.03         2.03         2.55         2.29         2.83         3.24         3.03         1.08       0.0718\n",
            "Wall time: 40.15074509500005\n",
            "! Best model       21    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     22    10      0.00649      0.00648     9.65e-06         1.97         2.47         1.88         2.08         1.98         2.38         2.56         2.47         1.23       0.0821\n",
            "     22    20       0.0114       0.0114     2.67e-05         2.54         3.26            2         3.15         2.58         2.67         3.83         3.25         2.27        0.151\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     22     5      0.00928      0.00928        6e-06          2.2         2.95         1.94         2.51         2.22         2.67         3.24         2.95        0.806       0.0538\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              22   41.836    0.005      0.00791     2.15e-05      0.00793         2.07         2.72         1.78         2.39         2.09         2.37         3.08         2.72          1.7        0.114\n",
            "! Validation         22   41.836    0.005      0.00934     8.63e-06      0.00935         2.22         2.96         1.97         2.51         2.24         2.75         3.18         2.97         1.05       0.0702\n",
            "Wall time: 41.83697648400005\n",
            "! Best model       22    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     23    10       0.0073      0.00729     8.94e-06            2         2.61         1.81         2.22         2.01         2.39         2.85         2.62         1.11        0.074\n",
            "     23    20      0.00653      0.00652     1.09e-05         1.91         2.47         1.84         1.99         1.91         2.49         2.45         2.47         1.21       0.0806\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     23     5      0.00889      0.00888     5.99e-06         2.16         2.89         1.88         2.48         2.18          2.6         3.18         2.89        0.828       0.0552\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              23   43.542    0.005      0.00887     2.16e-05      0.00889         2.22         2.88         1.86         2.62         2.24         2.44         3.32         2.88         1.69        0.113\n",
            "! Validation         23   43.542    0.005      0.00893     8.38e-06      0.00894         2.17         2.89         1.92         2.46         2.19         2.69         3.11          2.9         1.03       0.0686\n",
            "Wall time: 43.54291320700008\n",
            "! Best model       23    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     24    10      0.00851      0.00844     6.71e-05         2.21         2.81         2.08         2.36         2.22         2.62         3.02         2.82         3.67        0.245\n",
            "     24    20      0.00827       0.0081     0.000169         2.18         2.76          1.9         2.49         2.19         2.29         3.21         2.75         5.92        0.395\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     24     5      0.00852      0.00852     6.02e-06         2.11         2.83         1.83         2.44         2.13         2.53         3.13         2.83         0.85       0.0567\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              24   45.239    0.005      0.00893     5.22e-05      0.00898         2.23         2.89         1.86         2.65         2.26         2.43         3.35         2.89         2.78        0.185\n",
            "! Validation         24   45.239    0.005      0.00858     8.22e-06      0.00859         2.12         2.84         1.87         2.42         2.14         2.62         3.06         2.84        0.997       0.0665\n",
            "Wall time: 45.240207431000044\n",
            "! Best model       24    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     25    10      0.00537      0.00536     1.16e-05          1.7         2.24         1.73         1.68          1.7          2.3         2.17         2.24         1.22       0.0815\n",
            "     25    20       0.0054      0.00539     8.94e-06         1.73         2.25         1.57         1.92         1.74         2.18         2.32         2.25         1.16       0.0773\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     25     5      0.00819      0.00818     5.65e-06         2.07         2.77         1.78         2.41         2.09         2.47         3.08         2.77        0.795        0.053\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              25   47.036    0.005      0.00658     3.22e-05      0.00662         1.89         2.48         1.65         2.16          1.9          2.2         2.78         2.49          2.1         0.14\n",
            "! Validation         25   47.036    0.005      0.00827     7.72e-06      0.00827         2.08         2.78         1.82         2.38          2.1         2.57         3.01         2.79        0.985       0.0656\n",
            "Wall time: 47.037111465000066\n",
            "! Best model       25    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     26    10      0.00692      0.00677     0.000145         1.94         2.52         1.66         2.25         1.96         2.23         2.82         2.52         5.52        0.368\n",
            "     26    20       0.0052       0.0052     5.37e-06         1.72         2.21         1.54         1.94         1.74         2.01         2.42         2.21        0.797       0.0531\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     26     5      0.00788      0.00787     5.52e-06         2.03         2.72         1.74         2.36         2.05         2.41         3.03         2.72        0.814       0.0543\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              26   48.729    0.005      0.00608      3.5e-05      0.00612         1.81         2.39         1.57         2.09         1.83         2.11         2.67         2.39         2.19        0.146\n",
            "! Validation         26   48.729    0.005      0.00796     7.56e-06      0.00797         2.04         2.73         1.79         2.33         2.06         2.51         2.96         2.74        0.955       0.0637\n",
            "Wall time: 48.72997129500004\n",
            "! Best model       26    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     27    10      0.00746      0.00739     6.39e-05         1.89         2.63         1.46         2.38         1.92         2.03         3.18         2.61         3.26        0.217\n",
            "     27    20      0.00567      0.00557      9.2e-05         1.81         2.29         1.48         2.19         1.83          1.8         2.74         2.27         4.29        0.286\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     27     5      0.00761       0.0076     5.44e-06         1.99         2.67         1.69         2.32         2.01         2.36         2.99         2.67        0.836       0.0557\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              27   50.402    0.005      0.00604     6.97e-05      0.00611         1.79         2.38         1.52          2.1         1.81         2.06          2.7         2.38         3.16        0.211\n",
            "! Validation         27   50.402    0.005       0.0077     7.49e-06       0.0077            2         2.69         1.75         2.29         2.02         2.46         2.92         2.69        0.933       0.0622\n",
            "Wall time: 50.40349045800008\n",
            "! Best model       27    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     28    10      0.00592       0.0059     1.79e-05         1.83         2.35         1.65         2.04         1.84         2.22         2.49         2.36         1.67        0.111\n",
            "     28    20      0.00512      0.00489     0.000231          1.7         2.14         1.58         1.83         1.71         2.03         2.26         2.15         6.97        0.464\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     28     5      0.00741      0.00741     5.22e-06         1.96         2.64         1.66         2.31         1.99         2.32         2.96         2.64        0.808       0.0539\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              28   52.116    0.005      0.00635     7.39e-05      0.00643         1.86         2.44          1.6         2.17         1.88         2.13         2.75         2.44         3.24        0.216\n",
            "! Validation         28   52.116    0.005      0.00747     7.15e-06      0.00747         1.97         2.65         1.71         2.26         1.99         2.42         2.88         2.65        0.911       0.0607\n",
            "Wall time: 52.11664405300007\n",
            "! Best model       28    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     29    10      0.00452      0.00451     7.01e-06         1.54         2.06         1.35         1.76         1.56         1.77         2.34         2.06         1.01       0.0673\n",
            "     29    20      0.00795      0.00791     3.38e-05          2.1         2.72          1.8         2.44         2.12         2.41         3.04         2.73         2.64        0.176\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     29     5      0.00718      0.00718     5.14e-06         1.93         2.59         1.64         2.27         1.95         2.28         2.91          2.6        0.816       0.0544\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              29   53.781    0.005      0.00588     4.29e-05      0.00592         1.78         2.35         1.52         2.08          1.8         2.04         2.65         2.35         2.49        0.166\n",
            "! Validation         29   53.781    0.005      0.00725     6.97e-06      0.00726         1.94         2.61         1.68         2.23         1.96         2.38         2.85         2.61        0.898       0.0599\n",
            "Wall time: 53.782001395000066\n",
            "! Best model       29    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     30    10      0.00787      0.00756     0.000307         1.99         2.66         1.73         2.29         2.01         2.32         3.01         2.66         7.89        0.526\n",
            "     30    20      0.00731      0.00728     2.92e-05         1.97         2.61         1.76          2.2         1.98          2.4         2.84         2.62          2.4         0.16\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     30     5      0.00701        0.007     5.06e-06         1.91         2.56         1.61         2.25         1.93         2.25         2.88         2.56        0.809        0.054\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              30   55.463    0.005       0.0063     9.44e-05       0.0064         1.86         2.43         1.55         2.21         1.88         2.07         2.79         2.43         3.76        0.251\n",
            "! Validation         30   55.463    0.005      0.00706     6.83e-06      0.00707         1.91         2.57         1.66          2.2         1.93         2.34         2.81         2.58        0.892       0.0594\n",
            "Wall time: 55.463905949000036\n",
            "! Best model       30    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     31    10      0.00566      0.00565     1.03e-05         1.77          2.3         1.48         2.09         1.79         2.04         2.57          2.3         1.21       0.0804\n",
            "     31    20      0.00573      0.00572     1.73e-05         1.79         2.32         1.58         2.02          1.8         2.07         2.57         2.32         1.85        0.123\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     31     5      0.00684      0.00684     4.97e-06         1.89         2.53         1.59         2.23         1.91         2.21         2.85         2.53        0.814       0.0543\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              31   57.173    0.005      0.00553     7.22e-05       0.0056         1.74         2.28         1.48         2.04         1.76         1.98         2.58         2.28         3.41        0.227\n",
            "! Validation         31   57.173    0.005      0.00688     6.76e-06      0.00688         1.89         2.54         1.63         2.18         1.91         2.31         2.78         2.54        0.886       0.0591\n",
            "Wall time: 57.173635243000035\n",
            "! Best model       31    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     32    10       0.0054      0.00539     5.03e-06         1.68         2.25         1.53         1.85         1.69         2.07         2.44         2.25        0.819       0.0546\n",
            "     32    20      0.00522      0.00522     3.09e-06          1.7         2.21         1.19         2.28         1.74         1.57         2.77         2.17        0.653       0.0435\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     32     5      0.00669      0.00669     4.97e-06         1.87          2.5         1.57          2.2         1.89         2.19         2.82         2.51        0.803       0.0535\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              32   58.841    0.005      0.00588     8.28e-05      0.00596          1.8         2.35         1.53          2.1         1.82         2.04         2.66         2.35         3.61         0.24\n",
            "! Validation         32   58.841    0.005      0.00671      6.5e-06      0.00672         1.86         2.51         1.61         2.15         1.88         2.28         2.75         2.51        0.865       0.0576\n",
            "Wall time: 58.841977249000024\n",
            "! Best model       32    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     33    10      0.00708        0.007      7.4e-05         1.96         2.56         1.63         2.34         1.98         2.11            3         2.55         3.47        0.231\n",
            "     33    20      0.00524       0.0052     3.88e-05         1.69         2.21         1.31         2.13         1.72         1.67         2.69         2.18         2.81        0.187\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     33     5      0.00658      0.00657     4.92e-06         1.85         2.48         1.55         2.19         1.87         2.16         2.81         2.48        0.812       0.0542\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              33   60.527    0.005      0.00614     5.83e-05       0.0062         1.83          2.4         1.52         2.18         1.85         2.01         2.77         2.39         2.84        0.189\n",
            "! Validation         33   60.527    0.005      0.00656     6.57e-06      0.00657         1.84         2.48         1.59         2.13         1.86         2.24         2.73         2.48        0.879       0.0586\n",
            "Wall time: 60.52805565599999\n",
            "! Best model       33    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     34    10      0.00479      0.00467     0.000118         1.63         2.09          1.4         1.89         1.64         1.84         2.35         2.09         4.93        0.329\n",
            "     34    20      0.00485      0.00473     0.000118         1.65         2.11         1.24         2.11         1.68         1.65         2.53         2.09         4.93        0.328\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     34     5      0.00644      0.00644     4.85e-06         1.83         2.46         1.54         2.16         1.85         2.14         2.78         2.46        0.802       0.0534\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              34   62.201    0.005       0.0053     5.78e-05      0.00535         1.69         2.23         1.42            2         1.71         1.91         2.55         2.23          3.1        0.207\n",
            "! Validation         34   62.201    0.005      0.00642     6.32e-06      0.00643         1.82         2.45         1.57         2.11         1.84         2.22          2.7         2.46        0.864       0.0576\n",
            "Wall time: 62.20151494100003\n",
            "! Best model       34    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     35    10      0.00467      0.00463     4.43e-05         1.61         2.08         1.37         1.88         1.62         1.82         2.35         2.08         2.98        0.199\n",
            "     35    20      0.00503      0.00502     3.77e-06         1.68         2.17         1.34         2.07          1.7         1.72         2.59         2.16        0.872       0.0581\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     35     5      0.00629      0.00629     4.75e-06         1.81         2.43         1.52         2.13         1.83         2.11         2.74         2.43        0.789       0.0526\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              35   63.882    0.005      0.00533     2.73e-05      0.00536         1.72         2.24          1.5         1.97         1.73         1.98         2.49         2.24         1.97        0.131\n",
            "! Validation         35   63.882    0.005      0.00628     6.06e-06      0.00629          1.8         2.43         1.55         2.09         1.82         2.19         2.67         2.43        0.842       0.0561\n",
            "Wall time: 63.88290846000007\n",
            "! Best model       35    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     36    10      0.00517      0.00515     1.13e-05         1.57          2.2         1.38         1.78         1.58         1.82         2.57         2.19         1.39       0.0929\n",
            "     36    20      0.00435      0.00425     9.99e-05         1.59            2         1.35         1.86         1.61         1.68         2.31         1.99         4.57        0.305\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     36     5      0.00615      0.00614     4.75e-06         1.78          2.4          1.5         2.11          1.8         2.09         2.71          2.4        0.791       0.0527\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              36   65.567    0.005      0.00527     2.12e-05      0.00529         1.68         2.22         1.42         1.98          1.7         1.88         2.55         2.22         1.56        0.104\n",
            "! Validation         36   65.567    0.005      0.00616     6.09e-06      0.00616         1.78          2.4         1.53         2.07          1.8         2.17         2.65         2.41        0.847       0.0565\n",
            "Wall time: 65.56738824199999\n",
            "! Best model       36    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     37    10      0.00568      0.00564     3.53e-05         1.74          2.3         1.48         2.05         1.76         1.92         2.67         2.29         2.63        0.176\n",
            "     37    20      0.00509      0.00508     1.09e-05         1.68         2.18         1.42         1.98          1.7         1.87         2.49         2.18          1.4       0.0935\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     37     5      0.00603      0.00603     4.65e-06         1.77         2.38         1.48          2.1         1.79         2.05          2.7         2.38        0.781       0.0521\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              37   67.273    0.005      0.00534     7.75e-05      0.00542          1.7         2.24         1.43         2.01         1.72          1.9         2.57         2.24         3.44        0.229\n",
            "! Validation         37   67.273    0.005      0.00605     5.91e-06      0.00605         1.76         2.38         1.51         2.05         1.78         2.14         2.63         2.38        0.832       0.0555\n",
            "Wall time: 67.27412825700003\n",
            "! Best model       37    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     38    10      0.00563      0.00562     9.76e-06         1.77          2.3         1.67         1.89         1.78         2.16         2.45          2.3         1.33       0.0883\n",
            "     38    20      0.00393      0.00393     1.76e-06         1.47         1.92          1.2         1.77         1.48         1.63         2.21         1.92        0.453       0.0302\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     38     5      0.00595      0.00594     4.55e-06         1.76         2.36         1.47         2.08         1.78         2.03         2.69         2.36        0.772       0.0515\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              38   68.928    0.005       0.0052     5.25e-05      0.00526         1.69         2.21         1.43         1.98         1.71          1.9         2.52         2.21         2.62        0.175\n",
            "! Validation         38   68.928    0.005      0.00594     5.83e-06      0.00595         1.75         2.36         1.49         2.04         1.77         2.11         2.61         2.36        0.828       0.0552\n",
            "Wall time: 68.92902391300004\n",
            "! Best model       38    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     39    10      0.00442      0.00441     1.32e-05         1.58         2.03         1.29         1.92          1.6         1.65          2.4         2.02         1.52        0.101\n",
            "     39    20      0.00482      0.00479     2.95e-05         1.61         2.12          1.3         1.96         1.63         1.71         2.51         2.11         2.32        0.155\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     39     5      0.00584      0.00583     4.46e-06         1.74         2.34         1.46         2.06         1.76         2.01         2.66         2.34         0.75         0.05\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              39   70.643    0.005      0.00514      5.6e-05      0.00519         1.69         2.19         1.43         1.98         1.71         1.88         2.51         2.19         2.85         0.19\n",
            "! Validation         39   70.643    0.005      0.00583     5.45e-06      0.00584         1.73         2.34         1.48         2.02         1.75         2.09         2.59         2.34        0.797       0.0531\n",
            "Wall time: 70.64433841100004\n",
            "! Best model       39    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     40    10      0.00397      0.00396     1.06e-05         1.49         1.93          1.4          1.6          1.5         1.81         2.05         1.93         1.38       0.0921\n",
            "     40    20      0.00444      0.00431      0.00013         1.58         2.01         1.37         1.83          1.6         1.84         2.19         2.01         5.15        0.343\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     40     5      0.00572      0.00571     4.35e-06         1.72         2.31         1.44         2.04         1.74         1.99         2.64         2.31        0.738       0.0492\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              40   72.328    0.005      0.00453     3.79e-05      0.00457         1.57         2.06         1.35         1.82         1.59          1.8         2.32         2.06         2.31        0.154\n",
            "! Validation         40   72.328    0.005      0.00572     5.32e-06      0.00573         1.71         2.32         1.46         2.01         1.73         2.07         2.57         2.32        0.784       0.0522\n",
            "Wall time: 72.32873494400008\n",
            "! Best model       40    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     41    10      0.00674      0.00661     0.000127         1.88         2.49         1.69          2.1         1.89         2.34         2.65          2.5         5.13        0.342\n",
            "     41    20      0.00471      0.00468     2.94e-05         1.76         2.09         1.77         1.75         1.76         2.15         2.03         2.09         2.38        0.159\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     41     5      0.00562      0.00562     4.36e-06         1.71          2.3         1.43         2.03         1.73         1.97         2.62         2.29        0.731       0.0487\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              41   74.002    0.005      0.00512     6.53e-05      0.00518         1.68         2.19         1.44         1.96          1.7          1.9         2.48         2.19         3.07        0.205\n",
            "! Validation         41   74.002    0.005      0.00562     5.18e-06      0.00562          1.7          2.3         1.44         1.99         1.72         2.04         2.55          2.3        0.774       0.0516\n",
            "Wall time: 74.00234697300004\n",
            "! Best model       41    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     42    10      0.00459      0.00459      3.6e-06          1.6         2.07         1.43         1.79         1.61         1.92         2.23         2.08        0.797       0.0531\n",
            "     42    20      0.00294      0.00294     2.53e-06         1.22         1.66         1.27         1.17         1.22         1.75         1.55         1.65        0.709       0.0473\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     42     5      0.00552      0.00552     4.23e-06         1.69         2.27         1.42            2         1.71         1.95          2.6         2.27         0.73       0.0486\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              42   75.674    0.005      0.00483     1.16e-05      0.00485         1.63         2.13         1.37         1.92         1.65          1.8         2.45         2.13         1.34       0.0893\n",
            "! Validation         42   75.674    0.005      0.00552     5.21e-06      0.00552         1.68         2.27         1.43         1.98          1.7         2.02         2.54         2.28        0.778       0.0519\n",
            "Wall time: 75.67519421400004\n",
            "! Best model       42    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     43    10       0.0051      0.00479     0.000316         1.58         2.12         1.52         1.66         1.59         2.05         2.19         2.12         8.12        0.541\n",
            "     43    20      0.00337      0.00335     2.11e-05         1.35         1.77         1.24         1.47         1.36         1.64         1.92         1.78         2.01        0.134\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     43     5      0.00545      0.00544     4.24e-06         1.68         2.26          1.4            2          1.7         1.93         2.58         2.26        0.722       0.0481\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              43   77.350    0.005       0.0048     9.07e-05      0.00489         1.62         2.12         1.39         1.89         1.64         1.81         2.43         2.12         3.57        0.238\n",
            "! Validation         43   77.350    0.005      0.00543     5.03e-06      0.00544         1.67         2.26         1.41         1.96         1.69            2         2.52         2.26        0.762       0.0508\n",
            "Wall time: 77.350582175\n",
            "! Best model       43    0.005\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     44    10      0.00485      0.00485     6.55e-06         1.59         2.13         1.21         2.01         1.61         1.62          2.6         2.11        0.928       0.0619\n",
            "     44    20      0.00473      0.00472      9.9e-06         1.63          2.1         1.38         1.92         1.65         1.81         2.39          2.1         1.31       0.0871\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     44     5      0.00533      0.00533     4.15e-06         1.66         2.24         1.39         1.97         1.68         1.91         2.56         2.23         0.72        0.048\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              44   79.040    0.005      0.00453      5.3e-05      0.00458         1.58         2.06         1.33         1.86          1.6         1.75         2.36         2.06          2.8        0.187\n",
            "! Validation         44   79.040    0.005      0.00534     5.12e-06      0.00534         1.66         2.24          1.4         1.95         1.67         1.98          2.5         2.24        0.771       0.0514\n",
            "Wall time: 79.04082757100002\n",
            "! Best model       44    0.005\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     45    10      0.00508      0.00507     2.08e-06         1.71         2.18         1.41         2.05         1.73         1.77         2.57         2.17        0.647       0.0431\n",
            "     45    20      0.00356       0.0035     5.58e-05          1.4         1.81         1.35         1.46          1.4         1.76         1.87         1.82         3.35        0.223\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     45     5      0.00524      0.00523     4.17e-06         1.65         2.21         1.38         1.95         1.67         1.89         2.54         2.21        0.725       0.0483\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              45   80.715    0.005      0.00514     1.76e-05      0.00515          1.7         2.19          1.4         2.05         1.72         1.81         2.56         2.19         1.61        0.107\n",
            "! Validation         45   80.715    0.005      0.00525     5.13e-06      0.00525         1.64         2.22         1.38         1.94         1.66         1.95         2.49         2.22        0.771       0.0514\n",
            "Wall time: 80.71550285500007\n",
            "! Best model       45    0.005\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     46    10      0.00407      0.00404     2.49e-05         1.43         1.95          1.2          1.7         1.45          1.6         2.28         1.94         2.25         0.15\n",
            "     46    20      0.00372      0.00371     6.72e-06         1.55         1.87         1.37         1.75         1.56         1.66         2.08         1.87          1.1       0.0733\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     46     5      0.00516      0.00516     4.05e-06         1.64          2.2         1.37         1.94         1.66         1.87         2.52          2.2        0.706       0.0471\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              46   82.402    0.005      0.00411     1.48e-05      0.00412          1.5         1.96         1.26         1.77         1.51         1.66         2.26         1.96         1.34       0.0891\n",
            "! Validation         46   82.402    0.005      0.00516      4.9e-06      0.00516         1.63          2.2         1.37         1.92         1.65         1.93         2.47          2.2        0.748       0.0499\n",
            "Wall time: 82.40274405600007\n",
            "! Best model       46    0.005\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     47    10      0.00417      0.00416     1.68e-05         1.57         1.97         1.59         1.56         1.57         2.08         1.85         1.96         1.79        0.119\n",
            "     47    20      0.00505      0.00502     3.04e-05         1.66         2.17          1.5         1.84         1.67         1.97         2.37         2.17         2.41        0.161\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     47     5      0.00505      0.00504     4.11e-06         1.62         2.17         1.36         1.92         1.64         1.85         2.49         2.17        0.706       0.0471\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              47   84.089    0.005      0.00408     2.24e-05       0.0041         1.49         1.96         1.29         1.72          1.5          1.7         2.22         1.96         1.81        0.121\n",
            "! Validation         47   84.089    0.005      0.00506     4.95e-06      0.00507         1.61         2.18         1.35         1.91         1.63         1.91         2.45         2.18        0.754       0.0503\n",
            "Wall time: 84.09030797399998\n",
            "! Best model       47    0.005\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     48    10      0.00759      0.00755      4.5e-05         2.06         2.66         1.49         2.72          2.1         1.89         3.33         2.61         2.99          0.2\n",
            "     48    20      0.00492       0.0049     2.42e-05         1.66         2.14         1.26         2.11         1.69         1.64          2.6         2.12         2.24        0.149\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     48     5      0.00497      0.00497     4.03e-06         1.61         2.16         1.35          1.9         1.63         1.83         2.48         2.16        0.702       0.0468\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              48   85.768    0.005      0.00554     3.65e-05      0.00558         1.75         2.28         1.44         2.12         1.78         1.87         2.67         2.27         2.24        0.149\n",
            "! Validation         48   85.768    0.005      0.00497     4.85e-06      0.00498          1.6         2.16         1.34         1.89         1.62         1.89         2.43         2.16        0.746       0.0497\n",
            "Wall time: 85.76919720600006\n",
            "! Best model       48    0.005\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     49    10      0.00706      0.00699     7.27e-05            2         2.56          1.8         2.22         2.01         2.22          2.9         2.56         3.83        0.255\n",
            "     49    20      0.00425      0.00421     3.17e-05         1.52         1.99         1.35         1.71         1.53         1.76         2.22         1.99         2.48        0.165\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     49     5      0.00488      0.00488     3.96e-06         1.59         2.14         1.34         1.88         1.61         1.82         2.45         2.14        0.689       0.0459\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              49   87.455    0.005      0.00609     4.21e-05      0.00613         1.82         2.39         1.49         2.21         1.85         1.93         2.83         2.38         2.59        0.173\n",
            "! Validation         49   87.455    0.005      0.00489     4.76e-06      0.00489         1.59         2.14         1.33         1.88          1.6         1.88         2.41         2.14        0.738       0.0492\n",
            "Wall time: 87.45550388700008\n",
            "! Best model       49    0.005\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     50    10      0.00505      0.00494     0.000109          1.7         2.15         1.48         1.96         1.72         1.88         2.43         2.15         4.75        0.317\n",
            "     50    20      0.00539      0.00539     5.68e-06         1.73         2.25         1.33         2.18         1.76         1.76          2.7         2.23        0.766        0.051\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     50     5      0.00481      0.00481     4.04e-06         1.58         2.12         1.33         1.86          1.6         1.81         2.43         2.12        0.698       0.0466\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              50   89.132    0.005      0.00468     6.51e-05      0.00474         1.62         2.09         1.38         1.89         1.63         1.79         2.39         2.09         3.17        0.211\n",
            "! Validation         50   89.132    0.005      0.00481     4.47e-06      0.00482         1.57         2.12         1.32         1.86         1.59         1.86         2.39         2.13        0.715       0.0477\n",
            "Wall time: 89.13283661700007\n",
            "! Best model       50    0.005\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     51    10      0.00695      0.00695     3.07e-06         2.04         2.55         1.57         2.58         2.08         1.91         3.13         2.52        0.681       0.0454\n",
            "     51    20      0.00788      0.00788     4.14e-06         2.16         2.72         1.89         2.48         2.18         2.39         3.05         2.72        0.866       0.0577\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     51     5      0.00474      0.00474     3.88e-06         1.57         2.11         1.33         1.85         1.59         1.79         2.42         2.11        0.688       0.0458\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              51   90.819    0.005      0.00542     1.06e-05      0.00543         1.74         2.25         1.42         2.12         1.77         1.81         2.67         2.24         1.18       0.0788\n",
            "! Validation         51   90.819    0.005      0.00475     4.47e-06      0.00475         1.56         2.11         1.31         1.85         1.58         1.85         2.37         2.11        0.718       0.0479\n",
            "Wall time: 90.82011515400006\n",
            "! Best model       51    0.005\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     52    10      0.00581      0.00581     1.62e-06         1.88         2.33         1.58         2.23          1.9         1.91         2.74         2.32        0.472       0.0315\n",
            "     52    20      0.00617      0.00616     1.14e-05         1.83          2.4         1.48         2.23         1.86         1.97         2.82         2.39          1.4       0.0933\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     52     5      0.00467      0.00467     3.94e-06         1.56         2.09         1.32         1.84         1.58         1.78          2.4         2.09        0.689       0.0459\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              52   92.487    0.005      0.00496     1.24e-05      0.00497         1.68         2.16         1.38         2.02          1.7         1.77         2.53         2.15         1.34       0.0897\n",
            "! Validation         52   92.487    0.005      0.00469     4.27e-06       0.0047         1.55          2.1          1.3         1.84         1.57         1.83         2.36          2.1          0.7       0.0467\n",
            "Wall time: 92.48756182900001\n",
            "! Best model       52    0.005\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     53    10      0.00388      0.00379     8.88e-05         1.45         1.88         1.21         1.72         1.47         1.61         2.16         1.88         4.26        0.284\n",
            "     53    20      0.00469      0.00468     3.84e-06         1.52          2.1         1.31         1.76         1.53         1.85         2.35          2.1        0.678       0.0452\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     53     5      0.00461      0.00461     3.93e-06         1.55         2.08         1.31         1.82         1.57         1.77         2.39         2.08        0.686       0.0457\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              53   94.165    0.005      0.00373     3.34e-05      0.00376         1.42         1.87         1.18         1.69         1.44         1.56         2.17         1.87         2.35        0.156\n",
            "! Validation         53   94.165    0.005      0.00462     4.22e-06      0.00462         1.54         2.08         1.29         1.83         1.56         1.82         2.34         2.08        0.696       0.0464\n",
            "Wall time: 94.16612674800001\n",
            "! Best model       53    0.005\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     54    10      0.00371       0.0037     7.89e-06         1.44         1.86         1.15         1.77         1.46         1.49         2.21         1.85         1.22        0.081\n",
            "     54    20      0.00334      0.00334     8.17e-07          1.3         1.77        0.974         1.68         1.33         1.36         2.14         1.75        0.306       0.0204\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     54     5      0.00453      0.00453     3.79e-06         1.53         2.06          1.3          1.8         1.55         1.75         2.37         2.06        0.675        0.045\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              54   95.847    0.005      0.00355     2.45e-05      0.00358         1.39         1.83         1.15         1.66          1.4         1.53         2.12         1.82         1.87        0.125\n",
            "! Validation         54   95.847    0.005      0.00454     4.13e-06      0.00455         1.52         2.06         1.27         1.81         1.54          1.8         2.33         2.06        0.687       0.0458\n",
            "Wall time: 95.847895849\n",
            "! Best model       54    0.005\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     55    10      0.00327      0.00326     9.18e-06         1.39         1.75         1.22         1.58          1.4          1.6          1.9         1.75          1.3       0.0869\n",
            "     55    20      0.00264      0.00264     5.31e-06         1.22         1.57         1.09         1.38         1.23         1.45          1.7         1.58        0.909       0.0606\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     55     5      0.00447      0.00447     3.68e-06         1.52         2.05         1.29         1.79         1.54         1.73         2.35         2.04        0.664       0.0443\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              55   97.535    0.005        0.004     9.55e-06      0.00401         1.48         1.94         1.22         1.78          1.5         1.59         2.27         1.93         1.18       0.0788\n",
            "! Validation         55   97.535    0.005      0.00448     4.01e-06      0.00448         1.51         2.05         1.26          1.8         1.53         1.78         2.32         2.05        0.674       0.0449\n",
            "Wall time: 97.53624132700008\n",
            "! Best model       55    0.004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     56    10      0.00404      0.00403     1.03e-05          1.5         1.94         1.36         1.66         1.51         1.73         2.17         1.95         1.35       0.0898\n",
            "     56    20      0.00438      0.00434     4.36e-05         1.55         2.02         1.28         1.86         1.57         1.75         2.29         2.02         2.97        0.198\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     56     5       0.0044      0.00439     3.69e-06         1.51         2.03         1.27         1.78         1.53         1.71         2.34         2.03        0.667       0.0445\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              56   99.208    0.005      0.00341     1.41e-05      0.00343         1.37         1.79         1.16         1.61         1.38         1.53         2.05         1.79         1.45       0.0966\n",
            "! Validation         56   99.208    0.005       0.0044     3.89e-06      0.00441          1.5         2.03         1.25         1.78         1.52         1.77          2.3         2.03        0.664       0.0443\n",
            "Wall time: 99.209066312\n",
            "! Best model       56    0.004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     57    10      0.00356      0.00355     1.38e-05         1.35         1.82         1.11         1.63         1.37          1.5         2.14         1.82         1.58        0.105\n",
            "     57    20      0.00384      0.00384     3.35e-06          1.5          1.9         1.22         1.83         1.52         1.51         2.26         1.88        0.734        0.049\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     57     5      0.00433      0.00433     3.65e-06          1.5         2.01         1.26         1.76         1.51          1.7         2.32         2.01        0.656       0.0437\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              57  100.886    0.005      0.00309     1.42e-05      0.00311         1.31          1.7         1.11         1.54         1.32         1.45         1.95          1.7         1.43       0.0952\n",
            "! Validation         57  100.886    0.005      0.00433      3.9e-06      0.00433         1.49         2.02         1.24         1.77          1.5         1.75         2.28         2.01        0.662       0.0441\n",
            "Wall time: 100.88730400200006\n",
            "! Best model       57    0.004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     58    10      0.00311       0.0031      1.5e-05         1.28          1.7        0.984         1.61          1.3         1.38         2.01          1.7         1.74        0.116\n",
            "     58    20      0.00433       0.0043        3e-05         1.43         2.01        0.967         1.95         1.46         1.29          2.6         1.94         2.45        0.163\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     58     5      0.00426      0.00426     3.56e-06         1.48            2         1.25         1.75          1.5         1.68          2.3         1.99        0.652       0.0434\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              58  102.595    0.005      0.00356     1.83e-05      0.00358         1.39         1.83         1.11          1.7         1.41         1.46         2.18         1.82         1.68        0.112\n",
            "! Validation         58  102.595    0.005      0.00425      3.9e-06      0.00426         1.47            2         1.23         1.76         1.49         1.73         2.26            2        0.664       0.0443\n",
            "Wall time: 102.59632693500009\n",
            "! Best model       58    0.004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     59    10      0.00406      0.00403     2.49e-05         1.58         1.94         1.13         2.09         1.61         1.36         2.45          1.9         2.25         0.15\n",
            "     59    20      0.00536      0.00535     9.73e-06         1.74         2.24         1.57         1.94         1.75         2.02         2.47         2.24         1.19       0.0796\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     59     5      0.00417      0.00417     3.56e-06         1.47         1.98         1.24         1.73         1.48         1.66         2.28         1.97        0.644       0.0429\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              59  104.274    0.005      0.00419     3.32e-05      0.00423         1.49         1.98         1.21         1.82         1.52         1.57         2.37         1.97         2.24        0.149\n",
            "! Validation         59  104.274    0.005      0.00418      3.9e-06      0.00419         1.46         1.98         1.21         1.74         1.48         1.71         2.25         1.98        0.659        0.044\n",
            "Wall time: 104.27513960200008\n",
            "! Best model       59    0.004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     60    10      0.00542      0.00542     4.97e-06         1.73         2.25         1.38         2.14         1.76         1.77          2.7         2.24        0.759       0.0506\n",
            "     60    20      0.00472      0.00468     3.76e-05         1.61         2.09         1.44         1.81         1.63         1.94         2.26          2.1         2.73        0.182\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     60     5       0.0041      0.00409     3.44e-06         1.45         1.96         1.23         1.71         1.47         1.65         2.26         1.96        0.637       0.0425\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              60  105.953    0.005      0.00409     2.58e-05      0.00411          1.5         1.96         1.25         1.78         1.51         1.61         2.29         1.95         1.97        0.131\n",
            "! Validation         60  105.953    0.005      0.00412     3.72e-06      0.00413         1.45         1.97          1.2         1.73         1.47         1.69         2.24         1.97        0.644       0.0429\n",
            "Wall time: 105.95431512800008\n",
            "! Best model       60    0.004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     61    10       0.0031      0.00297     0.000124         1.32         1.67         1.01         1.67         1.34         1.35         1.97         1.66          5.1         0.34\n",
            "     61    20      0.00405      0.00373     0.000325         1.45         1.87         1.09         1.86         1.47         1.37         2.31         1.84         8.24         0.55\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     61     5      0.00402      0.00401     3.48e-06         1.44         1.94         1.22         1.69         1.46         1.64         2.24         1.94        0.625       0.0417\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              61  107.613    0.005      0.00397      0.00011      0.00408         1.48         1.93         1.17         1.83          1.5         1.53          2.3         1.92         4.23        0.282\n",
            "! Validation         61  107.613    0.005      0.00406     4.03e-06      0.00407         1.44         1.95         1.19         1.72         1.45         1.68         2.22         1.95        0.664       0.0443\n",
            "Wall time: 107.61345416699999\n",
            "! Best model       61    0.004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     62    10      0.00344      0.00344      9.7e-07         1.29          1.8        0.995         1.63         1.31         1.28         2.25         1.76        0.344       0.0229\n",
            "     62    20       0.0074       0.0073     9.89e-05         2.06         2.62         1.43         2.77          2.1         1.79         3.32         2.55         4.52        0.301\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     62     5      0.00395      0.00395     3.38e-06         1.43         1.92         1.21         1.68         1.44         1.63         2.22         1.92        0.627       0.0418\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              62  109.298    0.005      0.00403     5.85e-05      0.00409         1.48         1.94         1.18         1.82          1.5         1.53         2.33         1.93         2.96        0.197\n",
            "! Validation         62  109.298    0.005      0.00401     3.53e-06      0.00401         1.43         1.94         1.18         1.71         1.44         1.67         2.21         1.94        0.623       0.0415\n",
            "Wall time: 109.29845103100001\n",
            "! Best model       62    0.004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     63    10      0.00476      0.00467     8.31e-05         1.64         2.09         1.28         2.06         1.67         1.68         2.48         2.08         4.14        0.276\n",
            "     63    20      0.00332      0.00328     3.45e-05         1.33         1.75         1.09         1.61         1.35         1.46         2.04         1.75         2.62        0.175\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     63     5      0.00389      0.00388     3.37e-06         1.42         1.91          1.2         1.67         1.44         1.61          2.2          1.9        0.636       0.0424\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              63  110.979    0.005      0.00429     6.42e-05      0.00435         1.54         2.01         1.21          1.9         1.56         1.58          2.4         1.99         3.24        0.216\n",
            "! Validation         63  110.979    0.005      0.00395      3.4e-06      0.00396         1.42         1.93         1.17          1.7         1.43         1.65          2.2         1.92        0.615        0.041\n",
            "Wall time: 110.97936811800002\n",
            "! Best model       63    0.004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     64    10      0.00276      0.00276     7.21e-07         1.26         1.61         1.07         1.49         1.28         1.41         1.81         1.61        0.372       0.0248\n",
            "Error in sys.excepthook:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 47, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 449, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 418, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 376, in read_or_stop\n",
            "    return readline()\n",
            "KeyboardInterrupt\n",
            "\n",
            "Original exception was:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/nequip-train\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/nequip/scripts/train.py\", line 78, in main\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/nequip/train/trainer.py\", line 775, in train\n",
            "    self.epoch_step()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/nequip/train/trainer.py\", line 915, in epoch_step\n",
            "    validation=(category == VALIDATION),\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/nequip/train/trainer.py\", line 814, in batch_step\n",
            "    out = self.model(input_data)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/nequip/nn/_rescale.py\", line 140, in forward\n",
            "    data = self.model(data)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/nequip/nn/_grad_output.py\", line 85, in forward\n",
            "    data = self.func(data)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/nequip/nn/_graph_mixin.py\", line 356, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/nequip/nn/_convnetlayer.py\", line 160, in forward\n",
            "    data = self.conv(data)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/nequip/nn/_interaction_block.py\", line 169, in forward\n",
            "    x[edge_src], data[AtomicDataDict.EDGE_ATTRS_KEY], weight\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/e3nn/o3/_tensor_product/_tensor_product.py\", line 544, in forward\n",
            "    return self._compiled_main_left_right(x, y, real_weight)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "KeyboardInterrupt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      LR ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         cumulative_wall ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        training_C_f_mae █▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       training_C_f_rmse █▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        training_H_f_mae █▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       training_H_f_rmse █▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        training_e/N_mae █▆▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          training_e_mae █▆▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          training_f_mae █▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         training_f_rmse █▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           training_loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         training_loss_e █▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         training_loss_f █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_psavg_f_mae █▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_psavg_f_rmse █▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      validation_C_f_mae █▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     validation_C_f_rmse █▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      validation_H_f_mae █▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     validation_H_f_rmse █▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      validation_e/N_mae █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        validation_e_mae █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        validation_f_mae █▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       validation_f_rmse █▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         validation_loss █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       validation_loss_e █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       validation_loss_f █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_psavg_f_mae █▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_psavg_f_rmse █▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    wall ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      LR 0.005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         cumulative_wall 110.9786\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 63\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        training_C_f_mae 1.90282\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       training_C_f_rmse 2.39989\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        training_H_f_mae 1.21486\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       training_H_f_rmse 1.58122\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        training_e/N_mae 0.21581\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          training_e_mae 3.23719\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          training_f_mae 1.53591\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         training_f_rmse 2.0053\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           training_loss 0.00435\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         training_loss_e 6e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         training_loss_f 0.00429\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_psavg_f_mae 1.55884\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_psavg_f_rmse 1.99055\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      validation_C_f_mae 1.696\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     validation_C_f_rmse 2.19507\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      validation_H_f_mae 1.17274\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     validation_H_f_rmse 1.65353\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      validation_e/N_mae 0.04098\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        validation_e_mae 0.61469\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        validation_f_mae 1.41693\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       validation_f_rmse 1.9253\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         validation_loss 0.00396\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       validation_loss_e 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       validation_loss_f 0.00395\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_psavg_f_mae 1.43437\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_psavg_f_rmse 1.9243\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    wall 110.9786\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mexample-run-toluene\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/anony-moose-350753/toluene-example/runs/csw38ouy?apiKey=346134a58e9d74974f32b0ca7cfa6410207b04dc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220710_095935-csw38ouy/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EwuQZLDO4Cr"
      },
      "source": [
        "We see that the model has converged to an energy accuarcy < 1meV/atom and a force accuracy of approx. 40 meV/A within 5 minutes and trained on only 100 samples. That should give us a good first potential! Note that these numbers will decrease significantly if you increase the training set size and the number of epochs to train. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJitSZgLYNNF"
      },
      "source": [
        "### Deploy the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo_kIpYV00as"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/deploy.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VoeGtlA02KQ"
      },
      "source": [
        "We now convert the model to a potential file. This makes it independent of NequIP and we can use it any downstream application, such as LAMMPS. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3NJJgtDIDNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1046109-9e2c-430a-a7b3-396391ee7056"
      },
      "source": [
        "!nequip-deploy build --train-dir results/toluene/example-run-toluene toluene-deployed.pth"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Loading best_model from training session...\n",
            "INFO:root:Compiled & optimized model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXpcE3oP0LyD"
      },
      "source": [
        "## Evaluate Test Error on all remaining frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wRKKCZ2PRl3"
      },
      "source": [
        "Before running inference, we'd like to know how well the model is doing on a hold-out test set. We run the nequip-evaluate command to compute the test error on all data that we didn't use for training or validation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB54WSrN0PaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfd5e79f-f6ba-4de4-e2a5-fe8eb296f5a3"
      },
      "source": [
        "!nequip-evaluate --train-dir results/toluene/example-run-toluene --batch-size 50"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "WARNING: please note that models running on CUDA are usually nondeterministc and that this manifests in the final test errors; for a _more_ deterministic result, please use `--device cpu`\n",
            "Loading model... \n",
            "loaded model from training session\n",
            "Loading original dataset...\n",
            "Loaded dataset specified in config.yaml.\n",
            "Using origial training dataset (1000 frames) minus training (100 frames) and validation frames (50 frames), yielding a test set size of 850 frames.\n",
            "Starting...\n",
            "  0% 0/850 [00:00<?, ?it/s]\n",
            "\u001b[A\n",
            "  6% 50/850 [00:00<00:03, 202.98it/s]\n",
            " 12% 100/850 [00:00<00:07, 95.54it/s]\n",
            " 18% 150/850 [00:02<00:12, 53.98it/s]\n",
            " 24% 200/850 [00:03<00:14, 45.60it/s]\n",
            "\u001b[A\n",
            " 35% 300/850 [00:03<00:06, 90.86it/s]\n",
            "\u001b[A\n",
            " 47% 400/850 [00:03<00:03, 146.52it/s]\n",
            "\u001b[A\n",
            " 59% 500/850 [00:04<00:01, 212.21it/s]\n",
            "\u001b[A\n",
            " 71% 600/850 [00:04<00:00, 282.90it/s]\n",
            "\u001b[A\n",
            " 82% 700/850 [00:04<00:00, 358.61it/s]\n",
            "\u001b[A\n",
            " 94% 800/850 [00:04<00:00, 433.80it/s]\n",
            "100% 850/850 [00:04<00:00, 185.45it/s]\n",
            "\n",
            "\n",
            "--- Final result: ---\n",
            "               f_mae =  1.360746           \n",
            "              f_rmse =  1.873886           \n",
            "             H_f_mae =  1.122913           \n",
            "             C_f_mae =  1.632556           \n",
            "         psavg_f_mae =  1.377734           \n",
            "            H_f_rmse =  1.589736           \n",
            "            C_f_rmse =  2.153191           \n",
            "        psavg_f_rmse =  1.871464           \n",
            "               e_mae =  0.561195           \n",
            "             e/N_mae =  0.037413           \n",
            "               f_mae =  1.360746           \n",
            "              f_rmse =  1.873886           \n",
            "             H_f_mae =  1.122913           \n",
            "             C_f_mae =  1.632556           \n",
            "         psavg_f_mae =  1.377734           \n",
            "            H_f_rmse =  1.589736           \n",
            "            C_f_rmse =  2.153191           \n",
            "        psavg_f_rmse =  1.871464           \n",
            "               e_mae =  0.561195           \n",
            "             e/N_mae =  0.037413           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQHrMMnsPaJO"
      },
      "source": [
        "Again, energy errors of < 1meV/atom (converted from kcal/mol to eV), and force errors of ~45 meV/A 🎉"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4r5FBXaum9n"
      },
      "source": [
        "# LAMMPS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qIYIYyr1B4O"
      },
      "source": [
        "We are now in a position to run MD with our potential. Here, we will minimize the geometry of the toluene molecule we trained on from a perturbed initial state. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UirNBTlJ1BNZ"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/run.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQs0ijPhvAGb"
      },
      "source": [
        "Set up a simple LAMMPS input file\n",
        "\n",
        "CAUTION: the reference data here are in kcal/mol for the energies and kcal/mol/A for the forces. The NequIP model will therefore also be predicting outputs in these units. We are therefore using `units real` in LAMMPS (see [docs](https://docs.lammps.org/units.html)). If your reference data are in other units, you should using the corresponding units command in LAMMPS (e.g. if you use eV, A then `units metal` would be appropriate, which would then also change time units from `fs` to `ps`)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "W090KfMsd2Do"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tAHO8ODrpwG"
      },
      "source": [
        "lammps_input_minimize = \"\"\"\n",
        "units\treal\n",
        "atom_style atomic\n",
        "newton off\n",
        "thermo 1\n",
        "read_data structure.data\n",
        "\n",
        "pair_style\tnequip\n",
        "pair_coeff\t* * ../toluene-deployed.pth C H \n",
        "mass            1 15.9994\n",
        "mass            2 1.00794\n",
        "\n",
        "neighbor 1.0 bin\n",
        "neigh_modify delay 5 every 1\n",
        "\n",
        "minimize 0.0 1.0e-8 10000 1000000\n",
        "write_dump all custom output.dump id type x y z fx fy fz\n",
        "\"\"\"\n",
        "!mkdir lammps_run\n",
        "with open(\"lammps_run/toluene_minimize.in\", \"w\") as f:\n",
        "    f.write(lammps_input_minimize)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvWZCw1zvjRc"
      },
      "source": [
        "Here's starting configuration for Toluene at CCSD(T) accuracy. We will strongly perturb the inital positions by sampling from a uniform distribution $\\mathcal{U}([0, 0.5])$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEPfMeGnJUVH"
      },
      "source": [
        "toluene_example = \"\"\"15\n",
        " Lattice=\"100.0 0.0 0.0 0.0 100.0 0.0 0.0 0.0 100.0\" Properties=species:S:1:pos:R:3 -169777.5840406276=T pbc=\"F F F\"\n",
        " C       52.48936904      49.86911725      50.09520748\n",
        " C       51.01088202      49.89609925      50.17978049\n",
        " C       50.36647401      50.04650925      48.96054247\n",
        " C       48.95673398      50.29576626      48.71580846\n",
        " C       48.04533296      50.26023426      49.82589448\n",
        " C       48.70932398      49.85770925      51.01923950\n",
        " C       50.06326400      49.77782925      51.25691751\n",
        " H       52.94467905      50.48672926      50.86545150\n",
        " H       52.89060405      48.87175023      50.14480949\n",
        " H       53.02173405      50.05890725      49.03968247\n",
        " H       51.01439802      50.38234726      48.05314045\n",
        " H       48.80598498      50.64314926      47.68195744\n",
        " H       46.96754695      50.20586626      49.53998848\n",
        " H       48.16716997      49.75850325      51.88622952\n",
        " H       50.45791001      49.55387424      52.15303052\n",
        " \"\"\"\n",
        "\n",
        "with open('toluene.xyz', 'w') as f: \n",
        "    f.write(toluene_example)\n",
        "\n",
        "# read as ASE objects\n",
        "atoms = read('toluene.xyz', format='extxyz')\n",
        "\n",
        "# perturb positions\n",
        "p = atoms.get_positions()\n",
        "p += np.random.rand(15, 3) * 0.5\n",
        "atoms.set_positions(p)\n",
        "atoms.set_pbc(False)\n",
        "\n",
        "# write to a LAMMPS file\n",
        "write(\"lammps_run/structure.data\", atoms, format=\"lammps-data\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDuyueY11YBF"
      },
      "source": [
        "### Run the LAMMPS command: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gurLjNK5upvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d415879-d932-4588-f29e-2a73f5d55ee2"
      },
      "source": [
        "!cd lammps_run/ && ../lammps/build/lmp -in toluene_minimize.in"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LAMMPS (29 Sep 2021 - Update 2)\n",
            "OMP_NUM_THREADS environment is not set. Defaulting to 1 thread. (src/comm.cpp:98)\n",
            "  using 1 OpenMP thread(s) per MPI task\n",
            "Reading data file ...\n",
            "  orthogonal box = (0.0000000 0.0000000 0.0000000) to (100.00000 100.00000 100.00000)\n",
            "  1 by 1 by 1 MPI processor grid\n",
            "  reading atoms ...\n",
            "  15 atoms\n",
            "  read_data CPU = 0.001 seconds\n",
            "NEQUIP is using device cuda\n",
            "NequIP Coeff: type 1 is element C\n",
            "NequIP Coeff: type 2 is element H\n",
            "Loading model from ../toluene-deployed.pth\n",
            "Freezing TorchScript model...\n",
            "WARNING: Using 'neigh_modify every 1 delay 0 check yes' setting during minimization (src/min.cpp:188)\n",
            "Neighbor list info ...\n",
            "  update every 1 steps, delay 0 steps, check yes\n",
            "  max neighbors/atom: 2000, page size: 100000\n",
            "  master list distance cutoff = 5\n",
            "  ghost atom cutoff = 5\n",
            "  binsize = 2.5, bins = 40 40 40\n",
            "  1 neighbor lists, perpetual/occasional/extra = 1 0 0\n",
            "  (1) pair nequip, perpetual\n",
            "      attributes: full, newton off\n",
            "      pair build: full/bin/atomonly\n",
            "      stencil: full/bin/3d\n",
            "      bin: standard\n",
            "Setting up cg style minimization ...\n",
            "  Unit style    : real\n",
            "  Current step  : 0\n",
            "Per MPI rank memory allocation (min/avg/max) = 4.603 | 4.603 | 4.603 Mbytes\n",
            "Step Temp E_pair E_mol TotEng Press \n",
            "       0            0    -169576.8            0    -169576.8            0 \n",
            "       1            0   -169648.28            0   -169648.28            0 \n",
            "       2            0   -169724.03            0   -169724.03            0 \n",
            "       3            0   -169767.19            0   -169767.19            0 \n",
            "       4            0   -169785.98            0   -169785.98            0 \n",
            "       5            0   -169801.81            0   -169801.81            0 \n",
            "       6            0   -169805.38            0   -169805.38            0 \n",
            "       7            0   -169809.91            0   -169809.91            0 \n",
            "       8            0   -169811.58            0   -169811.58            0 \n",
            "       9            0   -169812.81            0   -169812.81            0 \n",
            "      10            0   -169813.62            0   -169813.62            0 \n",
            "      11            0   -169813.98            0   -169813.98            0 \n",
            "      12            0   -169814.31            0   -169814.31            0 \n",
            "      13            0   -169814.56            0   -169814.56            0 \n",
            "      14            0   -169814.89            0   -169814.89            0 \n",
            "      15            0   -169815.12            0   -169815.12            0 \n",
            "      16            0   -169815.33            0   -169815.33            0 \n",
            "      17            0   -169815.53            0   -169815.53            0 \n",
            "      18            0   -169815.66            0   -169815.66            0 \n",
            "      19            0   -169815.77            0   -169815.77            0 \n",
            "      20            0   -169815.84            0   -169815.84            0 \n",
            "      21            0   -169815.89            0   -169815.89            0 \n",
            "      22            0   -169815.94            0   -169815.94            0 \n",
            "      23            0   -169815.95            0   -169815.95            0 \n",
            "      24            0   -169816.02            0   -169816.02            0 \n",
            "      25            0   -169816.03            0   -169816.03            0 \n",
            "      26            0   -169816.05            0   -169816.05            0 \n",
            "      27            0   -169816.05            0   -169816.05            0 \n",
            "      28            0   -169816.05            0   -169816.05            0 \n",
            "      29            0   -169816.06            0   -169816.06            0 \n",
            "      30            0   -169816.06            0   -169816.06            0 \n",
            "      31            0   -169816.08            0   -169816.08            0 \n",
            "      32            0   -169816.08            0   -169816.08            0 \n",
            "Loop time of 8.95443 on 1 procs for 32 steps with 15 atoms\n",
            "\n",
            "98.7% CPU use with 1 MPI tasks x 1 OpenMP threads\n",
            "\n",
            "Minimization stats:\n",
            "  Stopping criterion = linesearch alpha is zero\n",
            "  Energy initial, next-to-last, final = \n",
            "        -169576.796875     -169816.078125     -169816.078125\n",
            "  Force two-norm initial, final = 477.18997 1.8789557\n",
            "  Force max component initial, final = 212.61981 0.75577152\n",
            "  Final line search alpha, max atom move = 1.9715860e-09 1.4900685e-09\n",
            "  Iterations, force evaluations = 32 112\n",
            "\n",
            "MPI task timing breakdown:\n",
            "Section |  min time  |  avg time  |  max time  |%varavg| %total\n",
            "---------------------------------------------------------------\n",
            "Pair    | 8.9524     | 8.9524     | 8.9524     |   0.0 | 99.98\n",
            "Neigh   | 0          | 0          | 0          |   0.0 |  0.00\n",
            "Comm    | 8.4714e-05 | 8.4714e-05 | 8.4714e-05 |   0.0 |  0.00\n",
            "Output  | 0.0011604  | 0.0011604  | 0.0011604  |   0.0 |  0.01\n",
            "Modify  | 0          | 0          | 0          |   0.0 |  0.00\n",
            "Other   |            | 0.0007875  |            |       |  0.01\n",
            "\n",
            "Nlocal:        15.0000 ave          15 max          15 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "Nghost:         0.00000 ave           0 max           0 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "Neighs:         0.00000 ave           0 max           0 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "FullNghs:      192.000 ave         192 max         192 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "\n",
            "Total # of neighbors = 192\n",
            "Ave neighs/atom = 12.800000\n",
            "Neighbor list builds = 0\n",
            "Dangerous builds = 0\n",
            "Total wall time: 0:00:12\n",
            "[c83070bc1ac0:02957] *** Process received signal ***\n",
            "[c83070bc1ac0:02957] Signal: Segmentation fault (11)\n",
            "[c83070bc1ac0:02957] Signal code: Address not mapped (1)\n",
            "[c83070bc1ac0:02957] Failing at address: 0x7f2502cda20d\n",
            "[c83070bc1ac0:02957] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f2505b87980]\n",
            "[c83070bc1ac0:02957] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7f25057c6775]\n",
            "[c83070bc1ac0:02957] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7f2506031e44]\n",
            "[c83070bc1ac0:02957] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7f25057c7605]\n",
            "[c83070bc1ac0:02957] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7f250602fcb3]\n",
            "[c83070bc1ac0:02957] *** End of error message ***\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOKfQ83JQESc"
      },
      "source": [
        "We see LAMMPS converges quickly to a minimum. Let's check how well we did. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPcQU9HbsaVl"
      },
      "source": [
        "# read the final structure back in \n",
        "minimized = read('./lammps_run/output.dump', format='lammps-dump-text')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brqGqVtdWpCF"
      },
      "source": [
        "### Compare optimized bond length to true coupled cluster reference from CCCBDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltFlaHrTRn97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c278f5ea-a655-4aa9-bc57-fb0fe9cde2dc"
      },
      "source": [
        "# get distances of optimized geometry (reference data: CCSD(T) [Psi4, cc-pVDZ])\n",
        "d_12 = minimized.get_distances(1, 2)\n",
        "\n",
        "# reference: https://cccbdb.nist.gov/geom3x.asp?method=6&basis=2, coupled cluster\n",
        "d_12_ccd = 1.4086\n",
        "\n",
        "print('Relative Error in bond length w.r.t. Coupled Cluster from CCCBDB: {:.3f}%'.format((100 * np.abs(d_12 - d_12_ccd) / d_12_ccd)[0]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relative Error in bond length w.r.t. Coupled Cluster from CCCBDB: 0.002%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT64gDUeQOvO"
      },
      "source": [
        "We find a final relative error close to Coupled Cluster accuracy 🎉"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4ZD6U0EkIp5"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "This concludes our tutorial. A next step would be to head over to https://github.com/mir-group/nequip, install NequIP and get started with your own system. If you have questions, please don't hesitate to reach out to batzner@g.harvard.edu, we're happy to help! \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ko53l9ShB6ty"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}