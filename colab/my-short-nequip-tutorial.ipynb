{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFpAi8g9XmUU"
      },
      "source": [
        "# Molecular Dynamics with NequIP \n",
        "\n",
        "### NequIP Authors: Simon Batzner, Albert Musaelian, Lixin Sun, Anders Johansson, Boris Kozinsky"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/nequip3.png?raw=true\" width=\"60%\">"
      ],
      "metadata": {
        "id": "6qtq3KJqLq9F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m1W9W9yvuKA"
      },
      "source": [
        "### Tutorial for MD for bulk water modified from this other [Colab Tutorial](https://bit.ly/mrs-nequip)\n",
        "#### by Gabriele Tocci"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The water trajectory is obtained with the SCAN functional at 300 K and is the last 10 ps of a trajectory used in the paper by Herrero et al.: [Connection between water's dynamical and structural properties: Insights from ab initio simulations ](https://www.pnas.org/doi/10.1073/pnas.2121641119)"
      ],
      "metadata": {
        "id": "gMg3YvVF2aBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open in colab and change the runtime to use the GPU\n",
        "<a href=\"https://colab.research.google.com/github/gabriele16/nequip/blob/main/colab/my-short-nequip-tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
      ],
      "metadata": {
        "id": "2shNisM863Wy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7bW4JWmmuyD",
        "outputId": "e108c403-8c51-4596-b0e7-7f5cc79c816f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.10\n",
            "  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.1 MB/s eta 0:00:43tcmalloc: large alloc 1147494400 bytes == 0x3aa04000 @  0x7fd1ca07e615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10) (4.1.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOLIFOJZaeZ5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import os\n",
        "data_dir = '/content/nequip/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZpOvFtImsy2",
        "outputId": "62901f69-8b5e-4230-f2a8-dd3ac6ce580a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.0+cu102\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "print(torch. __version__)\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7Uh7nyHnR-w",
        "outputId": "e082d1d8-5fdb-49d2-ca2a-b2ff4e115d64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIMyrDOEm2IB",
        "outputId": "c28b8859-a59f-4aa3-d4fe-3a82c4c125d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'lammps': No such file or directory\n",
            "Cloning into 'lammps'...\n",
            "remote: Enumerating objects: 11732, done.\u001b[K\n",
            "remote: Counting objects: 100% (11732/11732), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8603/8603), done.\u001b[K\n",
            "remote: Total 11732 (delta 3943), reused 6318 (delta 2930), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (11732/11732), 110.00 MiB | 13.75 MiB/s, done.\n",
            "Resolving deltas: 100% (3943/3943), done.\n",
            "Note: checking out '7586adbb6a61254125992709ef2fda9134cfca6c'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "Checking out files: 100% (11058/11058), done.\n",
            "Cloning into 'pair_nequip'...\n",
            "remote: Enumerating objects: 418, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 418 (delta 79), reused 85 (delta 65), pack-reused 302\u001b[K\n",
            "Receiving objects: 100% (418/418), 427.38 KiB | 3.10 MiB/s, done.\n",
            "Resolving deltas: 100% (207/207), done.\n",
            "Copying files...\n",
            "Updating CMakeLists.txt...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "!rm -r lammps\n",
        "!git clone -b \"stable_29Sep2021_update2\" --depth 1 \"https://github.com/lammps/lammps.git\"\n",
        "!git clone https://github.com/mir-group/pair_nequip\n",
        "!cd pair_nequip && ./patch_lammps.sh /content/lammps/\n",
        "!cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "100Be8B6m5am"
      },
      "outputs": [],
      "source": [
        "!cp /content/pair_nequip/*.cpp /content/lammps/src/\n",
        "!cp /content/pair_nequip/*.h /content/lammps/src/\n",
        "! sed -i 's/CMAKE_CXX_STANDARD 11/CMAKE_CXX_STANDARD 14/g'  /content/lammps/cmake/CMakeLists.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlgRSmyom9VQ",
        "outputId": "52b186fb-8798-4783-b113-66589421740a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.7/dist-packages (2019.0)\n",
            "Collecting mkl-include\n",
            "  Downloading mkl_include-2022.1.0-py2.py3-none-manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: intel-openmp in /usr/local/lib/python3.7/dist-packages (from mkl) (2022.1.0)\n",
            "Installing collected packages: mkl-include\n",
            "Successfully installed mkl-include-2022.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mkl mkl-include"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8vY6rwbnEgK",
        "outputId": "53fc069e-69a1-4bda-96ae-c37da1e06204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.17.1\") \n",
            "-- Appending /usr/local/cuda/lib64/stubs to CMAKE_LIBRARY_PATH: /usr/local/cuda/lib64/stubs\n",
            "-- Running check for auto-generated files from make-based build system\n",
            "-- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so (found version \"3.1\") \n",
            "-- Found MPI: TRUE (found version \"3.1\")  \n",
            "-- Looking for C++ include omp.h\n",
            "-- Looking for C++ include omp.h - found\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
            "-- Found JPEG: /usr/lib/x86_64-linux-gnu/libjpeg.so (found version \"80\") \n",
            "-- Found PNG: /usr/lib/x86_64-linux-gnu/libpng.so (found version \"1.6.34\") \n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
            "-- Found GZIP: /bin/gzip  \n",
            "-- Found FFMPEG: /usr/bin/ffmpeg  \n",
            "-- Looking for C++ include cmath\n",
            "-- Looking for C++ include cmath - found\n",
            "-- Generating style headers...\n",
            "-- Generating package headers...\n",
            "-- Generating lmpinstalledpkgs.h...\n",
            "-- Could NOT find ClangFormat (missing: ClangFormat_EXECUTABLE) (Required is at least version \"8.0\")\n",
            "-- The following tools and libraries have been found and configured:\n",
            " * Git\n",
            " * MPI\n",
            " * OpenMP\n",
            " * JPEG\n",
            " * PNG\n",
            " * ZLIB\n",
            "\n",
            "-- <<< Build configuration >>>\n",
            "   Operating System: Linux Ubuntu 18.04\n",
            "   Build type:       RelWithDebInfo\n",
            "   Install path:     /root/.local\n",
            "   Generator:        Unix Makefiles using /usr/bin/make\n",
            "-- Enabled packages: <None>\n",
            "-- <<< Compilers and Flags: >>>\n",
            "-- C++ Compiler:     /usr/bin/c++\n",
            "      Type:          GNU\n",
            "      Version:       7.5.0\n",
            "      C++ Flags:     -O2 -g -DNDEBUG\n",
            "      Defines:       LAMMPS_SMALLBIG;LAMMPS_MEMALIGN=64;LAMMPS_OMP_COMPAT=3;LAMMPS_JPEG;LAMMPS_PNG;LAMMPS_GZIP;LAMMPS_FFMPEG\n",
            "-- <<< Linker flags: >>>\n",
            "-- Executable name:  lmp\n",
            "-- Static library flags:    \n",
            "-- <<< MPI flags >>>\n",
            "-- MPI_defines:      MPICH_SKIP_MPICXX;OMPI_SKIP_MPICXX;_MPICC_H\n",
            "-- MPI includes:     /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include;/usr/lib/x86_64-linux-gnu/openmpi/include\n",
            "-- MPI libraries:    /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so;\n",
            "-- Looking for C++ include pthread.h\n",
            "-- Looking for C++ include pthread.h - found\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
            "-- Looking for pthread_create in pthreads\n",
            "-- Looking for pthread_create in pthreads - not found\n",
            "-- Looking for pthread_create in pthread\n",
            "-- Looking for pthread_create in pthread - found\n",
            "-- Found Threads: TRUE  \n",
            "-- Found CUDA: /usr/local/cuda (found version \"11.1\") \n",
            "-- Caffe2: CUDA detected: 11.1\n",
            "-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\n",
            "-- Caffe2: CUDA toolkit directory: /usr/local/cuda\n",
            "-- Caffe2: Header version is: 11.1\n",
            "-- Found CUDNN: /usr/lib/x86_64-linux-gnu/libcudnn.so  \n",
            "-- Found cuDNN: v7.6.5  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)\n",
            "-- /usr/local/cuda/lib64/libnvrtc.so shorthash is 3a20f2b6\n",
            "-- Autodetected CUDA architecture(s):  6.0\n",
            "-- Added CUDA NVCC flags for: -gencode;arch=compute_60,code=sm_60\n",
            "\u001b[33mCMake Warning at /usr/local/lib/python3.7/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\n",
            "  static library kineto_LIBRARY-NOTFOUND not found.\n",
            "Call Stack (most recent call first):\n",
            "  /usr/local/lib/python3.7/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:127 (append_torchlib_if_found)\n",
            "  CMakeLists.txt:922 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch: /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so  \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/lammps/build\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/variable.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/atom.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/angle.h\u001b[0m\n",
            "[  1%] Built target variable.h\n",
            "-- Git Directory: /content/lammps/.git\n",
            "[  1%] Built target atom.h\n",
            "[  1%] Built target angle.h\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/bond.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/citeme.h\u001b[0m\n",
            "[  1%] Built target bond.h\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/comm.h\u001b[0m\n",
            "[  1%] Built target citeme.h\n",
            "[  1%] Built target comm.h\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/compute.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/dihedral.h\u001b[0m\n",
            "[  2%] Built target compute.h\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/domain.h\u001b[0m\n",
            "-- Generating lmpgitversion.h...\n",
            "[  2%] Built target dihedral.h\n",
            "[  2%] Built target domain.h\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/error.h\u001b[0m\n",
            "[  2%] Built target gitversion\n",
            "[  2%] Built target error.h\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/fix.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/force.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/group.h\u001b[0m\n",
            "[  3%] Built target fix.h\n",
            "[  3%] Built target force.h\n",
            "[  3%] Built target group.h\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/improper.h\u001b[0m\n",
            "[  3%] Built target improper.h\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/input.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/kspace.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/info.h\u001b[0m\n",
            "[  4%] Built target input.h\n",
            "[  4%] Built target kspace.h\n",
            "[  4%] Built target info.h\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lammps.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lattice.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/library.h\u001b[0m\n",
            "[  5%] Built target lammps.h\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/lmppython.h\u001b[0m\n",
            "[  5%] Built target lattice.h\n",
            "[  5%] Built target library.h\n",
            "[  5%] Built target lmppython.h\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/lmptype.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/modify.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/memory.h\u001b[0m\n",
            "[  6%] Built target lmptype.h\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/neighbor.h\u001b[0m\n",
            "[  6%] Built target modify.h\n",
            "[  6%] Built target memory.h\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/neigh_list.h\u001b[0m\n",
            "[  6%] Built target neighbor.h\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/output.h\u001b[0m\n",
            "[  7%] Built target neigh_list.h\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/pair.h\u001b[0m\n",
            "[  7%] Built target output.h\n",
            "[  7%] Built target pair.h\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/pointers.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/region.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/timer.h\u001b[0m\n",
            "[  8%] Built target pointers.h\n",
            "[  8%] Built target region.h\n",
            "[  8%] Built target timer.h\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/universe.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/utils.h\u001b[0m\n",
            "[  8%] Built target universe.h\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/update.h\u001b[0m\n",
            "[  8%] Built target utils.h\n",
            "[  8%] Built target update.h\n",
            "[  8%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_deprecated.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_zero.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_hybrid.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/arg_info.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_map.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_atomic.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_body.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_charge.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_ellipsoid.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_hybrid.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_line.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_sphere.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_tri.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/balance.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/body.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_deprecated.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_hybrid.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_zero.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/change_box.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/citeme.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_brick.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_tiled.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_aggregate_atom.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle_local.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angmom_chunk.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond_local.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centro_atom.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centroid_stress_atom.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_atom.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_spread_atom.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cluster_atom.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cna_atom.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com_chunk.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_coord_atom.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_deprecated.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral_local.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole_chunk.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_displace_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_fragment_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_global_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_group_group.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration_chunk.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_heat_flux.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper_local.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_inertia_chunk.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke_atom.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd_chunk.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_omega_chunk.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_orientorder_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair_local.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe_atom.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pressure.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_atom.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_chunk.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_local.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_rdf.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_chunk.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_region.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_slice.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_stress_atom.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_chunk.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_com.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_deform.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_partial.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_profile.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_ramp.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_region.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_sphere.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_torque_chunk.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vacf.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vcm_chunk.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_atoms.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_bonds.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_box.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_atoms.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_bonds.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/deprecated.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_deprecated.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_hybrid.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_zero.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/displace_atoms.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/domain.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_atom.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_cfg.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_custom.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_deprecated.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_image.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_local.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_movie.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_xyz.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/error.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/finish.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_adapt.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_addforce.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_atom.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_chunk.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_correlate.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo_weight.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_time.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_aveforce.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_balance.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_box_relax.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deform.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deposit.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deprecated.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dt_reset.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dummy.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_efield.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_enforce2d.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_evaporate.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_external.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_gravity.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_group.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_halt.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_heat.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_indent.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_langevin.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_lineforce.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_minimize.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_momentum.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_move.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_neigh_history.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh_sphere.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph_sphere.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt_sphere.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_limit.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_noforce.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_sphere.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sllod.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sphere.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_planeforce.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_press_berendsen.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_print.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_property_atom.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_read_restart.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_recenter.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_respa.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_restrain.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_setforce.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_chunk.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_self.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_force.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_state.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_berendsen.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_rescale.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_thermal_conductivity.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_vector.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_viscous.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_harmonic.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj1043.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj126.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj93.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_morse.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_reflect.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_region.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_format.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_os.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/force.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/gridcomm.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/group.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/hashlittle.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/image.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_group.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_neigh.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_store.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_time.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_var.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_deprecated.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_hybrid.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_zero.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/info.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/input.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/integrate.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/irregular.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace_deprecated.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lammps.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lattice.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/library.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lmppython.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_eigen.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_extra.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_special.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/memory.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_cg.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_fire.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_fire_old.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_hftn.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_linesearch.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_quickmin.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_sd.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/minimize.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/modify.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/molecule.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_page.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_pool_chunk.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_multi.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_standard.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_list.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_request.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neighbor.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_copy.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin_atomonly.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin_ghost.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_multi.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_multi_old.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_nsq.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_nsq_ghost.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_atomonly_newton.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newtoff.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newtoff_ghost.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newton.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newton_tri.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newtoff.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newton.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newton_tri.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newtoff.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newton.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newton_tri.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newtoff.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newtoff_ghost.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newton.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newtoff.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newton.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newton_tri.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_nsq_newtoff.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_nsq_newton.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newtoff.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newton.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newton_tri.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newtoff.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newton.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newton_tri.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newtoff.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newton.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newton_tri.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_nsq_newtoff.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_nsq_newton.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_halffull_newtoff.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_halffull_newton.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_respa.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on_oneside.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_bin_2d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_bin_3d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_ghost_bin_2d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_ghost_bin_3d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_2d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_3d.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_old_2d.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_old_3d.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_2d.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_2d_tri.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_3d.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_3d_tri.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_2d.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_2d_tri.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_3d.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_3d_tri.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_2d.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_2d_tri.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_3d.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_3d_tri.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_all.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_partial.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_template.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_all.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_partial.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_template.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_all.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_partial.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_template.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_all.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_partial.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_template.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/output.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_born.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck_coul_cut.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_cut.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_debye.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_dsf.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_wolf.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_deprecated.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_overlay.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_scaled.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut_coul_cut.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_expand.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_morse.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_nequip.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_soft.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_table.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_yukawa.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zbl.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zero.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/potential_file_reader.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/procmap.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_mars.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_park.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rcb.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_data.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_dump.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_restart.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_native.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_xyz.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_block.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cone.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cylinder.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_deprecated.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_intersect.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_plane.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_prism.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_sphere.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_union.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/replicate.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rerun.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atom_ids.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_mol_ids.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/respa.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/run.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/set.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/special.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/table_file_reader.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tabular_function.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/text_file_reader.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/thermo.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/timer.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tokenizer.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/universe.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/update.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/utils.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/variable.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/lammps/src/variable.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint LAMMPS_NS::Variable::next(int, char**)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/lammps/src/variable.cpp:714:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "         \u001b[01;35m\u001b[Kfread(buf,1,64,fp)\u001b[m\u001b[K;\n",
            "         \u001b[01;35m\u001b[K~~~~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/velocity.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/verlet.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_coeff.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_data.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_dump.cpp.o\u001b[0m\n",
            "[100%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_restart.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX static library liblammps.a\u001b[0m\n",
            "[100%] Built target lammps\n",
            "[100%] \u001b[32mBuilding CXX object CMakeFiles/lmp.dir/content/lammps/src/main.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable lmp\u001b[0m\n",
            "[100%] Built target lmp\n"
          ]
        }
      ],
      "source": [
        "!cd lammps && mkdir -p build && cd build && cmake ../cmake -DCMAKE_PREFIX_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` && make -j4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone nequip repository with the AIMD data file"
      ],
      "metadata": {
        "id": "aG8wqxg43tiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone --depth 1 \"https://github.com/gabriele16/nequip.git\"\n",
        "! cd nequip && git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoYyUGf70c4H",
        "outputId": "0da1d40a-d836-4d67-c51b-f43e3b24f3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nequip'...\n",
            "remote: Enumerating objects: 183, done.\u001b[K\n",
            "remote: Counting objects: 100% (183/183), done.\u001b[K\n",
            "remote: Compressing objects: 100% (177/177), done.\u001b[K\n",
            "remote: Total 183 (delta 6), reused 86 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (183/183), 39.50 MiB | 13.31 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract configurations and forces of AIMD trajectory"
      ],
      "metadata": {
        "id": "FrjXfT6L3S51"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHCh2aC7WfKj",
        "outputId": "0dc0fdd4-c03b-4bb2-b9af-40ac684558d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "._AIMD_data\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n",
            "AIMD_data/\n",
            "AIMD_data/._WATER-frc-10k-1.xyz\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n",
            "AIMD_data/WATER-frc-10k-1.xyz\n",
            "AIMD_data/._celldata.dat\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n",
            "AIMD_data/celldata.dat\n",
            "AIMD_data/._WATER-pos-10k-1.xyz\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n",
            "AIMD_data/WATER-pos-10k-1.xyz\n",
            "celldata.dat  WATER-frc-10k-1.xyz  WATER-pos-10k-1.xyz\n"
          ]
        }
      ],
      "source": [
        "! tar -xzvf  /content/nequip/data/AIMD_data.tar.gz -C /content/nequip/data/\n",
        "! ls /content/nequip/data/AIMD_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J51CA0Bod1Jv",
        "outputId": "665fb165-5c92-4867-fe8b-2666f140ed29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.2 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.6.0-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 76.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 63.9 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=569719a4eb6efcbf727799b18feef246d27e85aa756c594fe67466fff1f63029\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.6.0 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.21\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./nequip\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (1.21.6)\n",
            "Collecting ase\n",
            "  Downloading ase-3.22.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (4.64.0)\n",
            "Requirement already satisfied: torch!=1.9.0,<=1.12,>=1.8 in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (1.10.0)\n",
            "Collecting e3nn<0.6.0,>=0.3.5\n",
            "  Downloading e3nn-0.5.0-py3-none-any.whl (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 11.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (3.13)\n",
            "Collecting torch-runstats>=0.2.0\n",
            "  Downloading torch_runstats-0.2.0-py3-none-any.whl (8.1 kB)\n",
            "Collecting torch-ema>=0.3.0\n",
            "  Downloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
            "Collecting scikit_learn<=1.0.1\n",
            "  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 23.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (4.1.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.4.1)\n",
            "Collecting opt-einsum-fx>=0.1.4\n",
            "  Downloading opt_einsum_fx-0.1.4-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (21.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (3.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn<=1.0.1->nequip==0.5.5) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn<=1.0.1->nequip==0.5.5) (3.1.0)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from ase->nequip==0.5.5) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (1.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1.0->ase->nequip==0.5.5) (1.15.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.2.1)\n",
            "Building wheels for collected packages: nequip\n",
            "  Building wheel for nequip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nequip: filename=nequip-0.5.5-py3-none-any.whl size=138681 sha256=74b26574d486434b07680682e4d9f3a2aa00cdd75e08a2594a03dba0a84da404\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-27vyin07/wheels/a8/8f/18/b30c4402c2d6ab52853310650b85822afe26aaae5f6d00356b\n",
            "Successfully built nequip\n",
            "Installing collected packages: opt-einsum-fx, torch-runstats, torch-ema, scikit-learn, e3nn, ase, nequip\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "Successfully installed ase-3.22.1 e3nn-0.5.0 nequip-0.5.5 opt-einsum-fx-0.1.4 scikit-learn-1.0.1 torch-ema-0.3 torch-runstats-0.2.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7efeab319df0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# install wandb\n",
        "!pip install wandb\n",
        "# install nequip\n",
        "!pip install nequip/\n",
        "# fix colab imports\n",
        "import site\n",
        "site.main()\n",
        "# set to allow anonymous WandB\n",
        "import os\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "import numpy as np\n",
        "from ase.io import read, write\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZixXbCiPcMGg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cc9becc7b8e24b43b56fdb84be6d3f8a"
          ]
        },
        "outputId": "730b2096-e98d-445b-a9ac-d48d86967ebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nglview\n",
            "  Downloading nglview-3.0.3.tar.gz (5.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7 MB 4.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jupyterlab-widgets in /usr/local/lib/python3.7/dist-packages (from nglview) (1.1.0)\n",
            "Requirement already satisfied: ipywidgets>=7 in /usr/local/lib/python3.7/dist-packages (from nglview) (7.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nglview) (1.21.6)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (3.6.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (5.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (5.5.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (5.4.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (4.10.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7->nglview) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7->nglview) (5.3.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (0.8.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7->nglview) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7->nglview) (4.10.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7->nglview) (2.15.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (4.11.4)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (5.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (21.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (4.1.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (3.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7->nglview) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7->nglview) (1.15.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (5.3.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (1.8.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (2.11.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7->nglview) (2.8.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7->nglview) (23.1.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (2.0.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.6.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (5.0.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.5.1)\n",
            "Building wheels for collected packages: nglview\n",
            "  Building wheel for nglview (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nglview: filename=nglview-3.0.3-py3-none-any.whl size=8057551 sha256=f5b3d37220f8654d8bd6eedb7205a8bc85338ce1121430ba8d3c015971ac1a03\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/0c/49/c6f79d8edba8fe89752bf20de2d99040bfa57db0548975c5d5\n",
            "Successfully built nglview\n",
            "Installing collected packages: nglview\n",
            "Successfully installed nglview-3.0.3\n",
            "Enabling notebook extension nglview-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/etc/jupyter/nbconfig/notebook.json\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/etc/jupyter/nbconfig/notebook.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc9becc7b8e24b43b56fdb84be6d3f8a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip3 install nglview\n",
        "!jupyter-nbextension enable nglview --py --sys-prefix\n",
        "import nglview as nv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDJ9Re0arOb0"
      },
      "outputs": [],
      "source": [
        "def MD_reader_xyz(f, data_dir, no_skip = 0):\n",
        "  filename = os.path.join(data_dir, f)\n",
        "  fo = open(filename, 'r')\n",
        "  natoms_str = fo.read().rsplit(' i = ')[0]\n",
        "  natoms = int(natoms_str.split('\\n')[0])\n",
        "  fo.close()  \n",
        "  fo = open(filename, 'r')\n",
        "  samples = fo.read().split(natoms_str)[1:]\n",
        "  steps = []\n",
        "  xyz = []\n",
        "  temperatures = []\n",
        "  energies = []\n",
        "  for sample in samples[::no_skip]:\n",
        "     entries = sample.split('\\n')[:-1]\n",
        "     energies.append(float(entries[0].split(\"=\")[-1]))\n",
        "     temp = np.array([list(map(float, lv.split()[1:])) for lv in entries[1:]])\n",
        "     xyz.append(temp[:,:])\n",
        "  return natoms_str, np.array(xyz), np.array(energies)     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DziQT_zjMKS4"
      },
      "outputs": [],
      "source": [
        "from ase.build import sort\n",
        "def MD_writer_xyz(positions,forces,cell_vec_abc,energies,\n",
        "                  data_dir,f,  conv_frc = 1.0 , conv_ener = 1.0 ):\n",
        "\n",
        "  filename = os.path.join(data_dir, f)\n",
        "  fo = open(filename, 'w')\n",
        "\n",
        "  for it, frame in enumerate(positions):\n",
        "    natoms = len(frame)\n",
        "    fo.write(\"{:5d}\\n\".format(natoms))\n",
        "    fo.write('Lattice=\"{:.5f} 0.0 0.0 0.0 {:.5f} 0.0 0.0 0.0 {:.5f}\" \\\n",
        "    Properties=\"species:S:1:pos:R:3:forces:R:3\" \\\n",
        "    energy={:.10f} pbc=\"T T T\"\\n'.format(cell_vec_abc[0],cell_vec_abc[1],cell_vec_abc[2],energies[it]*conv_ener)    \n",
        "    )\n",
        "    if it%1000 == 0.0:\n",
        "      print(it)\n",
        "    \n",
        "    sorted_frame = sort(frame)\n",
        "    sorted_forces = sort(forces[it])\n",
        "\n",
        "    fo.write(\"\".join(\"{:8s} {:.8f} {:16.8f} {:16.8f}\\\n",
        "     {:16.8f} {:16.8f} {:16.8f}\\n\".format(sorted_frame[iat].symbol,\n",
        "                                          sorted_frame[iat].position[0],\n",
        "                                          sorted_frame[iat].position[1],\n",
        "                                          sorted_frame[iat].position[2],\n",
        "                                          sorted_forces[iat].position[0]*conv_frc,\n",
        "                                          sorted_forces[iat].position[1]*conv_frc,\n",
        "                                          sorted_forces[iat].position[2]*conv_frc)\n",
        "                                          for iat in range(len(frame))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2U5i3IZqLBI",
        "outputId": "789ce1a5-8658-4c23-b138-729a993f8d30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.85, 9.85, 9.85])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "def read_cell(f,data_dir):\n",
        "  filename = os.path.join(data_dir,f)\n",
        "  fo = open(filename,'r')\n",
        "  cell_list_abc = fo.read().split('\\n')[:-1]\n",
        "  cell_vec_abc = np.array([list(map(float, lv.split())) for lv in cell_list_abc]).squeeze()\n",
        "  return(cell_vec_abc)\n",
        "\n",
        "cell_vec_abc = read_cell('celldata.dat',data_dir + '/AIMD_data')\n",
        "cell_vec_abc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read positions, energies, forces from of a 32 water molecules box in .xyz format generated with CP2K using the SCAN functional\n",
        "### The Energy is in Hartree while the forces are in eV/angstrom, therefore we convert also the energy in eV to make it consistent with LAMMPS \"metal\" units."
      ],
      "metadata": {
        "id": "wUcVzREa7XNR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9fcRMYnu5-V"
      },
      "outputs": [],
      "source": [
        "wat_traj = read(data_dir +'/AIMD_data/WATER-pos-10k-1.xyz',index=':')\n",
        "wat_frc = read(data_dir + '/AIMD_data/WATER-frc-10k-1.xyz', index=':')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The reader below is required to get the energies."
      ],
      "metadata": {
        "id": "tFK_vOgL7ul8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT7B4ryYu82t"
      },
      "outputs": [],
      "source": [
        "natoms, positions, energies = MD_reader_xyz('WATER-pos-10k-1.xyz', data_dir + '/AIMD_data/', no_skip=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The writer below is useful to convert 2 separate ase Atom objects of the positions and forces, and np.arrays of energies and cell, to an .extxyz file that can be read by nequip."
      ],
      "metadata": {
        "id": "BivsnMaz8NoG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgmmJAU5l5F2",
        "outputId": "c5f77da3-4f02-409b-c0d9-9a9fd99b1bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n"
          ]
        }
      ],
      "source": [
        "MD_writer_xyz(wat_traj, wat_frc, cell_vec_abc, energies, data_dir + '/AIMD_data/', 'wat_pos_frc-10k.extxyz',conv_frc = 1.0, conv_ener = 27.211399)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPqnt-SAXyvL"
      },
      "source": [
        "### Turn on GPU\n",
        "\n",
        "Make sure Runtime --> Change runtime type is set to GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HDmxkn3z8_m"
      },
      "source": [
        "## 3 Steps: \n",
        "* Train: using a data set, train the neural network 🧠 \n",
        "* Deploy: convert the Python-based model into a stand-alone potential file for fast execution ⚡\n",
        "* Run: run Molecular Dynamics, Monte Carlo, Structural Minimization, ...  with it in LAMMPS 🏃"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OD71eeDz7dA"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/all.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62aEgq6QYFIn"
      },
      "source": [
        "### Train a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELdBzH_8z4_2"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/train.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KuOIippfVfd"
      },
      "source": [
        "Here, we will train a NequIP potential on the following system\n",
        "\n",
        "* Water\n",
        "* sampled at T=300K from AIMD\n",
        "* Using 1000 training configurations\n",
        "* The units of the reference data are in eV and A."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mgoydrJW5lg0",
        "outputId": "ae208779-bb4e-4637-f309-4ecc59fd02b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.5.5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import nequip\n",
        "nequip.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Below is the configuration file used by nequip. We provide the dataset in an .extxyz file."
      ],
      "metadata": {
        "id": "ff4YA7aK1o9d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFCszShRk2RP",
        "outputId": "e36be2dc-724d-466d-f623-8132811bc98c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'root': 'results/water', 'run_name': 'example-run-water', 'seed': 123, 'dataset_seed': 456, 'append': True, 'default_dtype': 'float32', 'allow_tf32': False, 'r_max': 4.0, 'num_layers': 4, 'l_max': 1, 'parity': True, 'num_features': 32, 'nonlinearity_type': 'gate', 'resnet': False, 'nonlinearity_scalars': {'e': 'silu', 'o': 'tanh'}, 'nonlinearity_gates': {'e': 'silu', 'o': 'tanh'}, 'num_basis': 8, 'BesselBasis_trainable': True, 'PolynomialCutoff_p': 6, 'invariant_layers': 2, 'invariant_neurons': 64, 'avg_num_neighbors': 'auto', 'use_sc': True, 'dataset': 'ase', 'dataset_file_name': './nequip/data/AIMD_data/wat_pos_frc-10k.extxyz', 'ase_args': {'format': 'extxyz'}, 'include_keys': ['user_label'], 'key_mapping': {'user_label': 'label0'}, 'chemical_symbols': ['H', 'O'], 'verbose': 'info', 'log_batch_freq': 1, 'log_epoch_freq': 1, 'save_checkpoint_freq': -1, 'save_ema_checkpoint_freq': -1, 'n_train': 100, 'n_val': 50, 'learning_rate': 0.005, 'batch_size': 5, 'validation_batch_size': 10, 'max_epochs': 100000, 'train_val_split': 'random', 'shuffle': True, 'metrics_key': 'validation_loss', 'use_ema': True, 'ema_decay': 0.99, 'ema_use_num_updates': True, 'report_init_validation': True, 'early_stopping_patiences': {'validation_loss': 50}, 'early_stopping_delta': {'validation_loss': 0.005}, 'early_stopping_cumulative_delta': False, 'early_stopping_lower_bounds': {'LR': 1e-05}, 'early_stopping_upper_bounds': {'cumulative_wall': 1e+100}, 'loss_coeffs': {'forces': 1, 'total_energy': [1, 'PerAtomMSELoss']}, 'metrics_components': [['forces', 'mae'], ['forces', 'rmse'], ['forces', 'mae', {'PerSpecies': True, 'report_per_component': False}], ['forces', 'rmse', {'PerSpecies': True, 'report_per_component': False}], ['total_energy', 'mae'], ['total_energy', 'mae', {'PerAtom': True}]], 'optimizer_name': 'Adam', 'optimizer_amsgrad': False, 'optimizer_betas': (0.9, 0.999), 'optimizer_eps': 1e-08, 'optimizer_weight_decay': 0, 'max_gradient_norm': None, 'lr_scheduler_name': 'ReduceLROnPlateau', 'lr_scheduler_patience': 100, 'lr_scheduler_factor': 0.5, 'per_species_rescale_scales_trainable': False, 'per_species_rescale_shifts_trainable': False, 'per_species_rescale_shifts': 'dataset_per_atom_total_energy_mean', 'per_species_rescale_scales': 'dataset_forces_rms', 'global_rescale_shift': None, 'global_rescale_scale': 'dataset_forces_rms', 'global_rescale_shift_trainable': False, 'global_rescale_scale_trainable': False}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "from nequip.utils import Config\n",
        "config = Config.from_file('/content/nequip/configs/my-full-example.yaml')\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukSnt_QD5avu",
        "outputId": "b48c8520-aedd-46a8-85a7-f4ec7fa5d482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch device: cuda\n",
            "Processing dataset...\n",
            "Loaded data: Batch(atomic_numbers=[960000, 1], batch=[960000], cell=[10000, 3, 3], edge_cell_shift=[25556838, 3], edge_index=[2, 25556838], forces=[960000, 3], pbc=[10000, 3], pos=[960000, 3], ptr=[10001], total_energy=[10000, 1])\n",
            "Cached processed data to disk\n",
            "Done!\n",
            "Successfully loaded the data set of type ASEDataset(10000)...\n",
            "Replace string dataset_forces_rms to 0.7736424803733826\n",
            "Replace string dataset_per_atom_total_energy_mean to -156.0919189453125\n",
            "Atomic outputs are scaled by: [H, O: 0.773642], shifted by [H, O: -156.091919].\n",
            "Replace string dataset_forces_rms to 0.7736424803733826\n",
            "Initially outputs are globally scaled by: 0.7736424803733826, total_energy are globally shifted by None.\n",
            "Successfully built the network...\n",
            "Number of weights: 154200\n",
            "! Starting training ...\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      0     1          1.1         1.09       0.0142        0.606        0.807        0.515        0.787        0.651        0.688            1        0.846         8.82       0.0919\n",
            "      0     2         1.02         1.01       0.0135        0.577        0.776        0.491        0.749         0.62        0.662        0.965        0.813         8.61       0.0896\n",
            "      0     3        0.992        0.979       0.0132        0.574        0.765        0.494        0.734        0.614        0.659        0.943        0.801         8.52       0.0888\n",
            "      0     4        0.973         0.96       0.0134        0.564        0.758         0.48        0.733        0.606        0.653        0.933        0.793         8.58       0.0894\n",
            "      0     5        0.969        0.956       0.0131        0.571        0.756        0.491        0.732        0.612         0.65        0.934        0.792          8.5       0.0885\n",
            "\n",
            "\n",
            "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Initial Validation          0    4.157    0.005        0.998       0.0135         1.01        0.578        0.773        0.494        0.747        0.621        0.663        0.956        0.809         8.61       0.0897\n",
            "Wall time: 4.157472654000003\n",
            "! Best model        0    1.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      1     1        0.886        0.873       0.0135         0.55        0.723        0.481        0.687        0.584        0.633        0.875        0.754          8.6       0.0896\n",
            "      1     2         1.02         1.01      0.00641        0.576        0.778        0.498        0.733        0.616        0.664        0.968        0.816         5.93       0.0618\n",
            "      1     3         1.05         1.05      0.00212        0.586        0.792        0.499        0.759        0.629        0.675        0.986         0.83         3.37       0.0351\n",
            "      1     4         1.01         1.01     0.000403        0.573        0.777        0.492        0.735        0.613        0.672        0.954        0.813          1.4       0.0146\n",
            "      1     5        0.962        0.962        5e-05        0.576        0.759        0.499         0.73        0.615        0.655        0.933        0.794        0.481      0.00501\n",
            "      1     6        0.956        0.956     2.61e-05        0.573        0.756        0.491        0.737        0.614        0.647        0.938        0.792        0.331      0.00345\n",
            "      1     7        0.904        0.904     6.72e-05        0.553        0.736        0.472        0.715        0.594        0.632        0.908         0.77         0.53      0.00552\n",
            "      1     8        0.891        0.891     3.63e-05         0.53         0.73        0.457        0.677        0.567        0.626        0.903        0.764        0.414      0.00431\n",
            "      1     9        0.826        0.826     0.000137        0.533        0.703        0.457        0.685        0.571        0.597        0.878        0.738        0.814      0.00848\n",
            "      1    10        0.893        0.893      0.00066        0.558        0.731        0.474        0.726          0.6        0.627        0.903        0.765         1.89       0.0197\n",
            "      1    11        0.769        0.767      0.00131        0.505        0.678        0.435        0.646        0.541        0.581        0.839         0.71         2.69        0.028\n",
            "      1    12        0.654        0.651        0.003        0.474        0.624        0.403        0.615        0.509        0.526        0.785        0.655         4.05       0.0422\n",
            "      1    13        0.596        0.591      0.00545        0.458        0.595        0.394        0.586         0.49         0.51        0.735        0.623         5.47        0.057\n",
            "      1    14         0.59        0.581      0.00866        0.445         0.59        0.381        0.574        0.477          0.5        0.737        0.619          6.9       0.0719\n",
            "      1    15        0.528        0.516       0.0123         0.41        0.556        0.343        0.544        0.444        0.463        0.705        0.584         8.23       0.0857\n",
            "      1    16         0.46        0.445       0.0148        0.394        0.516        0.331         0.52        0.425        0.434         0.65        0.542         9.04       0.0942\n",
            "      1    17        0.346        0.333       0.0131        0.345        0.446        0.303        0.428        0.365        0.388        0.545        0.466         8.49       0.0884\n",
            "      1    18        0.335        0.325      0.00993         0.34        0.441        0.302        0.415        0.359        0.386        0.535         0.46         7.38       0.0769\n",
            "      1    19        0.295        0.293      0.00247         0.33        0.419        0.287        0.416        0.351         0.36        0.516        0.438         3.67       0.0382\n",
            "      1    20        0.273        0.272     0.000177        0.313        0.404        0.279        0.381         0.33        0.355        0.487        0.421        0.966       0.0101\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      1     1        0.298        0.298      0.00064        0.326        0.422        0.281        0.415        0.348        0.357        0.529        0.443         1.84       0.0191\n",
            "      1     2        0.273        0.272     0.000526        0.312        0.404        0.272        0.391        0.332        0.348        0.497        0.422         1.65       0.0172\n",
            "      1     3        0.253        0.253     0.000496        0.304        0.389        0.272        0.368         0.32        0.343        0.467        0.405         1.63       0.0169\n",
            "      1     4        0.264        0.263     0.000613        0.311        0.397        0.274        0.385        0.329        0.344        0.487        0.415          1.8       0.0187\n",
            "      1     5        0.265        0.264     0.000581        0.308        0.398        0.266        0.392        0.329        0.338        0.496        0.417         1.77       0.0184\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               1    7.959    0.005        0.707      0.00473        0.712        0.481        0.651        0.414        0.616        0.515        0.558        0.805        0.681         4.03        0.042\n",
            "! Validation          1    7.959    0.005         0.27     0.000571        0.271        0.312        0.402        0.273         0.39        0.332        0.346        0.495        0.421         1.73       0.0181\n",
            "Wall time: 7.959690197999976\n",
            "! Best model        1    0.271\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      2     1         0.27        0.266      0.00439        0.312        0.399        0.277        0.382         0.33         0.35        0.482        0.416         4.87       0.0507\n",
            "      2     2        0.263        0.254      0.00901        0.299         0.39        0.269        0.358        0.313        0.351        0.457        0.404         7.02       0.0731\n",
            "      2     3        0.212          0.2       0.0126        0.265        0.346         0.23        0.335        0.283        0.297        0.426        0.362         8.32       0.0866\n",
            "      2     4        0.207        0.199       0.0088        0.266        0.345        0.233        0.333        0.283        0.298        0.424        0.361         6.93       0.0722\n",
            "      2     5        0.204        0.202      0.00223        0.268        0.347        0.232        0.338        0.285        0.299        0.428        0.364         3.49       0.0364\n",
            "      2     6        0.161        0.161     0.000149        0.239         0.31         0.21        0.299        0.254        0.269        0.379        0.324        0.697      0.00726\n",
            "      2     7        0.167        0.166     0.000341        0.246        0.315        0.213        0.312        0.263        0.272        0.389         0.33         1.21       0.0126\n",
            "      2     8        0.152        0.152     0.000397         0.23        0.301        0.197        0.295        0.246        0.253         0.38        0.316         1.25        0.013\n",
            "      2     9        0.174        0.173     0.000926        0.246        0.322        0.211        0.317        0.264        0.272        0.404        0.338         2.24       0.0233\n",
            "      2    10        0.154        0.154     0.000708        0.236        0.303        0.204          0.3        0.252         0.26        0.375        0.317         1.93       0.0201\n",
            "      2    11        0.146        0.145      0.00094        0.231        0.295        0.204        0.284        0.244        0.256         0.36        0.308         2.23       0.0232\n",
            "      2    12        0.163        0.162     0.000592        0.246        0.312        0.215        0.309        0.262        0.271         0.38        0.325         1.78       0.0186\n",
            "      2    13        0.142        0.141     0.000745        0.226         0.29        0.195        0.289        0.242        0.245        0.364        0.305         1.99       0.0207\n",
            "      2    14         0.13        0.128      0.00154        0.214        0.277        0.184        0.274        0.229        0.233        0.349        0.291         2.88         0.03\n",
            "      2    15        0.138        0.136        0.002         0.22        0.286        0.194        0.273        0.233        0.247        0.351        0.299          3.3       0.0344\n",
            "      2    16        0.147        0.146      0.00121        0.224        0.295        0.189        0.292        0.241        0.242         0.38        0.311         2.54       0.0264\n",
            "      2    17        0.137        0.136     0.000426        0.222        0.286        0.194        0.279        0.236        0.244        0.355          0.3         1.49       0.0155\n",
            "      2    18        0.137        0.137     3.26e-05         0.22        0.286         0.19        0.278        0.234        0.244        0.356          0.3        0.353      0.00367\n",
            "      2    19        0.122        0.122     4.68e-05        0.208         0.27        0.182        0.262        0.222        0.233        0.332        0.282        0.402      0.00419\n",
            "      2    20        0.127        0.126     0.000243        0.211        0.275        0.179        0.274        0.227        0.231        0.347        0.289         1.14       0.0119\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      2     1        0.128        0.128        4e-05        0.211        0.277        0.181        0.272        0.226        0.229        0.353        0.291        0.433      0.00451\n",
            "      2     2        0.132        0.132      3.8e-05        0.215        0.281        0.186        0.275         0.23        0.239         0.35        0.294         0.42      0.00437\n",
            "      2     3        0.114        0.114     2.34e-05        0.203        0.261        0.178        0.253        0.216        0.226        0.319        0.272        0.291      0.00303\n",
            "      2     4         0.12         0.12     4.85e-05        0.205        0.268        0.179        0.257        0.218        0.229        0.333        0.281        0.392      0.00408\n",
            "      2     5        0.128        0.127     3.43e-05        0.211        0.276        0.179        0.274        0.227        0.229        0.353        0.291        0.365       0.0038\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               2   11.001    0.005        0.165      0.00237        0.168        0.241        0.314         0.21        0.304        0.257        0.271        0.388        0.329          2.8       0.0292\n",
            "! Validation          2   11.001    0.005        0.124     3.68e-05        0.124        0.209        0.273        0.181        0.266        0.223         0.23        0.342        0.286         0.38      0.00396\n",
            "Wall time: 11.001274817999956\n",
            "! Best model        2    0.124\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      3     1        0.123        0.122      0.00115        0.205        0.271        0.169        0.278        0.223        0.221        0.349        0.285         2.48       0.0258\n",
            "      3     2        0.126        0.124      0.00129        0.209        0.273        0.181        0.265        0.223        0.231        0.341        0.286         2.64       0.0275\n",
            "      3     3        0.128        0.127     0.000888        0.212        0.275        0.186        0.265        0.225        0.239        0.336        0.288         2.16       0.0225\n",
            "      3     4        0.104        0.104     0.000169        0.192        0.249        0.165        0.245        0.205        0.212        0.311        0.261        0.894      0.00931\n",
            "      3     5        0.122        0.122     0.000149        0.204         0.27        0.176        0.261        0.218        0.233        0.332        0.283        0.865      0.00901\n",
            "      3     6       0.0961       0.0959     0.000122        0.184         0.24        0.157        0.238        0.198        0.199        0.305        0.252        0.736      0.00766\n",
            "      3     7        0.104        0.103     0.000291        0.193        0.249        0.166        0.246        0.206        0.211        0.311        0.261         1.15        0.012\n",
            "      3     8        0.114        0.114     0.000185        0.199        0.261        0.171        0.256        0.214        0.221        0.325        0.273        0.971       0.0101\n",
            "      3     9        0.109        0.109     4.64e-05        0.196        0.255        0.169        0.252         0.21         0.22        0.314        0.267        0.332      0.00346\n",
            "      3    10       0.0941       0.0939     0.000193        0.184        0.237        0.162        0.228        0.195        0.205         0.29        0.248            1       0.0105\n",
            "      3    11        0.106        0.106     0.000142        0.189        0.252        0.164        0.239        0.201        0.213        0.314        0.264        0.786      0.00818\n",
            "      3    12       0.0905       0.0905     1.81e-05        0.181        0.233        0.157        0.228        0.193        0.199        0.289        0.244        0.253      0.00263\n",
            "      3    13        0.102        0.102     4.58e-05        0.193        0.247        0.169         0.24        0.204        0.214        0.302        0.258        0.488      0.00508\n",
            "      3    14       0.0898       0.0896     0.000192         0.18        0.232         0.16        0.221        0.191        0.205        0.277        0.241        0.888      0.00925\n",
            "      3    15       0.0931        0.093     3.24e-05        0.185        0.236        0.159        0.236        0.197        0.201        0.294        0.247         0.39      0.00406\n",
            "      3    16       0.0853       0.0851     0.000143        0.174        0.226        0.146         0.23        0.188        0.188        0.287        0.237        0.842      0.00877\n",
            "      3    17       0.0794       0.0791     0.000293        0.169        0.218        0.149         0.21        0.179        0.194        0.259        0.226         1.21       0.0126\n",
            "      3    18       0.0847       0.0844     0.000328        0.174        0.225        0.149        0.226        0.187        0.191         0.28        0.236         1.26       0.0131\n",
            "      3    19        0.098        0.098     3.49e-05        0.187        0.242        0.161        0.238          0.2        0.206        0.302        0.254         0.38      0.00396\n",
            "      3    20       0.0859       0.0858     8.77e-05        0.175        0.227         0.15        0.226        0.188        0.193        0.282        0.238         0.65      0.00677\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      3     1       0.0955       0.0955     2.49e-05        0.182        0.239        0.155        0.234        0.195        0.199        0.304        0.251        0.297      0.00309\n",
            "      3     2       0.0993       0.0992     3.77e-05        0.186        0.244        0.161        0.238        0.199         0.21          0.3        0.255        0.397      0.00413\n",
            "      3     3       0.0819       0.0819     2.86e-05        0.171        0.221        0.151        0.212        0.182        0.194        0.268        0.231        0.308      0.00321\n",
            "      3     4       0.0904       0.0904     1.67e-05        0.177        0.233        0.154        0.224        0.189        0.199        0.289        0.244        0.283      0.00294\n",
            "      3     5       0.0939       0.0939     1.96e-05        0.181        0.237        0.154        0.235        0.195        0.198        0.301        0.249        0.247      0.00257\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               3   14.050    0.005        0.101      0.00029        0.102        0.189        0.246        0.163        0.241        0.202         0.21        0.306        0.258         1.02       0.0106\n",
            "! Validation          3   14.050    0.005       0.0922     2.55e-05       0.0922         0.18        0.235        0.155        0.228        0.192          0.2        0.292        0.246        0.306      0.00319\n",
            "Wall time: 14.05102642899999\n",
            "! Best model        3    0.092\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      4     1       0.0771        0.077     4.63e-05        0.165        0.215        0.142        0.211        0.177        0.183        0.268        0.225        0.425      0.00443\n",
            "      4     2       0.0834       0.0834     5.63e-05        0.175        0.223         0.15        0.227        0.188        0.187        0.282        0.235        0.494      0.00515\n",
            "      4     3       0.0764       0.0763     8.15e-05        0.169        0.214        0.146        0.215        0.181        0.183        0.265        0.224         0.61      0.00635\n",
            "      4     4       0.0777       0.0775      0.00013        0.166        0.215        0.148        0.202        0.175        0.195        0.252        0.223        0.794      0.00827\n",
            "      4     5       0.0776       0.0775     9.64e-05        0.166        0.215        0.144        0.208        0.176        0.184        0.267        0.226        0.689      0.00718\n",
            "      4     6       0.0768       0.0762     0.000655        0.163        0.214        0.142        0.205        0.174        0.183        0.264        0.223         1.89       0.0197\n",
            "      4     7       0.0787       0.0782     0.000551        0.166        0.216        0.144        0.211        0.177        0.185        0.268        0.226         1.72       0.0179\n",
            "      4     8       0.0756       0.0756     3.21e-05        0.165        0.213        0.143        0.209        0.176        0.183        0.262        0.223        0.361      0.00376\n",
            "      4     9        0.074       0.0738     0.000186        0.161         0.21        0.138        0.207        0.172        0.177        0.265        0.221        0.938      0.00977\n",
            "      4    10       0.0778       0.0773     0.000588        0.166        0.215        0.142        0.215        0.178        0.185        0.265        0.225         1.77       0.0184\n",
            "      4    11       0.0726       0.0725     7.85e-05        0.159        0.208        0.135        0.207        0.171        0.175        0.263        0.219        0.616      0.00641\n",
            "      4    12       0.0609       0.0609     6.39e-05        0.148        0.191        0.128        0.189        0.158        0.164        0.235          0.2        0.544      0.00566\n",
            "      4    13       0.0593       0.0592     6.27e-05        0.146        0.188        0.127        0.185        0.156        0.162        0.231        0.197        0.562      0.00586\n",
            "      4    14       0.0751       0.0751      9.9e-06        0.163        0.212        0.141        0.207        0.174        0.182        0.262        0.222        0.198      0.00206\n",
            "      4    15       0.0636       0.0634     0.000251        0.152        0.195        0.131        0.195        0.163        0.168         0.24        0.204         1.16       0.0121\n",
            "      4    16        0.078       0.0776     0.000349        0.163        0.216        0.142        0.206        0.174        0.185        0.267        0.226         1.32       0.0138\n",
            "      4    17        0.053        0.053     5.16e-05        0.137        0.178         0.12         0.17        0.145        0.158        0.213        0.185        0.434      0.00452\n",
            "      4    18        0.065       0.0648     0.000272        0.151        0.197         0.13        0.195        0.162        0.168        0.244        0.206         1.17       0.0122\n",
            "      4    19       0.0543       0.0542     0.000137         0.14         0.18        0.124        0.172        0.148        0.158        0.218        0.188        0.862      0.00898\n",
            "      4    20       0.0703       0.0702     9.13e-05         0.16        0.205         0.14        0.201         0.17         0.18        0.248        0.214        0.627      0.00653\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      4     1       0.0703       0.0703     1.69e-05        0.156        0.205        0.135          0.2        0.167        0.174        0.257        0.215        0.258      0.00269\n",
            "      4     2       0.0727       0.0727     2.64e-05        0.159        0.209         0.14        0.199        0.169        0.184        0.251        0.217        0.313      0.00326\n",
            "      4     3       0.0591       0.0591      1.3e-05        0.146        0.188         0.13        0.178        0.154        0.166        0.225        0.196        0.228      0.00238\n",
            "      4     4       0.0644       0.0644     1.16e-05        0.151        0.196        0.131        0.191        0.161        0.168        0.243        0.206        0.206      0.00214\n",
            "      4     5       0.0637       0.0637     1.15e-05        0.149        0.195        0.127        0.192         0.16        0.167        0.243        0.205        0.193      0.00201\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               4   17.124    0.005       0.0712     0.000189       0.0714        0.159        0.206        0.138        0.202         0.17        0.178        0.254        0.216        0.859      0.00895\n",
            "! Validation          4   17.124    0.005        0.066     1.59e-05        0.066        0.152        0.199        0.132        0.192        0.162        0.172        0.244        0.208         0.24       0.0025\n",
            "Wall time: 17.124629892999906\n",
            "! Best model        4    0.066\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      5     1        0.066       0.0657     0.000242        0.151        0.198        0.133        0.186        0.159        0.174         0.24        0.207          1.1       0.0114\n",
            "      5     2        0.068        0.068     5.96e-05        0.155        0.202        0.136        0.193        0.164        0.175        0.246        0.211        0.523      0.00545\n",
            "      5     3       0.0584       0.0581     0.000315        0.142        0.186        0.122         0.18        0.151        0.161         0.23        0.195         1.24       0.0129\n",
            "      5     4        0.055       0.0545     0.000554        0.139        0.181         0.12        0.177        0.148        0.154        0.224        0.189         1.73        0.018\n",
            "      5     5       0.0494       0.0493      0.00014        0.133        0.172        0.117        0.166        0.141        0.149        0.209        0.179        0.838      0.00873\n",
            "      5     6       0.0516       0.0515     8.59e-05        0.136        0.176        0.118        0.173        0.146        0.153        0.214        0.183         0.62      0.00646\n",
            "      5     7       0.0525       0.0522     0.000235        0.138        0.177        0.119        0.178        0.148        0.149        0.222        0.186         1.11       0.0115\n",
            "      5     8       0.0561       0.0557     0.000399        0.142        0.183        0.125        0.175         0.15        0.161         0.22         0.19         1.47       0.0153\n",
            "      5     9       0.0497       0.0496     1.24e-05        0.133        0.172        0.116        0.166        0.141        0.152        0.208         0.18        0.227      0.00236\n",
            "      5    10       0.0543       0.0541     0.000178         0.14         0.18        0.124        0.171        0.148        0.159        0.216        0.187        0.966       0.0101\n",
            "      5    11       0.0636       0.0634     0.000141         0.15        0.195         0.13        0.189        0.159        0.169        0.239        0.204        0.871      0.00908\n",
            "      5    12       0.0526       0.0526     8.21e-06        0.138        0.177        0.122        0.171        0.146        0.157        0.213        0.185        0.161      0.00167\n",
            "      5    13       0.0531       0.0529      0.00026        0.137        0.178        0.122        0.167        0.145        0.158        0.212        0.185         1.17       0.0122\n",
            "      5    14       0.0441       0.0438      0.00033        0.127        0.162        0.112        0.158        0.135        0.142        0.196        0.169         1.35        0.014\n",
            "      5    15       0.0456       0.0454     0.000245        0.127        0.165        0.111         0.16        0.136        0.142        0.203        0.172         1.16       0.0121\n",
            "      5    16       0.0502       0.0502     7.97e-06        0.135        0.173        0.123        0.158        0.141         0.16        0.198        0.179        0.161      0.00168\n",
            "      5    17       0.0464       0.0457     0.000676        0.126        0.165        0.108        0.162        0.135        0.137        0.211        0.174         1.92         0.02\n",
            "      5    18       0.0435       0.0423      0.00118        0.122        0.159        0.108        0.149        0.129        0.142        0.188        0.165         2.54       0.0265\n",
            "      5    19       0.0496       0.0493     0.000295        0.134        0.172        0.117        0.169        0.143        0.151        0.208        0.179         1.26       0.0131\n",
            "      5    20       0.0523       0.0523     1.45e-05        0.135        0.177        0.121        0.163        0.142        0.157        0.211        0.184        0.277      0.00289\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      5     1       0.0568       0.0568     2.64e-05        0.141        0.184        0.123        0.177         0.15        0.159        0.227        0.193        0.315      0.00328\n",
            "      5     2        0.058       0.0579     3.86e-05        0.143        0.186        0.127        0.175        0.151        0.167         0.22        0.193        0.408      0.00425\n",
            "      5     3       0.0471       0.0471     5.01e-05         0.13        0.168        0.117        0.156        0.136        0.151        0.198        0.174        0.469      0.00488\n",
            "      5     4        0.051        0.051     3.32e-05        0.135        0.175        0.117         0.17        0.144        0.151        0.214        0.183         0.37      0.00385\n",
            "      5     5       0.0494       0.0494     4.73e-05        0.132        0.172        0.114        0.167        0.141        0.149         0.21         0.18        0.458      0.00477\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               5   20.172    0.005       0.0528     0.000269       0.0531        0.137        0.178         0.12        0.171        0.145        0.155        0.216        0.186         1.03       0.0108\n",
            "! Validation          5   20.172    0.005       0.0524     3.91e-05       0.0525        0.136        0.177        0.119        0.169        0.144        0.155        0.214        0.185        0.404      0.00421\n",
            "Wall time: 20.172223413999973\n",
            "! Best model        5    0.052\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      6     1       0.0443       0.0442     0.000107        0.125        0.163        0.108         0.16        0.134         0.14        0.201         0.17        0.749      0.00781\n",
            "      6     2       0.0422        0.042     0.000278        0.124        0.158        0.109        0.152        0.131        0.138        0.193        0.165         1.24       0.0129\n",
            "      6     3       0.0457       0.0455     0.000197        0.128        0.165        0.115        0.155        0.135        0.149        0.192        0.171         1.02       0.0107\n",
            "      6     4       0.0522       0.0522     8.15e-06        0.136        0.177        0.121        0.164        0.143        0.157        0.211        0.184        0.208      0.00216\n",
            "      6     5       0.0449       0.0448     7.63e-05        0.127        0.164        0.113        0.156        0.135        0.144        0.197        0.171        0.577      0.00601\n",
            "      6     6       0.0492       0.0492     9.64e-06         0.13        0.172        0.115         0.16        0.137        0.153        0.203        0.178        0.211       0.0022\n",
            "      6     7       0.0431       0.0431     1.13e-05        0.126        0.161        0.113        0.151        0.132        0.143        0.191        0.167        0.225      0.00235\n",
            "      6     8       0.0443        0.044     0.000308        0.125        0.162        0.108        0.157        0.133        0.139          0.2         0.17         1.27       0.0133\n",
            "      6     9       0.0378       0.0377      6.5e-05        0.118         0.15        0.103        0.148        0.126        0.132        0.181        0.157        0.596       0.0062\n",
            "      6    10       0.0475       0.0474     1.03e-05        0.129        0.169        0.112        0.162        0.137        0.147        0.205        0.176        0.183       0.0019\n",
            "      6    11       0.0531        0.053     5.87e-05        0.136        0.178        0.119        0.169        0.144        0.157        0.215        0.186        0.453      0.00472\n",
            "      6    12       0.0462       0.0458     0.000343        0.129        0.166        0.115        0.157        0.136        0.146        0.199        0.173         1.36       0.0142\n",
            "      6    13       0.0472       0.0468      0.00033         0.13        0.167        0.115        0.158        0.137        0.149          0.2        0.174         1.29       0.0134\n",
            "      6    14        0.045        0.045     2.34e-05        0.128        0.164        0.114        0.156        0.135        0.147        0.193         0.17         0.32      0.00333\n",
            "      6    15       0.0397       0.0397     9.33e-06        0.118        0.154        0.104        0.146        0.125        0.133        0.189        0.161        0.189      0.00197\n",
            "      6    16       0.0443       0.0442     7.85e-05        0.126        0.163        0.111        0.156        0.134        0.142        0.197         0.17        0.647      0.00674\n",
            "      6    17       0.0428       0.0425     0.000282        0.122         0.16        0.112        0.144        0.128        0.147        0.182        0.164         1.24       0.0129\n",
            "      6    18       0.0357       0.0357     1.28e-05        0.115        0.146        0.102        0.141        0.121        0.129        0.175        0.152        0.229      0.00239\n",
            "      6    19       0.0489       0.0487     0.000193        0.131        0.171        0.116        0.161        0.139        0.151        0.205        0.178         1.01       0.0106\n",
            "      6    20       0.0429       0.0429     1.11e-05        0.125         0.16        0.111        0.153        0.132        0.143         0.19        0.167        0.214      0.00223\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      6     1       0.0488       0.0488     7.97e-06        0.131        0.171        0.115        0.163        0.139        0.149        0.208        0.179        0.174      0.00181\n",
            "      6     2        0.049        0.049      1.6e-05        0.132        0.171        0.118         0.16        0.139        0.154        0.202        0.178        0.247      0.00257\n",
            "      6     3       0.0404       0.0404     1.72e-05        0.121        0.156        0.109        0.144        0.127         0.14        0.182        0.161        0.265      0.00276\n",
            "      6     4       0.0433       0.0433     1.15e-05        0.125        0.161        0.108        0.157        0.133         0.14        0.197        0.168        0.189      0.00197\n",
            "      6     5        0.042        0.042     1.91e-05        0.121        0.158        0.106        0.152        0.129        0.139        0.191        0.165        0.269       0.0028\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               6   23.230    0.005       0.0447     0.000121       0.0449        0.126        0.164        0.112        0.155        0.134        0.145        0.196         0.17        0.662      0.00689\n",
            "! Validation          6   23.230    0.005       0.0447     1.44e-05       0.0447        0.126        0.164        0.111        0.155        0.133        0.144        0.196         0.17        0.229      0.00238\n",
            "Wall time: 23.230524705999983\n",
            "! Best model        6    0.045\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      7     1       0.0443       0.0442     0.000102        0.125        0.163        0.112        0.152        0.132        0.146        0.192        0.169        0.735      0.00765\n",
            "      7     2       0.0428       0.0428     5.49e-05        0.123         0.16        0.107        0.156        0.131        0.139        0.196        0.167        0.526      0.00548\n",
            "      7     3       0.0454       0.0453     0.000154        0.127        0.165         0.11        0.161        0.135        0.141        0.204        0.172        0.918      0.00956\n",
            "      7     4       0.0437       0.0437     5.96e-05        0.125        0.162        0.115        0.146         0.13        0.147        0.187        0.167        0.521      0.00543\n",
            "      7     5       0.0417       0.0414     0.000277         0.12        0.157        0.108        0.145        0.127        0.143        0.183        0.163         1.19       0.0124\n",
            "      7     6       0.0383       0.0377     0.000555        0.118         0.15        0.105        0.142        0.124        0.136        0.176        0.156         1.75       0.0182\n",
            "      7     7       0.0407       0.0406     2.57e-05        0.121        0.156        0.109        0.145        0.127        0.138        0.186        0.162        0.341      0.00356\n",
            "      7     8       0.0394       0.0393     5.59e-06        0.118        0.153        0.105        0.143        0.124        0.137        0.182         0.16        0.132      0.00138\n",
            "      7     9       0.0413       0.0413     1.75e-05         0.12        0.157        0.109        0.143        0.126        0.139        0.188        0.164        0.264      0.00275\n",
            "      7    10       0.0417       0.0415     0.000165        0.122        0.158        0.106        0.154         0.13        0.135        0.195        0.165        0.926      0.00964\n",
            "      7    11       0.0487       0.0484     0.000287        0.131         0.17         0.12        0.153        0.137        0.159        0.191        0.175         1.25       0.0131\n",
            "      7    12       0.0407       0.0407     2.24e-05         0.12        0.156        0.107        0.147        0.127         0.14        0.185        0.162        0.314      0.00327\n",
            "      7    13       0.0411       0.0408     0.000251         0.12        0.156        0.109        0.143        0.126        0.142        0.182        0.162         1.17       0.0122\n",
            "      7    14       0.0446       0.0446     6.71e-06        0.127        0.163        0.114        0.154        0.134        0.146        0.193         0.17        0.155      0.00162\n",
            "      7    15       0.0349       0.0349     5.35e-05        0.112        0.144       0.0978         0.14        0.119        0.125        0.177        0.151        0.528       0.0055\n",
            "      7    16       0.0436       0.0436     7.99e-06        0.124        0.162        0.115        0.143        0.129        0.151         0.18        0.166        0.168      0.00175\n",
            "      7    17       0.0332       0.0331     4.79e-05        0.109        0.141       0.0959        0.135        0.115        0.122        0.172        0.147        0.473      0.00493\n",
            "      7    18       0.0403       0.0402     6.03e-05        0.121        0.155         0.11        0.142        0.126         0.14        0.181        0.161        0.563      0.00587\n",
            "      7    19       0.0379       0.0378     0.000112        0.116         0.15        0.101        0.145        0.123        0.131        0.183        0.157        0.776      0.00808\n",
            "      7    20       0.0397       0.0395     0.000183        0.118        0.154        0.104        0.146        0.125        0.134        0.187         0.16        0.972       0.0101\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      7     1       0.0437       0.0437     5.73e-06        0.124        0.162        0.109        0.155        0.132        0.141        0.197        0.169        0.154       0.0016\n",
            "      7     2        0.043        0.043     1.13e-05        0.124         0.16         0.11        0.151        0.131        0.143         0.19        0.167        0.199      0.00207\n",
            "      7     3       0.0362       0.0362     1.29e-05        0.114        0.147        0.103        0.138         0.12        0.132        0.174        0.153        0.233      0.00243\n",
            "      7     4       0.0385       0.0385     8.17e-06        0.117        0.152        0.102        0.148        0.125        0.132        0.185        0.159        0.155      0.00162\n",
            "      7     5       0.0376       0.0376     1.46e-05        0.115         0.15        0.101        0.143        0.122        0.132        0.181        0.156        0.234      0.00244\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               7   26.284    0.005       0.0411     0.000122       0.0412        0.121        0.157        0.108        0.147        0.127         0.14        0.186        0.163        0.683      0.00712\n",
            "! Validation          7   26.284    0.005       0.0398     1.05e-05       0.0398        0.119        0.154        0.105        0.147        0.126        0.136        0.185        0.161        0.195      0.00203\n",
            "Wall time: 26.285069922999924\n",
            "! Best model        7    0.040\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      8     1       0.0302       0.0302     5.42e-07        0.104        0.134       0.0922        0.127         0.11        0.119        0.162         0.14       0.0465     0.000484\n",
            "      8     2       0.0361       0.0361      4.2e-05        0.112        0.147        0.101        0.134        0.117        0.133        0.172        0.152        0.464      0.00484\n",
            "      8     3       0.0391       0.0391     6.04e-05        0.119        0.153        0.108        0.141        0.124        0.141        0.175        0.158        0.508      0.00529\n",
            "      8     4       0.0396       0.0395     5.85e-05         0.12        0.154        0.106        0.147        0.127        0.137        0.183         0.16        0.547       0.0057\n",
            "      8     5       0.0413       0.0412     5.73e-05         0.12        0.157        0.105         0.15        0.128        0.137         0.19        0.164        0.498      0.00518\n",
            "      8     6       0.0398       0.0397     4.99e-05        0.117        0.154        0.105        0.142        0.123        0.136        0.185        0.161        0.462      0.00482\n",
            "      8     7       0.0334       0.0333     0.000188         0.11        0.141        0.095        0.139        0.117        0.122        0.174        0.148         1.01       0.0105\n",
            "      8     8        0.039       0.0389     5.88e-05        0.116        0.153        0.103        0.142        0.122        0.134        0.184        0.159        0.525      0.00547\n",
            "      8     9       0.0291        0.029     7.03e-05        0.103        0.132       0.0926        0.125        0.109        0.117        0.158        0.137        0.619      0.00645\n",
            "      8    10       0.0376       0.0376     8.02e-06        0.116         0.15        0.104        0.141        0.122        0.134        0.178        0.156         0.12      0.00125\n",
            "      8    11       0.0376       0.0375     0.000189        0.116         0.15        0.103        0.144        0.123        0.131        0.181        0.156        0.998       0.0104\n",
            "      8    12       0.0327       0.0324     0.000348        0.108        0.139       0.0938        0.135        0.114         0.12        0.171        0.146         1.38       0.0144\n",
            "      8    13       0.0296       0.0295     0.000109        0.104        0.133       0.0933        0.124        0.109        0.122        0.153        0.137        0.753      0.00784\n",
            "      8    14       0.0333       0.0333     1.16e-05        0.109        0.141       0.0955        0.135        0.115        0.124         0.17        0.147        0.229      0.00239\n",
            "      8    15       0.0361        0.036     0.000134        0.112        0.147       0.0994        0.136        0.118        0.132        0.173        0.152        0.848      0.00884\n",
            "      8    16       0.0339       0.0336     0.000289         0.11        0.142       0.0984        0.134        0.116        0.128        0.166        0.147         1.25       0.0131\n",
            "      8    17       0.0305       0.0305     4.08e-05        0.106        0.135        0.092        0.133        0.113        0.117        0.166        0.141        0.411      0.00428\n",
            "      8    18       0.0375       0.0374     0.000107        0.115         0.15       0.0992        0.146        0.122        0.129        0.184        0.156        0.763      0.00795\n",
            "      8    19       0.0355       0.0354     9.46e-05        0.112        0.146       0.0989        0.139        0.119        0.129        0.175        0.152        0.704      0.00733\n",
            "      8    20       0.0295       0.0295     4.87e-06        0.104        0.133       0.0906        0.131        0.111        0.115        0.162        0.139         0.13      0.00136\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      8     1       0.0394       0.0394     4.63e-06        0.118        0.154        0.103        0.147        0.125        0.134        0.186         0.16        0.135       0.0014\n",
            "      8     2       0.0382       0.0382     8.09e-06        0.117        0.151        0.104        0.144        0.124        0.135         0.18        0.157        0.162      0.00169\n",
            "      8     3       0.0328       0.0327     9.26e-06        0.109         0.14       0.0975        0.132        0.115        0.125        0.166        0.146          0.2      0.00209\n",
            "      8     4       0.0347       0.0347     5.65e-06        0.111        0.144       0.0969         0.14        0.118        0.126        0.175         0.15        0.117      0.00122\n",
            "      8     5        0.034        0.034     1.02e-05         0.11        0.143       0.0963        0.136        0.116        0.126        0.171        0.149        0.187      0.00195\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               8   29.331    0.005        0.035      9.6e-05       0.0351        0.112        0.145       0.0988        0.137        0.118        0.128        0.173        0.151        0.613      0.00639\n",
            "! Validation          8   29.331    0.005       0.0358     7.57e-06       0.0358        0.113        0.146       0.0997         0.14         0.12        0.129        0.176        0.153         0.16      0.00167\n",
            "Wall time: 29.331530805999932\n",
            "! Best model        8    0.036\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      9     1       0.0371        0.037     0.000153        0.113        0.149          0.1        0.139         0.12        0.131        0.179        0.155        0.885      0.00922\n",
            "      9     2       0.0337       0.0332     0.000565        0.109        0.141       0.0963        0.134        0.115        0.123        0.171        0.147         1.76       0.0183\n",
            "      9     3        0.031       0.0309     0.000145        0.106        0.136       0.0949        0.129        0.112        0.121        0.162        0.141        0.865      0.00901\n",
            "      9     4        0.034       0.0338      0.00021        0.109        0.142          0.1        0.127        0.114        0.132        0.161        0.146         1.07       0.0111\n",
            "      9     5       0.0257       0.0253      0.00039       0.0966        0.123       0.0859        0.118        0.102        0.111        0.145        0.128         1.46       0.0152\n",
            "      9     6       0.0362       0.0361     0.000175        0.114        0.147        0.103        0.138         0.12        0.132        0.173        0.152        0.958      0.00998\n",
            "      9     7       0.0319       0.0318     9.99e-05        0.107        0.138       0.0944        0.132        0.113        0.122        0.166        0.144        0.735      0.00766\n",
            "      9     8       0.0346       0.0346     4.72e-06        0.112        0.144       0.0976        0.141        0.119        0.126        0.175         0.15        0.133      0.00139\n",
            "      9     9       0.0312        0.031     0.000217        0.104        0.136       0.0928        0.128         0.11        0.121        0.162        0.142         1.08       0.0112\n",
            "      9    10       0.0322       0.0322     1.41e-05        0.108        0.139       0.0936        0.136        0.115        0.119        0.171        0.145         0.24       0.0025\n",
            "      9    11        0.033        0.033     1.39e-05        0.109        0.141        0.097        0.133        0.115        0.123         0.17        0.147        0.254      0.00264\n",
            "      9    12       0.0354       0.0353     4.79e-05        0.113        0.145          0.1        0.138        0.119        0.129        0.174        0.151        0.504      0.00525\n",
            "      9    13       0.0301       0.0299     0.000234        0.103        0.134       0.0907        0.126        0.109        0.117        0.163         0.14         1.12       0.0116\n",
            "      9    14       0.0348       0.0348     2.57e-05        0.113        0.144        0.104        0.131        0.118        0.132        0.166        0.149         0.35      0.00364\n",
            "      9    15       0.0305       0.0305     6.23e-06        0.104        0.135       0.0932        0.127         0.11        0.119        0.162        0.141        0.144       0.0015\n",
            "      9    16       0.0369       0.0368     4.77e-05        0.114        0.148       0.0991        0.143        0.121        0.129        0.181        0.155        0.505      0.00526\n",
            "      9    17       0.0306       0.0302     0.000394        0.105        0.134       0.0944        0.125         0.11        0.121        0.157        0.139         1.47       0.0153\n",
            "      9    18       0.0337       0.0336     0.000176        0.109        0.142       0.0963        0.135        0.115        0.125        0.171        0.148        0.974       0.0101\n",
            "      9    19       0.0325       0.0325     1.02e-05        0.106        0.139       0.0962        0.126        0.111        0.128         0.16        0.144        0.185      0.00193\n",
            "      9    20       0.0244       0.0244     5.24e-05       0.0944        0.121       0.0813        0.121        0.101        0.103         0.15        0.127        0.529      0.00551\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      9     1       0.0357       0.0357     5.31e-06        0.112        0.146       0.0984         0.14        0.119        0.128        0.178        0.153         0.14      0.00146\n",
            "      9     2       0.0344       0.0344     6.47e-06        0.111        0.143       0.0985        0.137        0.118        0.127        0.171        0.149        0.138      0.00143\n",
            "      9     3       0.0296       0.0296     7.28e-06        0.104        0.133       0.0925        0.126        0.109        0.118        0.159        0.139        0.172       0.0018\n",
            "      9     4       0.0313       0.0313     4.65e-06        0.105        0.137       0.0922        0.132        0.112         0.12        0.166        0.143        0.115      0.00119\n",
            "      9     5       0.0308       0.0308     6.52e-06        0.105        0.136       0.0916        0.131        0.111         0.12        0.164        0.142        0.147      0.00153\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               9   32.382    0.005       0.0323     0.000149       0.0325        0.107        0.139       0.0956        0.131        0.113        0.123        0.166        0.145        0.761      0.00792\n",
            "! Validation          9   32.382    0.005       0.0324     6.05e-06       0.0324        0.107        0.139       0.0946        0.133        0.114        0.123        0.168        0.145        0.142      0.00148\n",
            "Wall time: 32.38251785099999\n",
            "! Best model        9    0.032\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     10     1       0.0302       0.0302     1.27e-05        0.104        0.134       0.0914        0.128         0.11        0.121        0.158        0.139        0.236      0.00246\n",
            "     10     2       0.0305       0.0305     2.47e-05        0.105        0.135       0.0935        0.128        0.111        0.121         0.16         0.14        0.329      0.00342\n",
            "     10     3       0.0313       0.0313     1.32e-05        0.104        0.137       0.0908        0.129         0.11         0.12        0.166        0.143        0.214      0.00223\n",
            "     10     4       0.0314       0.0314     8.94e-06        0.105        0.137       0.0924        0.131        0.112        0.121        0.165        0.143         0.19      0.00198\n",
            "     10     5        0.029       0.0289     3.72e-05        0.104        0.132       0.0911        0.129         0.11        0.116        0.158        0.137        0.418      0.00436\n",
            "     10     6       0.0286       0.0285       0.0001        0.101        0.131       0.0918        0.119        0.105        0.118        0.152        0.135        0.739      0.00769\n",
            "     10     7       0.0252       0.0252     4.48e-06       0.0952        0.123       0.0854        0.115          0.1        0.109        0.146        0.128        0.143      0.00149\n",
            "     10     8       0.0291       0.0291     2.76e-05        0.101        0.132       0.0849        0.133        0.109        0.108         0.17        0.139        0.353      0.00367\n",
            "     10     9       0.0265       0.0264      5.7e-05        0.096        0.126       0.0839         0.12        0.102        0.109        0.154        0.131        0.517      0.00539\n",
            "     10    10         0.03       0.0299     7.58e-05        0.102        0.134       0.0901        0.127        0.108        0.119         0.16        0.139        0.627      0.00653\n",
            "     10    11       0.0288       0.0288     1.01e-05       0.0998        0.131       0.0868        0.126        0.106        0.114         0.16        0.137        0.224      0.00233\n",
            "     10    12       0.0254       0.0253     2.68e-05       0.0951        0.123        0.083        0.119        0.101        0.106        0.151        0.129        0.348      0.00363\n",
            "     10    13       0.0268       0.0268      2.5e-05       0.0985        0.127       0.0874        0.121        0.104        0.112        0.152        0.132         0.36      0.00375\n",
            "     10    14        0.029       0.0289     7.74e-06        0.102        0.132       0.0882        0.129        0.109        0.113        0.162        0.138        0.161      0.00168\n",
            "     10    15       0.0253       0.0253     1.88e-05       0.0963        0.123       0.0825        0.124        0.103        0.104        0.153        0.129        0.299      0.00311\n",
            "     10    16       0.0325       0.0324     2.86e-05        0.107        0.139       0.0925        0.135        0.114        0.122        0.169        0.145        0.379      0.00395\n",
            "     10    17        0.024        0.024     1.94e-05       0.0931         0.12       0.0817        0.116       0.0988        0.103        0.148        0.125        0.279       0.0029\n",
            "     10    18       0.0244       0.0241     0.000288       0.0952         0.12       0.0874        0.111       0.0991        0.111        0.136        0.124         1.26       0.0131\n",
            "     10    19       0.0259       0.0257     0.000132       0.0953        0.124       0.0858        0.114          0.1         0.11        0.149        0.129         0.85      0.00885\n",
            "     10    20       0.0259       0.0258     1.14e-05       0.0951        0.124        0.084        0.117        0.101        0.109        0.151         0.13        0.208      0.00216\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     10     1       0.0324       0.0324     4.89e-06        0.107        0.139       0.0934        0.133        0.113        0.121         0.17        0.145        0.132      0.00137\n",
            "     10     2       0.0309       0.0309      5.3e-06        0.105        0.136       0.0928        0.131        0.112         0.12        0.164        0.142        0.135      0.00141\n",
            "     10     3       0.0268       0.0268     6.74e-06       0.0985        0.127       0.0873        0.121        0.104        0.112        0.153        0.132        0.164      0.00171\n",
            "     10     4       0.0283       0.0283     3.92e-06          0.1         0.13       0.0876        0.126        0.107        0.114        0.158        0.136        0.108      0.00112\n",
            "     10     5       0.0279       0.0279     5.58e-06       0.0996        0.129       0.0869        0.125        0.106        0.113        0.156        0.135        0.138      0.00143\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              10   35.413    0.005       0.0279     4.65e-05        0.028       0.0997        0.129       0.0877        0.124        0.106        0.113        0.156        0.135        0.407      0.00424\n",
            "! Validation         10   35.413    0.005       0.0293     5.28e-06       0.0293        0.102        0.132       0.0896        0.127        0.108        0.116         0.16        0.138        0.135      0.00141\n",
            "Wall time: 35.413477685999965\n",
            "! Best model       10    0.029\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     11     1       0.0268       0.0266     0.000116       0.0954        0.126       0.0816        0.123        0.102        0.106        0.159        0.133        0.778       0.0081\n",
            "     11     2       0.0264       0.0263     0.000102       0.0977        0.125       0.0852        0.123        0.104        0.108        0.155        0.131        0.722      0.00752\n",
            "     11     3       0.0248       0.0248     1.56e-05       0.0943        0.122       0.0825        0.118          0.1        0.106        0.148        0.127         0.24       0.0025\n",
            "     11     4       0.0266       0.0266     1.31e-05       0.0978        0.126        0.088        0.117        0.103        0.113        0.149        0.131        0.258      0.00269\n",
            "     11     5       0.0233       0.0233     5.31e-06       0.0916        0.118        0.081        0.113       0.0969        0.105        0.142        0.123         0.14      0.00146\n",
            "     11     6       0.0257       0.0256     0.000103       0.0959        0.124       0.0832        0.122        0.102        0.107        0.152         0.13        0.747      0.00778\n",
            "     11     7       0.0256       0.0255     0.000118       0.0962        0.124        0.085        0.119        0.102         0.11        0.148        0.129        0.795      0.00828\n",
            "     11     8        0.022        0.022      3.1e-05        0.087        0.115       0.0766        0.108       0.0922          0.1         0.14         0.12        0.402      0.00419\n",
            "     11     9       0.0242       0.0238     0.000421       0.0919        0.119       0.0799        0.116       0.0978        0.103        0.147        0.125         1.52       0.0158\n",
            "     11    10       0.0251       0.0246     0.000467       0.0928        0.121       0.0809        0.117       0.0988        0.106        0.147        0.127          1.6       0.0166\n",
            "     11    11        0.023       0.0229     0.000149       0.0916        0.117       0.0815        0.112       0.0967        0.104        0.139        0.122        0.901      0.00938\n",
            "     11    12       0.0225       0.0223     0.000167       0.0897        0.116       0.0778        0.113       0.0956       0.0994        0.142        0.121        0.941      0.00981\n",
            "     11    13       0.0255       0.0249     0.000654       0.0953        0.122       0.0833        0.119        0.101        0.107        0.147        0.127         1.89       0.0197\n",
            "     11    14       0.0276       0.0274     0.000138       0.0978        0.128       0.0846        0.124        0.104        0.111        0.156        0.134        0.857      0.00893\n",
            "     11    15       0.0231       0.0231     2.26e-05       0.0898        0.118       0.0807        0.108       0.0943        0.105        0.139        0.122        0.348      0.00363\n",
            "     11    16       0.0278       0.0277     6.41e-05       0.0972        0.129       0.0856         0.12        0.103        0.113        0.155        0.134        0.582      0.00606\n",
            "     11    17       0.0278       0.0277     8.89e-05       0.0969        0.129        0.085        0.121        0.103        0.113        0.156        0.134         0.69      0.00719\n",
            "     11    18       0.0268       0.0267     0.000121       0.0966        0.126       0.0831        0.124        0.103        0.109        0.156        0.132        0.808      0.00842\n",
            "     11    19       0.0237       0.0235     0.000165       0.0916        0.119       0.0794        0.116       0.0977        0.101        0.147        0.124        0.946      0.00986\n",
            "     11    20       0.0211       0.0211     1.03e-05       0.0868        0.112       0.0747        0.111       0.0928       0.0965        0.139        0.118        0.162      0.00169\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     11     1       0.0292       0.0292     6.04e-06        0.101        0.132       0.0883        0.127        0.108        0.115        0.162        0.138        0.141      0.00147\n",
            "     11     2       0.0278       0.0278     5.11e-06       0.0996        0.129       0.0872        0.125        0.106        0.113        0.156        0.135        0.147      0.00153\n",
            "     11     3       0.0242       0.0242     6.05e-06       0.0935         0.12       0.0822        0.116       0.0991        0.105        0.146        0.126        0.155      0.00162\n",
            "     11     4       0.0255       0.0255     4.24e-06        0.095        0.124       0.0829        0.119        0.101        0.108         0.15        0.129        0.125      0.00131\n",
            "     11     5       0.0253       0.0253     4.12e-06       0.0949        0.123       0.0824         0.12        0.101        0.107         0.15        0.129        0.123      0.00128\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              11   38.472    0.005       0.0248     0.000149        0.025       0.0937        0.122        0.082        0.117       0.0996        0.106        0.148        0.127        0.766      0.00798\n",
            "! Validation         11   38.472    0.005       0.0264     5.11e-06       0.0264       0.0968        0.126       0.0846        0.121        0.103         0.11        0.153        0.131        0.138      0.00144\n",
            "Wall time: 38.47289995699998\n",
            "! Best model       11    0.026\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     12     1       0.0248       0.0245     0.000266       0.0951        0.121       0.0815        0.122        0.102        0.103        0.151        0.127         1.21       0.0126\n",
            "     12     2       0.0346       0.0339     0.000629        0.112        0.142        0.102        0.131        0.116         0.13        0.164        0.147         1.85       0.0193\n",
            "     12     3       0.0285       0.0285     2.87e-05        0.101        0.131       0.0908        0.123        0.107        0.117        0.154        0.136        0.385      0.00401\n",
            "     12     4         0.02       0.0199     0.000128        0.085        0.109       0.0748        0.105       0.0901       0.0949        0.133        0.114        0.813      0.00847\n",
            "     12     5       0.0297       0.0293     0.000407        0.102        0.132       0.0882        0.129        0.109        0.113        0.164        0.139         1.49       0.0155\n",
            "     12     6       0.0348       0.0344     0.000388        0.112        0.144        0.105        0.127        0.116        0.132        0.163        0.148         1.46       0.0152\n",
            "     12     7       0.0294       0.0294     4.55e-05        0.104        0.133       0.0935        0.124        0.109        0.119        0.157        0.138        0.463      0.00482\n",
            "     12     8       0.0224       0.0221      0.00021       0.0896        0.115       0.0782        0.112       0.0952       0.0995        0.141         0.12         1.07       0.0112\n",
            "     12     9       0.0334       0.0334     8.96e-06        0.111        0.141        0.105        0.123        0.114        0.135        0.154        0.144        0.178      0.00186\n",
            "     12    10       0.0279       0.0278      3.6e-05          0.1        0.129       0.0855        0.129        0.107        0.109        0.162        0.135        0.434      0.00452\n",
            "     12    11       0.0208       0.0207      0.00012       0.0856        0.111       0.0749        0.107        0.091       0.0957        0.137        0.116        0.806       0.0084\n",
            "     12    12        0.028        0.028     3.35e-05        0.102        0.129       0.0881        0.129        0.108        0.112        0.158        0.135        0.351      0.00366\n",
            "     12    13       0.0273       0.0272     0.000118       0.0967        0.128       0.0826        0.125        0.104        0.108         0.16        0.134        0.772      0.00804\n",
            "     12    14       0.0215       0.0213     0.000151       0.0857        0.113       0.0755        0.106       0.0908       0.0991        0.136        0.118          0.9      0.00938\n",
            "     12    15       0.0217       0.0217     7.95e-05       0.0884        0.114       0.0764        0.112       0.0944       0.0988        0.139        0.119        0.656      0.00683\n",
            "     12    16       0.0207       0.0207     8.14e-06       0.0866        0.111       0.0786        0.103       0.0906        0.101        0.129        0.115        0.207      0.00216\n",
            "     12    17       0.0226       0.0226     7.73e-05       0.0893        0.116       0.0774        0.113       0.0952          0.1        0.143        0.122        0.644      0.00671\n",
            "     12    18       0.0249       0.0247     0.000176       0.0925        0.122       0.0811        0.115       0.0983        0.109        0.144        0.126        0.985       0.0103\n",
            "     12    19       0.0213       0.0213     1.08e-05       0.0873        0.113       0.0753        0.111       0.0933       0.0976        0.139        0.118        0.207      0.00216\n",
            "     12    20       0.0239       0.0239     4.01e-05       0.0911         0.12       0.0802        0.113       0.0965        0.105        0.144        0.125        0.459      0.00478\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     12     1       0.0266       0.0266     7.59e-06       0.0964        0.126       0.0837        0.122        0.103        0.109        0.155        0.132        0.162      0.00169\n",
            "     12     2       0.0254       0.0254     5.37e-06       0.0947        0.123       0.0821         0.12        0.101        0.107        0.151        0.129        0.156      0.00163\n",
            "     12     3       0.0222       0.0221     6.36e-06       0.0891        0.115       0.0777        0.112       0.0948       0.0997        0.141         0.12        0.153      0.00159\n",
            "     12     4       0.0234       0.0234     5.69e-06       0.0908        0.118       0.0788        0.115       0.0967        0.103        0.144        0.123        0.145      0.00151\n",
            "     12     5       0.0231       0.0231     3.59e-06       0.0908        0.118       0.0784        0.116        0.097        0.102        0.145        0.123        0.117      0.00121\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              12   41.530    0.005       0.0258     0.000148       0.0259       0.0958        0.124       0.0847        0.118        0.101         0.11        0.149        0.129        0.767      0.00799\n",
            "! Validation         12   41.530    0.005       0.0241     5.72e-06       0.0241       0.0923         0.12       0.0802        0.117       0.0984        0.104        0.147        0.126        0.147      0.00153\n",
            "Wall time: 41.530981082999915\n",
            "! Best model       12    0.024\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     13     1        0.024        0.024     1.05e-06        0.092         0.12       0.0791        0.118       0.0985        0.104        0.147        0.125       0.0662      0.00069\n",
            "     13     2       0.0206       0.0205     6.28e-05       0.0865        0.111       0.0742        0.111       0.0927       0.0958        0.136        0.116        0.583      0.00607\n",
            "     13     3       0.0239       0.0239      4.2e-05       0.0912         0.12       0.0767         0.12       0.0984        0.101         0.15        0.126        0.453      0.00472\n",
            "     13     4       0.0197       0.0197     8.23e-06       0.0849        0.109       0.0745        0.106       0.0901       0.0942        0.133        0.113        0.197      0.00206\n",
            "     13     5         0.02         0.02     4.93e-05       0.0849        0.109       0.0731        0.108       0.0907        0.093        0.136        0.115        0.512      0.00534\n",
            "     13     6       0.0272        0.027     0.000157          0.1        0.127       0.0921        0.116        0.104        0.117        0.146        0.131        0.919      0.00957\n",
            "     13     7       0.0232       0.0231     5.43e-05       0.0926        0.118       0.0795        0.119       0.0991          0.1        0.146        0.123        0.541      0.00563\n",
            "     13     8       0.0202       0.0202     1.04e-05       0.0837         0.11       0.0731        0.105        0.089       0.0962        0.133        0.115        0.222      0.00231\n",
            "     13     9       0.0249       0.0249     4.55e-05       0.0938        0.122       0.0805         0.12          0.1        0.105        0.151        0.128        0.481      0.00501\n",
            "     13    10       0.0208       0.0207     0.000104       0.0848        0.111       0.0764        0.102       0.0891       0.0999        0.132        0.116        0.744      0.00775\n",
            "     13    11       0.0234       0.0234     1.82e-05       0.0917        0.118       0.0793        0.116       0.0978        0.103        0.145        0.124        0.253      0.00263\n",
            "     13    12       0.0252       0.0251     7.68e-05       0.0958        0.123       0.0856        0.116        0.101         0.11        0.145        0.127        0.649      0.00676\n",
            "     13    13       0.0207       0.0206     5.44e-05       0.0855        0.111       0.0741        0.108       0.0912        0.095        0.138        0.116         0.54      0.00563\n",
            "     13    14       0.0247       0.0247     2.47e-06        0.092        0.122       0.0785        0.119       0.0988        0.106        0.149        0.127        0.114      0.00119\n",
            "     13    15       0.0221       0.0221     6.09e-06       0.0876        0.115       0.0765         0.11       0.0932       0.0989        0.142         0.12        0.178      0.00186\n",
            "     13    16        0.019        0.019     2.32e-05       0.0827        0.107       0.0717        0.105       0.0882        0.092        0.131        0.112        0.291      0.00303\n",
            "     13    17       0.0219       0.0219     1.19e-05        0.087        0.114       0.0743        0.113       0.0934        0.098        0.142         0.12        0.223      0.00233\n",
            "     13    18       0.0218       0.0217     5.08e-05       0.0864        0.114       0.0767        0.106       0.0913       0.0992        0.139        0.119        0.502      0.00522\n",
            "     13    19       0.0203       0.0203      1.2e-05       0.0853         0.11       0.0743        0.108       0.0909        0.095        0.135        0.115        0.227      0.00236\n",
            "     13    20       0.0231        0.023     9.22e-05       0.0905        0.117       0.0792        0.113       0.0961        0.103        0.141        0.122        0.709      0.00738\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     13     1       0.0245       0.0244     9.99e-06       0.0923        0.121       0.0798        0.117       0.0986        0.103         0.15        0.127        0.191      0.00198\n",
            "     13     2       0.0236       0.0236     6.19e-06       0.0908        0.119       0.0781        0.116       0.0971        0.102        0.146        0.124        0.164      0.00171\n",
            "     13     3       0.0205       0.0205     7.17e-06       0.0855        0.111       0.0739        0.109       0.0913       0.0951        0.137        0.116        0.159      0.00166\n",
            "     13     4       0.0216       0.0216     8.15e-06       0.0872        0.114       0.0756         0.11        0.093       0.0987        0.139        0.119        0.175      0.00182\n",
            "     13     5       0.0214       0.0214     4.25e-06       0.0872        0.113       0.0747        0.112       0.0934       0.0967         0.14        0.118        0.124      0.00129\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              13   44.572    0.005       0.0223     4.41e-05       0.0223       0.0889        0.116       0.0775        0.112       0.0947          0.1        0.141        0.121         0.42      0.00438\n",
            "! Validation         13   44.572    0.005       0.0223     7.15e-06       0.0223       0.0886        0.115       0.0764        0.113       0.0947       0.0993        0.142        0.121        0.163      0.00169\n",
            "Wall time: 44.57248224499995\n",
            "! Best model       13    0.022\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     14     1       0.0214       0.0213     8.31e-05       0.0866        0.113        0.075         0.11       0.0924       0.0961        0.141        0.118        0.673      0.00701\n",
            "     14     2       0.0196       0.0195      7.5e-05        0.084        0.108       0.0742        0.103       0.0889       0.0945        0.131        0.113        0.614       0.0064\n",
            "     14     3       0.0184       0.0184     2.38e-06       0.0816        0.105       0.0706        0.104       0.0871       0.0894        0.131         0.11        0.107      0.00112\n",
            "     14     4       0.0242        0.024     0.000163       0.0931         0.12       0.0812        0.117        0.099        0.105        0.146        0.125        0.929      0.00968\n",
            "     14     5       0.0218       0.0216     0.000202       0.0887        0.114       0.0749        0.116       0.0956       0.0963        0.143        0.119         1.03       0.0108\n",
            "     14     6       0.0209       0.0209     6.54e-05       0.0876        0.112       0.0794        0.104       0.0918        0.101        0.131        0.116        0.586      0.00611\n",
            "     14     7       0.0187       0.0187      5.1e-06       0.0814        0.106       0.0689        0.106       0.0876       0.0886        0.134        0.111        0.139      0.00145\n",
            "     14     8       0.0218       0.0217     5.53e-05       0.0878        0.114       0.0782        0.107       0.0926        0.101        0.136        0.119        0.535      0.00557\n",
            "     14     9       0.0254       0.0253     8.46e-05       0.0938        0.123       0.0795        0.122        0.101        0.101        0.158         0.13        0.663      0.00691\n",
            "     14    10       0.0208       0.0208      5.5e-05       0.0839        0.112       0.0725        0.107       0.0896       0.0974        0.135        0.116        0.546      0.00569\n",
            "     14    11       0.0189       0.0189     3.24e-05       0.0817        0.106       0.0696        0.106       0.0878       0.0893        0.134        0.112        0.404      0.00421\n",
            "     14    12       0.0175       0.0175     2.48e-06       0.0795        0.102       0.0693          0.1       0.0847       0.0894        0.124        0.107        0.116       0.0012\n",
            "     14    13       0.0165       0.0164     5.72e-05       0.0758       0.0991        0.065       0.0976       0.0813       0.0852        0.122        0.104        0.556      0.00579\n",
            "     14    14       0.0223       0.0222     3.77e-05       0.0861        0.115       0.0727        0.113       0.0928       0.0982        0.144        0.121         0.45      0.00469\n",
            "     14    15       0.0201       0.0201     1.92e-06       0.0834         0.11       0.0719        0.107       0.0892       0.0941        0.136        0.115       0.0961        0.001\n",
            "     14    16       0.0177       0.0177     1.34e-05       0.0791        0.103       0.0692       0.0988        0.084         0.09        0.125        0.107        0.244      0.00254\n",
            "     14    17       0.0212       0.0212     3.09e-06       0.0867        0.113       0.0739        0.112       0.0932       0.0956        0.141        0.118         0.11      0.00115\n",
            "     14    18       0.0197       0.0197     6.54e-06       0.0837        0.108       0.0729        0.105       0.0891       0.0937        0.133        0.113        0.172      0.00179\n",
            "     14    19       0.0196       0.0195     3.94e-05       0.0821        0.108       0.0703        0.106        0.088       0.0919        0.135        0.113         0.46      0.00479\n",
            "     14    20       0.0169       0.0169     1.92e-05       0.0785        0.101       0.0675          0.1        0.084        0.085        0.126        0.105        0.309      0.00321\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     14     1       0.0228       0.0227     8.61e-06       0.0889        0.117       0.0764        0.114       0.0952       0.0992        0.145        0.122        0.177      0.00184\n",
            "     14     2        0.022        0.022     5.14e-06       0.0875        0.115       0.0747        0.113       0.0939       0.0981        0.142         0.12        0.157      0.00163\n",
            "     14     3       0.0191       0.0191     6.14e-06       0.0825        0.107       0.0708        0.106       0.0884       0.0911        0.133        0.112        0.149      0.00155\n",
            "     14     4       0.0201       0.0201     6.68e-06        0.084         0.11       0.0725        0.107       0.0897       0.0948        0.135        0.115        0.151      0.00157\n",
            "     14     5         0.02         0.02     3.45e-06       0.0842        0.109       0.0717        0.109       0.0904       0.0925        0.137        0.115         0.11      0.00114\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              14   47.623    0.005       0.0201     5.02e-05       0.0202       0.0843         0.11       0.0728        0.107         0.09       0.0943        0.135        0.115        0.437      0.00455\n",
            "! Validation         14   47.623    0.005       0.0208     6.01e-06       0.0208       0.0854        0.112       0.0732         0.11       0.0915       0.0952        0.139        0.117        0.149      0.00155\n",
            "Wall time: 47.6235220719999\n",
            "! Best model       14    0.021\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     15     1       0.0193       0.0193     1.57e-05        0.082        0.108       0.0702        0.106       0.0879       0.0923        0.133        0.113        0.277      0.00288\n",
            "     15     2       0.0182       0.0181     0.000115       0.0801        0.104       0.0689        0.103       0.0857        0.089        0.129        0.109        0.781      0.00814\n",
            "     15     3       0.0173       0.0173     2.25e-05       0.0786        0.102       0.0669        0.102       0.0844       0.0853        0.129        0.107        0.331      0.00345\n",
            "     15     4       0.0201       0.0201     9.32e-06       0.0846         0.11       0.0738        0.106       0.0899       0.0949        0.135        0.115        0.193      0.00201\n",
            "     15     5       0.0197       0.0197     2.29e-05       0.0834        0.109       0.0713        0.108       0.0895       0.0921        0.136        0.114        0.331      0.00345\n",
            "     15     6       0.0181       0.0181      1.6e-06       0.0817        0.104       0.0721        0.101       0.0865       0.0912        0.126        0.109        0.082     0.000854\n",
            "     15     7       0.0202       0.0201     3.25e-06       0.0836         0.11       0.0721        0.107       0.0894       0.0937        0.136        0.115        0.115       0.0012\n",
            "     15     8       0.0194       0.0194     2.63e-06        0.083        0.108       0.0713        0.106       0.0888       0.0915        0.134        0.113        0.117      0.00122\n",
            "     15     9       0.0215       0.0215     1.42e-05       0.0879        0.113       0.0776        0.109        0.093        0.101        0.135        0.118        0.243      0.00253\n",
            "     15    10       0.0215       0.0214     0.000102       0.0866        0.113       0.0772        0.105       0.0912          0.1        0.135        0.118        0.742      0.00773\n",
            "     15    11       0.0197       0.0197     2.53e-05       0.0848        0.109       0.0731        0.108       0.0906       0.0923        0.135        0.114        0.332      0.00346\n",
            "     15    12       0.0221       0.0221     6.36e-05       0.0881        0.115       0.0772         0.11       0.0935          0.1         0.14         0.12        0.585      0.00609\n",
            "     15    13       0.0268       0.0266     0.000139       0.0979        0.126       0.0879        0.118        0.103        0.112         0.15        0.131         0.87      0.00906\n",
            "     15    14       0.0189       0.0187      0.00015       0.0809        0.106       0.0704        0.102       0.0861       0.0903        0.132        0.111        0.906      0.00944\n",
            "     15    15       0.0211       0.0211     1.32e-06       0.0855        0.112       0.0727        0.111       0.0918       0.0966        0.138        0.118       0.0725     0.000755\n",
            "     15    16       0.0218       0.0217     0.000129       0.0904        0.114       0.0811        0.109        0.095        0.101        0.135        0.118        0.826       0.0086\n",
            "     15    17       0.0168       0.0167     8.33e-05       0.0774          0.1       0.0658          0.1       0.0831       0.0853        0.124        0.105         0.67      0.00698\n",
            "     15    18       0.0222       0.0222     8.18e-07       0.0868        0.115       0.0738        0.113       0.0932       0.0982        0.143        0.121       0.0508     0.000529\n",
            "     15    19        0.019        0.019     1.35e-05       0.0833        0.107       0.0748          0.1       0.0875       0.0951        0.126        0.111         0.25      0.00261\n",
            "     15    20       0.0198       0.0198     2.16e-05       0.0816        0.109       0.0675         0.11       0.0887       0.0884        0.141        0.115        0.312      0.00325\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     15     1       0.0213       0.0213     8.34e-06        0.086        0.113       0.0735        0.111       0.0922       0.0954        0.142        0.119        0.176      0.00183\n",
            "     15     2       0.0207       0.0207     4.95e-06       0.0846        0.111       0.0717         0.11        0.091       0.0945        0.139        0.117        0.154      0.00161\n",
            "     15     3       0.0179       0.0179     5.69e-06       0.0799        0.104        0.068        0.104       0.0858       0.0876         0.13        0.109        0.146      0.00152\n",
            "     15     4        0.019        0.019     6.68e-06       0.0814        0.107         0.07        0.104       0.0871       0.0916        0.132        0.112        0.151      0.00157\n",
            "     15     5       0.0188       0.0188     3.45e-06       0.0815        0.106        0.069        0.107       0.0878       0.0889        0.134        0.111        0.109      0.00114\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              15   50.677    0.005       0.0201     4.68e-05       0.0202       0.0844         0.11       0.0733        0.107       0.0899       0.0947        0.135        0.115        0.404      0.00421\n",
            "! Validation         15   50.677    0.005       0.0195     5.82e-06       0.0195       0.0827        0.108       0.0704        0.107       0.0888       0.0916        0.135        0.113        0.147      0.00153\n",
            "Wall time: 50.67758054199999\n",
            "! Best model       15    0.020\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     16     1       0.0185       0.0185     4.55e-06        0.083        0.105       0.0741        0.101       0.0875        0.094        0.125        0.109        0.154      0.00161\n",
            "     16     2       0.0212       0.0212     5.73e-05       0.0863        0.113       0.0728        0.113       0.0931        0.094        0.143        0.118        0.549      0.00572\n",
            "     16     3       0.0212       0.0211     3.23e-06       0.0849        0.113       0.0703        0.114       0.0923       0.0934        0.143        0.118        0.108      0.00113\n",
            "     16     4       0.0186       0.0186     3.24e-05       0.0813        0.106       0.0685        0.107       0.0877       0.0873        0.135        0.111        0.417      0.00434\n",
            "     16     5       0.0181        0.018     3.59e-05       0.0796        0.104       0.0694          0.1       0.0848       0.0891        0.128        0.109        0.428      0.00445\n",
            "     16     6       0.0177       0.0177      2.3e-05       0.0789        0.103       0.0677        0.101       0.0845       0.0871        0.129        0.108        0.323      0.00337\n",
            "     16     7       0.0183       0.0183     2.59e-05       0.0811        0.105       0.0719       0.0995       0.0857       0.0931        0.125        0.109        0.371      0.00386\n",
            "     16     8       0.0207       0.0206     0.000102       0.0858        0.111       0.0741        0.109       0.0917       0.0943        0.139        0.116        0.745      0.00776\n",
            "     16     9       0.0208       0.0208     1.91e-05       0.0837        0.112        0.071        0.109         0.09       0.0955        0.138        0.117        0.271      0.00282\n",
            "     16    10       0.0175       0.0175     9.65e-06       0.0774        0.102        0.065        0.102       0.0836       0.0853         0.13        0.107        0.214      0.00223\n",
            "     16    11        0.024        0.024     8.92e-05       0.0916         0.12       0.0779        0.119       0.0985          0.1        0.152        0.126         0.66      0.00687\n",
            "     16    12       0.0174       0.0174      5.4e-05       0.0781        0.102        0.069       0.0964       0.0827       0.0895        0.123        0.106        0.529      0.00551\n",
            "     16    13        0.017        0.017     1.42e-06       0.0768        0.101       0.0649          0.1       0.0827        0.086        0.126        0.106       0.0719     0.000749\n",
            "     16    14       0.0182       0.0182     4.46e-06        0.079        0.104       0.0675        0.102       0.0848       0.0882        0.131        0.109        0.134      0.00139\n",
            "     16    15       0.0154       0.0154     2.09e-06       0.0738       0.0959       0.0622        0.097       0.0796       0.0812         0.12        0.101       0.0984      0.00103\n",
            "     16    16       0.0166       0.0166      1.7e-06       0.0768       0.0995        0.066       0.0983       0.0822       0.0857        0.123        0.104       0.0744     0.000775\n",
            "     16    17       0.0177       0.0176        4e-06       0.0787        0.103       0.0654        0.105       0.0854       0.0851        0.131        0.108        0.134       0.0014\n",
            "     16    18       0.0157       0.0157     4.69e-06       0.0741       0.0969       0.0622       0.0978         0.08        0.081        0.123        0.102        0.124      0.00129\n",
            "     16    19       0.0139       0.0138     3.47e-05       0.0707        0.091       0.0611       0.0899       0.0755       0.0764        0.115       0.0956        0.429      0.00447\n",
            "     16    20       0.0165       0.0165     7.37e-06       0.0759       0.0994       0.0648       0.0981       0.0815       0.0838        0.125        0.104        0.181      0.00189\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     16     1       0.0202       0.0202     6.94e-06       0.0836         0.11        0.071        0.109       0.0898       0.0923        0.138        0.115        0.159      0.00166\n",
            "     16     2       0.0196       0.0196     4.08e-06       0.0821        0.108       0.0691        0.108       0.0886       0.0913        0.136        0.114        0.142      0.00148\n",
            "     16     3        0.017        0.017     4.86e-06       0.0776        0.101       0.0657        0.101       0.0835       0.0848        0.127        0.106        0.136      0.00141\n",
            "     16     4        0.018        0.018     5.18e-06       0.0793        0.104        0.068        0.102       0.0849       0.0888        0.129        0.109        0.136      0.00142\n",
            "     16     5       0.0178       0.0178     2.85e-06        0.079        0.103       0.0665        0.104       0.0853       0.0858        0.131        0.108        0.105       0.0011\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              16   53.736    0.005       0.0182     2.58e-05       0.0182       0.0799        0.104       0.0683        0.103       0.0857       0.0887         0.13         0.11        0.301      0.00313\n",
            "! Validation         16   53.736    0.005       0.0185     4.78e-06       0.0185       0.0803        0.105       0.0681        0.105       0.0864       0.0887        0.132         0.11        0.136      0.00141\n",
            "Wall time: 53.73676139899999\n",
            "! Best model       16    0.019\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     17     1       0.0154       0.0154      4.3e-06       0.0743       0.0961       0.0623       0.0984       0.0804       0.0796        0.123        0.101        0.122      0.00127\n",
            "     17     2       0.0151       0.0151     2.15e-05       0.0734       0.0949       0.0627       0.0949       0.0788       0.0797         0.12       0.0997        0.328      0.00342\n",
            "     17     3       0.0167       0.0167     5.96e-05       0.0775       0.0999       0.0663          0.1       0.0832       0.0844        0.125        0.105        0.561      0.00584\n",
            "     17     4       0.0188       0.0187     3.33e-05       0.0802        0.106       0.0685        0.104        0.086       0.0906        0.131        0.111        0.385      0.00401\n",
            "     17     5       0.0159       0.0159     9.54e-06       0.0742       0.0975       0.0642       0.0943       0.0792       0.0843         0.12        0.102        0.217      0.00226\n",
            "     17     6       0.0159       0.0158     5.56e-05        0.074       0.0973       0.0618       0.0984       0.0801       0.0795        0.125        0.102        0.524      0.00545\n",
            "     17     7       0.0164       0.0163     6.29e-05       0.0762       0.0988       0.0635        0.101       0.0825       0.0818        0.126        0.104        0.579      0.00603\n",
            "     17     8       0.0192       0.0192     5.65e-06       0.0808        0.107       0.0676        0.107       0.0874       0.0895        0.136        0.113        0.143      0.00149\n",
            "     17     9       0.0151        0.015     0.000129        0.072       0.0947       0.0601       0.0957       0.0779       0.0798        0.119       0.0994         0.84      0.00875\n",
            "     17    10       0.0157       0.0155     0.000234       0.0748       0.0963       0.0639       0.0967       0.0803       0.0816         0.12        0.101         1.13       0.0118\n",
            "     17    11       0.0183       0.0182     1.22e-05        0.078        0.104       0.0643        0.105       0.0848       0.0852        0.135         0.11        0.203      0.00211\n",
            "     17    12       0.0163       0.0161     0.000221       0.0762       0.0982       0.0645       0.0998       0.0821        0.082        0.124        0.103         1.09       0.0114\n",
            "     17    13       0.0164        0.016     0.000381        0.074        0.098       0.0635       0.0949       0.0792       0.0837        0.122        0.103         1.45       0.0151\n",
            "     17    14       0.0153       0.0153     1.54e-06       0.0745       0.0958       0.0636       0.0961       0.0799       0.0798        0.122        0.101       0.0812     0.000846\n",
            "     17    15       0.0178       0.0173     0.000499       0.0774        0.102       0.0671       0.0981       0.0826       0.0878        0.125        0.106         1.66       0.0173\n",
            "     17    16       0.0154       0.0151     0.000379       0.0733       0.0949       0.0599          0.1         0.08       0.0755        0.125          0.1         1.44        0.015\n",
            "     17    17       0.0163       0.0163     6.47e-06       0.0756       0.0988       0.0647       0.0974       0.0811       0.0834        0.124        0.104         0.16      0.00167\n",
            "     17    18       0.0174        0.017     0.000416       0.0767        0.101       0.0652       0.0997       0.0824       0.0856        0.126        0.106         1.51       0.0158\n",
            "     17    19       0.0187       0.0184     0.000327       0.0799        0.105       0.0666        0.106       0.0865       0.0887        0.132         0.11         1.34        0.014\n",
            "     17    20       0.0154       0.0154     7.59e-06       0.0738       0.0961       0.0627        0.096       0.0794       0.0808        0.121        0.101         0.18      0.00188\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     17     1       0.0191       0.0191     7.27e-06       0.0813        0.107       0.0687        0.106       0.0875       0.0895        0.135        0.112        0.166      0.00173\n",
            "     17     2       0.0185       0.0185     4.01e-06       0.0797        0.105       0.0667        0.106       0.0862       0.0883        0.133        0.111        0.138      0.00144\n",
            "     17     3       0.0161       0.0161     4.97e-06       0.0754       0.0982       0.0635       0.0992       0.0814        0.082        0.124        0.103        0.138      0.00144\n",
            "     17     4       0.0171       0.0171     5.61e-06       0.0772        0.101        0.066       0.0996       0.0828       0.0861        0.126        0.106        0.141      0.00147\n",
            "     17     5       0.0169       0.0169     2.97e-06       0.0769          0.1       0.0643        0.102       0.0831       0.0831        0.128        0.106        0.101      0.00106\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              17   56.791    0.005       0.0164     0.000143       0.0166       0.0758       0.0992       0.0642       0.0992       0.0817       0.0833        0.125        0.104        0.697      0.00726\n",
            "! Validation         17   56.791    0.005       0.0176     4.97e-06       0.0176       0.0781        0.102       0.0658        0.103       0.0842       0.0858         0.13        0.108        0.137      0.00143\n",
            "Wall time: 56.79196194199994\n",
            "! Best model       17    0.018\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     18     1       0.0173        0.017     0.000272       0.0756        0.101       0.0662       0.0945       0.0804        0.088        0.123        0.105         1.22       0.0127\n",
            "     18     2       0.0149       0.0148     0.000147       0.0735        0.094       0.0623       0.0958        0.079       0.0782         0.12       0.0989        0.897      0.00934\n",
            "     18     3       0.0187       0.0187     3.53e-06       0.0803        0.106       0.0656         0.11       0.0876       0.0865        0.136        0.111        0.101      0.00105\n",
            "     18     4       0.0169       0.0168     0.000151       0.0765          0.1       0.0633        0.103       0.0831       0.0816         0.13        0.106         0.89      0.00927\n",
            "     18     5       0.0156       0.0155     0.000135       0.0723       0.0963       0.0595        0.098       0.0787       0.0795        0.123        0.101        0.857      0.00893\n",
            "     18     6       0.0149       0.0149     7.28e-06       0.0714       0.0945       0.0602       0.0938        0.077       0.0792        0.119       0.0993        0.153      0.00159\n",
            "     18     7        0.016       0.0159     0.000113       0.0743       0.0976       0.0615          0.1       0.0808       0.0804        0.125        0.103        0.773      0.00805\n",
            "     18     8       0.0166       0.0165     2.74e-05       0.0772       0.0995       0.0666       0.0984       0.0825       0.0847        0.124        0.104        0.382      0.00398\n",
            "     18     9       0.0158       0.0158      2.1e-06       0.0756       0.0973       0.0654       0.0959       0.0806       0.0831        0.121        0.102         0.09     0.000938\n",
            "     18    10       0.0169       0.0169     2.68e-05       0.0773        0.101       0.0662       0.0994       0.0828       0.0865        0.124        0.105        0.367      0.00382\n",
            "     18    11        0.015        0.015     1.07e-06       0.0738       0.0947       0.0648       0.0917       0.0782       0.0822        0.116       0.0989       0.0658     0.000686\n",
            "     18    12       0.0147       0.0147     2.76e-06       0.0726       0.0938       0.0612       0.0953       0.0783        0.077        0.121       0.0988        0.109      0.00114\n",
            "     18    13        0.016        0.016     9.73e-06       0.0763       0.0978       0.0661       0.0965       0.0813       0.0856        0.119        0.102        0.219      0.00228\n",
            "     18    14        0.017        0.017     5.33e-06       0.0785        0.101       0.0681       0.0993       0.0837       0.0857        0.126        0.106        0.138      0.00144\n",
            "     18    15       0.0178       0.0178     6.42e-06       0.0793        0.103       0.0664        0.105       0.0858       0.0844        0.133        0.109        0.179      0.00186\n",
            "     18    16       0.0153       0.0153     3.87e-05       0.0746       0.0957       0.0654       0.0931       0.0793       0.0828        0.117          0.1        0.454      0.00472\n",
            "     18    17       0.0144       0.0143     6.07e-05       0.0704       0.0926       0.0591        0.093       0.0761       0.0767        0.118       0.0974        0.564      0.00588\n",
            "     18    18       0.0153       0.0153     6.29e-06       0.0726       0.0956       0.0637       0.0906       0.0771       0.0831        0.117       0.0999        0.153       0.0016\n",
            "     18    19        0.019        0.019        9e-06       0.0824        0.107       0.0691        0.109       0.0891        0.088        0.137        0.112        0.206      0.00214\n",
            "     18    20       0.0176       0.0176     3.13e-05       0.0789        0.103       0.0689       0.0989       0.0839       0.0897        0.124        0.107        0.412       0.0043\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     18     1       0.0181       0.0181     7.03e-06       0.0791        0.104       0.0665        0.104       0.0853       0.0867        0.132         0.11        0.164      0.00171\n",
            "     18     2       0.0176       0.0176     3.62e-06       0.0776        0.103       0.0647        0.103        0.084       0.0855         0.13        0.108         0.13      0.00136\n",
            "     18     3       0.0153       0.0153     4.69e-06       0.0734       0.0958       0.0614       0.0973       0.0794       0.0794        0.122        0.101        0.135      0.00141\n",
            "     18     4       0.0164       0.0164     5.42e-06       0.0754        0.099       0.0642       0.0977       0.0809       0.0838        0.124        0.104         0.14      0.00146\n",
            "     18     5        0.016        0.016     2.85e-06       0.0748        0.098       0.0622       0.0999       0.0811       0.0806        0.126        0.103       0.0967      0.00101\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              18   59.850    0.005       0.0162     5.28e-05       0.0163       0.0757       0.0986       0.0645       0.0981       0.0813       0.0832        0.124        0.103        0.412      0.00429\n",
            "! Validation         18   59.850    0.005       0.0167     4.72e-06       0.0167        0.076          0.1       0.0638        0.101       0.0822       0.0832        0.127        0.105        0.133      0.00139\n",
            "Wall time: 59.85112176399991\n",
            "! Best model       18    0.017\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     19     1       0.0155       0.0155      4.1e-05       0.0731       0.0963       0.0644       0.0905       0.0774       0.0854        0.115          0.1        0.469      0.00488\n",
            "     19     2       0.0133       0.0133     4.27e-05       0.0674       0.0891       0.0555       0.0913       0.0734       0.0726        0.115       0.0939        0.477      0.00497\n",
            "     19     3       0.0136       0.0136     9.45e-07       0.0696       0.0901       0.0614       0.0861       0.0737       0.0786         0.11       0.0941       0.0529     0.000551\n",
            "     19     4       0.0143       0.0142     8.91e-05       0.0709       0.0923       0.0591       0.0947       0.0769       0.0755        0.119       0.0973        0.698      0.00728\n",
            "     19     5       0.0158       0.0158     7.76e-05       0.0741       0.0971        0.063       0.0964       0.0797       0.0829        0.121        0.102        0.649      0.00676\n",
            "     19     6       0.0162       0.0162     2.37e-05       0.0758       0.0985       0.0642        0.099       0.0816       0.0825        0.125        0.104        0.321      0.00335\n",
            "     19     7       0.0148       0.0148     1.55e-05       0.0719       0.0941       0.0584        0.099       0.0787       0.0758        0.123       0.0993         0.28      0.00292\n",
            "     19     8       0.0132        0.013     0.000199       0.0676       0.0883        0.058       0.0867       0.0723       0.0747         0.11       0.0926         1.04       0.0109\n",
            "     19     9       0.0142       0.0141     0.000141       0.0703       0.0918       0.0584        0.094       0.0762       0.0756        0.118       0.0967        0.879      0.00915\n",
            "     19    10       0.0189       0.0189     5.62e-06       0.0806        0.106        0.067        0.108       0.0873       0.0876        0.136        0.112        0.153      0.00159\n",
            "     19    11       0.0243       0.0242     3.21e-05       0.0935         0.12       0.0819        0.117       0.0993        0.105        0.146        0.126        0.401      0.00417\n",
            "     19    12       0.0244       0.0242     0.000251       0.0959         0.12       0.0883        0.111       0.0997         0.11        0.139        0.124         1.17       0.0122\n",
            "     19    13       0.0172       0.0172     2.29e-06       0.0781        0.102       0.0691        0.096       0.0825       0.0899        0.122        0.106        0.092     0.000958\n",
            "     19    14       0.0174       0.0174     1.89e-05       0.0787        0.102       0.0678          0.1       0.0841        0.088        0.125        0.107        0.317       0.0033\n",
            "     19    15       0.0244       0.0243     6.39e-05       0.0961        0.121       0.0915        0.105       0.0983        0.114        0.133        0.123        0.593      0.00617\n",
            "     19    16       0.0212       0.0212     3.07e-06       0.0885        0.113       0.0795        0.107        0.093       0.0991        0.136        0.117        0.109      0.00113\n",
            "     19    17       0.0156       0.0156      1.7e-06       0.0739       0.0967       0.0618       0.0981       0.0799       0.0812        0.122        0.102       0.0822     0.000857\n",
            "     19    18       0.0159       0.0159     3.69e-05       0.0749       0.0974       0.0639       0.0969       0.0804       0.0831        0.121        0.102        0.424      0.00441\n",
            "     19    19       0.0164       0.0163     0.000104        0.075       0.0988       0.0647       0.0957       0.0802       0.0844        0.123        0.103        0.758      0.00789\n",
            "     19    20       0.0174       0.0174     6.38e-06       0.0766        0.102       0.0643        0.101       0.0828       0.0853        0.129        0.107        0.178      0.00185\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     19     1       0.0173       0.0173     5.56e-06       0.0772        0.102       0.0646        0.102       0.0834       0.0844         0.13        0.107        0.141      0.00147\n",
            "     19     2       0.0168       0.0168     2.91e-06       0.0757          0.1       0.0629        0.101       0.0821       0.0832        0.128        0.105        0.117      0.00122\n",
            "     19     3       0.0146       0.0146     3.66e-06       0.0716       0.0935       0.0597       0.0953       0.0775       0.0773        0.119       0.0984        0.123      0.00128\n",
            "     19     4       0.0157       0.0157     3.71e-06       0.0737       0.0969       0.0626        0.096       0.0793       0.0816        0.122        0.102         0.12      0.00125\n",
            "     19     5       0.0154       0.0154      2.2e-06        0.073       0.0959       0.0606       0.0979       0.0792       0.0785        0.123        0.101       0.0939     0.000979\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              19   62.915    0.005       0.0171     5.79e-05       0.0172       0.0776        0.101       0.0671       0.0986       0.0829       0.0873        0.125        0.106        0.457      0.00476\n",
            "! Validation         19   62.915    0.005        0.016     3.61e-06        0.016       0.0742       0.0977       0.0621       0.0985       0.0803        0.081        0.125        0.103        0.119      0.00124\n",
            "Wall time: 62.91584973699992\n",
            "! Best model       19    0.016\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     20     1       0.0157       0.0157     6.52e-06       0.0746       0.0968       0.0634       0.0968       0.0801       0.0822        0.121        0.101        0.137      0.00142\n",
            "     20     2       0.0135       0.0135     6.09e-06       0.0689       0.0897        0.058       0.0906       0.0743       0.0743        0.115       0.0944        0.168      0.00175\n",
            "     20     3       0.0169       0.0168     2.42e-05        0.076          0.1       0.0652       0.0976       0.0814       0.0857        0.125        0.105        0.338      0.00352\n",
            "     20     4       0.0174       0.0174     4.75e-06       0.0773        0.102       0.0647        0.103       0.0836       0.0851        0.129        0.107        0.141      0.00147\n",
            "     20     5       0.0143       0.0142     1.75e-05       0.0701       0.0923       0.0581       0.0941       0.0761       0.0773        0.117        0.097        0.297       0.0031\n",
            "     20     6       0.0139       0.0139     2.43e-05       0.0705       0.0911       0.0594       0.0927        0.076       0.0761        0.115       0.0957        0.358      0.00373\n",
            "     20     7       0.0141       0.0141      9.6e-06       0.0696       0.0917       0.0577       0.0933       0.0755        0.075        0.118       0.0967        0.209      0.00218\n",
            "     20     8       0.0125       0.0125     3.91e-06       0.0664       0.0866       0.0542       0.0909       0.0725       0.0692        0.114       0.0914        0.112      0.00116\n",
            "     20     9       0.0132       0.0132     5.51e-06       0.0682        0.089       0.0583       0.0879       0.0731       0.0757        0.111       0.0934         0.16      0.00166\n",
            "     20    10       0.0179       0.0178     9.22e-05       0.0805        0.103       0.0678        0.106       0.0869       0.0853        0.132        0.109        0.696      0.00725\n",
            "     20    11       0.0169       0.0169     1.27e-05       0.0768          0.1       0.0662       0.0979       0.0821       0.0869        0.123        0.105        0.239      0.00249\n",
            "     20    12       0.0136       0.0136     3.88e-06       0.0692       0.0902       0.0581       0.0914       0.0748       0.0751        0.114       0.0948        0.121      0.00126\n",
            "     20    13       0.0131       0.0131      2.6e-05       0.0671       0.0885        0.055       0.0912       0.0731       0.0705        0.117       0.0935        0.374       0.0039\n",
            "     20    14       0.0133       0.0133     3.14e-05       0.0688       0.0891       0.0581       0.0902       0.0742       0.0737        0.114       0.0938        0.392      0.00409\n",
            "     20    15       0.0137       0.0137     1.08e-05       0.0695       0.0904       0.0578       0.0928       0.0753        0.075        0.115       0.0951        0.227      0.00237\n",
            "     20    16       0.0137       0.0136     7.47e-06       0.0679       0.0904       0.0555       0.0925        0.074        0.073        0.118       0.0953        0.196      0.00204\n",
            "     20    17       0.0163       0.0163     1.05e-05       0.0755       0.0989       0.0642       0.0981       0.0811       0.0827        0.125        0.104        0.239      0.00249\n",
            "     20    18       0.0177       0.0177     6.26e-06        0.081        0.103       0.0725       0.0981       0.0853        0.092        0.122        0.107        0.119      0.00124\n",
            "     20    19       0.0199       0.0199     1.05e-05       0.0826        0.109         0.07        0.108       0.0889       0.0935        0.135        0.114        0.216      0.00225\n",
            "     20    20       0.0132       0.0132     3.23e-05       0.0681       0.0887       0.0576       0.0891       0.0733       0.0746        0.112       0.0932        0.412      0.00429\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     20     1       0.0166       0.0166     5.08e-06       0.0755       0.0997       0.0631          0.1       0.0817       0.0824        0.127        0.105        0.134       0.0014\n",
            "     20     2       0.0161       0.0161     2.64e-06        0.074       0.0981       0.0613       0.0993       0.0803        0.081        0.125        0.103        0.111      0.00115\n",
            "     20     3        0.014        0.014     3.44e-06         0.07       0.0916       0.0582       0.0937       0.0759       0.0754        0.118       0.0965        0.119      0.00124\n",
            "     20     4       0.0151       0.0151     3.34e-06       0.0722        0.095       0.0612       0.0944       0.0778       0.0797         0.12       0.0998        0.114      0.00119\n",
            "     20     5       0.0147       0.0147     2.18e-06       0.0713       0.0938        0.059        0.096       0.0775       0.0766        0.121       0.0989       0.0936     0.000975\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              20   65.981    0.005        0.015     1.73e-05        0.015       0.0724       0.0948       0.0611       0.0951       0.0781       0.0794         0.12       0.0996        0.258      0.00268\n",
            "! Validation         20   65.981    0.005       0.0153     3.34e-06       0.0153       0.0726       0.0957       0.0606       0.0967       0.0787       0.0791        0.122        0.101        0.114      0.00119\n",
            "Wall time: 65.98207843499995\n",
            "! Best model       20    0.015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     21     1       0.0124       0.0124     7.35e-05       0.0664       0.0861       0.0561        0.087       0.0716        0.071         0.11       0.0906         0.63      0.00657\n",
            "     21     2       0.0172       0.0172     1.66e-06       0.0782        0.102       0.0658        0.103       0.0844       0.0863        0.127        0.106       0.0762     0.000793\n",
            "     21     3       0.0169       0.0169     4.68e-05       0.0766          0.1       0.0653       0.0991       0.0822       0.0846        0.126        0.105        0.485      0.00506\n",
            "     21     4       0.0174       0.0172      0.00015        0.079        0.101       0.0692       0.0987       0.0839       0.0882        0.124        0.106        0.904      0.00942\n",
            "     21     5       0.0149       0.0149     6.44e-06       0.0731       0.0944       0.0612       0.0971       0.0791       0.0782         0.12       0.0993        0.171      0.00178\n",
            "     21     6       0.0147       0.0147     4.94e-06       0.0718       0.0937       0.0607        0.094       0.0773       0.0787        0.118       0.0984        0.152      0.00158\n",
            "     21     7        0.016       0.0159     7.62e-05       0.0729       0.0975       0.0609        0.097        0.079       0.0788        0.127        0.103        0.636      0.00663\n",
            "     21     8       0.0187       0.0186     0.000146       0.0828        0.105       0.0736        0.101       0.0874       0.0925        0.127         0.11        0.896      0.00934\n",
            "     21     9        0.012        0.012     2.36e-06       0.0659       0.0848       0.0562       0.0853       0.0707       0.0705        0.108       0.0892       0.0912      0.00095\n",
            "     21    10       0.0124       0.0124     5.77e-05        0.067        0.086        0.057        0.087        0.072       0.0719        0.109       0.0904        0.554      0.00577\n",
            "     21    11       0.0156       0.0155     8.58e-05       0.0751       0.0963       0.0674       0.0906        0.079       0.0848        0.116          0.1        0.677      0.00705\n",
            "     21    12       0.0151        0.015     7.41e-05        0.072       0.0948       0.0595        0.097       0.0782       0.0788        0.121       0.0997        0.625      0.00651\n",
            "     21    13       0.0138       0.0138     3.04e-06       0.0685        0.091       0.0577       0.0902       0.0739       0.0756        0.116       0.0957        0.126      0.00131\n",
            "     21    14       0.0161       0.0161     4.48e-05       0.0752       0.0982       0.0609        0.104       0.0823       0.0796        0.127        0.104        0.494      0.00514\n",
            "     21    15       0.0147       0.0147     2.67e-05       0.0717       0.0938       0.0624       0.0901       0.0763         0.08        0.117       0.0983        0.371      0.00386\n",
            "     21    16        0.015        0.015     8.31e-06       0.0727       0.0947       0.0606        0.097       0.0788       0.0786        0.121       0.0996        0.188      0.00195\n",
            "     21    17       0.0129       0.0129      3.3e-06       0.0667       0.0878       0.0538       0.0924       0.0731       0.0694        0.116       0.0928        0.118      0.00123\n",
            "     21    18       0.0148       0.0148     1.65e-06       0.0727       0.0942       0.0629       0.0922       0.0775       0.0799        0.118       0.0988       0.0924     0.000962\n",
            "     21    19       0.0163       0.0163      2.8e-05       0.0753       0.0987       0.0652       0.0956       0.0804       0.0836        0.123        0.104        0.385      0.00401\n",
            "     21    20        0.015        0.015     2.46e-05       0.0727       0.0947       0.0633       0.0916       0.0774       0.0821        0.116        0.099        0.362      0.00377\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     21     1        0.016        0.016     5.17e-06        0.074       0.0977       0.0616       0.0986       0.0801       0.0806        0.125        0.103        0.139      0.00144\n",
            "     21     2       0.0154       0.0154     2.56e-06       0.0724       0.0961       0.0598       0.0976       0.0787       0.0791        0.123        0.101        0.109      0.00113\n",
            "     21     3       0.0135       0.0135     3.51e-06       0.0686       0.0898       0.0569       0.0921       0.0745       0.0736        0.116       0.0946         0.12      0.00125\n",
            "     21     4       0.0146       0.0146     3.71e-06       0.0709       0.0933       0.0598       0.0929       0.0764       0.0779        0.118       0.0981        0.117      0.00122\n",
            "     21     5       0.0142       0.0142     2.22e-06         0.07       0.0921       0.0578       0.0943       0.0761        0.075        0.119       0.0971       0.0906     0.000944\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              21   69.029    0.005       0.0151     4.33e-05       0.0151       0.0728       0.0949        0.062       0.0945       0.0782       0.0799         0.12       0.0997        0.402      0.00418\n",
            "! Validation         21   69.029    0.005       0.0147     3.43e-06       0.0147       0.0712       0.0939       0.0592       0.0951       0.0771       0.0773         0.12       0.0988        0.115       0.0012\n",
            "Wall time: 69.02945741899998\n",
            "! Best model       21    0.015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     22     1       0.0139       0.0138     8.56e-05       0.0686       0.0909       0.0574       0.0911       0.0743       0.0751        0.116       0.0957        0.678      0.00706\n",
            "     22     2       0.0168       0.0167     6.31e-05       0.0763          0.1       0.0654        0.098       0.0817       0.0849        0.125        0.105        0.578      0.00602\n",
            "     22     3       0.0183       0.0183     4.97e-06       0.0796        0.105       0.0703       0.0982       0.0842       0.0937        0.123        0.109        0.112      0.00117\n",
            "     22     4       0.0135       0.0134     8.88e-05       0.0696       0.0895       0.0598       0.0891       0.0744       0.0753        0.113        0.094        0.697      0.00726\n",
            "     22     5       0.0129       0.0128     9.52e-05       0.0679       0.0875       0.0581       0.0875       0.0728       0.0739         0.11       0.0918        0.715      0.00745\n",
            "     22     6       0.0148       0.0148      4.1e-06       0.0718       0.0941       0.0595       0.0963       0.0779       0.0758        0.123       0.0993        0.121      0.00126\n",
            "     22     7       0.0158       0.0158     8.63e-07       0.0738       0.0971       0.0606          0.1       0.0804       0.0781        0.127        0.102       0.0631     0.000657\n",
            "     22     8       0.0154       0.0154     5.84e-05       0.0737        0.096       0.0622       0.0968       0.0795       0.0788        0.123        0.101        0.566      0.00589\n",
            "     22     9        0.013        0.013     3.16e-05       0.0682       0.0882       0.0565       0.0917       0.0741       0.0706        0.116       0.0931        0.408      0.00425\n",
            "     22    10       0.0153       0.0153     5.82e-06       0.0729       0.0958       0.0612       0.0963       0.0788       0.0801        0.121        0.101        0.139      0.00145\n",
            "     22    11       0.0127       0.0127     2.41e-06       0.0657       0.0871       0.0548       0.0876       0.0712        0.072        0.111       0.0916       0.0947     0.000987\n",
            "     22    12       0.0137       0.0137     5.39e-06        0.069       0.0906       0.0575       0.0921       0.0748        0.075        0.116       0.0954        0.167      0.00174\n",
            "     22    13       0.0121       0.0121     3.01e-05       0.0638        0.085       0.0524       0.0865       0.0695       0.0695         0.11       0.0895        0.401      0.00418\n",
            "     22    14        0.012        0.012     2.72e-06       0.0659       0.0848       0.0551       0.0875       0.0713       0.0703        0.108       0.0892       0.0957     0.000997\n",
            "     22    15       0.0135       0.0135     1.25e-05       0.0693       0.0899       0.0565       0.0949       0.0757       0.0735        0.116       0.0947        0.255      0.00266\n",
            "     22    16       0.0117       0.0117     5.56e-06        0.064       0.0836       0.0539       0.0842       0.0691       0.0693        0.107        0.088        0.164      0.00171\n",
            "     22    17       0.0127       0.0126     3.34e-05       0.0657        0.087       0.0544       0.0883       0.0714       0.0719        0.111       0.0916        0.414      0.00432\n",
            "     22    18       0.0118       0.0118     1.73e-05       0.0634       0.0841       0.0526       0.0849       0.0688       0.0688        0.108       0.0887        0.295      0.00307\n",
            "     22    19        0.012        0.012     1.03e-05       0.0651       0.0847        0.056       0.0832       0.0696        0.071        0.107        0.089        0.227      0.00236\n",
            "     22    20        0.013        0.013     4.54e-05       0.0669       0.0881       0.0537       0.0933       0.0735       0.0685        0.118       0.0932        0.495      0.00516\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     22     1       0.0153       0.0153     4.72e-06       0.0725       0.0958       0.0602        0.097       0.0786       0.0788        0.123        0.101        0.131      0.00136\n",
            "     22     2       0.0148       0.0148     2.19e-06        0.071       0.0942       0.0584        0.096       0.0772       0.0772        0.121       0.0992       0.0994      0.00104\n",
            "     22     3        0.013        0.013      3.2e-06       0.0673       0.0882       0.0557       0.0907       0.0732        0.072        0.114        0.093        0.115       0.0012\n",
            "     22     4       0.0141       0.0141     3.17e-06       0.0695       0.0917       0.0586       0.0915        0.075       0.0762        0.117       0.0965         0.11      0.00115\n",
            "     22     5       0.0137       0.0137     2.07e-06       0.0686       0.0904       0.0566       0.0927       0.0746       0.0735        0.117       0.0953       0.0882     0.000919\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              22   72.079    0.005       0.0137     3.02e-05       0.0137       0.0691       0.0906       0.0579       0.0914       0.0746        0.075        0.116       0.0953        0.334      0.00348\n",
            "! Validation         22   72.079    0.005       0.0142     3.07e-06       0.0142       0.0698       0.0921       0.0579       0.0936       0.0757       0.0756        0.118        0.097        0.109      0.00113\n",
            "Wall time: 72.079395481\n",
            "! Best model       22    0.014\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     23     1       0.0115       0.0115     4.06e-07        0.064       0.0829       0.0532       0.0854       0.0693       0.0692        0.105       0.0872       0.0465     0.000484\n",
            "     23     2       0.0126       0.0126     2.59e-05       0.0682       0.0868       0.0581       0.0882       0.0732       0.0733        0.109       0.0911        0.371      0.00387\n",
            "     23     3       0.0134       0.0133     5.17e-05       0.0681       0.0894       0.0561       0.0922       0.0741       0.0734        0.115       0.0941        0.531      0.00554\n",
            "     23     4       0.0141       0.0141     1.55e-05       0.0693       0.0918       0.0567       0.0946       0.0756       0.0747        0.119       0.0968         0.28      0.00292\n",
            "     23     5       0.0128       0.0127     9.02e-05       0.0658       0.0872       0.0547       0.0881       0.0714       0.0713        0.113       0.0919        0.696      0.00725\n",
            "     23     6       0.0142       0.0142     1.83e-05       0.0703       0.0921       0.0584       0.0939       0.0762       0.0759        0.118       0.0969        0.314      0.00327\n",
            "     23     7       0.0127       0.0127     6.27e-05       0.0664        0.087       0.0554       0.0885       0.0719       0.0726         0.11       0.0915        0.573      0.00597\n",
            "     23     8       0.0141        0.014     7.41e-05       0.0697       0.0917        0.057        0.095        0.076       0.0746        0.119       0.0967        0.619      0.00644\n",
            "     23     9       0.0132       0.0132     6.34e-06        0.067        0.089       0.0561       0.0887       0.0724       0.0729        0.115       0.0938         0.16      0.00167\n",
            "     23    10       0.0125       0.0124      8.1e-05       0.0656       0.0861       0.0549       0.0869       0.0709        0.072        0.109       0.0905         0.65      0.00677\n",
            "     23    11       0.0135       0.0135     1.16e-05       0.0689       0.0898       0.0586       0.0897       0.0741       0.0745        0.114       0.0945        0.235      0.00244\n",
            "     23    12       0.0131       0.0131     5.85e-06       0.0685       0.0886       0.0582       0.0892       0.0737       0.0739        0.112       0.0931        0.156      0.00163\n",
            "     23    13       0.0116       0.0115     0.000105       0.0635        0.083       0.0527       0.0849       0.0688       0.0688        0.106       0.0873        0.756      0.00788\n",
            "     23    14       0.0127       0.0127     3.58e-06       0.0671        0.087       0.0568       0.0875       0.0722       0.0728         0.11       0.0915        0.124      0.00129\n",
            "     23    15       0.0165       0.0164     7.52e-05       0.0776       0.0992       0.0714       0.0899       0.0806       0.0895        0.116        0.103        0.635      0.00661\n",
            "     23    16        0.016        0.016     4.75e-06       0.0772       0.0977         0.07       0.0914       0.0807       0.0882        0.114        0.101        0.137      0.00142\n",
            "     23    17       0.0144       0.0144      4.4e-06       0.0715       0.0928       0.0596       0.0954       0.0775        0.075        0.121       0.0979        0.154       0.0016\n",
            "     23    18       0.0146       0.0146     4.29e-05       0.0701       0.0934       0.0582        0.094       0.0761       0.0776        0.119       0.0982        0.478      0.00498\n",
            "     23    19        0.014        0.014     1.09e-06       0.0704       0.0916       0.0568       0.0976       0.0772       0.0719        0.122       0.0968       0.0609     0.000635\n",
            "     23    20       0.0186       0.0185     9.53e-05       0.0825        0.105       0.0731        0.101       0.0873       0.0912        0.129         0.11        0.718      0.00748\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     23     1       0.0148       0.0148     4.45e-06       0.0712       0.0941        0.059       0.0955       0.0772       0.0772        0.121       0.0991        0.129      0.00134\n",
            "     23     2       0.0143       0.0143     1.99e-06       0.0695       0.0924       0.0571       0.0945       0.0758       0.0755        0.119       0.0974       0.0942     0.000982\n",
            "     23     3       0.0126       0.0126     3.02e-06       0.0661       0.0867       0.0545       0.0892       0.0719       0.0705        0.112       0.0914        0.112      0.00117\n",
            "     23     4       0.0136       0.0136     2.98e-06       0.0683       0.0902       0.0574       0.0901       0.0738       0.0747        0.115       0.0949        0.108      0.00113\n",
            "     23     5       0.0132       0.0132     2.03e-06       0.0673       0.0888       0.0554       0.0911       0.0733       0.0721        0.115       0.0937       0.0871     0.000907\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              23   75.126    0.005       0.0138     3.88e-05       0.0138       0.0696       0.0908       0.0588       0.0911        0.075       0.0759        0.115       0.0954        0.385      0.00401\n",
            "! Validation         23   75.126    0.005       0.0137      2.9e-06       0.0137       0.0685       0.0905       0.0567       0.0921       0.0744        0.074        0.117       0.0953        0.106      0.00111\n",
            "Wall time: 75.12670188599998\n",
            "! Best model       23    0.014\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     24     1       0.0199       0.0199      1.5e-06       0.0861        0.109       0.0786        0.101       0.0899       0.0984        0.128        0.113       0.0764     0.000795\n",
            "     24     2       0.0146       0.0146     1.57e-05       0.0718       0.0934        0.062       0.0913       0.0767       0.0789        0.117        0.098        0.287      0.00299\n",
            "     24     3       0.0117       0.0116     4.55e-05       0.0628       0.0834       0.0516       0.0852       0.0684       0.0673        0.109        0.088        0.486      0.00506\n",
            "     24     4       0.0147       0.0147     4.57e-06       0.0733       0.0939       0.0629        0.094       0.0785        0.079        0.118       0.0986        0.153       0.0016\n",
            "     24     5       0.0137       0.0136     9.65e-05        0.069       0.0903       0.0586       0.0898       0.0742       0.0752        0.115       0.0949        0.729      0.00759\n",
            "     24     6       0.0122       0.0122     2.62e-05       0.0641       0.0855       0.0526       0.0873       0.0699       0.0701         0.11       0.0901        0.366      0.00381\n",
            "     24     7       0.0125       0.0125     2.15e-05       0.0668       0.0865       0.0564       0.0875        0.072       0.0723        0.109       0.0909        0.334      0.00347\n",
            "     24     8       0.0137       0.0137     2.66e-05       0.0697       0.0906       0.0591        0.091       0.0751       0.0759        0.114       0.0952        0.369      0.00385\n",
            "     24     9       0.0137       0.0137     1.99e-05       0.0681       0.0904       0.0555       0.0932       0.0743       0.0742        0.116       0.0952        0.317       0.0033\n",
            "     24    10       0.0131       0.0131     8.38e-06       0.0687       0.0887       0.0577       0.0905       0.0741       0.0741        0.112       0.0932        0.207      0.00215\n",
            "     24    11       0.0143       0.0143     7.68e-05       0.0711       0.0924       0.0577        0.098       0.0778       0.0727        0.123       0.0976        0.645      0.00671\n",
            "     24    12       0.0126       0.0126     4.34e-06       0.0653       0.0867       0.0558       0.0845       0.0701        0.073        0.109        0.091        0.151      0.00157\n",
            "     24    13       0.0126       0.0126     1.58e-05       0.0681        0.087       0.0593       0.0856       0.0724       0.0751        0.107       0.0909        0.275      0.00287\n",
            "     24    14       0.0127       0.0127      1.5e-05       0.0669       0.0872       0.0553       0.0901       0.0727       0.0709        0.113        0.092        0.268      0.00279\n",
            "     24    15       0.0162       0.0162     7.21e-06       0.0754       0.0985       0.0642       0.0978        0.081       0.0828        0.124        0.103        0.195      0.00203\n",
            "     24    16       0.0133       0.0132      6.9e-05       0.0685        0.089       0.0555       0.0944        0.075       0.0699        0.118       0.0941        0.608      0.00634\n",
            "     24    17       0.0141       0.0141     4.42e-06        0.071       0.0919       0.0577       0.0975       0.0776       0.0735        0.121        0.097        0.121      0.00126\n",
            "     24    18       0.0126       0.0125     3.67e-05       0.0674       0.0866       0.0577       0.0868       0.0723       0.0739        0.108       0.0907        0.448      0.00467\n",
            "     24    19       0.0121       0.0121     2.99e-05        0.064       0.0851       0.0531       0.0858       0.0694       0.0696         0.11       0.0896          0.4      0.00417\n",
            "     24    20        0.011        0.011      4.6e-07       0.0623       0.0813       0.0527       0.0815       0.0671       0.0679        0.103       0.0854       0.0381     0.000397\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     24     1       0.0143       0.0143     3.95e-06         0.07       0.0925       0.0579       0.0941        0.076       0.0758        0.119       0.0974         0.12      0.00125\n",
            "     24     2       0.0138       0.0138     1.74e-06       0.0684        0.091        0.056       0.0932       0.0746       0.0741        0.118       0.0959       0.0873     0.000909\n",
            "     24     3       0.0122       0.0122     2.68e-06        0.065       0.0853       0.0535       0.0879       0.0707       0.0692        0.111         0.09        0.107      0.00111\n",
            "     24     4       0.0132       0.0132     2.44e-06       0.0672       0.0889       0.0564       0.0889       0.0726       0.0733        0.114       0.0936       0.0997      0.00104\n",
            "     24     5       0.0128       0.0128     1.85e-06       0.0662       0.0875       0.0545       0.0897       0.0721       0.0709        0.114       0.0922       0.0836     0.000871\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              24   78.184    0.005       0.0136     2.63e-05       0.0136        0.069       0.0901       0.0582       0.0907       0.0744        0.075        0.114       0.0947        0.324      0.00337\n",
            "! Validation         24   78.184    0.005       0.0133     2.53e-06       0.0133       0.0674       0.0891       0.0557       0.0907       0.0732       0.0727        0.115       0.0939       0.0995      0.00104\n",
            "Wall time: 78.18476133599995\n",
            "! Best model       24    0.013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     25     1        0.015       0.0149     4.69e-05       0.0717       0.0945         0.06       0.0951       0.0776       0.0792        0.119       0.0993        0.501      0.00522\n",
            "     25     2       0.0142       0.0142     6.19e-06       0.0707       0.0923       0.0584       0.0953       0.0768       0.0735        0.121       0.0975        0.131      0.00137\n",
            "     25     3       0.0137       0.0137      2.2e-06       0.0703       0.0905       0.0612       0.0887       0.0749       0.0764        0.114        0.095       0.0758     0.000789\n",
            "     25     4       0.0121       0.0121     5.51e-05       0.0665       0.0851       0.0571       0.0853       0.0712       0.0724        0.106       0.0892        0.524      0.00545\n",
            "     25     5       0.0109       0.0109     1.06e-06       0.0606       0.0806       0.0502       0.0813       0.0658       0.0659        0.104        0.085       0.0666     0.000694\n",
            "     25     6       0.0121       0.0119     0.000203       0.0655       0.0845       0.0565       0.0835         0.07       0.0721        0.105       0.0885         1.06        0.011\n",
            "     25     7       0.0137       0.0136     6.68e-05       0.0661       0.0903       0.0527       0.0929       0.0728       0.0725        0.118       0.0953        0.606      0.00632\n",
            "     25     8       0.0117       0.0117     3.82e-05       0.0639       0.0836       0.0536       0.0846       0.0691       0.0683        0.108        0.088        0.451       0.0047\n",
            "     25     9       0.0111        0.011      0.00011       0.0625        0.081       0.0519       0.0836       0.0678       0.0659        0.105       0.0854        0.777       0.0081\n",
            "     25    10       0.0121       0.0121     8.14e-05       0.0654        0.085       0.0545       0.0872       0.0708       0.0691         0.11       0.0896        0.665      0.00693\n",
            "     25    11       0.0128       0.0128     2.22e-05       0.0677       0.0874        0.059       0.0852       0.0721       0.0746        0.108       0.0915        0.347      0.00362\n",
            "     25    12        0.012       0.0119     7.19e-05       0.0648       0.0846       0.0535       0.0873       0.0704       0.0683         0.11       0.0892        0.626      0.00652\n",
            "     25    13       0.0133       0.0133     1.02e-06       0.0674       0.0892       0.0562       0.0897        0.073       0.0743        0.113       0.0938       0.0594     0.000618\n",
            "     25    14       0.0113       0.0113     2.43e-05        0.063       0.0822       0.0534       0.0822       0.0678        0.068        0.105       0.0865        0.351      0.00366\n",
            "     25    15        0.012        0.012     1.61e-05       0.0645       0.0846       0.0539       0.0857       0.0698       0.0702        0.108        0.089        0.277      0.00288\n",
            "     25    16       0.0117       0.0117     2.08e-05       0.0621       0.0836       0.0493       0.0877       0.0685       0.0654        0.111       0.0884        0.323      0.00337\n",
            "     25    17       0.0125       0.0125     4.64e-05       0.0655       0.0864        0.053       0.0906       0.0718       0.0676        0.115       0.0914        0.495      0.00516\n",
            "     25    18        0.012        0.012     1.92e-06       0.0644       0.0847       0.0527        0.088       0.0703       0.0689         0.11       0.0893       0.0873     0.000909\n",
            "     25    19       0.0125       0.0125      2.9e-05       0.0661       0.0865        0.054       0.0904       0.0722       0.0711        0.111       0.0911        0.391      0.00407\n",
            "     25    20       0.0106       0.0106     9.29e-06       0.0616       0.0796       0.0503       0.0843       0.0673       0.0629        0.105       0.0841        0.225      0.00235\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     25     1       0.0138       0.0138     3.71e-06       0.0689        0.091       0.0569       0.0928       0.0749       0.0744        0.117       0.0959        0.118      0.00122\n",
            "     25     2       0.0134       0.0134     1.65e-06       0.0672       0.0895       0.0549       0.0919       0.0734       0.0726        0.116       0.0944       0.0847     0.000882\n",
            "     25     3       0.0118       0.0118     2.48e-06        0.064       0.0841       0.0526       0.0867       0.0697        0.068        0.109       0.0887        0.104      0.00108\n",
            "     25     4       0.0128       0.0128        2e-06       0.0662       0.0876       0.0553       0.0878       0.0716       0.0719        0.113       0.0923       0.0924     0.000962\n",
            "     25     5       0.0124       0.0124     1.82e-06       0.0652       0.0861       0.0536       0.0883       0.0709       0.0698        0.112       0.0908       0.0839     0.000874\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              25   81.238    0.005       0.0123     4.27e-05       0.0124       0.0655       0.0859       0.0546       0.0874        0.071       0.0704         0.11       0.0905        0.402      0.00418\n",
            "! Validation         25   81.238    0.005       0.0129     2.33e-06       0.0129       0.0663       0.0877       0.0547       0.0895       0.0721       0.0714        0.114       0.0925       0.0964        0.001\n",
            "Wall time: 81.23834470299994\n",
            "! Best model       25    0.013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     26     1       0.0136       0.0136     1.27e-05       0.0684       0.0903       0.0565        0.092       0.0743       0.0743        0.116        0.095         0.19      0.00198\n",
            "     26     2       0.0129       0.0129     5.07e-06        0.068       0.0878         0.06       0.0838       0.0719       0.0765        0.107       0.0917        0.148      0.00154\n",
            "     26     3       0.0122       0.0122      7.9e-06       0.0662       0.0854       0.0562       0.0863       0.0713       0.0707        0.109       0.0899        0.203      0.00211\n",
            "     26     4       0.0125       0.0124     1.67e-05       0.0654       0.0863       0.0536       0.0891       0.0713       0.0703        0.112       0.0909        0.293      0.00305\n",
            "     26     5         0.01         0.01     2.18e-05       0.0588       0.0774       0.0469       0.0824       0.0647       0.0607        0.103       0.0818        0.343      0.00357\n",
            "     26     6       0.0104       0.0104     2.32e-06       0.0603       0.0789       0.0484       0.0839       0.0662       0.0622        0.105       0.0834       0.0945     0.000985\n",
            "     26     7        0.012        0.012     1.66e-05       0.0634       0.0848        0.052       0.0862       0.0691       0.0687         0.11       0.0894        0.299      0.00311\n",
            "     26     8       0.0127       0.0127     4.12e-05       0.0668       0.0871        0.057       0.0866       0.0718        0.072        0.111       0.0916        0.472      0.00491\n",
            "     26     9       0.0137       0.0137     2.36e-06       0.0695       0.0904       0.0597       0.0892       0.0744       0.0762        0.114       0.0949       0.0986      0.00103\n",
            "     26    10       0.0139       0.0139     1.76e-05       0.0689       0.0912       0.0568        0.093       0.0749       0.0748        0.117        0.096        0.299      0.00311\n",
            "     26    11       0.0109       0.0108     1.81e-05       0.0612       0.0806         0.05       0.0835       0.0668       0.0641        0.106       0.0851        0.314      0.00327\n",
            "     26    12       0.0106       0.0106     9.36e-06       0.0619       0.0796       0.0519       0.0818       0.0669       0.0656        0.102       0.0838        0.198      0.00206\n",
            "     26    13       0.0137       0.0137     4.81e-06       0.0684       0.0904       0.0569       0.0913       0.0741       0.0728        0.118       0.0954        0.151      0.00157\n",
            "     26    14        0.017        0.017     1.96e-05        0.079        0.101       0.0714       0.0942       0.0828       0.0904        0.119        0.105        0.318      0.00332\n",
            "     26    15       0.0158       0.0158     2.26e-06       0.0766       0.0973       0.0707       0.0884       0.0796       0.0889        0.112        0.101       0.0879     0.000916\n",
            "     26    16       0.0114       0.0114     1.48e-05       0.0637       0.0826       0.0535        0.084       0.0688        0.068        0.106        0.087        0.281      0.00293\n",
            "     26    17        0.012        0.012     4.11e-05        0.065       0.0847       0.0554       0.0842       0.0698       0.0717        0.106       0.0888        0.464      0.00484\n",
            "     26    18       0.0172       0.0172      1.4e-05       0.0788        0.102       0.0693       0.0978       0.0836       0.0899        0.121        0.106        0.272      0.00283\n",
            "     26    19       0.0193       0.0191     0.000192       0.0848        0.107       0.0775       0.0994       0.0885       0.0962        0.125        0.111         1.02       0.0107\n",
            "     26    20       0.0184       0.0183      8.2e-05       0.0821        0.105       0.0703        0.106       0.0881       0.0877        0.132         0.11        0.668      0.00696\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     26     1       0.0134       0.0134     3.32e-06       0.0678       0.0896        0.056       0.0915       0.0737       0.0732        0.116       0.0944        0.109      0.00113\n",
            "     26     2        0.013        0.013     1.59e-06       0.0662       0.0881       0.0539       0.0907       0.0723       0.0713        0.115        0.093       0.0838     0.000873\n",
            "     26     3       0.0115       0.0115     2.25e-06        0.063       0.0829       0.0517       0.0855       0.0686       0.0668        0.108       0.0875        0.098      0.00102\n",
            "     26     4       0.0125       0.0125     1.64e-06       0.0652       0.0864       0.0544       0.0868       0.0706       0.0707        0.111       0.0911       0.0844     0.000879\n",
            "     26     5        0.012        0.012     1.83e-06       0.0641       0.0849       0.0527       0.0869       0.0698       0.0688         0.11       0.0895       0.0838     0.000873\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              26   84.288    0.005       0.0135     2.71e-05       0.0135       0.0689       0.0898       0.0587       0.0892       0.0739       0.0758        0.113       0.0943        0.311      0.00324\n",
            "! Validation         26   84.288    0.005       0.0125     2.13e-06       0.0125       0.0653       0.0864       0.0538       0.0883        0.071       0.0702        0.112       0.0911       0.0917     0.000956\n",
            "Wall time: 84.28849297099998\n",
            "! Best model       26    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     27     1       0.0134       0.0134     4.87e-06       0.0687       0.0894       0.0579       0.0903       0.0741       0.0729        0.116       0.0943        0.159      0.00166\n",
            "     27     2       0.0111       0.0109     0.000205         0.06       0.0808       0.0497       0.0806       0.0651       0.0643        0.106       0.0853         1.06       0.0111\n",
            "     27     3       0.0147       0.0145     0.000228        0.072       0.0931       0.0602       0.0957       0.0779       0.0767        0.119        0.098         1.11       0.0116\n",
            "     27     4       0.0226       0.0226     6.56e-05       0.0929        0.116       0.0869        0.105       0.0959        0.109        0.129        0.119        0.597      0.00622\n",
            "     27     5       0.0159       0.0158     2.45e-05       0.0754       0.0974       0.0672       0.0919       0.0795       0.0854        0.118        0.102        0.358      0.00373\n",
            "     27     6       0.0142       0.0141     2.59e-05        0.068        0.092       0.0545       0.0949       0.0747       0.0751        0.119       0.0969        0.376      0.00392\n",
            "     27     7       0.0148       0.0148     2.15e-06       0.0738       0.0942       0.0661       0.0893       0.0777       0.0836        0.112        0.098       0.0961        0.001\n",
            "     27     8       0.0194       0.0193     5.35e-05       0.0852        0.108       0.0755        0.105         0.09        0.093        0.132        0.112        0.533      0.00555\n",
            "     27     9       0.0165       0.0165     1.47e-05       0.0765       0.0994       0.0658       0.0979       0.0819       0.0838        0.125        0.104        0.241      0.00251\n",
            "     27    10       0.0113       0.0113     5.81e-06       0.0624       0.0822       0.0514       0.0842       0.0678       0.0668        0.106       0.0866        0.147      0.00154\n",
            "     27    11       0.0177       0.0177     4.45e-05       0.0813        0.103       0.0724       0.0991       0.0858       0.0908        0.124        0.107        0.483      0.00503\n",
            "     27    12       0.0146       0.0146     1.52e-06       0.0723       0.0935       0.0647       0.0877       0.0762       0.0832        0.111       0.0972       0.0732     0.000763\n",
            "     27    13       0.0117       0.0117     2.32e-05       0.0646       0.0836       0.0522       0.0894       0.0708       0.0664         0.11       0.0883         0.34      0.00354\n",
            "     27    14       0.0181        0.018     3.82e-05       0.0819        0.104       0.0763       0.0931       0.0847       0.0958        0.119        0.107        0.452      0.00471\n",
            "     27    15       0.0173       0.0172     7.16e-05       0.0797        0.102        0.072       0.0952       0.0836       0.0896        0.122        0.106        0.625      0.00651\n",
            "     27    16       0.0109       0.0109     5.57e-05       0.0611       0.0807       0.0511       0.0812       0.0661       0.0662        0.104       0.0851        0.549      0.00572\n",
            "     27    17       0.0122       0.0122     6.14e-06       0.0669       0.0853       0.0581       0.0844       0.0713       0.0737        0.105       0.0893        0.176      0.00183\n",
            "     27    18       0.0155       0.0155     4.71e-06       0.0766       0.0963       0.0651       0.0997       0.0824       0.0804        0.122        0.101        0.152      0.00158\n",
            "     27    19       0.0139       0.0139      2.4e-05       0.0693       0.0912       0.0588       0.0904       0.0746       0.0777        0.114       0.0956        0.364      0.00379\n",
            "     27    20       0.0141        0.014     4.71e-05       0.0698       0.0916       0.0574       0.0946        0.076       0.0748        0.118       0.0966        0.494      0.00515\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     27     1       0.0131       0.0131     3.42e-06        0.067       0.0886       0.0552       0.0906       0.0729       0.0721        0.115       0.0933        0.113      0.00118\n",
            "     27     2       0.0127       0.0127     1.42e-06       0.0653       0.0871       0.0531       0.0896       0.0714       0.0703        0.113       0.0919        0.079     0.000823\n",
            "     27     3       0.0112       0.0112     2.27e-06       0.0622       0.0819       0.0511       0.0845       0.0678       0.0659        0.107       0.0864       0.0989      0.00103\n",
            "     27     4       0.0122       0.0122      1.8e-06       0.0645       0.0855       0.0536       0.0862       0.0699       0.0697        0.111       0.0902        0.088     0.000917\n",
            "     27     5       0.0118       0.0118     1.75e-06       0.0633       0.0839        0.052       0.0857       0.0689        0.068        0.109       0.0885       0.0798     0.000831\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              27   87.336    0.005       0.0149     4.73e-05        0.015       0.0729       0.0946       0.0632       0.0924       0.0778       0.0812        0.117        0.099         0.42      0.00437\n",
            "! Validation         27   87.336    0.005       0.0122     2.13e-06       0.0122       0.0644       0.0854        0.053       0.0873       0.0702       0.0692        0.111       0.0901       0.0917     0.000956\n",
            "Wall time: 87.33689786999992\n",
            "! Best model       27    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     28     1       0.0165       0.0165     1.94e-06       0.0755       0.0995       0.0637       0.0991       0.0814       0.0842        0.125        0.104       0.0924     0.000962\n",
            "     28     2       0.0132       0.0132     3.72e-05       0.0678       0.0889       0.0569       0.0896       0.0732       0.0745        0.112       0.0934        0.431      0.00449\n",
            "     28     3       0.0128       0.0128     1.06e-06       0.0659       0.0874        0.056       0.0857       0.0708       0.0734         0.11       0.0919       0.0611     0.000637\n",
            "     28     4       0.0153       0.0153     5.09e-05       0.0747       0.0955       0.0646        0.095       0.0798       0.0819        0.118          0.1        0.516      0.00537\n",
            "     28     5       0.0155       0.0155     2.31e-05       0.0748       0.0962       0.0641       0.0962       0.0802       0.0811        0.121        0.101        0.348      0.00363\n",
            "     28     6       0.0126       0.0126     7.19e-06       0.0661       0.0869       0.0552       0.0878       0.0715       0.0712        0.112       0.0916        0.178      0.00185\n",
            "     28     7       0.0119       0.0118      3.8e-05       0.0657       0.0841       0.0547       0.0877       0.0712       0.0694        0.108       0.0886        0.449      0.00468\n",
            "     28     8       0.0133       0.0132     9.07e-05       0.0684       0.0889       0.0572       0.0907       0.0739       0.0726        0.115       0.0937          0.7      0.00729\n",
            "     28     9       0.0119       0.0119     1.77e-06       0.0646       0.0844       0.0515       0.0907       0.0711       0.0651        0.114       0.0893       0.0838     0.000873\n",
            "     28    10       0.0102       0.0102     3.09e-05       0.0585        0.078       0.0485       0.0784       0.0635       0.0642          0.1       0.0822        0.402      0.00418\n",
            "     28    11       0.0108       0.0108      1.5e-05       0.0611       0.0804       0.0519       0.0794       0.0657       0.0677        0.101       0.0845         0.27      0.00281\n",
            "     28    12       0.0119       0.0119     2.18e-06       0.0649       0.0843       0.0538       0.0871       0.0705       0.0675        0.111        0.089        0.092     0.000958\n",
            "     28    13         0.01         0.01     2.48e-05       0.0586       0.0774       0.0487       0.0785       0.0636       0.0634       0.0997       0.0815         0.36      0.00375\n",
            "     28    14      0.00913      0.00913     5.97e-06       0.0564       0.0739       0.0461       0.0769       0.0615       0.0585       0.0976       0.0781        0.172      0.00179\n",
            "     28    15       0.0113       0.0113     7.51e-05       0.0631       0.0821       0.0517        0.086       0.0689       0.0667        0.106       0.0865        0.637      0.00664\n",
            "     28    16       0.0114       0.0114      7.2e-06       0.0628       0.0825       0.0529       0.0827       0.0678       0.0683        0.105       0.0868        0.183      0.00191\n",
            "     28    17      0.00979      0.00973     6.06e-05       0.0588       0.0763       0.0491       0.0782       0.0637        0.063       0.0976       0.0803        0.572      0.00596\n",
            "     28    18       0.0126       0.0126     8.35e-05       0.0665       0.0867       0.0555       0.0885        0.072       0.0714        0.111       0.0913        0.679      0.00707\n",
            "     28    19       0.0135       0.0135     1.96e-06       0.0676       0.0898       0.0563       0.0901       0.0732       0.0743        0.115       0.0945       0.0854     0.000889\n",
            "     28    20        0.011       0.0109     0.000158       0.0617       0.0806       0.0511       0.0829        0.067       0.0658        0.104        0.085         0.93      0.00969\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     28     1       0.0128       0.0128     3.11e-06       0.0662       0.0875       0.0545       0.0897       0.0721       0.0713        0.113       0.0922        0.104      0.00108\n",
            "     28     2       0.0124       0.0124     1.67e-06       0.0646       0.0862       0.0525       0.0888       0.0707       0.0693        0.113       0.0909       0.0835      0.00087\n",
            "     28     3        0.011        0.011     2.12e-06       0.0616       0.0811       0.0505       0.0837       0.0671       0.0652        0.106       0.0856        0.093     0.000968\n",
            "     28     4        0.012        0.012     1.31e-06       0.0638       0.0847       0.0529       0.0854       0.0692       0.0688         0.11       0.0893       0.0764     0.000795\n",
            "     28     5       0.0115       0.0115     2.02e-06       0.0625       0.0829       0.0514       0.0847       0.0681       0.0672        0.108       0.0874       0.0865     0.000901\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              28   90.381    0.005       0.0122     3.58e-05       0.0122       0.0652       0.0855       0.0545       0.0866       0.0705       0.0705        0.109       0.0899        0.362      0.00377\n",
            "! Validation         28   90.381    0.005       0.0119     2.05e-06       0.0119       0.0637       0.0845       0.0524       0.0865       0.0694       0.0684         0.11       0.0891       0.0886     0.000923\n",
            "Wall time: 90.38169640199999\n",
            "! Best model       28    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     29     1       0.0108       0.0107      7.4e-05        0.061       0.0801       0.0505       0.0819       0.0662       0.0653        0.103       0.0844        0.637      0.00664\n",
            "     29     2       0.0137       0.0137      2.6e-06         0.07       0.0905       0.0597       0.0906       0.0752       0.0763        0.114       0.0949       0.0953     0.000993\n",
            "     29     3       0.0116       0.0115     3.78e-05       0.0628        0.083       0.0525       0.0836        0.068       0.0681        0.107       0.0874        0.442       0.0046\n",
            "     29     4      0.00982       0.0097     0.000118       0.0583       0.0762       0.0475       0.0798       0.0636       0.0612       0.0997       0.0804          0.8      0.00834\n",
            "     29     5       0.0122       0.0122     2.73e-05       0.0658       0.0854       0.0547       0.0879       0.0713       0.0681        0.112       0.0902        0.362      0.00377\n",
            "     29     6       0.0117       0.0116      6.3e-05       0.0631       0.0835       0.0526       0.0842       0.0684       0.0679        0.108        0.088        0.573      0.00597\n",
            "     29     7      0.00941      0.00933     7.38e-05        0.057       0.0747       0.0463       0.0783       0.0623        0.059        0.099        0.079        0.635      0.00662\n",
            "     29     8       0.0102       0.0102      1.1e-06       0.0597       0.0781       0.0487       0.0819       0.0653        0.062        0.103       0.0825       0.0568     0.000592\n",
            "     29     9       0.0144       0.0144     2.05e-05       0.0711       0.0928       0.0608       0.0919       0.0763       0.0777        0.117       0.0976        0.315      0.00328\n",
            "     29    10       0.0131       0.0131     1.58e-05       0.0665       0.0887       0.0554       0.0886        0.072       0.0728        0.114       0.0934        0.291      0.00304\n",
            "     29    11       0.0127       0.0127     7.56e-06       0.0655       0.0873       0.0539       0.0888       0.0713       0.0731         0.11       0.0917        0.195      0.00203\n",
            "     29    12       0.0148       0.0147     4.93e-05       0.0725       0.0939       0.0661       0.0853       0.0757       0.0856        0.109       0.0971         0.52      0.00542\n",
            "     29    13       0.0118       0.0118     3.08e-06       0.0663       0.0839       0.0584       0.0821       0.0702       0.0722        0.103       0.0878       0.0898     0.000936\n",
            "     29    14      0.00951       0.0095     1.14e-05       0.0572       0.0754       0.0472       0.0771       0.0622       0.0601       0.0992       0.0796         0.23       0.0024\n",
            "     29    15       0.0163       0.0163     3.83e-05       0.0768       0.0986       0.0661       0.0983       0.0822       0.0839        0.123        0.103        0.454      0.00473\n",
            "     29    16       0.0182       0.0182     6.73e-05       0.0812        0.104       0.0718          0.1       0.0859       0.0911        0.127        0.109        0.606      0.00632\n",
            "     29    17       0.0126       0.0126      5.7e-05       0.0663       0.0867        0.056       0.0867       0.0714       0.0722         0.11       0.0912        0.556      0.00579\n",
            "     29    18       0.0107       0.0106      0.00011       0.0603       0.0797       0.0487       0.0834       0.0661        0.063        0.105       0.0842        0.768        0.008\n",
            "     29    19       0.0127       0.0127     1.08e-06        0.069       0.0873       0.0629       0.0813       0.0721       0.0783        0.103       0.0907       0.0748     0.000779\n",
            "     29    20       0.0141       0.0141     2.33e-05       0.0705       0.0917       0.0599       0.0916       0.0757       0.0757        0.117       0.0966        0.355       0.0037\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     29     1       0.0125       0.0125     3.12e-06       0.0655       0.0865       0.0538       0.0888       0.0713       0.0704        0.112       0.0912        0.106       0.0011\n",
            "     29     2       0.0121       0.0121     1.35e-06       0.0639       0.0852       0.0518        0.088       0.0699       0.0683        0.111       0.0899       0.0757     0.000788\n",
            "     29     3       0.0108       0.0108     2.07e-06       0.0609       0.0803       0.0499       0.0829       0.0664       0.0643        0.105       0.0847       0.0933     0.000971\n",
            "     29     4       0.0117       0.0117     1.45e-06       0.0631       0.0838       0.0523       0.0847       0.0685       0.0679        0.109       0.0884       0.0795     0.000828\n",
            "     29     5       0.0112       0.0112     1.82e-06       0.0618        0.082       0.0508       0.0838       0.0673       0.0664        0.107       0.0865         0.08     0.000833\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              29   93.417    0.005       0.0125     4.01e-05       0.0125        0.066       0.0864        0.056       0.0862       0.0711       0.0722        0.109       0.0908        0.403       0.0042\n",
            "! Validation         29   93.417    0.005       0.0117     1.96e-06       0.0117        0.063       0.0836       0.0517       0.0856       0.0687       0.0675        0.109       0.0882       0.0868     0.000904\n",
            "Wall time: 93.41748802199993\n",
            "! Best model       29    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     30     1       0.0131       0.0131     7.15e-05       0.0678       0.0884       0.0555       0.0924        0.074       0.0714        0.115       0.0933        0.625      0.00651\n",
            "     30     2       0.0109       0.0109     5.66e-06        0.062       0.0806        0.054       0.0781        0.066       0.0684        0.101       0.0845        0.159      0.00166\n",
            "     30     3       0.0138       0.0138     4.35e-05       0.0702       0.0909       0.0597       0.0912       0.0754       0.0744        0.117       0.0957        0.485      0.00505\n",
            "     30     4       0.0132       0.0132     1.05e-05       0.0664        0.089       0.0535       0.0923       0.0729       0.0713        0.117       0.0939        0.224      0.00234\n",
            "     30     5      0.00987      0.00987     2.12e-07       0.0583       0.0769       0.0473       0.0801       0.0637        0.061        0.101       0.0812       0.0256     0.000267\n",
            "     30     6       0.0107       0.0107     1.23e-05       0.0616       0.0801       0.0515       0.0818       0.0666       0.0665        0.102       0.0843         0.25       0.0026\n",
            "     30     7      0.00952      0.00952      2.5e-06       0.0584       0.0755       0.0494       0.0763       0.0628       0.0619        0.097       0.0795       0.0857     0.000893\n",
            "     30     8       0.0109       0.0109     2.82e-06       0.0617       0.0807       0.0525         0.08       0.0663       0.0677        0.102       0.0848       0.0977      0.00102\n",
            "     30     9       0.0115       0.0115     1.67e-06        0.063       0.0828       0.0514       0.0861       0.0687       0.0665        0.108       0.0874       0.0797      0.00083\n",
            "     30    10        0.012       0.0119     5.52e-05       0.0628       0.0844       0.0511       0.0863       0.0687       0.0673        0.111       0.0892        0.551      0.00574\n",
            "     30    11       0.0125       0.0125     3.44e-05        0.066       0.0864       0.0543       0.0893       0.0718       0.0702        0.112       0.0911        0.432       0.0045\n",
            "     30    12       0.0113       0.0113     4.54e-06       0.0622       0.0823       0.0519       0.0828       0.0673       0.0683        0.105       0.0866        0.138      0.00143\n",
            "     30    13       0.0106       0.0105     1.67e-05       0.0607       0.0794        0.049       0.0841       0.0665        0.063        0.105       0.0839         0.27      0.00281\n",
            "     30    14       0.0104       0.0103     6.84e-05       0.0606       0.0786       0.0523       0.0772       0.0647       0.0673       0.0974       0.0823        0.613      0.00638\n",
            "     30    15       0.0112       0.0112     5.18e-06       0.0626       0.0818       0.0525       0.0828       0.0677       0.0677        0.104       0.0861        0.143      0.00149\n",
            "     30    16       0.0119       0.0119     8.11e-07       0.0641       0.0844       0.0553       0.0818       0.0686       0.0711        0.106       0.0886       0.0625     0.000651\n",
            "     30    17       0.0164       0.0164     1.44e-06       0.0789       0.0992       0.0695       0.0978       0.0837       0.0855        0.122        0.104       0.0746     0.000777\n",
            "     30    18       0.0153       0.0152     6.15e-05       0.0735       0.0954       0.0615       0.0976       0.0796       0.0772        0.124        0.101         0.58      0.00605\n",
            "     30    19       0.0106       0.0106     8.86e-06       0.0606       0.0796       0.0495       0.0829       0.0662       0.0639        0.104        0.084        0.205      0.00214\n",
            "     30    20       0.0137       0.0137     6.71e-06       0.0704       0.0905        0.059       0.0932       0.0761       0.0742        0.116       0.0953        0.147      0.00154\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     30     1       0.0122       0.0122     3.05e-06       0.0648       0.0856       0.0531        0.088       0.0706       0.0695        0.111       0.0902        0.105       0.0011\n",
            "     30     2       0.0119       0.0119     1.29e-06       0.0633       0.0843       0.0513       0.0872       0.0693       0.0675        0.111        0.089       0.0732     0.000763\n",
            "     30     3       0.0106       0.0106     1.98e-06       0.0603       0.0796       0.0493       0.0822       0.0658       0.0636        0.104        0.084       0.0917     0.000955\n",
            "     30     4       0.0115       0.0115     1.39e-06       0.0624       0.0831       0.0516       0.0841       0.0679       0.0671        0.108       0.0876       0.0771     0.000804\n",
            "     30     5        0.011        0.011     1.83e-06       0.0611       0.0812       0.0503       0.0828       0.0666       0.0658        0.105       0.0856       0.0794     0.000827\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              30   96.471    0.005       0.0119     2.07e-05        0.012       0.0646       0.0846        0.054       0.0857       0.0699       0.0695        0.109       0.0891        0.262      0.00273\n",
            "! Validation         30   96.471    0.005       0.0114     1.91e-06       0.0114       0.0624       0.0828       0.0511       0.0849        0.068       0.0667        0.108       0.0873       0.0853     0.000889\n",
            "Wall time: 96.47196906699992\n",
            "! Best model       30    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     31     1       0.0147       0.0147     4.98e-06       0.0726       0.0937       0.0641       0.0896       0.0769       0.0813        0.114       0.0979        0.161      0.00167\n",
            "     31     2       0.0125       0.0125     2.32e-05       0.0672       0.0864       0.0583       0.0849       0.0716        0.075        0.106       0.0903        0.348      0.00362\n",
            "     31     3       0.0101         0.01     3.17e-05       0.0585       0.0775       0.0482       0.0791       0.0637       0.0619        0.102       0.0818        0.416      0.00433\n",
            "     31     4       0.0113       0.0113     1.06e-05       0.0635       0.0823       0.0554       0.0795       0.0675       0.0708        0.101       0.0861        0.236      0.00246\n",
            "     31     5       0.0108       0.0107     1.03e-05       0.0616       0.0802       0.0503       0.0844       0.0673       0.0642        0.105       0.0847        0.198      0.00207\n",
            "     31     6       0.0096       0.0096      3.6e-06       0.0589       0.0758       0.0498       0.0772       0.0635       0.0626       0.0969       0.0798        0.109      0.00114\n",
            "     31     7       0.0121       0.0121     5.63e-05       0.0648       0.0851        0.054       0.0864       0.0702       0.0702        0.109       0.0895        0.555      0.00578\n",
            "     31     8       0.0106       0.0106     3.96e-05       0.0609       0.0796       0.0487       0.0852       0.0669       0.0624        0.106       0.0842        0.451       0.0047\n",
            "     31     9       0.0106       0.0105     3.69e-06       0.0604       0.0795       0.0495       0.0822       0.0659       0.0637        0.104       0.0839        0.138      0.00143\n",
            "     31    10       0.0121       0.0121     2.53e-06       0.0655       0.0851       0.0559       0.0849       0.0704       0.0703        0.109       0.0896        0.103      0.00107\n",
            "     31    11       0.0101         0.01     3.85e-05       0.0592       0.0775       0.0493        0.079       0.0641       0.0627        0.101       0.0817        0.454      0.00473\n",
            "     31    12       0.0119       0.0119     3.39e-05       0.0637       0.0843       0.0526       0.0858       0.0692       0.0693        0.108       0.0888        0.428      0.00446\n",
            "     31    13       0.0107       0.0107     7.57e-07       0.0608       0.0801       0.0507       0.0812       0.0659       0.0655        0.103       0.0844       0.0572     0.000596\n",
            "     31    14       0.0101       0.0101     9.36e-06       0.0575       0.0778       0.0448        0.083       0.0639       0.0601        0.105       0.0824        0.202      0.00211\n",
            "     31    15      0.00989      0.00987      1.8e-05       0.0581       0.0769       0.0465       0.0814        0.064       0.0605        0.102       0.0813        0.308      0.00321\n",
            "     31    16      0.00973      0.00973     2.59e-06       0.0574       0.0763       0.0474       0.0773       0.0624       0.0613       0.0997       0.0805       0.0961        0.001\n",
            "     31    17       0.0107       0.0107     2.13e-05       0.0603         0.08        0.049       0.0829       0.0659       0.0644        0.104       0.0844        0.334      0.00348\n",
            "     31    18      0.00932      0.00932     4.46e-07       0.0573       0.0747       0.0476       0.0767       0.0621       0.0605        0.097       0.0787       0.0363     0.000378\n",
            "     31    19       0.0109       0.0109     2.34e-05       0.0612       0.0809       0.0499       0.0839       0.0669       0.0646        0.106       0.0854        0.341      0.00355\n",
            "     31    20         0.01         0.01     1.31e-06       0.0589       0.0774       0.0478        0.081       0.0644       0.0613        0.102       0.0818       0.0596     0.000621\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     31     1        0.012        0.012      2.9e-06       0.0641       0.0847       0.0525       0.0873       0.0699       0.0686         0.11       0.0893          0.1      0.00105\n",
            "     31     2       0.0116       0.0116     1.32e-06       0.0626       0.0835       0.0506       0.0865       0.0686       0.0666         0.11       0.0881       0.0719     0.000749\n",
            "     31     3       0.0104       0.0104     1.85e-06       0.0597       0.0789       0.0488       0.0816       0.0652       0.0629        0.104       0.0833       0.0868     0.000904\n",
            "     31     4       0.0113       0.0113     1.22e-06       0.0618       0.0823        0.051       0.0835       0.0672       0.0662        0.107       0.0868       0.0747     0.000778\n",
            "     31     5       0.0108       0.0108     1.93e-06       0.0605       0.0804       0.0497        0.082       0.0659       0.0651        0.104       0.0847       0.0811     0.000844\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              31   99.526    0.005       0.0109     1.68e-05       0.0109       0.0614       0.0807        0.051       0.0823       0.0666       0.0659        0.104        0.085        0.252      0.00262\n",
            "! Validation         31   99.526    0.005       0.0112     1.84e-06       0.0112       0.0617       0.0819       0.0505       0.0842       0.0674       0.0659        0.107       0.0865        0.083     0.000864\n",
            "Wall time: 99.52700737800001\n",
            "! Best model       31    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     32     1       0.0108       0.0108      1.2e-05       0.0609       0.0803       0.0504       0.0818       0.0661       0.0669        0.102       0.0844        0.245      0.00256\n",
            "     32     2       0.0086      0.00858     2.07e-05       0.0555       0.0717       0.0442       0.0782       0.0612       0.0558       0.0958       0.0758        0.332      0.00346\n",
            "     32     3       0.0129       0.0129     1.29e-06       0.0676        0.088       0.0579       0.0869       0.0724       0.0743         0.11       0.0923       0.0715     0.000745\n",
            "     32     4       0.0111       0.0111     5.69e-05        0.064       0.0815       0.0566        0.079       0.0678       0.0708       0.0993       0.0851        0.549      0.00571\n",
            "     32     5      0.00982       0.0098      2.4e-05       0.0581       0.0766       0.0465       0.0814       0.0639       0.0593        0.103        0.081        0.363      0.00378\n",
            "     32     6       0.0105       0.0105     5.08e-06        0.061       0.0792       0.0512       0.0805       0.0659        0.065        0.102       0.0834        0.145      0.00151\n",
            "     32     7       0.0128       0.0127     8.46e-05       0.0653       0.0871       0.0533       0.0892       0.0712       0.0706        0.113       0.0918        0.679      0.00707\n",
            "     32     8        0.011       0.0109     0.000126       0.0619       0.0807       0.0513       0.0831       0.0672       0.0647        0.106       0.0851        0.832      0.00867\n",
            "     32     9       0.0112       0.0111     2.63e-05       0.0625       0.0816       0.0526       0.0824       0.0675       0.0676        0.104       0.0859         0.38      0.00396\n",
            "     32    10       0.0118       0.0117     0.000176       0.0639       0.0835       0.0527       0.0863       0.0695       0.0673        0.109       0.0881         0.98       0.0102\n",
            "     32    11       0.0122       0.0121     4.65e-05       0.0655       0.0853       0.0559       0.0846       0.0703       0.0733        0.105       0.0892        0.496      0.00517\n",
            "     32    12       0.0098       0.0098     3.04e-06       0.0586       0.0766       0.0475       0.0809       0.0642       0.0608        0.101       0.0809        0.113      0.00118\n",
            "     32    13      0.00982      0.00974     7.83e-05       0.0583       0.0764       0.0505       0.0737       0.0621       0.0655       0.0945         0.08        0.651      0.00679\n",
            "     32    14       0.0103       0.0102     3.91e-05       0.0604       0.0782       0.0508       0.0795       0.0652       0.0639        0.101       0.0825        0.452       0.0047\n",
            "     32    15       0.0107       0.0107     1.56e-05       0.0599       0.0799       0.0492       0.0813       0.0653       0.0647        0.104       0.0843         0.29      0.00302\n",
            "     32    16      0.00999      0.00994     5.23e-05       0.0586       0.0771       0.0489       0.0781       0.0635       0.0618        0.101       0.0814        0.527      0.00549\n",
            "     32    17       0.0114       0.0114     8.34e-06       0.0618       0.0824       0.0498       0.0858       0.0678        0.064         0.11       0.0872        0.209      0.00218\n",
            "     32    18        0.013        0.013     2.89e-05        0.068       0.0881       0.0599       0.0842        0.072       0.0764        0.108        0.092        0.396      0.00413\n",
            "     32    19       0.0106       0.0105     3.97e-05       0.0614       0.0794       0.0527       0.0788       0.0658       0.0679       0.0983       0.0831        0.454      0.00473\n",
            "     32    20       0.0118       0.0118     1.02e-05       0.0626       0.0839       0.0507       0.0865       0.0686       0.0672         0.11       0.0885        0.226      0.00235\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     32     1       0.0117       0.0117     2.91e-06       0.0634       0.0838       0.0518       0.0865       0.0692       0.0678        0.109       0.0884        0.104      0.00109\n",
            "     32     2       0.0114       0.0114     1.19e-06        0.062       0.0826       0.0501       0.0858       0.0679       0.0658        0.109       0.0873       0.0689     0.000718\n",
            "     32     3       0.0102       0.0102      1.9e-06       0.0592       0.0782       0.0483        0.081       0.0646       0.0622        0.103       0.0826        0.089     0.000927\n",
            "     32     4       0.0111       0.0111     1.32e-06       0.0612       0.0815       0.0504       0.0829       0.0666       0.0655        0.107       0.0861       0.0764     0.000795\n",
            "     32     5       0.0106       0.0106     1.83e-06       0.0599       0.0796       0.0492       0.0812       0.0652       0.0645        0.103       0.0839       0.0773     0.000806\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              32  102.567    0.005        0.011     4.27e-05        0.011       0.0618        0.081       0.0516       0.0821       0.0669       0.0666        0.104       0.0853         0.42      0.00437\n",
            "! Validation         32  102.567    0.005        0.011     1.83e-06        0.011       0.0611       0.0812         0.05       0.0835       0.0667       0.0652        0.106       0.0857       0.0832     0.000866\n",
            "Wall time: 102.567964695\n",
            "! Best model       32    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     33     1       0.0105       0.0103     0.000178       0.0613       0.0784       0.0531       0.0777       0.0654        0.067       0.0974       0.0822        0.989       0.0103\n",
            "     33     2       0.0132       0.0132     2.74e-06       0.0667        0.089       0.0519       0.0964       0.0741        0.066        0.123       0.0943        0.111      0.00115\n",
            "     33     3       0.0109       0.0109     3.47e-07       0.0624       0.0807       0.0519       0.0835       0.0677       0.0641        0.106       0.0852         0.04     0.000417\n",
            "     33     4      0.00956       0.0095     6.57e-05       0.0577       0.0754       0.0477       0.0776       0.0626       0.0604       0.0988       0.0796        0.598      0.00623\n",
            "     33     5       0.0134       0.0134     1.81e-05       0.0672       0.0895       0.0533       0.0949       0.0741       0.0694         0.12       0.0947        0.311      0.00324\n",
            "     33     6        0.017       0.0169      5.6e-05       0.0795        0.101       0.0726       0.0933       0.0829       0.0907        0.118        0.104        0.554      0.00577\n",
            "     33     7       0.0148       0.0148     8.37e-06       0.0741       0.0941       0.0677       0.0869       0.0773       0.0847        0.111       0.0976        0.206      0.00214\n",
            "     33     8         0.01         0.01     4.61e-06       0.0592       0.0775       0.0485       0.0804       0.0645       0.0617        0.102       0.0818        0.145      0.00151\n",
            "     33     9       0.0127       0.0127     3.16e-06       0.0675       0.0872       0.0595       0.0834       0.0715       0.0754        0.107       0.0912         0.13      0.00136\n",
            "     33    10       0.0144       0.0144     1.11e-05        0.071       0.0928       0.0593       0.0943       0.0768       0.0762        0.119       0.0978        0.237      0.00247\n",
            "     33    11       0.0113       0.0112     5.92e-05       0.0614        0.082       0.0499       0.0843       0.0671       0.0644        0.109       0.0867         0.57      0.00593\n",
            "     33    12        0.011        0.011     2.46e-05       0.0616        0.081       0.0485       0.0878       0.0681       0.0627        0.109       0.0857        0.362      0.00377\n",
            "     33    13       0.0131       0.0131     5.65e-05       0.0682       0.0885       0.0574       0.0899       0.0736       0.0739        0.112        0.093        0.556      0.00579\n",
            "     33    14       0.0123       0.0123     1.27e-05       0.0668       0.0858       0.0591       0.0822       0.0707       0.0743        0.105       0.0897        0.239      0.00249\n",
            "     33    15      0.00948      0.00947     3.29e-06       0.0575       0.0753       0.0478       0.0768       0.0623       0.0614       0.0973       0.0794        0.115       0.0012\n",
            "     33    16      0.00964      0.00963     9.41e-06       0.0587       0.0759       0.0497       0.0767       0.0632       0.0637       0.0958       0.0797         0.22      0.00229\n",
            "     33    17       0.0111       0.0111     2.43e-05       0.0621       0.0813       0.0503       0.0858        0.068       0.0663        0.105       0.0857        0.341      0.00355\n",
            "     33    18       0.0113       0.0113     4.09e-06        0.062       0.0821       0.0506       0.0848       0.0677       0.0668        0.106       0.0866         0.13      0.00135\n",
            "     33    19       0.0113       0.0113     5.82e-06        0.063       0.0822       0.0524       0.0842       0.0683        0.068        0.105       0.0865        0.155      0.00162\n",
            "     33    20      0.00992      0.00991     5.25e-07       0.0575        0.077       0.0477       0.0771       0.0624       0.0633        0.099       0.0811         0.04     0.000417\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     33     1       0.0115       0.0115     2.84e-06       0.0628        0.083       0.0512       0.0859       0.0685        0.067        0.108       0.0875        0.102      0.00106\n",
            "     33     2       0.0112       0.0112     1.21e-06       0.0614       0.0819       0.0496       0.0851       0.0673       0.0652        0.108       0.0865       0.0687     0.000715\n",
            "     33     3         0.01         0.01     1.83e-06       0.0587       0.0775       0.0478       0.0804       0.0641       0.0616        0.102       0.0819       0.0855     0.000891\n",
            "     33     4       0.0109       0.0109      1.2e-06       0.0607       0.0809       0.0498       0.0824       0.0661       0.0647        0.106       0.0854       0.0743     0.000774\n",
            "     33     5       0.0104       0.0104     1.81e-06       0.0593       0.0789       0.0488       0.0805       0.0646       0.0639        0.103       0.0832       0.0766     0.000798\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              33  105.618    0.005       0.0118     2.74e-05       0.0118       0.0643       0.0841       0.0539       0.0849       0.0694       0.0695        0.108       0.0885        0.302      0.00315\n",
            "! Validation         33  105.618    0.005       0.0108     1.78e-06       0.0108       0.0606       0.0805       0.0494       0.0828       0.0661       0.0645        0.105       0.0849       0.0813     0.000847\n",
            "Wall time: 105.61824654599991\n",
            "! Best model       33    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     34     1       0.0102       0.0102      1.5e-06       0.0596       0.0782       0.0497       0.0794       0.0646        0.063        0.102       0.0825       0.0682      0.00071\n",
            "     34     2       0.0124       0.0123     4.17e-06       0.0658        0.086       0.0546       0.0883       0.0714       0.0696        0.112       0.0907        0.142      0.00148\n",
            "     34     3      0.00926      0.00926     1.16e-06       0.0582       0.0744       0.0498       0.0752       0.0625       0.0621       0.0944       0.0782       0.0537     0.000559\n",
            "     34     4       0.0113       0.0113     6.52e-06       0.0618       0.0823       0.0502        0.085       0.0676       0.0669        0.107       0.0868        0.169      0.00176\n",
            "     34     5      0.00993      0.00993     1.99e-06       0.0578       0.0771       0.0483       0.0767       0.0625       0.0644       0.0977        0.081       0.0885     0.000922\n",
            "     34     6       0.0102       0.0102      3.3e-05       0.0601       0.0781       0.0511       0.0781       0.0646       0.0653       0.0988       0.0821        0.425      0.00443\n",
            "     34     7      0.00904      0.00903     4.58e-06       0.0555       0.0735       0.0443       0.0778        0.061       0.0575       0.0981       0.0778        0.144       0.0015\n",
            "     34     8      0.00976      0.00975     6.37e-06       0.0583       0.0764       0.0478       0.0793       0.0636       0.0619       0.0993       0.0806        0.174      0.00181\n",
            "     34     9      0.00991      0.00991     1.33e-06       0.0587        0.077       0.0475       0.0811       0.0643       0.0607        0.102       0.0814       0.0826     0.000861\n",
            "     34    10      0.00896      0.00895     8.43e-06        0.056       0.0732       0.0463       0.0754       0.0609       0.0593       0.0951       0.0772        0.195      0.00203\n",
            "     34    11       0.0105       0.0105     5.91e-06       0.0606       0.0792       0.0511       0.0795       0.0653       0.0644        0.103       0.0835        0.161      0.00168\n",
            "     34    12       0.0102       0.0102        3e-06       0.0592        0.078       0.0471       0.0833       0.0652       0.0615        0.103       0.0824       0.0957     0.000997\n",
            "     34    13       0.0106       0.0106     4.86e-07       0.0597       0.0795       0.0483       0.0825       0.0654       0.0634        0.104       0.0839        0.042     0.000437\n",
            "     34    14       0.0106       0.0106     1.04e-06        0.059       0.0798       0.0469       0.0832        0.065       0.0615        0.107       0.0844       0.0674     0.000702\n",
            "     34    15      0.00892      0.00892     1.37e-06       0.0566       0.0731       0.0466       0.0765       0.0615       0.0593       0.0948        0.077        0.065     0.000677\n",
            "     34    16      0.00965      0.00965     3.97e-06       0.0571        0.076        0.045       0.0812       0.0631       0.0579        0.103       0.0805        0.136      0.00141\n",
            "     34    17       0.0104       0.0104     2.43e-05        0.059       0.0789       0.0485         0.08       0.0643       0.0617        0.105       0.0834        0.363      0.00378\n",
            "     34    18       0.0121       0.0121     1.14e-06       0.0652        0.085       0.0561       0.0835       0.0698       0.0715        0.107       0.0892       0.0494     0.000515\n",
            "     34    19        0.012        0.012     3.52e-06       0.0646       0.0848       0.0547       0.0845       0.0696       0.0702        0.108       0.0892        0.119      0.00124\n",
            "     34    20      0.00992      0.00988     4.69e-05       0.0587       0.0769        0.048       0.0802       0.0641       0.0613        0.101       0.0812        0.508      0.00529\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     34     1       0.0113       0.0113     2.75e-06       0.0622       0.0822       0.0507       0.0852       0.0679       0.0662        0.107       0.0867       0.0997      0.00104\n",
            "     34     2        0.011        0.011     1.19e-06       0.0608       0.0812       0.0491       0.0844       0.0667       0.0644        0.107       0.0857       0.0663     0.000691\n",
            "     34     3      0.00989      0.00989     1.74e-06       0.0582       0.0769       0.0473         0.08       0.0636        0.061        0.102       0.0813       0.0816      0.00085\n",
            "     34     4       0.0107       0.0107     1.17e-06       0.0601       0.0802       0.0493       0.0818       0.0655        0.064        0.105       0.0847       0.0732     0.000763\n",
            "     34     5       0.0102       0.0102     1.86e-06       0.0588       0.0782       0.0483       0.0797        0.064       0.0633        0.102       0.0825       0.0752     0.000783\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              34  108.677    0.005       0.0103     8.03e-06       0.0103       0.0596       0.0785       0.0491       0.0805       0.0648       0.0633        0.102       0.0828        0.157      0.00164\n",
            "! Validation         34  108.677    0.005       0.0106     1.74e-06       0.0106         0.06       0.0798       0.0489       0.0822       0.0656       0.0638        0.105       0.0842       0.0792     0.000825\n",
            "Wall time: 108.6774715549999\n",
            "! Best model       34    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     35     1       0.0098      0.00973     6.63e-05       0.0575       0.0763       0.0461       0.0802       0.0632       0.0604        0.101       0.0806        0.599      0.00624\n",
            "     35     2      0.00995      0.00995     2.04e-06       0.0591       0.0772       0.0508       0.0756       0.0632       0.0649       0.0972        0.081       0.0828     0.000863\n",
            "     35     3       0.0103       0.0103     2.56e-05        0.059       0.0786       0.0477       0.0816       0.0646       0.0632        0.103       0.0829        0.372      0.00387\n",
            "     35     4       0.0098      0.00974     5.32e-05       0.0577       0.0764       0.0476       0.0779       0.0628       0.0619       0.0992       0.0805        0.536      0.00559\n",
            "     35     5       0.0115       0.0115     8.44e-06       0.0627       0.0828       0.0511       0.0859       0.0685       0.0663        0.109       0.0875        0.191      0.00199\n",
            "     35     6        0.011       0.0109      8.1e-05       0.0613       0.0807       0.0504        0.083       0.0667       0.0647        0.106       0.0852        0.666      0.00694\n",
            "     35     7        0.011        0.011     1.26e-06       0.0609        0.081       0.0502       0.0824       0.0663       0.0651        0.106       0.0855       0.0725     0.000755\n",
            "     35     8       0.0101       0.0101     1.57e-05       0.0583       0.0776       0.0471       0.0808       0.0639       0.0616        0.102        0.082        0.287      0.00298\n",
            "     35     9       0.0103       0.0103     1.36e-06       0.0608       0.0785       0.0511         0.08       0.0656       0.0651       0.0999       0.0825       0.0846     0.000881\n",
            "     35    10       0.0132       0.0132     2.28e-06       0.0683       0.0889       0.0577       0.0894       0.0736       0.0744        0.112       0.0935        0.103      0.00107\n",
            "     35    11       0.0133       0.0132      2.7e-05       0.0686        0.089       0.0553        0.095       0.0752       0.0698        0.118       0.0941        0.371      0.00387\n",
            "     35    12        0.013        0.013     1.41e-05       0.0676       0.0882       0.0565       0.0897       0.0731       0.0718        0.114        0.093        0.259       0.0027\n",
            "     35    13       0.0108       0.0108     1.12e-05       0.0609       0.0804       0.0498       0.0829       0.0664       0.0647        0.105       0.0849        0.242      0.00252\n",
            "     35    14      0.00821       0.0082     6.81e-06       0.0544       0.0701       0.0445       0.0741       0.0593       0.0564       0.0914       0.0739        0.189      0.00197\n",
            "     35    15      0.00998      0.00998        2e-07        0.058       0.0773       0.0476       0.0787       0.0632       0.0613        0.102       0.0816       0.0285     0.000297\n",
            "     35    16       0.0107       0.0107     7.06e-05       0.0621       0.0799       0.0525       0.0814       0.0669       0.0661        0.102       0.0841        0.617      0.00642\n",
            "     35    17       0.0125       0.0125     1.88e-06       0.0666       0.0865       0.0588       0.0823       0.0705        0.075        0.106       0.0904       0.0863     0.000899\n",
            "     35    18      0.00934      0.00932     2.19e-05       0.0578       0.0747       0.0486       0.0762       0.0624       0.0615       0.0958       0.0786        0.344      0.00358\n",
            "     35    19      0.00814      0.00809      5.7e-05       0.0535       0.0696       0.0449       0.0707       0.0578       0.0569       0.0897       0.0733        0.553      0.00576\n",
            "     35    20       0.0108       0.0108     1.09e-05       0.0616       0.0804       0.0522       0.0803       0.0662       0.0668        0.102       0.0845        0.222      0.00232\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     35     1       0.0111       0.0111     2.77e-06       0.0616       0.0814       0.0502       0.0846       0.0674       0.0655        0.106       0.0859        0.101      0.00105\n",
            "     35     2       0.0108       0.0108      1.1e-06       0.0603       0.0805       0.0486       0.0837       0.0662       0.0637        0.106        0.085       0.0643     0.000669\n",
            "     35     3      0.00975      0.00975     1.74e-06       0.0577       0.0764       0.0469       0.0795       0.0632       0.0604        0.101       0.0807       0.0829     0.000864\n",
            "     35     4       0.0106       0.0106     1.19e-06       0.0596       0.0796       0.0487       0.0813        0.065       0.0633        0.105        0.084       0.0733     0.000764\n",
            "     35     5         0.01         0.01     1.75e-06       0.0583       0.0775       0.0479       0.0791       0.0635       0.0628        0.101       0.0818       0.0724     0.000754\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              35  111.752    0.005       0.0107     2.39e-05       0.0107       0.0608       0.0799       0.0505       0.0814        0.066       0.0651        0.103       0.0842        0.295      0.00308\n",
            "! Validation         35  111.752    0.005       0.0105     1.71e-06       0.0105       0.0595       0.0791       0.0485       0.0816       0.0651       0.0632        0.104       0.0835       0.0788     0.000821\n",
            "Wall time: 111.752698571\n",
            "! Best model       35    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     36     1       0.0113       0.0111     0.000107       0.0636       0.0817       0.0532       0.0842       0.0687       0.0691        0.102       0.0857        0.766      0.00798\n",
            "     36     2      0.00976       0.0097     6.16e-05       0.0589       0.0762       0.0502       0.0764       0.0633       0.0641       0.0958         0.08        0.581      0.00605\n",
            "     36     3      0.00914      0.00914     2.22e-06       0.0564       0.0739        0.047       0.0752       0.0611       0.0599        0.096        0.078        0.101      0.00106\n",
            "     36     4       0.0125       0.0125     2.13e-05       0.0663       0.0864       0.0586       0.0818       0.0702       0.0757        0.105       0.0901        0.332      0.00346\n",
            "     36     5       0.0113       0.0112     2.24e-05       0.0628        0.082       0.0545       0.0795        0.067         0.07        0.102       0.0859        0.341      0.00355\n",
            "     36     6      0.00947      0.00946     3.18e-06       0.0571       0.0753       0.0457       0.0799       0.0628       0.0591          0.1       0.0796       0.0896     0.000934\n",
            "     36     7       0.0109       0.0109     4.05e-06       0.0621       0.0807       0.0525       0.0815        0.067       0.0682        0.101       0.0847         0.12      0.00125\n",
            "     36     8       0.0107       0.0106     3.14e-05       0.0613       0.0798       0.0501       0.0835       0.0668       0.0641        0.104       0.0842        0.407      0.00424\n",
            "     36     9      0.00897      0.00897     3.91e-06       0.0554       0.0733       0.0427       0.0808       0.0617       0.0537        0.102       0.0777        0.133      0.00138\n",
            "     36    10      0.00885      0.00885     9.48e-07       0.0557       0.0728       0.0455       0.0761       0.0608       0.0572       0.0967       0.0769       0.0643     0.000669\n",
            "     36    11      0.00848      0.00848     6.83e-07        0.054       0.0712        0.044       0.0742       0.0591       0.0556       0.0951       0.0754       0.0447     0.000466\n",
            "     36    12      0.00967      0.00966     7.88e-06       0.0576        0.076       0.0476       0.0774       0.0625       0.0623       0.0979       0.0801        0.202       0.0021\n",
            "     36    13      0.00955      0.00954     1.12e-06       0.0576       0.0756       0.0468       0.0792        0.063       0.0588        0.101         0.08       0.0586      0.00061\n",
            "     36    14      0.00942      0.00941     3.32e-06       0.0562       0.0751       0.0448        0.079       0.0619       0.0575        0.101       0.0795        0.106       0.0011\n",
            "     36    15       0.0089      0.00889     1.33e-05       0.0558       0.0729       0.0459       0.0755       0.0607       0.0573        0.097       0.0771        0.257      0.00268\n",
            "     36    16       0.0117       0.0117     2.28e-06       0.0627       0.0837       0.0511        0.086       0.0685       0.0678        0.109       0.0883       0.0973      0.00101\n",
            "     36    17       0.0118       0.0118      3.4e-05       0.0638        0.084       0.0534       0.0848       0.0691       0.0693        0.108       0.0885         0.43      0.00448\n",
            "     36    18       0.0103       0.0103     1.95e-05       0.0596       0.0785       0.0502       0.0784       0.0643       0.0642        0.101       0.0827        0.309      0.00322\n",
            "     36    19      0.00892      0.00891     1.46e-05       0.0556        0.073       0.0459       0.0749       0.0604       0.0587       0.0954        0.077        0.277      0.00289\n",
            "     36    20       0.0105       0.0105     2.49e-06       0.0608       0.0793       0.0523       0.0778       0.0651       0.0672       0.0992       0.0832        0.098      0.00102\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     36     1       0.0109       0.0109     2.79e-06       0.0611       0.0807       0.0497        0.084       0.0668       0.0649        0.106       0.0852        0.103      0.00107\n",
            "     36     2       0.0106       0.0106     9.67e-07       0.0598       0.0798       0.0481       0.0832       0.0656       0.0631        0.106       0.0844       0.0596     0.000621\n",
            "     36     3      0.00961      0.00961     1.69e-06       0.0573       0.0758       0.0464        0.079       0.0627       0.0598          0.1       0.0801       0.0819     0.000853\n",
            "     36     4       0.0104       0.0104     1.24e-06       0.0591       0.0789       0.0483       0.0808       0.0645       0.0627        0.104       0.0834       0.0734     0.000765\n",
            "     36     5      0.00988      0.00988     1.73e-06       0.0579       0.0769       0.0475       0.0786        0.063       0.0622       0.0999       0.0811       0.0702     0.000731\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              36  114.818    0.005       0.0101     1.78e-05       0.0101       0.0592       0.0777       0.0491       0.0793       0.0642       0.0633        0.101       0.0819        0.241      0.00251\n",
            "! Validation         36  114.818    0.005       0.0103     1.68e-06       0.0103        0.059       0.0785        0.048       0.0811       0.0645       0.0626        0.103       0.0829       0.0776     0.000808\n",
            "Wall time: 114.81918186099995\n",
            "! Best model       36    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     37     1       0.0112       0.0112      3.9e-06       0.0633        0.082       0.0526       0.0848       0.0687       0.0665        0.106       0.0865         0.14      0.00146\n",
            "     37     2       0.0107       0.0107     2.15e-05       0.0609         0.08       0.0478        0.087       0.0674       0.0617        0.108       0.0847        0.323      0.00336\n",
            "     37     3       0.0102       0.0102     9.69e-06       0.0583       0.0782       0.0467       0.0815       0.0641       0.0611        0.104       0.0827        0.226      0.00236\n",
            "     37     4       0.0105       0.0105     5.65e-06       0.0588       0.0794       0.0452        0.086       0.0656       0.0604        0.108       0.0841         0.17      0.00177\n",
            "     37     5       0.0097      0.00965     5.04e-05       0.0572        0.076       0.0465       0.0785       0.0625       0.0606       0.0999       0.0803        0.522      0.00544\n",
            "     37     6      0.00969      0.00967     2.26e-05       0.0574       0.0761       0.0471       0.0781       0.0626       0.0594        0.102       0.0805        0.349      0.00364\n",
            "     37     7       0.0101         0.01     4.82e-05       0.0584       0.0775       0.0473       0.0806        0.064       0.0603        0.104        0.082         0.51      0.00531\n",
            "     37     8      0.00957      0.00947     0.000104       0.0566       0.0753       0.0462       0.0773       0.0618        0.061       0.0978       0.0794        0.751      0.00782\n",
            "     37     9         0.01         0.01     2.76e-05        0.059       0.0774       0.0501       0.0769       0.0635       0.0635       0.0996       0.0815        0.389      0.00405\n",
            "     37    10         0.01         0.01     2.39e-05        0.059       0.0774       0.0482       0.0807       0.0644        0.062        0.101       0.0817        0.358      0.00373\n",
            "     37    11      0.00909      0.00894     0.000148       0.0561       0.0732       0.0457       0.0768       0.0612       0.0586       0.0958       0.0772        0.902      0.00939\n",
            "     37    12         0.01         0.01     8.06e-06       0.0596       0.0775       0.0483       0.0824       0.0653       0.0625        0.101       0.0818        0.202      0.00211\n",
            "     37    13      0.00875      0.00875      7.6e-06       0.0563       0.0724       0.0476       0.0737       0.0607       0.0592       0.0933       0.0762        0.169      0.00176\n",
            "     37    14      0.00871      0.00869     1.72e-05        0.055       0.0721       0.0438       0.0772       0.0605       0.0558       0.0969       0.0763        0.294      0.00307\n",
            "     37    15       0.0086      0.00859     1.02e-05       0.0548       0.0717       0.0447        0.075       0.0599       0.0575       0.0939       0.0757        0.222      0.00232\n",
            "     37    16       0.0107       0.0107     8.94e-06       0.0605         0.08       0.0482        0.085       0.0666        0.063        0.106       0.0846          0.2      0.00209\n",
            "     37    17      0.00854      0.00853     6.21e-06       0.0542       0.0715       0.0454       0.0717       0.0586        0.059       0.0914       0.0752        0.161      0.00167\n",
            "     37    18      0.00855      0.00853     1.39e-05       0.0546       0.0715       0.0442       0.0753       0.0597       0.0562       0.0948       0.0755        0.263      0.00274\n",
            "     37    19      0.00932      0.00931     2.84e-06       0.0569       0.0747       0.0463       0.0781       0.0622       0.0594       0.0983       0.0789        0.104      0.00108\n",
            "     37    20      0.00936      0.00933     3.69e-05       0.0568       0.0747       0.0458       0.0787       0.0622       0.0588       0.0992        0.079        0.448      0.00467\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     37     1       0.0107       0.0107     2.83e-06       0.0606       0.0801       0.0493       0.0834       0.0663       0.0642        0.105       0.0845        0.105      0.00109\n",
            "     37     2       0.0105       0.0105      9.3e-07       0.0592       0.0792       0.0476       0.0825       0.0651       0.0624        0.105       0.0837       0.0584     0.000608\n",
            "     37     3      0.00948      0.00948     1.69e-06       0.0569       0.0753        0.046       0.0785       0.0623       0.0593       0.0999       0.0796       0.0807      0.00084\n",
            "     37     4       0.0103       0.0103     1.26e-06       0.0587       0.0784       0.0478       0.0804       0.0641        0.062        0.104       0.0828       0.0734     0.000765\n",
            "     37     5      0.00972      0.00972     1.69e-06       0.0574       0.0763       0.0471        0.078       0.0626       0.0617       0.0992       0.0804       0.0689     0.000718\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              37  117.873    0.005      0.00965     2.89e-05      0.00968       0.0577        0.076       0.0469       0.0793       0.0631       0.0604          0.1       0.0803        0.335      0.00349\n",
            "! Validation         37  117.873    0.005       0.0101     1.68e-06       0.0101       0.0586       0.0779       0.0476       0.0806       0.0641        0.062        0.103       0.0822       0.0773     0.000805\n",
            "Wall time: 117.8740828199999\n",
            "! Best model       37    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     38     1       0.0086      0.00857     3.47e-05       0.0548       0.0716       0.0444       0.0757       0.0601       0.0566       0.0948       0.0757        0.434      0.00452\n",
            "     38     2       0.0102       0.0102     5.67e-06       0.0587       0.0783       0.0459       0.0842       0.0651       0.0597        0.106       0.0829        0.153       0.0016\n",
            "     38     3       0.0096      0.00956     3.47e-05       0.0579       0.0756       0.0475       0.0787       0.0631       0.0603       0.0995       0.0799        0.433      0.00451\n",
            "     38     4       0.0101         0.01     2.87e-05       0.0586       0.0775       0.0488       0.0784       0.0636       0.0638       0.0994       0.0816        0.391      0.00407\n",
            "     38     5      0.00873      0.00873     1.38e-06       0.0556       0.0723        0.046       0.0748       0.0604       0.0589       0.0934       0.0762       0.0834     0.000869\n",
            "     38     6       0.0103       0.0103     1.19e-06       0.0605       0.0786        0.052       0.0776       0.0648       0.0665       0.0985       0.0825       0.0738     0.000769\n",
            "     38     7      0.00859      0.00859     3.39e-06       0.0541       0.0717       0.0447       0.0727       0.0587       0.0572       0.0942       0.0757        0.134       0.0014\n",
            "     38     8      0.00908      0.00906     1.41e-05       0.0562       0.0737       0.0457       0.0771       0.0614       0.0586       0.0969       0.0778        0.253      0.00264\n",
            "     38     9       0.0103       0.0103     4.26e-05       0.0595       0.0785       0.0493         0.08       0.0646       0.0633        0.102       0.0828        0.469      0.00488\n",
            "     38    10       0.0108       0.0108     1.55e-05       0.0602       0.0803       0.0482       0.0841       0.0662       0.0633        0.106       0.0849        0.284      0.00296\n",
            "     38    11       0.0105       0.0104     5.57e-05       0.0614        0.079       0.0516       0.0811       0.0664       0.0663       0.0996        0.083        0.551      0.00574\n",
            "     38    12      0.00995      0.00993     2.19e-05       0.0597       0.0771       0.0515       0.0763       0.0639       0.0645       0.0976        0.081        0.343      0.00357\n",
            "     38    13       0.0105       0.0105     7.45e-07       0.0607       0.0793       0.0492       0.0838       0.0665       0.0638        0.104       0.0837       0.0482     0.000503\n",
            "     38    14      0.00942      0.00938      3.7e-05       0.0572       0.0749       0.0465       0.0788       0.0626       0.0594       0.0989       0.0792        0.449      0.00467\n",
            "     38    15       0.0111       0.0111     7.66e-06       0.0614       0.0813        0.049       0.0862       0.0676       0.0635        0.109        0.086        0.191      0.00199\n",
            "     38    16      0.00887      0.00877     0.000105       0.0549       0.0724       0.0438        0.077       0.0604       0.0552       0.0982       0.0767        0.759       0.0079\n",
            "     38    17      0.00866      0.00863     3.11e-05       0.0543       0.0719       0.0441       0.0748       0.0595       0.0563       0.0957        0.076        0.409      0.00426\n",
            "     38    18      0.00954      0.00952     2.05e-05       0.0555       0.0755       0.0453        0.076       0.0606       0.0597       0.0998       0.0798        0.331      0.00345\n",
            "     38    19       0.0082      0.00808      0.00012       0.0528       0.0695       0.0435       0.0713       0.0574       0.0552       0.0917       0.0734         0.81      0.00844\n",
            "     38    20      0.00886      0.00883     3.02e-05       0.0558       0.0727       0.0461       0.0752       0.0607       0.0592        0.094       0.0766        0.392      0.00409\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     38     1       0.0105       0.0105     2.77e-06       0.0602       0.0794       0.0488       0.0829       0.0658       0.0636        0.104       0.0839        0.103      0.00108\n",
            "     38     2       0.0103       0.0103     9.61e-07       0.0587       0.0785       0.0472       0.0819       0.0645       0.0618        0.104        0.083       0.0592     0.000616\n",
            "     38     3      0.00936      0.00935      1.7e-06       0.0565       0.0748       0.0456       0.0781       0.0619       0.0588       0.0994       0.0791       0.0812     0.000846\n",
            "     38     4       0.0101       0.0101     1.22e-06       0.0582       0.0778       0.0474         0.08       0.0637       0.0614        0.103       0.0822       0.0725     0.000755\n",
            "     38     5      0.00956      0.00956     1.72e-06        0.057       0.0756       0.0467       0.0774       0.0621       0.0612       0.0984       0.0798       0.0703     0.000732\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              38  120.926    0.005      0.00956     3.06e-05       0.0096       0.0575       0.0757       0.0472       0.0782       0.0627       0.0607       0.0991       0.0799         0.35      0.00364\n",
            "! Validation         38  120.926    0.005      0.00997     1.67e-06      0.00998       0.0581       0.0773       0.0471       0.0801       0.0636       0.0614        0.102       0.0816       0.0773     0.000805\n",
            "Wall time: 120.9270802179999\n",
            "! Best model       38    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     39     1       0.0106       0.0106     8.32e-05       0.0607       0.0795       0.0502       0.0817        0.066        0.065        0.103       0.0838        0.672        0.007\n",
            "     39     2        0.011       0.0109      5.9e-05       0.0612       0.0807       0.0501       0.0835       0.0668       0.0644        0.106       0.0853        0.567      0.00591\n",
            "     39     3       0.0108       0.0108     2.17e-06       0.0608       0.0805       0.0493       0.0838       0.0665       0.0639        0.106        0.085       0.0869     0.000905\n",
            "     39     4       0.0105       0.0104     0.000107       0.0606       0.0791       0.0491       0.0835       0.0663       0.0619        0.105       0.0836        0.765      0.00797\n",
            "     39     5        0.009      0.00899     3.72e-06       0.0556       0.0734       0.0444       0.0779       0.0612       0.0573       0.0978       0.0776        0.117      0.00122\n",
            "     39     6       0.0102       0.0102     3.59e-05       0.0592        0.078       0.0502       0.0773       0.0638       0.0657       0.0981       0.0819        0.439      0.00458\n",
            "     39     7      0.00877      0.00877      3.3e-06       0.0555       0.0724       0.0479       0.0705       0.0592       0.0609       0.0913       0.0761        0.123      0.00129\n",
            "     39     8       0.0104       0.0104     1.37e-05       0.0603       0.0789       0.0496       0.0816       0.0656       0.0625        0.104       0.0834        0.266      0.00277\n",
            "     39     9       0.0114       0.0113      9.3e-05       0.0626       0.0823       0.0528       0.0821       0.0675       0.0679        0.105       0.0866        0.714      0.00744\n",
            "     39    10       0.0104       0.0103     5.39e-05       0.0614       0.0785       0.0538       0.0768       0.0653       0.0679       0.0964       0.0821        0.545      0.00567\n",
            "     39    11       0.0104       0.0103      2.8e-05       0.0606       0.0787       0.0513        0.079       0.0652       0.0664       0.0988       0.0826        0.383      0.00399\n",
            "     39    12      0.00735      0.00734     2.19e-06       0.0501       0.0663        0.041       0.0683       0.0546       0.0522       0.0879       0.0701       0.0988      0.00103\n",
            "     39    13       0.0102       0.0101     3.83e-05       0.0589       0.0779       0.0495       0.0775       0.0635       0.0639          0.1        0.082        0.448      0.00467\n",
            "     39    14       0.0101         0.01     2.16e-05       0.0595       0.0775        0.049       0.0805       0.0647       0.0627        0.101       0.0818        0.319      0.00332\n",
            "     39    15      0.00971      0.00961     9.56e-05       0.0572       0.0758       0.0453       0.0809       0.0631       0.0588        0.102       0.0802        0.726      0.00756\n",
            "     39    16      0.00783      0.00776     7.25e-05       0.0518       0.0681       0.0424       0.0707       0.0565       0.0544       0.0895        0.072        0.624       0.0065\n",
            "     39    17       0.0106       0.0106     4.52e-05       0.0612       0.0796       0.0517       0.0802        0.066        0.066        0.102       0.0838        0.489      0.00509\n",
            "     39    18         0.01      0.00992     0.000124       0.0577       0.0771       0.0466         0.08       0.0633       0.0603        0.103       0.0815        0.828      0.00863\n",
            "     39    19      0.00946      0.00943      2.5e-05        0.057       0.0751       0.0458       0.0794       0.0626       0.0596       0.0991       0.0794        0.353      0.00368\n",
            "     39    20      0.00987       0.0098     6.49e-05        0.058       0.0766       0.0472       0.0795       0.0634        0.061        0.101       0.0809        0.593      0.00618\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     39     1       0.0104       0.0104     2.74e-06       0.0597       0.0788       0.0484       0.0823       0.0653       0.0629        0.103       0.0832        0.103      0.00108\n",
            "     39     2       0.0102       0.0102     9.39e-07       0.0583        0.078       0.0468       0.0814       0.0641       0.0613        0.104       0.0825       0.0566      0.00059\n",
            "     39     3      0.00924      0.00924     1.63e-06       0.0561       0.0744       0.0453       0.0778       0.0615       0.0583       0.0989       0.0786       0.0791     0.000824\n",
            "     39     4      0.00997      0.00997     1.22e-06       0.0578       0.0772       0.0469       0.0796       0.0632       0.0608        0.102       0.0816       0.0724     0.000754\n",
            "     39     5      0.00942      0.00941      1.7e-06       0.0565       0.0751       0.0463        0.077       0.0616       0.0607       0.0977       0.0792       0.0695     0.000724\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              39  123.972    0.005      0.00988     4.86e-05      0.00993       0.0585       0.0769       0.0484       0.0787       0.0636       0.0623       0.0999       0.0811        0.458      0.00477\n",
            "! Validation         39  123.972    0.005      0.00983     1.65e-06      0.00983       0.0577       0.0767       0.0467       0.0796       0.0632       0.0608        0.101        0.081       0.0762     0.000793\n",
            "Wall time: 123.97253540499992\n",
            "! Best model       39    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     40     1      0.00888      0.00882     6.05e-05       0.0558       0.0726       0.0456       0.0761       0.0609       0.0577       0.0958       0.0767        0.572      0.00595\n",
            "     40     2      0.00923      0.00922     1.02e-05       0.0558       0.0743       0.0454       0.0765        0.061       0.0584       0.0987       0.0785        0.223      0.00232\n",
            "     40     3       0.0102       0.0101     2.59e-05       0.0582       0.0779       0.0464       0.0816        0.064       0.0605        0.104       0.0824        0.375      0.00391\n",
            "     40     4         0.01         0.01     7.38e-07       0.0606       0.0775       0.0527       0.0764       0.0646       0.0668       0.0953       0.0811       0.0629     0.000655\n",
            "     40     5       0.0148       0.0148     1.67e-06       0.0752       0.0941       0.0689       0.0878       0.0784       0.0857        0.109       0.0973       0.0844     0.000879\n",
            "     40     6       0.0148       0.0148     1.63e-05       0.0725        0.094       0.0594       0.0987        0.079       0.0767        0.121       0.0991        0.294      0.00306\n",
            "     40     7       0.0117       0.0116     9.45e-05       0.0635       0.0833       0.0521       0.0862       0.0692       0.0661         0.11        0.088        0.716      0.00746\n",
            "     40     8      0.00986      0.00986     4.64e-06       0.0583       0.0768       0.0487       0.0776       0.0632       0.0627       0.0992       0.0809        0.135      0.00141\n",
            "     40     9       0.0104       0.0103     7.49e-05       0.0588       0.0786       0.0477       0.0809       0.0643       0.0622        0.104        0.083        0.641      0.00668\n",
            "     40    10      0.00979      0.00969     9.96e-05       0.0569       0.0762       0.0449       0.0809       0.0629       0.0591        0.102       0.0806        0.739       0.0077\n",
            "     40    11      0.00985      0.00984     5.74e-06       0.0574       0.0767       0.0456       0.0811       0.0633       0.0595        0.103       0.0812        0.172      0.00179\n",
            "     40    12      0.00999       0.0099     8.86e-05       0.0582        0.077       0.0468       0.0811       0.0639       0.0606        0.102       0.0814        0.698      0.00727\n",
            "     40    13       0.0116       0.0115     4.44e-05       0.0655        0.083       0.0608        0.075       0.0679       0.0763        0.095       0.0857        0.489       0.0051\n",
            "     40    14       0.0111       0.0111     1.19e-05       0.0626       0.0816       0.0544        0.079       0.0667       0.0697        0.101       0.0855        0.242      0.00252\n",
            "     40    15       0.0105       0.0105     3.88e-06       0.0593       0.0791       0.0467       0.0846       0.0656       0.0592        0.108       0.0838        0.141      0.00146\n",
            "     40    16      0.00892      0.00889     2.56e-05       0.0556       0.0729       0.0437       0.0792       0.0615       0.0551       0.0995       0.0773        0.346       0.0036\n",
            "     40    17      0.00913      0.00913     3.33e-06       0.0563       0.0739        0.046        0.077       0.0615       0.0591        0.097        0.078       0.0924     0.000962\n",
            "     40    18      0.00933      0.00932     5.69e-06       0.0573       0.0747       0.0463       0.0792       0.0628       0.0594       0.0984       0.0789        0.156      0.00163\n",
            "     40    19      0.00951      0.00951     5.92e-06       0.0581       0.0754       0.0501        0.074       0.0621       0.0628       0.0958       0.0793        0.176      0.00183\n",
            "     40    20      0.00944      0.00944     2.69e-06       0.0581       0.0752       0.0492       0.0759       0.0626       0.0627       0.0954        0.079        0.119      0.00124\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     40     1       0.0102       0.0102     2.75e-06       0.0593       0.0782       0.0479        0.082       0.0649       0.0624        0.103       0.0826        0.103      0.00107\n",
            "     40     2         0.01         0.01     9.55e-07       0.0579       0.0774       0.0464       0.0809       0.0636       0.0607        0.103       0.0819       0.0577     0.000601\n",
            "     40     3      0.00913      0.00913     1.64e-06       0.0557       0.0739       0.0449       0.0774       0.0611       0.0579       0.0985       0.0782       0.0786     0.000819\n",
            "     40     4      0.00984      0.00984      1.2e-06       0.0574       0.0767       0.0465       0.0792       0.0629       0.0602        0.102       0.0811       0.0733     0.000764\n",
            "     40     5      0.00928      0.00928     1.67e-06       0.0562       0.0745        0.046       0.0765       0.0612       0.0602       0.0971       0.0786       0.0684     0.000712\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              40  127.034    0.005       0.0104     2.93e-05       0.0104       0.0602        0.079       0.0501       0.0804       0.0653       0.0645        0.102       0.0832        0.324      0.00337\n",
            "! Validation         40  127.034    0.005       0.0097     1.64e-06       0.0097       0.0573       0.0762       0.0463       0.0792       0.0628       0.0603        0.101       0.0805       0.0762     0.000794\n",
            "Wall time: 127.03483626999991\n",
            "! Best model       40    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     41     1      0.00963       0.0096     3.15e-05       0.0576       0.0758       0.0462       0.0805       0.0633       0.0591        0.101       0.0802        0.414      0.00431\n",
            "     41     2      0.00914      0.00912     1.76e-05        0.056       0.0739       0.0455       0.0771       0.0613       0.0585       0.0977       0.0781        0.294      0.00306\n",
            "     41     3      0.00835      0.00831     3.95e-05       0.0537       0.0705       0.0438       0.0735       0.0587       0.0553       0.0938       0.0746        0.465      0.00484\n",
            "     41     4       0.0095       0.0095     5.03e-07        0.059       0.0754       0.0513       0.0746       0.0629       0.0643       0.0938        0.079       0.0471      0.00049\n",
            "     41     5       0.0106       0.0106     2.09e-06        0.062       0.0798       0.0532       0.0794       0.0663       0.0667        0.101       0.0839       0.0963        0.001\n",
            "     41     6      0.00887      0.00885     1.95e-05       0.0546       0.0728       0.0435       0.0768       0.0602       0.0556       0.0986       0.0771        0.326       0.0034\n",
            "     41     7      0.00806      0.00806     2.36e-06       0.0532       0.0695        0.043       0.0738       0.0584        0.055       0.0918       0.0734        0.104      0.00108\n",
            "     41     8      0.00846      0.00845      4.9e-06        0.054       0.0711       0.0454       0.0712       0.0583       0.0584       0.0914       0.0749        0.149      0.00155\n",
            "     41     9      0.00854      0.00854     1.18e-06        0.054       0.0715       0.0446       0.0729       0.0587       0.0568       0.0943       0.0755       0.0715     0.000745\n",
            "     41    10      0.00923      0.00922     6.45e-06        0.057       0.0743       0.0481       0.0747       0.0614       0.0617       0.0946       0.0781        0.167      0.00174\n",
            "     41    11      0.00945      0.00943     1.16e-05       0.0552       0.0751       0.0424       0.0809       0.0616       0.0556        0.104       0.0797        0.243      0.00253\n",
            "     41    12         0.01      0.00998     3.71e-05       0.0585       0.0773       0.0489       0.0778       0.0634       0.0631       0.0997       0.0814         0.45      0.00469\n",
            "     41    13      0.00904      0.00903     1.18e-06       0.0557       0.0735       0.0456       0.0757       0.0607         0.06        0.095       0.0775       0.0557      0.00058\n",
            "     41    14       0.0087      0.00869     1.41e-05       0.0541       0.0721       0.0419       0.0785       0.0602       0.0531       0.0998       0.0765        0.263      0.00274\n",
            "     41    15      0.00868      0.00868     8.58e-07       0.0551       0.0721       0.0447       0.0757       0.0602       0.0567       0.0957       0.0762       0.0629     0.000655\n",
            "     41    16       0.0139       0.0139     1.64e-06       0.0692       0.0913       0.0581       0.0914       0.0748       0.0757        0.116        0.096       0.0844     0.000879\n",
            "     41    17       0.0145       0.0144     0.000121       0.0733       0.0928       0.0648       0.0902       0.0775       0.0816        0.112       0.0967         0.81      0.00844\n",
            "     41    18       0.0126       0.0126     2.27e-06       0.0691        0.087       0.0614       0.0845        0.073       0.0763        0.105       0.0907       0.0875     0.000911\n",
            "     41    19      0.00996      0.00991     4.95e-05       0.0591        0.077       0.0476        0.082       0.0648         0.06        0.103       0.0815        0.519      0.00541\n",
            "     41    20       0.0095      0.00938     0.000118       0.0561       0.0749        0.046       0.0762       0.0611       0.0596       0.0987       0.0791        0.805      0.00839\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     41     1       0.0101       0.0101     2.75e-06       0.0589       0.0777       0.0475       0.0816       0.0646       0.0619        0.102       0.0821        0.103      0.00107\n",
            "     41     2       0.0099       0.0099     9.32e-07       0.0575        0.077        0.046       0.0805       0.0632       0.0603        0.103       0.0814       0.0571     0.000595\n",
            "     41     3      0.00904      0.00904     1.64e-06       0.0554       0.0736       0.0446       0.0771       0.0608       0.0575       0.0981       0.0778       0.0793     0.000826\n",
            "     41     4      0.00972      0.00972     1.21e-06        0.057       0.0763       0.0461       0.0789       0.0625       0.0597        0.102       0.0806       0.0728     0.000758\n",
            "     41     5      0.00916      0.00916     1.66e-06       0.0558        0.074       0.0457       0.0761       0.0609       0.0597       0.0965       0.0781       0.0682      0.00071\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              41  130.087    0.005      0.00982     2.41e-05      0.00984       0.0583       0.0767       0.0483       0.0784       0.0633       0.0621       0.0996       0.0808        0.276      0.00287\n",
            "! Validation         41  130.087    0.005      0.00958     1.64e-06      0.00958       0.0569       0.0757        0.046       0.0788       0.0624       0.0598          0.1         0.08        0.076     0.000791\n",
            "Wall time: 130.08745759099997\n",
            "! Best model       41    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     42     1       0.0105       0.0105     2.46e-05       0.0591       0.0792       0.0485       0.0803       0.0644       0.0641        0.103       0.0835        0.357      0.00372\n",
            "     42     2       0.0101      0.00979     0.000282       0.0575       0.0765       0.0463         0.08       0.0631       0.0599        0.102       0.0809         1.25        0.013\n",
            "     42     3       0.0119       0.0119     1.36e-05       0.0662       0.0844       0.0591       0.0804       0.0698       0.0748        0.101       0.0879        0.262      0.00273\n",
            "     42     4       0.0123       0.0122      7.7e-05       0.0673       0.0854       0.0613       0.0794       0.0703        0.076        0.102       0.0888        0.649      0.00676\n",
            "     42     5       0.0112        0.011     0.000189       0.0618       0.0811       0.0504       0.0848       0.0676       0.0645        0.107       0.0857         1.02       0.0106\n",
            "     42     6      0.00962      0.00961      1.8e-05       0.0573       0.0758       0.0464       0.0792       0.0628         0.06          0.1       0.0801        0.312      0.00325\n",
            "     42     7      0.00826      0.00815     0.000108       0.0532       0.0698       0.0428       0.0738       0.0583       0.0542       0.0936       0.0739         0.77      0.00802\n",
            "     42     8      0.00891      0.00891     6.78e-07       0.0544        0.073       0.0429       0.0776       0.0602       0.0548       0.0999       0.0774       0.0533     0.000555\n",
            "     42     9      0.00885      0.00879     6.72e-05       0.0542       0.0725       0.0443        0.074       0.0591       0.0584       0.0946       0.0765        0.606      0.00631\n",
            "     42    10      0.00802      0.00799     3.07e-05       0.0525       0.0692       0.0419       0.0736       0.0578       0.0536       0.0928       0.0732        0.406      0.00423\n",
            "     42    11      0.00827      0.00824     2.93e-05       0.0536       0.0702       0.0444       0.0721       0.0582       0.0562        0.092       0.0741        0.398      0.00415\n",
            "     42    12      0.00901      0.00892     8.73e-05       0.0554       0.0731       0.0443       0.0775       0.0609       0.0568       0.0978       0.0773        0.678      0.00706\n",
            "     42    13      0.00854      0.00854     2.06e-06       0.0542       0.0715       0.0436       0.0753       0.0594       0.0561        0.095       0.0756       0.0996      0.00104\n",
            "     42    14      0.00853       0.0085     2.91e-05       0.0539       0.0713       0.0439       0.0739       0.0589       0.0562       0.0945       0.0754        0.392      0.00408\n",
            "     42    15      0.00885      0.00883     1.94e-05       0.0545       0.0727       0.0435       0.0764         0.06        0.057       0.0967       0.0769        0.322      0.00335\n",
            "     42    16      0.00854      0.00854     8.35e-07       0.0543       0.0715       0.0417       0.0794       0.0605       0.0527       0.0989       0.0758       0.0611     0.000637\n",
            "     42    17       0.0102       0.0101     3.98e-05       0.0601       0.0779       0.0504       0.0794       0.0649        0.064          0.1        0.082        0.465      0.00484\n",
            "     42    18       0.0132       0.0132     1.84e-05       0.0695        0.089        0.061       0.0866       0.0738       0.0767        0.109       0.0931        0.308      0.00321\n",
            "     42    19       0.0131       0.0131     1.88e-06         0.07       0.0886       0.0619       0.0863       0.0741       0.0761        0.109       0.0927       0.0967      0.00101\n",
            "     42    20      0.00977      0.00977     4.87e-07       0.0583       0.0765        0.048       0.0789       0.0635       0.0608        0.101       0.0808        0.042     0.000437\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     42     1      0.00997      0.00996     2.78e-06       0.0585       0.0772       0.0472       0.0812       0.0642       0.0613        0.102       0.0816        0.104      0.00108\n",
            "     42     2      0.00978      0.00978      8.6e-07       0.0571       0.0765       0.0457         0.08       0.0628       0.0598        0.102       0.0809       0.0538     0.000561\n",
            "     42     3      0.00895      0.00895     1.62e-06       0.0551       0.0732       0.0443       0.0768       0.0605       0.0571       0.0977       0.0774       0.0802     0.000835\n",
            "     42     4       0.0096       0.0096     1.32e-06       0.0567       0.0758       0.0457       0.0786       0.0621       0.0592        0.101       0.0802       0.0746     0.000777\n",
            "     42     5      0.00904      0.00904     1.64e-06       0.0555       0.0736       0.0453       0.0758       0.0606       0.0593       0.0959       0.0776       0.0683     0.000711\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              42  133.136    0.005      0.00983      5.2e-05      0.00988       0.0584       0.0767       0.0483       0.0784       0.0634       0.0621       0.0996       0.0809        0.427      0.00445\n",
            "! Validation         42  133.136    0.005      0.00947     1.64e-06      0.00947       0.0566       0.0753       0.0456       0.0785        0.062       0.0594       0.0998       0.0796       0.0762     0.000794\n",
            "Wall time: 133.136799807\n",
            "! Best model       42    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     43     1      0.00829      0.00829     2.06e-06       0.0524       0.0704       0.0417       0.0737       0.0577       0.0547       0.0943       0.0745        0.101      0.00105\n",
            "     43     2       0.0122       0.0122     1.34e-05       0.0648       0.0855       0.0562       0.0819       0.0691       0.0734        0.105       0.0894        0.259       0.0027\n",
            "     43     3       0.0149       0.0149      5.4e-05       0.0746       0.0944        0.066        0.092        0.079       0.0817        0.116       0.0987        0.537      0.00559\n",
            "     43     4       0.0142       0.0142     3.74e-06       0.0725       0.0922        0.063       0.0914       0.0772       0.0789        0.114       0.0966        0.138      0.00144\n",
            "     43     5      0.00999      0.00995     3.25e-05       0.0591       0.0772       0.0497       0.0779       0.0638       0.0634       0.0991       0.0813         0.42      0.00437\n",
            "     43     6      0.00853      0.00852     5.87e-06       0.0533       0.0714       0.0419       0.0762       0.0591        0.054       0.0974       0.0757        0.158      0.00164\n",
            "     43     7      0.00711      0.00707     4.87e-05       0.0491        0.065       0.0393       0.0688       0.0541       0.0494       0.0884       0.0689        0.516      0.00537\n",
            "     43     8      0.00859      0.00858     1.01e-05       0.0541       0.0717       0.0442        0.074       0.0591       0.0574       0.0939       0.0757        0.215      0.00224\n",
            "     43     9      0.00878      0.00878     2.69e-06       0.0549       0.0725       0.0447       0.0753         0.06       0.0579       0.0951       0.0765       0.0834     0.000869\n",
            "     43    10      0.00806      0.00804     1.23e-05       0.0525       0.0694       0.0422       0.0731       0.0576       0.0541       0.0927       0.0734        0.254      0.00265\n",
            "     43    11       0.0087       0.0087     1.92e-06        0.055       0.0722        0.045       0.0751         0.06       0.0574        0.095       0.0762       0.0842     0.000877\n",
            "     43    12      0.00832       0.0083     2.19e-05       0.0528       0.0705       0.0429       0.0726       0.0577       0.0551       0.0939       0.0745        0.345       0.0036\n",
            "     43    13      0.00949      0.00948     4.67e-06       0.0567       0.0753       0.0463       0.0776        0.062       0.0601       0.0991       0.0796        0.151      0.00157\n",
            "     43    14      0.00826      0.00818     8.31e-05       0.0533         0.07       0.0439        0.072        0.058       0.0555       0.0923       0.0739         0.67      0.00698\n",
            "     43    15      0.00935      0.00929     6.42e-05       0.0563       0.0746       0.0457       0.0774       0.0615       0.0594       0.0981       0.0787         0.59      0.00615\n",
            "     43    16      0.00807      0.00801     5.85e-05       0.0519       0.0692       0.0425       0.0708       0.0566       0.0545       0.0919       0.0732        0.566      0.00589\n",
            "     43    17      0.00878      0.00861     0.000171       0.0544       0.0718       0.0446       0.0741       0.0594       0.0568        0.095       0.0759        0.971       0.0101\n",
            "     43    18      0.00822      0.00822     5.59e-06       0.0529       0.0701       0.0419       0.0749       0.0584       0.0535        0.095       0.0743        0.167      0.00174\n",
            "     43    19      0.00893      0.00887     5.52e-05       0.0553       0.0729       0.0435        0.079       0.0612       0.0551       0.0993       0.0772         0.55      0.00573\n",
            "     43    20      0.00873      0.00868     5.72e-05       0.0548       0.0721       0.0441       0.0764       0.0602       0.0563       0.0961       0.0762        0.559      0.00582\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     43     1      0.00984      0.00984     2.78e-06       0.0581       0.0767       0.0468       0.0808       0.0638       0.0608        0.101       0.0811        0.104      0.00109\n",
            "     43     2      0.00968      0.00967     9.33e-07       0.0568       0.0761       0.0453       0.0796       0.0625       0.0593        0.102       0.0805       0.0557      0.00058\n",
            "     43     3      0.00886      0.00886     1.59e-06       0.0548       0.0728        0.044       0.0765       0.0602       0.0567       0.0973        0.077       0.0773     0.000806\n",
            "     43     4      0.00949      0.00949     1.21e-06       0.0563       0.0754       0.0453       0.0782       0.0618       0.0587        0.101       0.0797       0.0722     0.000752\n",
            "     43     5      0.00893      0.00893     1.67e-06       0.0552       0.0731        0.045       0.0754       0.0602       0.0589       0.0954       0.0771       0.0682      0.00071\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              43  136.187    0.005      0.00934     3.54e-05      0.00938       0.0565       0.0748       0.0465       0.0767       0.0616         0.06       0.0978       0.0789        0.367      0.00382\n",
            "! Validation         43  136.187    0.005      0.00936     1.64e-06      0.00936       0.0562       0.0748       0.0453       0.0781       0.0617       0.0589       0.0993       0.0791       0.0755     0.000787\n",
            "Wall time: 136.1874814549999\n",
            "! Best model       43    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     44     1      0.00792      0.00792     4.75e-06       0.0521       0.0688        0.042       0.0723       0.0572       0.0541       0.0915       0.0728        0.156      0.00162\n",
            "     44     2       0.0093      0.00927     2.37e-05       0.0558       0.0745       0.0456        0.076       0.0608         0.06       0.0972       0.0786        0.353      0.00368\n",
            "     44     3       0.0092       0.0092     1.52e-07        0.057       0.0742       0.0471       0.0769        0.062       0.0599       0.0968       0.0783        0.023      0.00024\n",
            "     44     4      0.00876       0.0087     5.55e-05       0.0546       0.0722       0.0435       0.0769       0.0602       0.0566        0.096       0.0763        0.542      0.00565\n",
            "     44     5      0.00823      0.00823     3.64e-06       0.0531       0.0702       0.0425       0.0744       0.0585       0.0546       0.0939       0.0742        0.123      0.00128\n",
            "     44     6       0.0104       0.0103     8.03e-05       0.0598       0.0785       0.0501       0.0793       0.0647        0.065          0.1       0.0825         0.66      0.00688\n",
            "     44     7      0.00921      0.00917     4.59e-05       0.0568       0.0741       0.0471       0.0762       0.0617       0.0601       0.0962       0.0781        0.499       0.0052\n",
            "     44     8      0.00944      0.00944     2.82e-06       0.0582       0.0752         0.05       0.0747       0.0623       0.0627       0.0953        0.079       0.0996      0.00104\n",
            "     44     9      0.00873      0.00862     0.000109        0.054       0.0718       0.0427       0.0764       0.0596       0.0544       0.0978       0.0761        0.767      0.00799\n",
            "     44    10      0.00891      0.00887     4.16e-05        0.055       0.0729       0.0436        0.078       0.0608       0.0556       0.0987       0.0772        0.465      0.00485\n",
            "     44    11      0.00936      0.00936     2.04e-06       0.0557       0.0748       0.0441       0.0788       0.0615       0.0574        0.101       0.0792       0.0996      0.00104\n",
            "     44    12       0.0115       0.0114     9.53e-06       0.0635       0.0828       0.0511       0.0885       0.0698       0.0646         0.11       0.0875        0.223      0.00233\n",
            "     44    13       0.0117       0.0116     0.000115       0.0648       0.0834       0.0568       0.0806       0.0687       0.0713        0.103       0.0873        0.796      0.00829\n",
            "     44    14       0.0108       0.0108      1.3e-05       0.0622       0.0803       0.0543        0.078       0.0662       0.0699       0.0978       0.0839        0.246      0.00257\n",
            "     44    15      0.00868      0.00863     4.22e-05       0.0543       0.0719       0.0436       0.0758       0.0597       0.0555       0.0967       0.0761        0.475      0.00495\n",
            "     44    16      0.00954      0.00954     7.52e-07       0.0575       0.0755       0.0489       0.0749       0.0619       0.0627       0.0962       0.0795       0.0586      0.00061\n",
            "     44    17       0.0103       0.0103     6.59e-07       0.0602       0.0786        0.052       0.0767       0.0643       0.0659       0.0992       0.0826       0.0531     0.000553\n",
            "     44    18       0.0089      0.00889     4.17e-06       0.0553        0.073       0.0463       0.0734       0.0598       0.0583       0.0957        0.077        0.145      0.00151\n",
            "     44    19      0.00855      0.00854     2.71e-06       0.0536       0.0715       0.0426       0.0756       0.0591       0.0543       0.0972       0.0757       0.0906     0.000944\n",
            "     44    20      0.00997      0.00996     6.22e-06       0.0578       0.0772       0.0468       0.0798       0.0633       0.0614        0.102       0.0816        0.161      0.00167\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     44     1      0.00972      0.00972     2.73e-06       0.0578       0.0763       0.0465       0.0804       0.0634       0.0604        0.101       0.0806        0.103      0.00107\n",
            "     44     2      0.00957      0.00957     9.32e-07       0.0564       0.0757        0.045       0.0793       0.0621       0.0589        0.101       0.0801       0.0555     0.000578\n",
            "     44     3      0.00877      0.00877     1.55e-06       0.0545       0.0725       0.0437       0.0762       0.0599       0.0563        0.097       0.0767       0.0779     0.000812\n",
            "     44     4      0.00939      0.00939     1.23e-06        0.056        0.075        0.045       0.0779       0.0615       0.0582          0.1       0.0793       0.0733     0.000764\n",
            "     44     5      0.00883      0.00883      1.6e-06       0.0549       0.0727       0.0448       0.0751       0.0599       0.0585        0.095       0.0767       0.0655     0.000683\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              44  139.256    0.005      0.00944     2.82e-05      0.00947       0.0571       0.0752        0.047       0.0772       0.0621       0.0604       0.0982       0.0793        0.302      0.00314\n",
            "! Validation         44  139.256    0.005      0.00926     1.61e-06      0.00926       0.0559       0.0744        0.045       0.0778       0.0614       0.0585       0.0989       0.0787        0.075     0.000781\n",
            "Wall time: 139.2565434889999\n",
            "! Best model       44    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     45     1      0.00941       0.0094     1.21e-05       0.0575        0.075       0.0478       0.0769       0.0624       0.0603       0.0981       0.0792         0.24       0.0025\n",
            "     45     2       0.0103       0.0103     2.36e-06       0.0606       0.0787       0.0516       0.0786       0.0651       0.0659       0.0995       0.0827       0.0891     0.000928\n",
            "     45     3      0.00888      0.00888     1.21e-06       0.0553       0.0729       0.0462       0.0737       0.0599       0.0585       0.0954       0.0769       0.0629     0.000655\n",
            "     45     4       0.0104       0.0104      4.8e-06         0.06        0.079       0.0508       0.0785       0.0647       0.0655        0.101       0.0832        0.108      0.00113\n",
            "     45     5       0.0146       0.0146     1.96e-05       0.0741       0.0934        0.067       0.0884       0.0777       0.0829        0.112       0.0973        0.319      0.00332\n",
            "     45     6        0.013        0.013     1.32e-05       0.0679       0.0881       0.0575       0.0888       0.0731       0.0723        0.113       0.0927        0.259       0.0027\n",
            "     45     7      0.00904      0.00904     6.32e-07       0.0564       0.0735        0.045       0.0791       0.0621       0.0571       0.0985       0.0778       0.0562     0.000586\n",
            "     45     8      0.00731      0.00731     3.05e-06       0.0497       0.0661       0.0393       0.0704       0.0548       0.0499       0.0903       0.0701        0.121      0.00126\n",
            "     45     9      0.00948      0.00948     6.28e-06        0.057       0.0753       0.0464       0.0782       0.0623       0.0591          0.1       0.0796         0.18      0.00187\n",
            "     45    10       0.0112       0.0112      1.1e-06       0.0633       0.0817       0.0534        0.083       0.0682       0.0674        0.105        0.086        0.065     0.000677\n",
            "     45    11       0.0118       0.0118     2.85e-05       0.0657       0.0841       0.0583       0.0804       0.0694       0.0742        0.101       0.0876        0.393       0.0041\n",
            "     45    12      0.00903      0.00902     4.14e-06       0.0558       0.0735       0.0436       0.0803       0.0619       0.0556          0.1       0.0778        0.121      0.00126\n",
            "     45    13      0.00875      0.00871      3.2e-05       0.0544       0.0722       0.0436       0.0761       0.0598       0.0575       0.0951       0.0763        0.406      0.00423\n",
            "     45    14       0.0096      0.00958     2.15e-05        0.057       0.0757       0.0465       0.0782       0.0623       0.0594        0.101         0.08        0.337      0.00351\n",
            "     45    15      0.00931      0.00931     5.34e-06       0.0565       0.0746       0.0455       0.0784       0.0619       0.0588        0.099       0.0789        0.162      0.00169\n",
            "     45    16       0.0105       0.0104     8.42e-05       0.0595        0.079       0.0485       0.0816        0.065       0.0636        0.103       0.0834        0.678      0.00706\n",
            "     45    17      0.00821      0.00815     6.35e-05       0.0529       0.0698       0.0423        0.074       0.0581       0.0531       0.0949        0.074        0.591      0.00616\n",
            "     45    18       0.0103       0.0103     2.66e-05       0.0602       0.0784       0.0537       0.0732       0.0635       0.0687       0.0948       0.0818         0.38      0.00396\n",
            "     45    19      0.00921      0.00913     8.52e-05       0.0564       0.0739       0.0466        0.076       0.0613       0.0597       0.0963        0.078        0.676      0.00704\n",
            "     45    20      0.00996      0.00996     6.71e-06        0.059       0.0772       0.0482       0.0806       0.0644        0.062        0.101       0.0815        0.167      0.00174\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     45     1      0.00961      0.00961     2.78e-06       0.0574       0.0758       0.0461       0.0801       0.0631       0.0599          0.1       0.0801        0.104      0.00108\n",
            "     45     2      0.00948      0.00947     8.95e-07       0.0561       0.0753       0.0447       0.0789       0.0618       0.0585        0.101       0.0797       0.0536     0.000558\n",
            "     45     3       0.0087       0.0087     1.59e-06       0.0543       0.0721       0.0434       0.0759       0.0597        0.056       0.0967       0.0763       0.0794     0.000827\n",
            "     45     4      0.00929      0.00929     1.27e-06       0.0557       0.0746       0.0447       0.0777       0.0612       0.0578          0.1       0.0789       0.0746     0.000777\n",
            "     45     5      0.00873      0.00873     1.56e-06       0.0546       0.0723       0.0444       0.0748       0.0596        0.058       0.0945       0.0763       0.0658     0.000686\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              45  142.317    0.005         0.01     2.11e-05         0.01        0.059       0.0774       0.0491       0.0787       0.0639        0.063          0.1       0.0815        0.271      0.00282\n",
            "! Validation         45  142.317    0.005      0.00916     1.62e-06      0.00916       0.0556        0.074       0.0447       0.0775       0.0611       0.0581       0.0985       0.0783       0.0754     0.000786\n",
            "Wall time: 142.31750640099995\n",
            "! Best model       45    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     46     1      0.00805      0.00794     0.000109       0.0519       0.0689       0.0413       0.0731       0.0572       0.0534       0.0925       0.0729        0.772      0.00804\n",
            "     46     2       0.0114       0.0113     8.88e-05       0.0623       0.0824       0.0508       0.0854       0.0681        0.065        0.109       0.0871        0.695      0.00724\n",
            "     46     3       0.0109       0.0108      9.5e-05       0.0614       0.0802       0.0517       0.0807       0.0662       0.0668        0.102       0.0844        0.717      0.00747\n",
            "     46     4      0.00976      0.00967     8.56e-05       0.0596       0.0761       0.0533       0.0723       0.0628       0.0665       0.0923       0.0794        0.684      0.00712\n",
            "     46     5      0.00929      0.00929      3.9e-07        0.057       0.0746       0.0456         0.08       0.0628        0.058       0.0998       0.0789       0.0398     0.000415\n",
            "     46     6      0.00816      0.00804     0.000116       0.0537       0.0694       0.0431       0.0749        0.059       0.0543       0.0925       0.0734        0.799      0.00833\n",
            "     46     7      0.00886      0.00885     9.84e-06       0.0552       0.0728        0.045       0.0755       0.0603       0.0581       0.0957       0.0769        0.227      0.00237\n",
            "     46     8      0.00994      0.00983     0.000107       0.0579       0.0767       0.0477       0.0783        0.063       0.0618          0.1       0.0809        0.765      0.00797\n",
            "     46     9         0.01         0.01     7.73e-06       0.0607       0.0774       0.0524       0.0774       0.0649        0.066       0.0964       0.0812        0.185      0.00192\n",
            "     46    10       0.0106       0.0106     2.47e-05       0.0607       0.0795       0.0512       0.0797       0.0654       0.0656        0.102       0.0837        0.366      0.00381\n",
            "     46    11      0.00785      0.00776     9.07e-05       0.0516       0.0681       0.0392       0.0765       0.0578       0.0499       0.0946       0.0722        0.702      0.00731\n",
            "     46    12      0.00723      0.00723     1.02e-06       0.0488       0.0658        0.039       0.0683       0.0537       0.0498       0.0896       0.0697       0.0672       0.0007\n",
            "     46    13         0.01      0.00993      7.1e-05       0.0577       0.0771       0.0458       0.0815       0.0637       0.0593        0.104       0.0816         0.62      0.00646\n",
            "     46    14      0.00728      0.00728      3.4e-06       0.0507        0.066       0.0412       0.0698       0.0555       0.0522       0.0873       0.0698        0.113      0.00117\n",
            "     46    15      0.00924      0.00923     6.31e-06       0.0566       0.0743       0.0474        0.075       0.0612       0.0617       0.0947       0.0782        0.158      0.00165\n",
            "     46    16      0.00874      0.00874     1.01e-06       0.0546       0.0723       0.0445       0.0747       0.0596       0.0563       0.0967       0.0765       0.0631     0.000657\n",
            "     46    17      0.00973      0.00973     4.81e-06       0.0584       0.0763       0.0485        0.078       0.0633        0.062       0.0989       0.0804        0.159      0.00166\n",
            "     46    18       0.0101       0.0101     7.72e-06       0.0593       0.0776       0.0485       0.0809       0.0647       0.0625        0.101       0.0819        0.173      0.00181\n",
            "     46    19      0.00961      0.00956     4.19e-05       0.0571       0.0757       0.0455       0.0803       0.0629       0.0585        0.102       0.0801        0.463      0.00482\n",
            "     46    20       0.0084      0.00839     9.72e-06       0.0536       0.0709        0.043       0.0749       0.0589       0.0557       0.0942       0.0749        0.219      0.00228\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     46     1      0.00951       0.0095     2.67e-06       0.0571       0.0754       0.0458       0.0797       0.0627       0.0595       0.0999       0.0797        0.103      0.00107\n",
            "     46     2      0.00939      0.00939     9.33e-07       0.0559        0.075       0.0445       0.0787       0.0616       0.0582          0.1       0.0793       0.0542     0.000565\n",
            "     46     3      0.00863      0.00863     1.54e-06        0.054       0.0719       0.0432       0.0757       0.0594       0.0557       0.0964        0.076       0.0775     0.000808\n",
            "     46     4       0.0092       0.0092     1.24e-06       0.0554       0.0742       0.0443       0.0774       0.0609       0.0574       0.0997       0.0785       0.0737     0.000768\n",
            "     46     5      0.00863      0.00863     1.52e-06       0.0542       0.0719       0.0441       0.0745       0.0593       0.0576       0.0941       0.0758       0.0654     0.000682\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              46  145.383    0.005      0.00921     4.41e-05      0.00926       0.0564       0.0743       0.0462       0.0769       0.0615       0.0594       0.0974       0.0784        0.399      0.00416\n",
            "! Validation         46  145.383    0.005      0.00907     1.58e-06      0.00907       0.0553       0.0737       0.0444       0.0772       0.0608       0.0577       0.0981       0.0779       0.0747     0.000778\n",
            "Wall time: 145.38379435699994\n",
            "! Best model       46    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     47     1       0.0102       0.0102     2.74e-06       0.0578       0.0781       0.0465       0.0804       0.0634       0.0611        0.104       0.0826        0.109      0.00113\n",
            "     47     2       0.0108       0.0108     1.09e-05       0.0621       0.0804       0.0534       0.0795       0.0664       0.0673        0.102       0.0844        0.213      0.00222\n",
            "     47     3      0.00864      0.00863     1.63e-05        0.055       0.0719        0.045        0.075         0.06       0.0576       0.0941       0.0759        0.298      0.00311\n",
            "     47     4       0.0087       0.0087     5.47e-06       0.0545       0.0722       0.0443        0.075       0.0596       0.0566       0.0959       0.0763         0.16      0.00166\n",
            "     47     5      0.00975      0.00975     1.77e-06       0.0582       0.0764       0.0455       0.0835       0.0645        0.057        0.105        0.081       0.0703     0.000732\n",
            "     47     6       0.0108       0.0108     1.28e-05       0.0607       0.0804       0.0488       0.0845       0.0667       0.0633        0.107        0.085        0.262      0.00273\n",
            "     47     7      0.00969      0.00965     3.67e-05       0.0595        0.076       0.0526       0.0734        0.063        0.066       0.0928       0.0794        0.443      0.00461\n",
            "     47     8      0.00748      0.00746     2.23e-05       0.0509       0.0668       0.0417       0.0693       0.0555       0.0532       0.0879       0.0706        0.345      0.00359\n",
            "     47     9      0.00912      0.00911     8.98e-06       0.0558       0.0738       0.0467       0.0742       0.0604       0.0588       0.0972        0.078         0.21      0.00219\n",
            "     47    10       0.0121       0.0121     1.66e-05       0.0659       0.0852       0.0563       0.0852       0.0707       0.0707        0.108       0.0896        0.297       0.0031\n",
            "     47    11       0.0107       0.0106      3.2e-05       0.0618       0.0797       0.0523        0.081       0.0666       0.0663        0.101       0.0838        0.402      0.00419\n",
            "     47    12      0.00879      0.00876     3.05e-05       0.0549       0.0724       0.0431       0.0785       0.0608       0.0565       0.0966       0.0766        0.394      0.00411\n",
            "     47    13      0.00962      0.00955     7.03e-05       0.0589       0.0756       0.0507       0.0753        0.063       0.0646       0.0938       0.0792        0.618      0.00643\n",
            "     47    14       0.0118       0.0117     1.09e-05       0.0648       0.0838       0.0556       0.0832       0.0694       0.0705        0.106        0.088        0.243      0.00253\n",
            "     47    15       0.0113       0.0113     3.41e-05        0.064       0.0821       0.0544       0.0833       0.0689       0.0681        0.105       0.0864        0.428      0.00446\n",
            "     47    16      0.00919      0.00918     5.21e-06        0.056       0.0741       0.0438       0.0804       0.0621       0.0558        0.101       0.0785        0.167      0.00174\n",
            "     47    17      0.00848      0.00846      2.3e-05        0.054       0.0711       0.0426       0.0768       0.0597       0.0554       0.0951       0.0753        0.343      0.00357\n",
            "     47    18       0.0124       0.0124     2.69e-05       0.0665       0.0861       0.0579       0.0837       0.0708       0.0726        0.108       0.0904        0.384        0.004\n",
            "     47    19       0.0138       0.0138     3.61e-05       0.0721       0.0907       0.0656       0.0853       0.0754       0.0802        0.109       0.0945         0.44      0.00458\n",
            "     47    20      0.00847      0.00845     1.76e-05       0.0553       0.0711       0.0474       0.0712       0.0593       0.0598       0.0897       0.0747        0.308      0.00321\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     47     1       0.0094       0.0094     2.66e-06       0.0568        0.075       0.0455       0.0794       0.0624        0.059       0.0995       0.0793        0.102      0.00106\n",
            "     47     2      0.00929      0.00929     9.74e-07       0.0556       0.0746       0.0442       0.0783       0.0612       0.0578          0.1       0.0789       0.0565     0.000589\n",
            "     47     3      0.00855      0.00854     1.48e-06       0.0537       0.0715       0.0429       0.0754       0.0592       0.0553       0.0961       0.0757       0.0744     0.000775\n",
            "     47     4      0.00911       0.0091     1.16e-06       0.0551       0.0738        0.044       0.0771       0.0606       0.0569       0.0993       0.0781       0.0714     0.000744\n",
            "     47     5      0.00855      0.00855     1.59e-06        0.054       0.0715       0.0439       0.0742       0.0591       0.0573       0.0937       0.0755       0.0649     0.000676\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              47  148.448    0.005       0.0101     2.11e-05       0.0101       0.0594       0.0776       0.0497       0.0789       0.0643       0.0634          0.1       0.0818        0.307      0.00319\n",
            "! Validation         47  148.448    0.005      0.00898     1.57e-06      0.00898        0.055       0.0733       0.0441       0.0769       0.0605       0.0573       0.0978       0.0775       0.0738     0.000769\n",
            "Wall time: 148.448677094\n",
            "! Best model       47    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     48     1      0.00926      0.00926     2.29e-06       0.0569       0.0744       0.0473       0.0763       0.0618       0.0598       0.0973       0.0786        0.107      0.00112\n",
            "     48     2       0.0151       0.0151     2.58e-06       0.0761        0.095       0.0692       0.0897       0.0795       0.0855        0.112       0.0986       0.0918     0.000956\n",
            "     48     3       0.0135       0.0134     8.92e-06       0.0704       0.0897       0.0646       0.0818       0.0732       0.0806        0.106       0.0931        0.201      0.00209\n",
            "     48     4      0.00945      0.00945     1.96e-06       0.0551       0.0752       0.0434       0.0785       0.0609       0.0583        0.101       0.0796       0.0898     0.000936\n",
            "     48     5      0.00931      0.00929     2.22e-05       0.0567       0.0746       0.0469       0.0762       0.0616       0.0595        0.098       0.0787        0.329      0.00343\n",
            "     48     6       0.0116       0.0115     9.04e-05       0.0642       0.0831       0.0544       0.0837       0.0691       0.0689        0.106       0.0875        0.701       0.0073\n",
            "     48     7        0.009      0.00898     2.14e-05       0.0554       0.0733       0.0461       0.0741       0.0601       0.0591       0.0956       0.0774        0.338      0.00352\n",
            "     48     8      0.00799      0.00786     0.000126       0.0519       0.0686       0.0411       0.0736       0.0573       0.0526       0.0926       0.0726        0.833      0.00867\n",
            "     48     9      0.00941      0.00938     2.38e-05       0.0576       0.0749       0.0484       0.0759       0.0622       0.0616       0.0963       0.0789        0.358      0.00373\n",
            "     48    10      0.00993      0.00993     1.07e-06       0.0598       0.0771       0.0516       0.0763       0.0639       0.0653       0.0964       0.0809       0.0488     0.000509\n",
            "     48    11      0.00821      0.00813     8.11e-05       0.0525       0.0698       0.0424       0.0727       0.0576        0.055       0.0925       0.0737        0.666      0.00693\n",
            "     48    12      0.00879      0.00877     2.52e-05       0.0558       0.0724       0.0456        0.076       0.0608       0.0572       0.0959       0.0765        0.371      0.00386\n",
            "     48    13      0.00858      0.00858     1.25e-06       0.0548       0.0717       0.0437        0.077       0.0603       0.0558       0.0958       0.0758       0.0701      0.00073\n",
            "     48    14      0.00958      0.00958      3.5e-06       0.0567       0.0757       0.0442       0.0816       0.0629        0.059        0.101       0.0801        0.132      0.00137\n",
            "     48    15      0.00891      0.00891     8.73e-07       0.0553        0.073       0.0452       0.0755       0.0604        0.058       0.0963       0.0771       0.0625     0.000651\n",
            "     48    16      0.00873      0.00873     1.34e-06       0.0552       0.0723       0.0438        0.078       0.0609        0.056        0.097       0.0765        0.067     0.000698\n",
            "     48    17      0.00804      0.00803     5.27e-06        0.053       0.0693        0.043       0.0728       0.0579        0.055       0.0915       0.0733        0.146      0.00152\n",
            "     48    18      0.00817      0.00814      2.9e-05       0.0533       0.0698       0.0425        0.075       0.0588       0.0543       0.0934       0.0738         0.38      0.00396\n",
            "     48    19      0.00787      0.00787     5.26e-06       0.0515       0.0686       0.0413       0.0719       0.0566       0.0528       0.0925       0.0726        0.152      0.00159\n",
            "     48    20      0.00807      0.00806     2.02e-06       0.0515       0.0695       0.0409       0.0727       0.0568       0.0535       0.0936       0.0735        0.092     0.000958\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     48     1      0.00932      0.00932     2.73e-06       0.0565       0.0747       0.0452       0.0791       0.0622       0.0587       0.0992       0.0789        0.104      0.00108\n",
            "     48     2      0.00922      0.00922     9.12e-07       0.0553       0.0743       0.0439        0.078        0.061       0.0575       0.0997       0.0786       0.0528      0.00055\n",
            "     48     3      0.00848      0.00848     1.49e-06       0.0535       0.0712       0.0426       0.0752       0.0589        0.055       0.0958       0.0754       0.0753     0.000784\n",
            "     48     4      0.00903      0.00902     1.21e-06       0.0548       0.0735       0.0438       0.0769       0.0603       0.0566        0.099       0.0778       0.0726     0.000756\n",
            "     48     5      0.00845      0.00845     1.56e-06       0.0537       0.0711       0.0436       0.0739       0.0588       0.0569       0.0933       0.0751       0.0649     0.000676\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              48  151.501    0.005      0.00945     2.28e-05      0.00947       0.0572       0.0752       0.0473        0.077       0.0621        0.061       0.0976       0.0793        0.262      0.00273\n",
            "! Validation         48  151.501    0.005       0.0089     1.58e-06       0.0089       0.0548        0.073       0.0438       0.0766       0.0602       0.0569       0.0974       0.0772       0.0739     0.000769\n",
            "Wall time: 151.501396215\n",
            "! Best model       48    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     49     1      0.00795      0.00791      4.4e-05       0.0517       0.0688       0.0425         0.07       0.0562       0.0542       0.0913       0.0727        0.481      0.00501\n",
            "     49     2       0.0082      0.00818     1.94e-05       0.0525         0.07       0.0424       0.0727       0.0575        0.055       0.0929        0.074        0.324      0.00338\n",
            "     49     3      0.00814      0.00814     2.52e-06       0.0526       0.0698       0.0425       0.0728       0.0577       0.0548       0.0927       0.0738        0.107      0.00112\n",
            "     49     4      0.00861      0.00861     4.92e-06       0.0541       0.0718       0.0436        0.075       0.0593       0.0569       0.0947       0.0758        0.144       0.0015\n",
            "     49     5         0.01      0.00999     6.08e-05       0.0597       0.0773       0.0503       0.0783       0.0643       0.0636       0.0992       0.0814        0.574      0.00598\n",
            "     49     6       0.0109       0.0109     9.66e-06       0.0622       0.0806        0.053       0.0805       0.0668        0.068        0.101       0.0846        0.219      0.00228\n",
            "     49     7      0.00808      0.00808     9.62e-07       0.0533       0.0695       0.0431       0.0736       0.0584       0.0548       0.0922       0.0735         0.05     0.000521\n",
            "     49     8      0.00886      0.00886     3.51e-06       0.0555       0.0728       0.0453       0.0759       0.0606       0.0571       0.0969        0.077        0.128      0.00134\n",
            "     49     9      0.00926      0.00921     5.41e-05        0.059       0.0742       0.0531       0.0709        0.062       0.0651       0.0897       0.0774        0.541      0.00564\n",
            "     49    10      0.00792      0.00791     6.83e-06       0.0524       0.0688       0.0425       0.0724       0.0574       0.0542       0.0913       0.0727        0.175      0.00182\n",
            "     49    11        0.009      0.00899     2.34e-06        0.055       0.0734       0.0458       0.0735       0.0596       0.0593       0.0955       0.0774       0.0916     0.000954\n",
            "     49    12       0.0111       0.0111     1.18e-05       0.0636       0.0815       0.0546       0.0814        0.068       0.0682        0.103       0.0857         0.25       0.0026\n",
            "     49    13       0.0138       0.0137     6.11e-05       0.0699       0.0906       0.0572       0.0953       0.0763        0.072        0.119       0.0957        0.578      0.00602\n",
            "     49    14       0.0108       0.0108     9.32e-06       0.0617       0.0804       0.0502       0.0847       0.0675       0.0633        0.107        0.085        0.223      0.00232\n",
            "     49    15      0.00821       0.0082     5.75e-06       0.0543       0.0701        0.045       0.0728       0.0589       0.0568        0.091       0.0739         0.16      0.00166\n",
            "     49    16       0.0098      0.00978     1.43e-05       0.0573       0.0765       0.0479        0.076        0.062        0.063       0.0981       0.0806         0.28      0.00292\n",
            "     49    17       0.0129       0.0129     1.43e-05       0.0695        0.088       0.0636       0.0813       0.0725       0.0797        0.103       0.0911        0.266      0.00277\n",
            "     49    18      0.00936      0.00934     1.59e-05       0.0575       0.0748       0.0477       0.0772       0.0624       0.0602       0.0976       0.0789        0.278      0.00289\n",
            "     49    19      0.00752       0.0075     2.18e-05       0.0504        0.067       0.0399       0.0714       0.0556       0.0497       0.0924        0.071         0.34      0.00355\n",
            "     49    20       0.0086      0.00859     9.17e-06       0.0544       0.0717       0.0434       0.0764       0.0599       0.0558        0.096       0.0759        0.221       0.0023\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     49     1      0.00923      0.00922     2.71e-06       0.0562       0.0743       0.0449       0.0788       0.0618       0.0583       0.0988       0.0786        0.103      0.00108\n",
            "     49     2      0.00914      0.00913     9.99e-07        0.055       0.0739       0.0437       0.0778       0.0607       0.0571       0.0994       0.0782       0.0567     0.000591\n",
            "     49     3      0.00842      0.00842     1.48e-06       0.0533        0.071       0.0424       0.0751       0.0587       0.0547       0.0956       0.0751       0.0736     0.000767\n",
            "     49     4      0.00894      0.00894      1.2e-06       0.0545       0.0732       0.0435       0.0766         0.06       0.0562       0.0987       0.0774       0.0723     0.000753\n",
            "     49     5      0.00837      0.00836      1.5e-06       0.0534       0.0708       0.0433       0.0736       0.0585       0.0565       0.0929       0.0747       0.0632     0.000658\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              49  154.548    0.005      0.00943     1.86e-05      0.00945       0.0573       0.0751       0.0477       0.0766       0.0621        0.061       0.0975       0.0792        0.272      0.00283\n",
            "! Validation         49  154.548    0.005      0.00882     1.58e-06      0.00882       0.0545       0.0726       0.0436       0.0764         0.06       0.0566       0.0971       0.0768       0.0738     0.000769\n",
            "Wall time: 154.548477649\n",
            "! Best model       49    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     50     1      0.00875       0.0087     5.85e-05       0.0541       0.0721       0.0433       0.0758       0.0596       0.0548        0.098       0.0764        0.565      0.00589\n",
            "     50     2      0.00816      0.00815     3.73e-06       0.0528       0.0698       0.0431       0.0723       0.0577       0.0545       0.0933       0.0739        0.139      0.00145\n",
            "     50     3      0.00836      0.00835     1.06e-05        0.053       0.0707       0.0429       0.0732        0.058       0.0566       0.0927       0.0746        0.229      0.00238\n",
            "     50     4      0.00785      0.00783     1.41e-05       0.0521       0.0685       0.0412       0.0739       0.0576       0.0517       0.0934       0.0725        0.273      0.00284\n",
            "     50     5      0.00759      0.00758     1.21e-06       0.0513       0.0674       0.0419       0.0702        0.056        0.053       0.0895       0.0712       0.0801     0.000834\n",
            "     50     6      0.00723      0.00723     1.86e-06       0.0502       0.0658       0.0401       0.0705       0.0553       0.0501       0.0892       0.0697       0.0883      0.00092\n",
            "     50     7      0.00874      0.00873     1.13e-05       0.0557       0.0723       0.0483       0.0705       0.0594       0.0611       0.0906       0.0758        0.237      0.00247\n",
            "     50     8      0.00944      0.00943     1.48e-05       0.0571       0.0751       0.0484       0.0746       0.0615       0.0628       0.0951       0.0789        0.261      0.00272\n",
            "     50     9      0.00779      0.00779     1.99e-06       0.0522       0.0683       0.0421       0.0724       0.0572       0.0536       0.0908       0.0722       0.0967      0.00101\n",
            "     50    10      0.00998      0.00996     2.06e-05       0.0593       0.0772       0.0504        0.077       0.0637       0.0649       0.0973       0.0811        0.335      0.00349\n",
            "     50    11       0.0106       0.0106     7.73e-07       0.0611       0.0796       0.0506       0.0822       0.0664       0.0637        0.104        0.084       0.0555     0.000578\n",
            "     50    12      0.00799      0.00799     7.82e-06       0.0517       0.0691       0.0408       0.0736       0.0572       0.0532       0.0932       0.0732        0.195      0.00203\n",
            "     50    13      0.00877      0.00876      1.2e-05       0.0546       0.0724       0.0431       0.0777       0.0604       0.0554       0.0979       0.0767        0.224      0.00233\n",
            "     50    14      0.00743       0.0074     2.27e-05        0.051       0.0666       0.0413       0.0704       0.0558       0.0526       0.0881       0.0704        0.349      0.00364\n",
            "     50    15      0.00942      0.00942     4.62e-07       0.0559       0.0751       0.0434       0.0808       0.0621       0.0577        0.101       0.0795       0.0361     0.000376\n",
            "     50    16      0.00838      0.00837     6.44e-06       0.0531       0.0708       0.0417        0.076       0.0588       0.0546       0.0953       0.0749         0.15      0.00156\n",
            "     50    17      0.00984      0.00984      7.4e-07       0.0581       0.0767       0.0465       0.0814       0.0639       0.0584        0.104       0.0813       0.0547      0.00057\n",
            "     50    18      0.00933      0.00933     1.19e-06        0.057       0.0747       0.0447       0.0817       0.0632       0.0567        0.102       0.0791       0.0656     0.000684\n",
            "     50    19      0.00745      0.00745     1.68e-06       0.0502       0.0668       0.0404       0.0698       0.0551        0.052       0.0892       0.0706       0.0934     0.000972\n",
            "     50    20      0.00967      0.00967     1.46e-06       0.0567       0.0761       0.0446        0.081       0.0628       0.0583        0.103       0.0805       0.0611     0.000637\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     50     1      0.00913      0.00913     2.65e-06       0.0559       0.0739       0.0446       0.0784       0.0615       0.0579       0.0984       0.0782        0.102      0.00106\n",
            "     50     2      0.00906      0.00905     1.04e-06       0.0548       0.0736       0.0435       0.0775       0.0605       0.0568       0.0991       0.0779       0.0579     0.000603\n",
            "     50     3      0.00836      0.00836     1.43e-06       0.0531       0.0707       0.0422       0.0748       0.0585       0.0544       0.0953       0.0749       0.0718     0.000748\n",
            "     50     4      0.00886      0.00886     1.21e-06       0.0542       0.0728       0.0432       0.0763       0.0598       0.0558       0.0983       0.0771       0.0729     0.000759\n",
            "     50     5      0.00829      0.00829     1.54e-06       0.0532       0.0704       0.0431       0.0734       0.0582       0.0562       0.0925       0.0744        0.064     0.000666\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              50  157.597    0.005      0.00863     9.69e-06      0.00864       0.0544       0.0719       0.0439       0.0753       0.0596       0.0564       0.0955        0.076        0.179      0.00187\n",
            "! Validation         50  157.597    0.005      0.00874     1.57e-06      0.00874       0.0542       0.0723       0.0433       0.0761       0.0597       0.0562       0.0968       0.0765       0.0737     0.000768\n",
            "Wall time: 157.59782722499995\n",
            "! Best model       50    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     51     1       0.0113       0.0113     4.26e-05       0.0643       0.0821       0.0555       0.0821       0.0688       0.0712          0.1       0.0858        0.479      0.00499\n",
            "     51     2      0.00959      0.00958     7.47e-07       0.0587       0.0757       0.0493       0.0773       0.0633       0.0613       0.0984       0.0799       0.0617     0.000643\n",
            "     51     3      0.00813      0.00813     8.87e-07       0.0519       0.0698       0.0403       0.0751       0.0577       0.0512       0.0968        0.074         0.06     0.000625\n",
            "     51     4      0.00977      0.00975     1.39e-05       0.0578       0.0764       0.0463        0.081       0.0636       0.0593        0.102       0.0808        0.266      0.00277\n",
            "     51     5       0.0137       0.0136     0.000104       0.0702       0.0901       0.0618       0.0871       0.0744       0.0769        0.112       0.0944         0.75      0.00781\n",
            "     51     6        0.012        0.012     4.05e-05       0.0664       0.0847       0.0597       0.0799       0.0698       0.0752        0.101       0.0881        0.472      0.00492\n",
            "     51     7      0.00846      0.00843     2.29e-05       0.0539        0.071        0.044       0.0736       0.0588       0.0564       0.0937       0.0751        0.353      0.00368\n",
            "     51     8      0.00871       0.0087     5.51e-06       0.0552       0.0722        0.045       0.0755       0.0603       0.0576       0.0948       0.0762        0.123      0.00128\n",
            "     51     9      0.00941       0.0094     4.89e-06       0.0586        0.075       0.0502       0.0754       0.0628       0.0635        0.094       0.0787        0.159      0.00165\n",
            "     51    10      0.00987      0.00986     7.14e-06       0.0589       0.0768       0.0482       0.0803       0.0643       0.0613        0.101       0.0811        0.186      0.00194\n",
            "     51    11      0.00957      0.00952     4.27e-05       0.0579       0.0755       0.0462       0.0812       0.0637       0.0586        0.101       0.0799        0.484      0.00505\n",
            "     51    12      0.00843      0.00842     8.37e-06       0.0537        0.071       0.0433       0.0746       0.0589        0.055       0.0953       0.0751        0.206      0.00214\n",
            "     51    13      0.00824      0.00815      8.7e-05       0.0526       0.0699       0.0423       0.0732       0.0578       0.0545       0.0933       0.0739        0.692      0.00721\n",
            "     51    14      0.00942       0.0094     1.77e-05       0.0571        0.075       0.0478       0.0758       0.0618       0.0616       0.0963        0.079        0.298       0.0031\n",
            "     51    15       0.0089      0.00885     5.63e-05        0.055       0.0728       0.0442       0.0766       0.0604       0.0563       0.0977        0.077        0.554      0.00577\n",
            "     51    16       0.0085      0.00842     8.07e-05       0.0536        0.071       0.0431       0.0746       0.0588       0.0544       0.0959       0.0751        0.663       0.0069\n",
            "     51    17       0.0073       0.0073     2.01e-06        0.052       0.0661       0.0453       0.0654       0.0554       0.0568       0.0816       0.0692       0.0787      0.00082\n",
            "     51    18      0.00836      0.00828     8.18e-05       0.0532       0.0704       0.0423        0.075       0.0586       0.0539       0.0951       0.0745        0.671      0.00699\n",
            "     51    19       0.0116       0.0116      6.3e-06       0.0624       0.0834       0.0493       0.0887        0.069       0.0642        0.112       0.0883        0.183      0.00191\n",
            "     51    20      0.00922      0.00922     1.85e-06       0.0576       0.0743       0.0495       0.0738       0.0617       0.0634       0.0923       0.0778       0.0805     0.000838\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     51     1      0.00905      0.00905     2.71e-06       0.0556       0.0736       0.0443       0.0782       0.0612       0.0576       0.0981       0.0778        0.104      0.00108\n",
            "     51     2      0.00898      0.00898     9.46e-07       0.0546       0.0733       0.0432       0.0773       0.0603       0.0564       0.0988       0.0776       0.0542     0.000565\n",
            "     51     3       0.0083       0.0083     1.45e-06       0.0529       0.0705        0.042       0.0746       0.0583       0.0542       0.0951       0.0746       0.0737     0.000768\n",
            "     51     4      0.00878      0.00878     1.27e-06        0.054       0.0725       0.0429       0.0761       0.0595       0.0555        0.098       0.0767       0.0747     0.000778\n",
            "     51     5      0.00822      0.00821     1.47e-06       0.0529       0.0701       0.0429       0.0731        0.058       0.0559       0.0922        0.074       0.0646     0.000672\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              51  160.815    0.005      0.00949     3.14e-05      0.00952       0.0576       0.0754       0.0477       0.0773       0.0625        0.061        0.098       0.0795        0.341      0.00355\n",
            "! Validation         51  160.815    0.005      0.00866     1.57e-06      0.00867        0.054        0.072       0.0431       0.0759       0.0595       0.0559       0.0965       0.0762       0.0741     0.000772\n",
            "Wall time: 160.81556739999996\n",
            "! Best model       51    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     52     1      0.00832      0.00831     7.26e-06       0.0528       0.0705       0.0414       0.0756       0.0585       0.0527       0.0968       0.0747        0.178      0.00185\n",
            "     52     2       0.0104       0.0104     1.03e-05       0.0623        0.079       0.0561       0.0748       0.0654       0.0696       0.0951       0.0824        0.224      0.00233\n",
            "     52     3      0.00893      0.00893     1.51e-06       0.0559       0.0731       0.0478       0.0721         0.06       0.0615        0.092       0.0768       0.0832     0.000867\n",
            "     52     4      0.00805      0.00804     9.26e-07       0.0526       0.0694       0.0411       0.0755       0.0583       0.0532       0.0937       0.0735       0.0588     0.000612\n",
            "     52     5      0.00778      0.00776     2.17e-05        0.052       0.0682       0.0418       0.0723       0.0571       0.0536       0.0905       0.0721        0.327       0.0034\n",
            "     52     6      0.00917      0.00916     2.45e-06       0.0564       0.0741       0.0453       0.0785       0.0619       0.0584       0.0982       0.0783        0.107      0.00112\n",
            "     52     7       0.0085      0.00848     1.72e-05       0.0545       0.0712       0.0431       0.0773       0.0602       0.0542       0.0967       0.0754        0.304      0.00317\n",
            "     52     8      0.00954      0.00953      3.5e-06       0.0567       0.0755       0.0459       0.0782       0.0621       0.0598       0.0998       0.0798       0.0924     0.000962\n",
            "     52     9      0.00786      0.00783     3.41e-05       0.0516       0.0685       0.0419       0.0709       0.0564       0.0535       0.0913       0.0724        0.432       0.0045\n",
            "     52    10       0.0091       0.0091     1.16e-06       0.0568       0.0738       0.0481       0.0743       0.0612       0.0608       0.0946       0.0777        0.074     0.000771\n",
            "     52    11      0.00903      0.00896     6.18e-05       0.0565       0.0732       0.0488        0.072       0.0604       0.0626       0.0909       0.0767        0.579      0.00603\n",
            "     52    12       0.0076      0.00757     3.08e-05       0.0514       0.0673       0.0426       0.0689       0.0558       0.0537       0.0885       0.0711        0.407      0.00424\n",
            "     52    13      0.00824      0.00822     1.92e-05       0.0525       0.0701       0.0427       0.0721       0.0574       0.0554       0.0928       0.0741        0.324      0.00337\n",
            "     52    14       0.0094      0.00934     6.77e-05       0.0577       0.0748       0.0476       0.0779       0.0627       0.0601       0.0977       0.0789        0.609      0.00635\n",
            "     52    15       0.0102       0.0102     4.21e-06       0.0584       0.0779       0.0455       0.0843       0.0649       0.0583        0.107       0.0826        0.128      0.00134\n",
            "     52    16      0.00734      0.00733     8.24e-06        0.051       0.0663       0.0421       0.0688       0.0554       0.0525       0.0875         0.07        0.199      0.00207\n",
            "     52    17      0.00879      0.00879     1.39e-06       0.0535       0.0725       0.0415       0.0774       0.0595       0.0542       0.0995       0.0769       0.0736     0.000767\n",
            "     52    18      0.00875      0.00875     3.81e-06       0.0561       0.0724       0.0458       0.0768       0.0613       0.0578        0.095       0.0764        0.141      0.00146\n",
            "     52    19      0.00815      0.00815     8.09e-07       0.0521       0.0698        0.041       0.0743       0.0576       0.0531       0.0948        0.074       0.0652      0.00068\n",
            "     52    20       0.0094      0.00939     1.52e-05       0.0578        0.075        0.051       0.0714       0.0612       0.0659       0.0904       0.0782         0.27      0.00281\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     52     1      0.00896      0.00896     2.68e-06       0.0553       0.0732       0.0441       0.0778        0.061       0.0572       0.0977       0.0775        0.102      0.00107\n",
            "     52     2       0.0089       0.0089     1.08e-06       0.0543        0.073        0.043        0.077         0.06       0.0561       0.0984       0.0773       0.0573     0.000597\n",
            "     52     3      0.00824      0.00824     1.45e-06       0.0527       0.0702       0.0418       0.0744       0.0581       0.0539       0.0949       0.0744       0.0713     0.000743\n",
            "     52     4       0.0087       0.0087     1.23e-06       0.0537       0.0721       0.0427       0.0758       0.0593       0.0551       0.0977       0.0764       0.0736     0.000767\n",
            "     52     5      0.00813      0.00813     1.47e-06       0.0527       0.0697       0.0426       0.0728       0.0577       0.0555       0.0918       0.0737       0.0626     0.000652\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              52  163.980    0.005      0.00871     1.57e-05      0.00873       0.0549       0.0722       0.0451       0.0747       0.0599       0.0577       0.0947       0.0762        0.234      0.00243\n",
            "! Validation         52  163.980    0.005      0.00858     1.58e-06      0.00859       0.0538       0.0717       0.0428       0.0756       0.0592       0.0556       0.0961       0.0758       0.0734     0.000765\n",
            "Wall time: 163.9805650699999\n",
            "! Best model       52    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     53     1      0.00924      0.00921     2.62e-05       0.0575       0.0743       0.0474       0.0778       0.0626       0.0604       0.0962       0.0783        0.373      0.00389\n",
            "     53     2       0.0102       0.0102     1.16e-05       0.0604       0.0782       0.0477        0.086       0.0668       0.0599        0.106       0.0828         0.24       0.0025\n",
            "     53     3      0.00793      0.00791     1.87e-05       0.0517       0.0688        0.041       0.0732       0.0571       0.0518        0.094       0.0729        0.312      0.00325\n",
            "     53     4      0.00806      0.00805     1.87e-05       0.0536       0.0694       0.0442       0.0723       0.0583       0.0558       0.0907       0.0732        0.301      0.00314\n",
            "     53     5      0.00844      0.00844     8.65e-07       0.0544       0.0711       0.0453       0.0726        0.059       0.0567       0.0934        0.075       0.0645     0.000671\n",
            "     53     6      0.00978      0.00973      4.8e-05       0.0565       0.0763       0.0447       0.0801       0.0624       0.0584        0.103       0.0808        0.512      0.00533\n",
            "     53     7      0.00742      0.00739     2.21e-05       0.0512       0.0665       0.0413       0.0709       0.0561       0.0518       0.0889       0.0704        0.345      0.00359\n",
            "     53     8      0.00978      0.00973     4.55e-05       0.0588       0.0763       0.0499       0.0765       0.0632       0.0638       0.0966       0.0802        0.493      0.00513\n",
            "     53     9      0.00706      0.00702     3.64e-05       0.0491       0.0648       0.0382       0.0711       0.0546       0.0483       0.0891       0.0687        0.444      0.00462\n",
            "     53    10       0.0088      0.00879     8.17e-06        0.057       0.0725       0.0509       0.0692         0.06       0.0638       0.0874       0.0756        0.202       0.0021\n",
            "     53    11      0.00823      0.00821     1.89e-05       0.0546       0.0701       0.0471       0.0696       0.0584       0.0586       0.0888       0.0737        0.314      0.00328\n",
            "     53    12      0.00786      0.00786     2.73e-06       0.0513       0.0686        0.039       0.0759       0.0574       0.0503       0.0951       0.0727        0.109      0.00113\n",
            "     53    13      0.00811      0.00804     6.79e-05       0.0518       0.0694       0.0395       0.0765        0.058       0.0502        0.097       0.0736        0.611      0.00637\n",
            "     53    14      0.00853      0.00849     3.62e-05       0.0538       0.0713       0.0423       0.0768       0.0595        0.054       0.0971       0.0755        0.444      0.00462\n",
            "     53    15      0.00729      0.00721     7.89e-05       0.0487       0.0657       0.0375       0.0712       0.0543       0.0492       0.0901       0.0696        0.655      0.00683\n",
            "     53    16      0.00821      0.00813     7.47e-05       0.0525       0.0698       0.0433        0.071       0.0571       0.0558       0.0916       0.0737        0.638      0.00665\n",
            "     53    17      0.00783      0.00783     2.91e-06       0.0519       0.0685       0.0411       0.0734       0.0572       0.0525       0.0925       0.0725        0.108      0.00112\n",
            "     53    18      0.00788      0.00783     5.43e-05       0.0519       0.0685       0.0424       0.0709       0.0567       0.0542       0.0904       0.0723        0.539      0.00561\n",
            "     53    19         0.01         0.01     1.48e-06       0.0584       0.0774       0.0465        0.082       0.0643       0.0599        0.104       0.0819       0.0746     0.000777\n",
            "     53    20      0.00804      0.00804     1.04e-06        0.052       0.0694       0.0419        0.072        0.057       0.0539       0.0928       0.0734       0.0621     0.000647\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     53     1      0.00888      0.00888      2.8e-06       0.0551       0.0729       0.0438       0.0775       0.0607       0.0569       0.0973       0.0771        0.105       0.0011\n",
            "     53     2      0.00883      0.00882     1.05e-06       0.0541       0.0727       0.0428       0.0768       0.0598       0.0558       0.0981       0.0769       0.0569     0.000593\n",
            "     53     3      0.00818      0.00818     1.47e-06       0.0525         0.07       0.0416       0.0742       0.0579       0.0536       0.0946       0.0741       0.0726     0.000756\n",
            "     53     4      0.00862      0.00862      1.3e-06       0.0535       0.0718       0.0424       0.0756        0.059       0.0548       0.0974       0.0761       0.0758     0.000789\n",
            "     53     5      0.00806      0.00806     1.45e-06       0.0525       0.0694       0.0424       0.0726       0.0575       0.0552       0.0915       0.0734       0.0632     0.000658\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              53  167.085    0.005      0.00841     2.88e-05      0.00844       0.0539       0.0709       0.0436       0.0744        0.059       0.0557       0.0944        0.075        0.342      0.00356\n",
            "! Validation         53  167.085    0.005      0.00851     1.61e-06      0.00851       0.0535       0.0714       0.0426       0.0753        0.059       0.0552       0.0958       0.0755       0.0748     0.000779\n",
            "Wall time: 167.0854384449999\n",
            "! Best model       53    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     54     1      0.00731      0.00731     1.08e-06       0.0502       0.0661       0.0409       0.0688       0.0549       0.0525       0.0872       0.0699       0.0703     0.000732\n",
            "     54     2      0.00891      0.00891      6.3e-06       0.0548        0.073       0.0426       0.0792       0.0609       0.0547          0.1       0.0774        0.181      0.00189\n",
            "     54     3      0.00782      0.00781     1.05e-05       0.0531       0.0684       0.0457        0.068       0.0569       0.0576       0.0859       0.0718        0.239      0.00249\n",
            "     54     4      0.00841      0.00838     2.69e-05       0.0539       0.0708       0.0433        0.075       0.0592       0.0546       0.0953        0.075         0.37      0.00386\n",
            "     54     5      0.00802      0.00795     6.72e-05       0.0525        0.069       0.0409       0.0759       0.0584       0.0518       0.0944       0.0731        0.606      0.00632\n",
            "     54     6      0.00706      0.00704     2.36e-05         0.05       0.0649       0.0407       0.0687       0.0547       0.0511       0.0861       0.0686        0.356      0.00371\n",
            "     54     7       0.0103       0.0103     1.42e-05       0.0603       0.0785       0.0476       0.0855       0.0666       0.0609        0.105       0.0831        0.264      0.00275\n",
            "     54     8       0.0118       0.0116     0.000184        0.064       0.0835       0.0528       0.0865       0.0696       0.0682        0.108        0.088            1       0.0105\n",
            "     54     9      0.00914      0.00914     3.29e-06       0.0573        0.074       0.0509       0.0703       0.0606       0.0644       0.0901       0.0772        0.104      0.00108\n",
            "     54    10      0.00837      0.00834      2.7e-05       0.0528       0.0707       0.0415       0.0754       0.0584       0.0536       0.0961       0.0748        0.383      0.00399\n",
            "     54    11      0.00874      0.00874     4.47e-06       0.0552       0.0723       0.0461       0.0735       0.0598       0.0589       0.0936       0.0762        0.156      0.00163\n",
            "     54    12       0.0096      0.00959     3.13e-06       0.0589       0.0758       0.0493       0.0781       0.0637       0.0618        0.098       0.0799        0.127      0.00132\n",
            "     54    13      0.00895      0.00894     1.33e-06       0.0555       0.0732       0.0439       0.0785       0.0612       0.0561       0.0989       0.0775       0.0633     0.000659\n",
            "     54    14      0.00792      0.00785     7.13e-05       0.0513       0.0685       0.0404       0.0731       0.0568       0.0526       0.0926       0.0726        0.622      0.00648\n",
            "     54    15      0.00772      0.00771     6.94e-06       0.0514        0.068       0.0418       0.0708       0.0563       0.0527       0.0911       0.0719        0.195      0.00203\n",
            "     54    16       0.0084      0.00834     5.98e-05       0.0528       0.0707       0.0423       0.0738       0.0581       0.0548       0.0947       0.0748        0.568      0.00591\n",
            "     54    17      0.00932      0.00925     7.09e-05        0.055       0.0744        0.042       0.0808       0.0614       0.0548        0.103       0.0789        0.621      0.00647\n",
            "     54    18       0.0079      0.00789     8.38e-06       0.0522       0.0687       0.0416       0.0735       0.0575       0.0533       0.0922       0.0727        0.204      0.00212\n",
            "     54    19      0.00915      0.00905     9.61e-05       0.0559       0.0736       0.0472       0.0733       0.0602       0.0604       0.0947       0.0775        0.728      0.00758\n",
            "     54    20       0.0084      0.00834     5.28e-05       0.0547       0.0707       0.0465       0.0711       0.0588       0.0583       0.0905       0.0744        0.536      0.00558\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     54     1      0.00879      0.00878     2.63e-06       0.0548       0.0725       0.0435       0.0772       0.0604       0.0564        0.097       0.0767        0.101      0.00105\n",
            "     54     2      0.00874      0.00874     1.11e-06       0.0539       0.0723       0.0425       0.0765       0.0595       0.0554       0.0977       0.0766       0.0586      0.00061\n",
            "     54     3      0.00812      0.00812     1.45e-06       0.0523       0.0697       0.0414        0.074       0.0577       0.0533       0.0943       0.0738       0.0704     0.000733\n",
            "     54     4      0.00854      0.00854      1.2e-06       0.0532       0.0715       0.0422       0.0753       0.0587       0.0544        0.097       0.0757       0.0727     0.000757\n",
            "     54     5      0.00799      0.00798     1.43e-06       0.0522       0.0691       0.0421       0.0724       0.0573       0.0549       0.0911        0.073       0.0617     0.000643\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              54  170.138    0.005      0.00863      3.7e-05      0.00866       0.0546       0.0719       0.0444        0.075       0.0597       0.0568        0.095       0.0759         0.37      0.00385\n",
            "! Validation         54  170.138    0.005      0.00843     1.56e-06      0.00843       0.0533        0.071       0.0424       0.0751       0.0587       0.0549       0.0955       0.0752       0.0728     0.000759\n",
            "Wall time: 170.1385134809999\n",
            "! Best model       54    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     55     1       0.0065       0.0065     1.34e-06       0.0476       0.0624       0.0381       0.0665       0.0523       0.0479       0.0841        0.066       0.0799     0.000832\n",
            "     55     2      0.00764      0.00759     4.47e-05       0.0508       0.0674       0.0404       0.0716        0.056       0.0511       0.0917       0.0714        0.494      0.00515\n",
            "     55     3      0.00911      0.00907     3.44e-05       0.0563       0.0737       0.0462       0.0764       0.0613       0.0587        0.097       0.0778        0.428      0.00446\n",
            "     55     4      0.00794      0.00792     1.81e-05       0.0517       0.0689       0.0412       0.0728        0.057       0.0527       0.0932       0.0729        0.314      0.00327\n",
            "     55     5      0.00769      0.00769     1.81e-06       0.0514       0.0679       0.0408       0.0725       0.0566       0.0513       0.0924       0.0719       0.0873     0.000909\n",
            "     55     6      0.00806      0.00805     4.78e-06       0.0514       0.0694       0.0401       0.0739        0.057       0.0533       0.0937       0.0735        0.155      0.00161\n",
            "     55     7      0.00879      0.00878     3.56e-06       0.0552       0.0725       0.0443        0.077       0.0606       0.0569       0.0964       0.0767        0.122      0.00127\n",
            "     55     8      0.00995      0.00994     6.94e-06       0.0593       0.0771       0.0497       0.0785       0.0641       0.0626          0.1       0.0813         0.19      0.00198\n",
            "     55     9       0.0132       0.0131     0.000112       0.0694       0.0886       0.0611        0.086       0.0735       0.0767        0.109       0.0927        0.786      0.00819\n",
            "     55    10       0.0124       0.0123     1.28e-05       0.0679       0.0859       0.0606       0.0825       0.0715       0.0754        0.104       0.0896        0.246      0.00256\n",
            "     55    11       0.0081      0.00805     4.49e-05       0.0535       0.0694       0.0437       0.0732       0.0584       0.0552       0.0915       0.0733        0.494      0.00515\n",
            "     55    12      0.00753       0.0075     3.02e-05       0.0508        0.067       0.0414       0.0695       0.0554       0.0529       0.0888       0.0708        0.403       0.0042\n",
            "     55    13      0.00972      0.00971     6.15e-06        0.058       0.0762       0.0486       0.0766       0.0626       0.0626       0.0979       0.0803        0.163       0.0017\n",
            "     55    14       0.0113       0.0112      5.1e-05       0.0626        0.082       0.0519        0.084       0.0679       0.0666        0.106       0.0865        0.528       0.0055\n",
            "     55    15      0.00855      0.00855      2.3e-06        0.053       0.0715       0.0421       0.0748       0.0585       0.0567       0.0944       0.0756        0.102      0.00106\n",
            "     55    16      0.00785      0.00784     2.52e-06       0.0527       0.0685       0.0418       0.0743       0.0581       0.0534       0.0916       0.0725       0.0941     0.000981\n",
            "     55    17         0.01      0.00999     6.99e-06       0.0596       0.0773       0.0486       0.0815        0.065        0.061        0.103       0.0817        0.174      0.00182\n",
            "     55    18      0.00971      0.00971     6.08e-06       0.0595       0.0762       0.0526       0.0733       0.0629       0.0664       0.0928       0.0796        0.165      0.00172\n",
            "     55    19      0.00874      0.00874     9.35e-07       0.0547       0.0723       0.0448       0.0745       0.0597       0.0564       0.0966       0.0765       0.0461      0.00048\n",
            "     55    20      0.00785      0.00779     5.57e-05       0.0516       0.0683       0.0409        0.073        0.057       0.0526       0.0919       0.0723        0.543      0.00566\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     55     1      0.00871      0.00871     2.81e-06       0.0545       0.0722       0.0433        0.077       0.0601       0.0561       0.0967       0.0764        0.105      0.00109\n",
            "     55     2      0.00867      0.00867     1.05e-06       0.0536       0.0721       0.0423       0.0763       0.0593       0.0551       0.0974       0.0763        0.056     0.000583\n",
            "     55     3      0.00806      0.00806     1.43e-06        0.052       0.0694       0.0412       0.0737       0.0575        0.053       0.0941       0.0735       0.0731     0.000762\n",
            "     55     4      0.00847      0.00847     1.31e-06        0.053       0.0712       0.0419       0.0751       0.0585       0.0541       0.0968       0.0754       0.0757     0.000788\n",
            "     55     5      0.00792      0.00792     1.41e-06        0.052       0.0688       0.0419       0.0721        0.057       0.0546       0.0908       0.0727       0.0619     0.000645\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              55  173.200    0.005      0.00901     2.24e-05      0.00903       0.0558       0.0734       0.0459       0.0756       0.0608        0.059        0.096       0.0775        0.281      0.00292\n",
            "! Validation         55  173.200    0.005      0.00836      1.6e-06      0.00837        0.053       0.0708       0.0421       0.0748       0.0585       0.0546       0.0952       0.0749       0.0743     0.000774\n",
            "Wall time: 173.20059271799994\n",
            "! Best model       55    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     56     1      0.00833      0.00828     4.23e-05       0.0533       0.0704       0.0441       0.0716       0.0579       0.0571       0.0914       0.0742        0.476      0.00496\n",
            "     56     2      0.00896      0.00895     9.19e-06        0.055       0.0732       0.0438       0.0774       0.0606       0.0568       0.0981       0.0774        0.219      0.00228\n",
            "     56     3      0.00804      0.00802     1.98e-05       0.0534       0.0693       0.0439       0.0723       0.0581       0.0553       0.0911       0.0732        0.325      0.00338\n",
            "     56     4      0.00808      0.00807     1.03e-05       0.0535       0.0695       0.0428       0.0749       0.0589       0.0537       0.0934       0.0736        0.227      0.00237\n",
            "     56     5      0.00739      0.00739     5.03e-07         0.05       0.0665       0.0394        0.071       0.0552       0.0504       0.0905       0.0704        0.041     0.000427\n",
            "     56     6      0.00789      0.00788     2.92e-06       0.0524       0.0687       0.0421       0.0731       0.0576       0.0531       0.0923       0.0727        0.123      0.00128\n",
            "     56     7      0.00913      0.00912     9.01e-06       0.0564       0.0739       0.0461       0.0771       0.0616       0.0591        0.097        0.078         0.22      0.00229\n",
            "     56     8      0.00854      0.00854     6.18e-06       0.0542       0.0715       0.0441       0.0744       0.0593       0.0556       0.0956       0.0756        0.183      0.00191\n",
            "     56     9      0.00772      0.00772     5.49e-07       0.0511        0.068       0.0406        0.072       0.0563       0.0519       0.0921        0.072        0.043     0.000448\n",
            "     56    10       0.0125       0.0125     9.21e-07       0.0654       0.0864       0.0527       0.0908       0.0718        0.068        0.115       0.0914       0.0607     0.000633\n",
            "     56    11       0.0113       0.0112     6.12e-05       0.0655        0.082       0.0588       0.0789       0.0689       0.0715       0.0997       0.0856        0.575      0.00599\n",
            "     56    12       0.0107       0.0107     3.87e-06       0.0622       0.0799       0.0534       0.0798       0.0666       0.0681       0.0995       0.0838         0.12      0.00125\n",
            "     56    13      0.00839      0.00835      4.2e-05       0.0529       0.0707        0.041       0.0765       0.0588       0.0532       0.0966       0.0749        0.476      0.00496\n",
            "     56    14      0.00802      0.00799     2.81e-05       0.0525       0.0692       0.0408       0.0757       0.0583       0.0521       0.0945       0.0733        0.388      0.00404\n",
            "     56    15      0.00744      0.00743     4.43e-06       0.0501       0.0667       0.0394       0.0716       0.0555       0.0502       0.0912       0.0707        0.151      0.00157\n",
            "     56    16      0.00696      0.00694     2.37e-05       0.0481       0.0644       0.0377        0.069       0.0533       0.0476        0.089       0.0683        0.353      0.00368\n",
            "     56    17      0.00904      0.00903     1.19e-05       0.0558       0.0735       0.0444       0.0786       0.0615       0.0578       0.0976       0.0777        0.248      0.00258\n",
            "     56    18      0.00747      0.00747     2.44e-06       0.0508       0.0669       0.0415       0.0693       0.0554       0.0523       0.0891       0.0707        0.103      0.00107\n",
            "     56    19      0.00707      0.00706     3.26e-06       0.0497        0.065       0.0414       0.0665       0.0539       0.0522        0.085       0.0686        0.109      0.00113\n",
            "     56    20      0.00758      0.00758     3.19e-06       0.0505       0.0674       0.0393        0.073       0.0561       0.0509       0.0918       0.0714        0.113      0.00118\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     56     1      0.00864      0.00863     2.73e-06       0.0543       0.0719        0.043       0.0767       0.0599       0.0558       0.0963       0.0761        0.102      0.00107\n",
            "     56     2      0.00861      0.00861      1.1e-06       0.0534       0.0718       0.0421        0.076       0.0591       0.0549       0.0972        0.076       0.0576       0.0006\n",
            "     56     3        0.008        0.008     1.44e-06       0.0518       0.0692        0.041       0.0736       0.0573       0.0527       0.0939       0.0733       0.0715     0.000745\n",
            "     56     4      0.00841       0.0084     1.25e-06       0.0528       0.0709       0.0417       0.0749       0.0583       0.0537       0.0965       0.0751       0.0734     0.000765\n",
            "     56     5      0.00785      0.00785     1.41e-06       0.0517       0.0685       0.0417       0.0719       0.0568       0.0543       0.0905       0.0724       0.0605     0.000631\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              56  176.258    0.005      0.00851     1.43e-05      0.00853       0.0541       0.0714       0.0439       0.0747       0.0593       0.0562       0.0947       0.0754        0.228      0.00237\n",
            "! Validation         56  176.258    0.005       0.0083     1.58e-06       0.0083       0.0528       0.0705       0.0419       0.0746       0.0583       0.0543       0.0949       0.0746       0.0731     0.000761\n",
            "Wall time: 176.25919745099998\n",
            "! Best model       56    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     57     1      0.00817      0.00817     2.01e-06       0.0536       0.0699       0.0436       0.0736       0.0586       0.0551       0.0927       0.0739       0.0844     0.000879\n",
            "     57     2      0.00766      0.00765     5.47e-06       0.0514       0.0677       0.0418       0.0705       0.0562       0.0533       0.0898       0.0715         0.14      0.00145\n",
            "     57     3      0.00665      0.00665     3.12e-06       0.0477       0.0631       0.0379       0.0672       0.0526       0.0481       0.0855       0.0668        0.123      0.00129\n",
            "     57     4      0.00739      0.00739     2.88e-06       0.0497       0.0665       0.0396         0.07       0.0548       0.0512       0.0897       0.0704        0.099      0.00103\n",
            "     57     5      0.00781       0.0078     4.86e-06       0.0521       0.0683       0.0414       0.0734       0.0574       0.0526       0.0921       0.0723        0.155      0.00162\n",
            "     57     6      0.00935      0.00926     8.66e-05       0.0574       0.0745       0.0451       0.0819       0.0635       0.0565        0.101       0.0789        0.688      0.00717\n",
            "     57     7      0.00876      0.00876     6.02e-06        0.054       0.0724       0.0441        0.074        0.059       0.0574       0.0955       0.0765        0.161      0.00167\n",
            "     57     8      0.00832      0.00828     3.74e-05       0.0529       0.0704       0.0423       0.0741       0.0582       0.0554       0.0935       0.0744        0.451       0.0047\n",
            "     57     9      0.00743      0.00742     6.25e-06       0.0501       0.0667         0.04       0.0703       0.0551        0.052        0.089       0.0705        0.158      0.00165\n",
            "     57    10      0.00756      0.00755     5.28e-06       0.0513       0.0672       0.0417       0.0706       0.0562       0.0531        0.089       0.0711        0.156      0.00162\n",
            "     57    11      0.00731       0.0073     1.05e-05       0.0501       0.0661       0.0399       0.0706       0.0552       0.0512       0.0886       0.0699        0.234      0.00244\n",
            "     57    12      0.00738      0.00738     5.68e-06       0.0492       0.0665       0.0374       0.0728       0.0551       0.0476       0.0933       0.0705        0.173       0.0018\n",
            "     57    13      0.00706      0.00706     6.65e-07       0.0494        0.065       0.0399       0.0684       0.0541       0.0514       0.0859       0.0687       0.0535     0.000557\n",
            "     57    14      0.00764      0.00763     1.54e-05       0.0516       0.0676       0.0408       0.0734       0.0571       0.0512        0.092       0.0716        0.289      0.00301\n",
            "     57    15      0.00801      0.00801     2.95e-06       0.0525       0.0692       0.0436       0.0704        0.057       0.0553       0.0909       0.0731        0.104      0.00109\n",
            "     57    16      0.00818      0.00817     6.45e-06       0.0531       0.0699       0.0447       0.0697       0.0572       0.0567       0.0908       0.0738        0.188      0.00195\n",
            "     57    17      0.00948      0.00946     2.51e-05       0.0565       0.0752       0.0445       0.0804       0.0625       0.0579        0.101       0.0796         0.37      0.00385\n",
            "     57    18      0.00811      0.00811     1.61e-06       0.0527       0.0697       0.0415       0.0752       0.0583       0.0531       0.0944       0.0738       0.0764     0.000795\n",
            "     57    19      0.00989      0.00987     2.25e-05       0.0577       0.0769       0.0469       0.0795       0.0632       0.0609        0.102       0.0812        0.352      0.00367\n",
            "     57    20       0.0103       0.0102     3.25e-05       0.0596       0.0782       0.0491       0.0807       0.0649       0.0619        0.103       0.0827        0.421      0.00439\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     57     1      0.00856      0.00856     2.65e-06        0.054       0.0716       0.0428       0.0764       0.0596       0.0555        0.096       0.0757        0.101      0.00105\n",
            "     57     2      0.00854      0.00854     1.18e-06       0.0532       0.0715       0.0419       0.0759       0.0589       0.0545       0.0969       0.0757       0.0601     0.000626\n",
            "     57     3      0.00795      0.00794     1.41e-06       0.0517        0.069       0.0408       0.0734       0.0571       0.0524       0.0937        0.073       0.0692     0.000721\n",
            "     57     4      0.00833      0.00833     1.26e-06       0.0525       0.0706       0.0415       0.0747       0.0581       0.0534       0.0962       0.0748        0.074     0.000771\n",
            "     57     5      0.00778      0.00778      1.4e-06       0.0515       0.0682       0.0414       0.0717       0.0565        0.054       0.0902       0.0721       0.0608     0.000634\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              57  179.319    0.005      0.00811     1.42e-05      0.00812       0.0526       0.0697       0.0423       0.0733       0.0578       0.0542       0.0932       0.0737        0.224      0.00233\n",
            "! Validation         57  179.319    0.005      0.00823     1.58e-06      0.00823       0.0526       0.0702       0.0417       0.0744        0.058        0.054       0.0946       0.0743       0.0731     0.000761\n",
            "Wall time: 179.31974561699997\n",
            "! Best model       57    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     58     1      0.00785      0.00785     3.71e-06       0.0526       0.0685       0.0414        0.075       0.0582       0.0521       0.0931       0.0726        0.138      0.00143\n",
            "     58     2      0.00659      0.00658     1.27e-05        0.047       0.0627       0.0371        0.067        0.052       0.0474       0.0856       0.0665        0.258      0.00269\n",
            "     58     3      0.00885      0.00881     4.16e-05       0.0544       0.0726       0.0437       0.0758       0.0598       0.0563       0.0973       0.0768         0.47       0.0049\n",
            "     58     4       0.0084      0.00838     1.78e-05        0.054       0.0708       0.0447       0.0727       0.0587       0.0568       0.0927       0.0747        0.306      0.00318\n",
            "     58     5      0.00868      0.00867     1.04e-05       0.0544        0.072       0.0434       0.0765       0.0599       0.0551       0.0974       0.0763         0.22      0.00229\n",
            "     58     6      0.00771      0.00769     1.78e-05       0.0516       0.0679       0.0414        0.072       0.0567       0.0533       0.0902       0.0717        0.306      0.00319\n",
            "     58     7      0.00666      0.00663     2.54e-05       0.0486        0.063       0.0398       0.0663       0.0531       0.0493       0.0839       0.0666        0.369      0.00385\n",
            "     58     8       0.0076       0.0076     2.82e-06       0.0511       0.0674       0.0414       0.0705        0.056       0.0538       0.0887       0.0712        0.109      0.00113\n",
            "     58     9      0.00787      0.00787     7.54e-06        0.052       0.0686       0.0408       0.0742       0.0575       0.0521       0.0933       0.0727        0.199      0.00208\n",
            "     58    10       0.0101       0.0101     3.76e-05       0.0594       0.0776       0.0484       0.0812       0.0648       0.0619        0.102        0.082        0.432       0.0045\n",
            "     58    11      0.00868      0.00867     5.14e-06       0.0548        0.072       0.0475       0.0694       0.0584       0.0601       0.0914       0.0757        0.158      0.00165\n",
            "     58    12      0.00748      0.00747     6.75e-06       0.0513       0.0669       0.0408       0.0724       0.0566       0.0518       0.0897       0.0708        0.172      0.00179\n",
            "     58    13      0.00837      0.00836     5.33e-06       0.0531       0.0707       0.0409       0.0775       0.0592       0.0529       0.0971        0.075        0.163       0.0017\n",
            "     58    14      0.00716      0.00715     1.81e-05       0.0499       0.0654       0.0399       0.0698       0.0549       0.0508       0.0876       0.0692        0.306      0.00319\n",
            "     58    15      0.00868      0.00867      1.2e-05       0.0543        0.072       0.0444        0.074       0.0592       0.0578       0.0943        0.076        0.255      0.00266\n",
            "     58    16      0.00782      0.00782     3.26e-06       0.0514       0.0684       0.0408       0.0725       0.0566        0.052       0.0929       0.0725        0.121      0.00126\n",
            "     58    17       0.0078      0.00777     2.98e-05       0.0515       0.0682       0.0402        0.074       0.0571       0.0508       0.0937       0.0723        0.403      0.00419\n",
            "     58    18      0.00769      0.00769     4.41e-07        0.051       0.0679       0.0409       0.0713       0.0561       0.0524       0.0913       0.0718       0.0424     0.000441\n",
            "     58    19      0.00701        0.007     9.79e-06       0.0485       0.0647       0.0389       0.0677       0.0533       0.0495       0.0875       0.0685        0.219      0.00228\n",
            "     58    20      0.00752      0.00752     9.37e-07       0.0504       0.0671       0.0395       0.0722       0.0558       0.0512       0.0909       0.0711       0.0664     0.000692\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     58     1       0.0085       0.0085     2.66e-06       0.0538       0.0713       0.0426       0.0762       0.0594       0.0552       0.0957       0.0755        0.101      0.00105\n",
            "     58     2      0.00848      0.00847     1.11e-06        0.053       0.0712       0.0417       0.0757       0.0587       0.0542       0.0966       0.0754       0.0568     0.000592\n",
            "     58     3       0.0079      0.00789     1.42e-06       0.0515       0.0687       0.0406       0.0732       0.0569       0.0521       0.0935       0.0728       0.0704     0.000733\n",
            "     58     4      0.00827      0.00826     1.27e-06       0.0523       0.0703       0.0412       0.0744       0.0578       0.0531       0.0959       0.0745       0.0739      0.00077\n",
            "     58     5      0.00772      0.00771     1.37e-06       0.0513        0.068       0.0412       0.0714       0.0563       0.0537       0.0899       0.0718       0.0598     0.000623\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              58  182.370    0.005      0.00791     1.34e-05      0.00793       0.0521       0.0688       0.0418       0.0726       0.0572       0.0535       0.0921       0.0728        0.236      0.00245\n",
            "! Validation         58  182.370    0.005      0.00817     1.56e-06      0.00817       0.0524       0.0699       0.0415       0.0742       0.0578       0.0537       0.0944        0.074       0.0724     0.000754\n",
            "Wall time: 182.37048200899994\n",
            "! Best model       58    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     59     1      0.00664      0.00664     3.22e-06       0.0472        0.063       0.0378       0.0661       0.0519       0.0481       0.0854       0.0668         0.12      0.00125\n",
            "     59     2      0.00712      0.00712      8.1e-07       0.0491       0.0653        0.038       0.0713       0.0546       0.0481       0.0903       0.0692       0.0559     0.000582\n",
            "     59     3      0.00893      0.00892     3.08e-06        0.056       0.0731       0.0459        0.076        0.061       0.0587       0.0956       0.0771        0.112      0.00117\n",
            "     59     4      0.00872       0.0087     1.71e-05       0.0553       0.0722       0.0468       0.0724       0.0596       0.0595       0.0924        0.076        0.301      0.00314\n",
            "     59     5      0.00832      0.00832     2.48e-06       0.0523       0.0706       0.0404       0.0759       0.0582        0.053       0.0966       0.0748          0.1      0.00104\n",
            "     59     6      0.00755      0.00754     1.22e-05       0.0514       0.0672       0.0414       0.0716       0.0565       0.0524       0.0896        0.071        0.256      0.00266\n",
            "     59     7      0.00741       0.0074     1.04e-05       0.0513       0.0665       0.0423       0.0693       0.0558       0.0535        0.087       0.0702        0.239      0.00249\n",
            "     59     8      0.00881       0.0088     1.29e-05       0.0542       0.0726       0.0412       0.0801       0.0607       0.0531        0.101       0.0769        0.266      0.00277\n",
            "     59     9      0.00736      0.00732      4.4e-05       0.0498       0.0662       0.0399       0.0695       0.0547       0.0515       0.0885         0.07        0.492      0.00512\n",
            "     59    10      0.00795      0.00789     5.87e-05       0.0524       0.0687       0.0426       0.0722       0.0574       0.0549       0.0902       0.0725        0.561      0.00584\n",
            "     59    11      0.00667      0.00666     3.51e-06       0.0481       0.0632       0.0385       0.0673       0.0529       0.0487        0.085       0.0668        0.132      0.00138\n",
            "     59    12      0.00867      0.00865      1.9e-05       0.0539       0.0719       0.0433       0.0751       0.0592       0.0555       0.0968       0.0761        0.313      0.00326\n",
            "     59    13      0.00779      0.00777     1.88e-05       0.0508       0.0682       0.0404       0.0714       0.0559       0.0514       0.0931       0.0723        0.306      0.00318\n",
            "     59    14      0.00767      0.00767     1.19e-06       0.0515       0.0677       0.0402        0.074       0.0571       0.0512       0.0923       0.0718       0.0697     0.000726\n",
            "     59    15      0.00695      0.00695     1.76e-06       0.0484       0.0645       0.0386       0.0679       0.0533       0.0488       0.0879       0.0683       0.0824     0.000859\n",
            "     59    16      0.00771       0.0077     3.86e-06       0.0514       0.0679       0.0401       0.0738        0.057       0.0521       0.0917       0.0719        0.128      0.00133\n",
            "     59    17      0.00709      0.00708     5.23e-06         0.05       0.0651       0.0409       0.0682       0.0546       0.0513       0.0863       0.0688        0.162      0.00169\n",
            "     59    18      0.00825      0.00824     1.46e-05       0.0536       0.0702       0.0427       0.0755       0.0591       0.0541       0.0945       0.0743        0.265      0.00276\n",
            "     59    19        0.008      0.00798     1.73e-05       0.0521       0.0691       0.0417       0.0729       0.0573       0.0544       0.0917        0.073        0.294      0.00306\n",
            "     59    20      0.00963      0.00962     1.74e-05       0.0589       0.0759       0.0501       0.0765       0.0633       0.0634        0.096       0.0797        0.306      0.00319\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     59     1      0.00842      0.00842     2.66e-06       0.0535        0.071       0.0423       0.0759       0.0591       0.0548       0.0954       0.0751          0.1      0.00104\n",
            "     59     2       0.0084       0.0084     1.26e-06       0.0528       0.0709       0.0414       0.0754       0.0584       0.0539       0.0963       0.0751        0.061     0.000636\n",
            "     59     3      0.00784      0.00784     1.38e-06       0.0513       0.0685       0.0404        0.073       0.0567       0.0518       0.0933       0.0726       0.0683     0.000711\n",
            "     59     4      0.00819      0.00819     1.27e-06       0.0521         0.07        0.041       0.0742       0.0576       0.0528       0.0956       0.0742       0.0731     0.000762\n",
            "     59     5      0.00764      0.00764     1.38e-06       0.0511       0.0676        0.041       0.0712       0.0561       0.0533       0.0896       0.0715       0.0601     0.000626\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              59  185.442    0.005      0.00785     1.34e-05      0.00786       0.0519       0.0685       0.0417       0.0724        0.057       0.0533       0.0917       0.0725        0.228      0.00237\n",
            "! Validation         59  185.442    0.005       0.0081     1.59e-06       0.0081       0.0521       0.0696       0.0412       0.0739       0.0576       0.0533       0.0941       0.0737       0.0725     0.000756\n",
            "Wall time: 185.442393585\n",
            "! Best model       59    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     60     1       0.0109       0.0109     1.39e-05       0.0632       0.0808       0.0533        0.083       0.0681       0.0678        0.102       0.0848        0.268       0.0028\n",
            "     60     2       0.0122       0.0122     3.24e-05       0.0658       0.0855       0.0539       0.0894       0.0717        0.068        0.112       0.0903        0.421      0.00439\n",
            "     60     3      0.00982      0.00982     7.28e-06       0.0587       0.0766       0.0477       0.0806       0.0641       0.0602        0.102        0.081        0.192        0.002\n",
            "     60     4      0.00815      0.00814     6.58e-06       0.0523       0.0698       0.0416       0.0736       0.0576       0.0541       0.0937       0.0739        0.185      0.00192\n",
            "     60     5      0.00722      0.00716      5.3e-05       0.0492       0.0655       0.0407       0.0663       0.0535       0.0521       0.0862       0.0692        0.538      0.00561\n",
            "     60     6      0.00859      0.00858     9.24e-06       0.0557       0.0716       0.0479       0.0714       0.0596       0.0607       0.0896       0.0752        0.207      0.00216\n",
            "     60     7      0.00801      0.00797     3.89e-05       0.0517       0.0691       0.0409       0.0731        0.057       0.0533       0.0929       0.0731        0.462      0.00481\n",
            "     60     8      0.00771      0.00766     5.75e-05       0.0514       0.0677       0.0415       0.0712       0.0563       0.0518       0.0915       0.0717        0.558      0.00581\n",
            "     60     9        0.009      0.00898     1.48e-05       0.0568       0.0733       0.0489       0.0727       0.0608       0.0617       0.0923        0.077        0.273      0.00285\n",
            "     60    10      0.00804      0.00799      4.5e-05       0.0533       0.0692       0.0434       0.0732       0.0583       0.0542       0.0921       0.0731        0.497      0.00518\n",
            "     60    11      0.00859      0.00857     2.23e-05       0.0546       0.0716        0.042       0.0797       0.0609       0.0528       0.0991       0.0759        0.343      0.00357\n",
            "     60    12      0.00701      0.00701     3.62e-06       0.0493       0.0648       0.0387       0.0705       0.0546        0.049       0.0883       0.0686        0.113      0.00117\n",
            "     60    13      0.00764      0.00763     4.58e-06       0.0511       0.0676       0.0402       0.0728       0.0565       0.0511       0.0921       0.0716        0.134       0.0014\n",
            "     60    14       0.0082      0.00815     4.55e-05       0.0536       0.0699       0.0434       0.0739       0.0587       0.0539        0.094       0.0739        0.495      0.00516\n",
            "     60    15       0.0112       0.0112     1.03e-05       0.0629       0.0817       0.0541       0.0806       0.0674       0.0683        0.103       0.0859        0.211       0.0022\n",
            "     60    16       0.0122       0.0122     3.33e-06       0.0653       0.0856       0.0536       0.0888       0.0712       0.0693        0.111       0.0903        0.121      0.00127\n",
            "     60    17         0.01         0.01     5.42e-06       0.0591       0.0774       0.0488       0.0796       0.0642       0.0613        0.102       0.0817        0.167      0.00174\n",
            "     60    18      0.00859      0.00859     1.62e-06       0.0538       0.0717       0.0445       0.0725       0.0585       0.0576       0.0937       0.0757       0.0834     0.000869\n",
            "     60    19      0.00865      0.00864     2.01e-06       0.0537       0.0719       0.0418       0.0776       0.0597       0.0549       0.0975       0.0762       0.0967      0.00101\n",
            "     60    20        0.011       0.0109      5.2e-05       0.0626       0.0809       0.0551       0.0776       0.0663       0.0705       0.0984       0.0844         0.53      0.00552\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     60     1      0.00836      0.00835     2.65e-06       0.0533       0.0707       0.0421       0.0757       0.0589       0.0545       0.0952       0.0748        0.101      0.00105\n",
            "     60     2      0.00834      0.00833     1.04e-06       0.0526       0.0706       0.0412       0.0752       0.0582       0.0536        0.096       0.0748        0.055     0.000573\n",
            "     60     3      0.00779      0.00778     1.38e-06       0.0511       0.0683       0.0402       0.0728       0.0565       0.0516       0.0931       0.0723       0.0709     0.000739\n",
            "     60     4      0.00814      0.00813     1.25e-06       0.0519       0.0698       0.0408       0.0741       0.0574       0.0524       0.0954       0.0739       0.0743     0.000774\n",
            "     60     5      0.00758      0.00758     1.29e-06       0.0508       0.0674       0.0408        0.071       0.0559        0.053       0.0894       0.0712       0.0586      0.00061\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              60  188.491    0.005      0.00912     2.15e-05      0.00914       0.0562       0.0739       0.0461       0.0764       0.0613        0.059        0.097        0.078        0.295      0.00307\n",
            "! Validation         60  188.491    0.005      0.00804     1.52e-06      0.00804       0.0519       0.0694        0.041       0.0738       0.0574        0.053       0.0938       0.0734       0.0719     0.000749\n",
            "Wall time: 188.49157177899997\n",
            "! Best model       60    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     61     1       0.0133       0.0133     2.85e-06       0.0701       0.0891       0.0625       0.0853       0.0739       0.0775        0.109       0.0931        0.123      0.00128\n",
            "     61     2       0.0093       0.0093     5.12e-06       0.0568       0.0746       0.0465       0.0775        0.062       0.0596       0.0979       0.0788        0.149      0.00155\n",
            "     61     3      0.00683      0.00679     3.67e-05        0.048       0.0638       0.0375       0.0692       0.0533       0.0478       0.0874       0.0676        0.447      0.00466\n",
            "     61     4      0.00762      0.00761     1.45e-06       0.0514       0.0675       0.0407       0.0728       0.0567       0.0511       0.0919       0.0715       0.0615     0.000641\n",
            "     61     5      0.00796      0.00789     7.24e-05       0.0529       0.0687       0.0446       0.0694        0.057       0.0554       0.0897       0.0725        0.628      0.00655\n",
            "     61     6      0.00675      0.00675      2.8e-07       0.0486       0.0636       0.0402       0.0653       0.0528       0.0508       0.0834       0.0671       0.0346      0.00036\n",
            "     61     7      0.00726      0.00724     2.73e-05       0.0497       0.0658       0.0393       0.0705       0.0549       0.0506       0.0887       0.0697        0.379      0.00394\n",
            "     61     8       0.0077       0.0077      2.8e-06       0.0522       0.0679       0.0426       0.0713       0.0569       0.0541       0.0893       0.0717        0.111      0.00116\n",
            "     61     9      0.00906      0.00904     1.68e-05       0.0567       0.0736        0.048       0.0742       0.0611       0.0602       0.0948       0.0775        0.282      0.00293\n",
            "     61    10      0.00775      0.00774     1.26e-05       0.0516       0.0681       0.0406       0.0735        0.057       0.0514       0.0928       0.0721        0.259       0.0027\n",
            "     61    11      0.00749      0.00747     1.14e-05       0.0502       0.0669       0.0409       0.0686       0.0548        0.053       0.0883       0.0707        0.244      0.00254\n",
            "     61    12      0.00979      0.00974     4.44e-05       0.0578       0.0764       0.0465       0.0803       0.0634       0.0599        0.102       0.0807         0.49      0.00511\n",
            "     61    13      0.00976      0.00975     1.27e-05       0.0595       0.0764       0.0507       0.0769       0.0638       0.0631       0.0977       0.0804        0.261      0.00271\n",
            "     61    14      0.00946      0.00944     1.93e-05       0.0569       0.0752        0.047       0.0767       0.0619       0.0599       0.0989       0.0794        0.321      0.00334\n",
            "     61    15      0.00794      0.00794     3.47e-06        0.052       0.0689       0.0398       0.0763       0.0581       0.0514       0.0947        0.073        0.106       0.0011\n",
            "     61    16      0.00886      0.00885     6.81e-06       0.0562       0.0728       0.0481       0.0724       0.0602        0.061        0.092       0.0765        0.187      0.00194\n",
            "     61    17       0.0104       0.0104     3.14e-05       0.0612       0.0788        0.053       0.0775       0.0652       0.0667       0.0987       0.0827        0.406      0.00423\n",
            "     61    18      0.00889      0.00888     9.06e-06       0.0552       0.0729        0.044       0.0777       0.0608       0.0559       0.0985       0.0772        0.213      0.00221\n",
            "     61    19      0.00857      0.00857     5.68e-06       0.0541       0.0716       0.0415       0.0793       0.0604        0.053       0.0988       0.0759        0.152      0.00159\n",
            "     61    20       0.0072      0.00719     1.53e-05       0.0495       0.0656       0.0403       0.0681       0.0542       0.0517       0.0869       0.0693        0.287      0.00299\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     61     1      0.00829      0.00829     2.65e-06       0.0531       0.0704       0.0419       0.0755       0.0587       0.0542       0.0949       0.0746        0.101      0.00105\n",
            "     61     2      0.00827      0.00827     1.07e-06       0.0523       0.0704        0.041        0.075        0.058       0.0533       0.0957       0.0745       0.0547      0.00057\n",
            "     61     3      0.00773      0.00773     1.38e-06       0.0509        0.068         0.04       0.0726       0.0563       0.0513       0.0928       0.0721       0.0688     0.000716\n",
            "     61     4      0.00807      0.00807     1.31e-06       0.0516       0.0695       0.0405       0.0739       0.0572       0.0521       0.0952       0.0737       0.0752     0.000783\n",
            "     61     5      0.00753      0.00752     1.28e-06       0.0506       0.0671       0.0405       0.0708       0.0557       0.0528       0.0891       0.0709       0.0575     0.000599\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              61  191.543    0.005      0.00858     1.69e-05       0.0086       0.0545       0.0717       0.0447       0.0741       0.0594       0.0571       0.0942       0.0757        0.257      0.00268\n",
            "! Validation         61  191.543    0.005      0.00798     1.54e-06      0.00798       0.0517       0.0691       0.0408       0.0735       0.0572       0.0528       0.0936       0.0732       0.0714     0.000743\n",
            "Wall time: 191.5434609969999\n",
            "! Best model       61    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     62     1      0.00708      0.00707     1.81e-06       0.0499       0.0651       0.0393       0.0711       0.0552       0.0493       0.0885       0.0689       0.0787      0.00082\n",
            "     62     2      0.00768      0.00766     2.33e-05        0.051       0.0677        0.041       0.0709        0.056        0.053       0.0902       0.0716        0.354      0.00369\n",
            "     62     3      0.00726      0.00724     2.04e-05       0.0495       0.0658        0.039       0.0705       0.0548       0.0507       0.0886       0.0697        0.325      0.00338\n",
            "     62     4      0.00661       0.0066     3.21e-06       0.0468       0.0629       0.0353       0.0699       0.0526       0.0451       0.0883       0.0667        0.107      0.00111\n",
            "     62     5      0.00843      0.00842     1.16e-05       0.0536        0.071        0.043       0.0749       0.0589        0.055       0.0953       0.0751         0.25      0.00261\n",
            "     62     6      0.00723      0.00722     2.92e-06       0.0496       0.0658       0.0388        0.071       0.0549       0.0492       0.0902       0.0697        0.104      0.00109\n",
            "     62     7      0.00705      0.00704     4.47e-06       0.0502       0.0649       0.0408       0.0691       0.0549       0.0508       0.0865       0.0687        0.147      0.00153\n",
            "     62     8      0.00727      0.00727     7.51e-07       0.0497       0.0659         0.04       0.0692       0.0546       0.0512       0.0883       0.0698       0.0492     0.000513\n",
            "     62     9      0.00806      0.00803     2.77e-05       0.0532       0.0693       0.0447       0.0702       0.0574       0.0558       0.0905       0.0732        0.386      0.00402\n",
            "     62    10      0.00982      0.00982     1.76e-06       0.0587       0.0766       0.0484       0.0792       0.0638       0.0621       0.0996       0.0808       0.0859     0.000895\n",
            "     62    11      0.00705        0.007     5.04e-05       0.0487       0.0647       0.0371       0.0718       0.0545       0.0465       0.0908       0.0686        0.525      0.00546\n",
            "     62    12      0.00825      0.00822     3.24e-05       0.0522       0.0701       0.0411       0.0746       0.0578       0.0534       0.0952       0.0743        0.417      0.00434\n",
            "     62    13      0.00813      0.00809     3.95e-05       0.0523       0.0696       0.0433       0.0703       0.0568       0.0561       0.0908       0.0734        0.463      0.00483\n",
            "     62    14      0.00782      0.00774     8.31e-05       0.0502       0.0681       0.0391       0.0726       0.0558       0.0499       0.0944       0.0722        0.675      0.00703\n",
            "     62    15      0.00784      0.00784     3.04e-06       0.0511       0.0685       0.0401       0.0732       0.0566       0.0527       0.0924       0.0725        0.109      0.00113\n",
            "     62    16      0.00719      0.00717        2e-05       0.0493       0.0655       0.0385        0.071       0.0547       0.0491       0.0898       0.0694        0.328      0.00342\n",
            "     62    17      0.00795      0.00795     1.97e-06        0.052        0.069       0.0425       0.0712       0.0568       0.0546       0.0912       0.0729        0.093     0.000968\n",
            "     62    18      0.00892      0.00892     4.83e-06       0.0556       0.0731        0.047        0.073         0.06       0.0596       0.0944        0.077        0.144       0.0015\n",
            "     62    19      0.00685      0.00684     7.42e-06       0.0487        0.064       0.0391       0.0679       0.0535       0.0491       0.0864       0.0677        0.188      0.00196\n",
            "     62    20      0.00736      0.00736     6.22e-07       0.0512       0.0664       0.0422       0.0694       0.0558       0.0532       0.0869         0.07       0.0461      0.00048\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     62     1      0.00823      0.00822     2.63e-06       0.0529       0.0702       0.0417       0.0752       0.0584       0.0539       0.0946       0.0743          0.1      0.00105\n",
            "     62     2      0.00822      0.00822     1.19e-06       0.0521       0.0701       0.0408       0.0748       0.0578       0.0531       0.0955       0.0743       0.0595      0.00062\n",
            "     62     3      0.00769      0.00769     1.41e-06       0.0507       0.0678       0.0398       0.0725       0.0562       0.0511       0.0926       0.0719       0.0691      0.00072\n",
            "     62     4      0.00801      0.00801     1.27e-06       0.0514       0.0693       0.0403       0.0737        0.057       0.0519       0.0949       0.0734       0.0739      0.00077\n",
            "     62     5      0.00747      0.00747     1.29e-06       0.0504       0.0669       0.0404       0.0706       0.0555       0.0525       0.0889       0.0707       0.0584     0.000608\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              62  194.593    0.005      0.00767     1.71e-05      0.00769       0.0512       0.0678        0.041       0.0715       0.0563       0.0525        0.091       0.0717        0.244      0.00254\n",
            "! Validation         62  194.593    0.005      0.00792     1.56e-06      0.00792       0.0515       0.0689       0.0406       0.0733        0.057       0.0525       0.0933       0.0729       0.0723     0.000753\n",
            "Wall time: 194.59377163299996\n",
            "! Best model       62    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     63     1      0.00846      0.00845     1.15e-05       0.0532       0.0711       0.0427       0.0742       0.0584       0.0558       0.0945       0.0752        0.245      0.00255\n",
            "     63     2      0.00761      0.00758     2.83e-05       0.0513       0.0673         0.04       0.0741        0.057       0.0499       0.0929       0.0714        0.393      0.00409\n",
            "     63     3      0.00935      0.00934     1.09e-05       0.0558       0.0748       0.0427        0.082       0.0623       0.0553        0.103       0.0793        0.237      0.00246\n",
            "     63     4      0.00824      0.00824     3.55e-06       0.0542       0.0702       0.0464       0.0698       0.0581       0.0591       0.0883       0.0737        0.118      0.00123\n",
            "     63     5      0.00978      0.00977     9.84e-06       0.0591       0.0765       0.0509       0.0756       0.0632       0.0644       0.0962       0.0803        0.214      0.00223\n",
            "     63     6       0.0078       0.0078     5.54e-06       0.0525       0.0683       0.0429       0.0718       0.0573       0.0545       0.0898       0.0721        0.163       0.0017\n",
            "     63     7      0.00766      0.00765     3.83e-06       0.0507       0.0677       0.0401       0.0717       0.0559       0.0525       0.0908       0.0716        0.113      0.00118\n",
            "     63     8      0.00981       0.0098     3.31e-06        0.059       0.0766       0.0499       0.0773       0.0636       0.0627       0.0987       0.0807        0.125       0.0013\n",
            "     63     9       0.0124       0.0124     1.57e-05       0.0681       0.0862       0.0614       0.0814       0.0714       0.0755        0.104       0.0899        0.281      0.00292\n",
            "     63    10      0.00842      0.00841     8.63e-06       0.0542        0.071       0.0445       0.0737       0.0591       0.0558       0.0942        0.075        0.216      0.00225\n",
            "     63    11      0.00706      0.00706     3.96e-06       0.0487        0.065       0.0384       0.0692       0.0538       0.0494       0.0883       0.0688        0.131      0.00136\n",
            "     63    12      0.00928      0.00928     4.55e-06       0.0573       0.0745       0.0465       0.0788       0.0627       0.0589       0.0986       0.0787        0.146      0.00152\n",
            "     63    13       0.0108       0.0107     2.35e-05       0.0634       0.0802       0.0551       0.0802       0.0676       0.0682          0.1       0.0841        0.357      0.00372\n",
            "     63    14       0.0106       0.0106      1.1e-06       0.0621       0.0797       0.0543       0.0776        0.066       0.0676       0.0995       0.0836       0.0652      0.00068\n",
            "     63    15      0.00765      0.00765     4.71e-06       0.0515       0.0676       0.0426       0.0692       0.0559       0.0543       0.0885       0.0714        0.152      0.00158\n",
            "     63    16      0.00753      0.00752     1.34e-05       0.0506       0.0671       0.0397       0.0725       0.0561       0.0507       0.0914        0.071        0.265      0.00276\n",
            "     63    17       0.0102       0.0102     6.29e-05       0.0604       0.0781       0.0521       0.0771       0.0646       0.0656       0.0984        0.082        0.586       0.0061\n",
            "     63    18       0.0127       0.0127     7.84e-06        0.068        0.087       0.0575        0.089       0.0733       0.0722        0.111       0.0916        0.197      0.00205\n",
            "     63    19       0.0106       0.0106     1.83e-05       0.0605       0.0796       0.0517       0.0782        0.065       0.0655        0.102       0.0838        0.316       0.0033\n",
            "     63    20      0.00684      0.00682     1.81e-05       0.0484       0.0639        0.039        0.067        0.053       0.0486       0.0867       0.0677        0.311      0.00324\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     63     1      0.00817      0.00817     2.59e-06       0.0526       0.0699       0.0415        0.075       0.0582       0.0537       0.0944        0.074       0.0987      0.00103\n",
            "     63     2      0.00816      0.00816     1.23e-06       0.0519       0.0699       0.0406       0.0746       0.0576       0.0528       0.0953        0.074       0.0593     0.000617\n",
            "     63     3      0.00764      0.00764     1.39e-06       0.0505       0.0676       0.0396       0.0723        0.056       0.0508       0.0925       0.0717       0.0684     0.000712\n",
            "     63     4      0.00796      0.00796     1.31e-06       0.0513        0.069       0.0401       0.0735       0.0568       0.0516       0.0947       0.0731       0.0741     0.000772\n",
            "     63     5      0.00741      0.00741     1.28e-06       0.0502       0.0666       0.0402       0.0704       0.0553       0.0522       0.0886       0.0704       0.0576       0.0006\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              63  197.642    0.005      0.00913      1.3e-05      0.00914       0.0565       0.0739       0.0469       0.0755       0.0612       0.0598       0.0961       0.0779        0.231      0.00241\n",
            "! Validation         63  197.642    0.005      0.00787     1.56e-06      0.00787       0.0513       0.0686       0.0404       0.0732       0.0568       0.0522       0.0931       0.0727       0.0716     0.000746\n",
            "Wall time: 197.64239043099997\n",
            "! Best model       63    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     64     1      0.00852      0.00852     1.27e-06       0.0535       0.0714       0.0432       0.0742       0.0587       0.0561       0.0948       0.0755       0.0684     0.000712\n",
            "     64     2      0.00956      0.00949      6.5e-05       0.0591       0.0754       0.0502        0.077       0.0636       0.0618        0.097       0.0794        0.598      0.00623\n",
            "     64     3      0.00859      0.00859     2.27e-06       0.0542       0.0717       0.0445       0.0738       0.0591        0.057       0.0944       0.0757        0.111      0.00116\n",
            "     64     4      0.00728      0.00728     3.29e-06       0.0499        0.066       0.0395       0.0708       0.0551       0.0496       0.0903       0.0699        0.122      0.00127\n",
            "     64     5      0.00802      0.00801     8.68e-06       0.0519       0.0693       0.0421       0.0715       0.0568       0.0539       0.0927       0.0733         0.21      0.00218\n",
            "     64     6      0.00771       0.0077     7.65e-06       0.0504       0.0679       0.0395       0.0721       0.0558       0.0507       0.0932        0.072        0.192        0.002\n",
            "     64     7      0.00754      0.00754     1.06e-06       0.0503       0.0672       0.0395        0.072       0.0557       0.0518       0.0904       0.0711       0.0594     0.000618\n",
            "     64     8      0.00771       0.0077     1.16e-05        0.051       0.0679       0.0416       0.0699       0.0557       0.0538       0.0897       0.0717        0.215      0.00224\n",
            "     64     9      0.00713      0.00713     1.85e-06       0.0488       0.0653       0.0395       0.0673       0.0534        0.051       0.0872       0.0691       0.0854     0.000889\n",
            "     64    10      0.00809      0.00808     2.37e-06       0.0524       0.0696       0.0414       0.0745        0.058       0.0534       0.0938       0.0736         0.11      0.00115\n",
            "     64    11      0.00657      0.00656     2.69e-06       0.0474       0.0627       0.0382        0.066       0.0521       0.0482       0.0845       0.0663        0.112      0.00117\n",
            "     64    12      0.00808      0.00807     4.67e-06       0.0518       0.0695       0.0393       0.0768       0.0581       0.0498       0.0977       0.0737        0.149      0.00155\n",
            "     64    13      0.00837      0.00837      9.7e-07       0.0525       0.0708       0.0419       0.0738       0.0579       0.0532       0.0968        0.075       0.0703     0.000732\n",
            "     64    14      0.00766      0.00761     5.09e-05       0.0519       0.0675       0.0437       0.0684       0.0561       0.0551       0.0871       0.0711        0.527      0.00549\n",
            "     64    15      0.00729      0.00729     1.11e-06       0.0496        0.066       0.0409        0.067        0.054       0.0524       0.0871       0.0698       0.0717     0.000747\n",
            "     64    16      0.00788      0.00785     3.63e-05       0.0523       0.0685       0.0411       0.0745       0.0578       0.0529       0.0922       0.0725        0.446      0.00465\n",
            "     64    17      0.00791      0.00788     2.63e-05       0.0518       0.0687        0.041       0.0735       0.0572       0.0533       0.0921       0.0727        0.375      0.00391\n",
            "     64    18      0.00773      0.00773     3.34e-06       0.0497        0.068       0.0376        0.074       0.0558       0.0502        0.094       0.0721        0.121      0.00126\n",
            "     64    19      0.00843       0.0084     3.35e-05       0.0532       0.0709       0.0396       0.0805       0.0601       0.0507       0.0997       0.0752        0.413      0.00431\n",
            "     64    20      0.00716      0.00715     8.25e-06       0.0495       0.0654       0.0398       0.0688       0.0543       0.0509       0.0875       0.0692        0.208      0.00216\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     64     1      0.00811       0.0081     2.66e-06       0.0524       0.0696       0.0412       0.0748        0.058       0.0534       0.0941       0.0737          0.1      0.00105\n",
            "     64     2      0.00811      0.00811     1.24e-06       0.0518       0.0697       0.0404       0.0744       0.0574       0.0526        0.095       0.0738       0.0594     0.000618\n",
            "     64     3       0.0076       0.0076     1.44e-06       0.0504       0.0674       0.0395       0.0722       0.0558       0.0506       0.0923       0.0715       0.0708     0.000738\n",
            "     64     4       0.0079       0.0079      1.3e-06        0.051       0.0688       0.0399       0.0733       0.0566       0.0513       0.0944       0.0729       0.0745     0.000776\n",
            "     64     5      0.00737      0.00737     1.23e-06       0.0501       0.0664         0.04       0.0702       0.0551        0.052       0.0885       0.0702       0.0566      0.00059\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              64  200.699    0.005      0.00785     1.37e-05      0.00786       0.0516       0.0685       0.0412       0.0723       0.0568       0.0529       0.0922       0.0725        0.213      0.00222\n",
            "! Validation         64  200.699    0.005      0.00782     1.57e-06      0.00782       0.0511       0.0684       0.0402        0.073       0.0566        0.052       0.0929       0.0724       0.0724     0.000754\n",
            "Wall time: 200.69946229699997\n",
            "! Best model       64    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     65     1      0.00775      0.00775     4.02e-07       0.0508       0.0681       0.0397       0.0731       0.0564       0.0515       0.0928       0.0721       0.0312     0.000326\n",
            "     65     2      0.00784      0.00783     4.95e-06       0.0512       0.0685        0.041       0.0716       0.0563       0.0523       0.0927       0.0725         0.16      0.00167\n",
            "     65     3      0.00855      0.00854     1.58e-05       0.0551       0.0715       0.0471       0.0712       0.0591       0.0594        0.091       0.0752        0.276      0.00287\n",
            "     65     4      0.00765      0.00765     1.92e-06       0.0511       0.0677       0.0413       0.0706        0.056       0.0518       0.0915       0.0717       0.0889     0.000926\n",
            "     65     5       0.0105       0.0105     7.21e-06       0.0626       0.0792       0.0578       0.0721        0.065       0.0725       0.0913       0.0819        0.196      0.00204\n",
            "     65     6      0.00963      0.00963     6.61e-07       0.0599       0.0759       0.0561       0.0673       0.0617       0.0696       0.0873       0.0784        0.042     0.000437\n",
            "     65     7      0.00794      0.00794     2.95e-07       0.0518       0.0689       0.0397       0.0759       0.0578         0.05       0.0962       0.0731       0.0391     0.000407\n",
            "     65     8      0.00762       0.0076     1.98e-05       0.0523       0.0675       0.0445       0.0679       0.0562       0.0555       0.0866        0.071        0.329      0.00342\n",
            "     65     9      0.00997      0.00997     5.47e-07       0.0586       0.0772        0.047       0.0817       0.0644       0.0602        0.103       0.0817       0.0461      0.00048\n",
            "     65    10      0.00964      0.00951     0.000124       0.0562       0.0755        0.044       0.0805       0.0623       0.0565        0.103         0.08        0.823      0.00858\n",
            "     65    11      0.00829      0.00829     3.41e-06        0.055       0.0704       0.0462       0.0726       0.0594       0.0581       0.0902       0.0741        0.114      0.00119\n",
            "     65    12      0.00812      0.00799      0.00013       0.0527       0.0691       0.0427       0.0727       0.0577       0.0542       0.0921       0.0731        0.846      0.00881\n",
            "     65    13      0.00831       0.0083     1.22e-05       0.0546       0.0705       0.0476       0.0685       0.0581       0.0594       0.0887        0.074         0.25       0.0026\n",
            "     65    14      0.00814      0.00809     5.39e-05       0.0532       0.0696       0.0428        0.074       0.0584       0.0553       0.0917       0.0735         0.54      0.00563\n",
            "     65    15      0.00812      0.00806     5.61e-05       0.0529       0.0695       0.0438        0.071       0.0574       0.0565         0.09       0.0732        0.546      0.00569\n",
            "     65    16      0.00848      0.00847     1.24e-05       0.0558       0.0712       0.0488       0.0699       0.0593        0.061       0.0881       0.0745        0.247      0.00257\n",
            "     65    17      0.00799       0.0079     8.84e-05       0.0525       0.0688       0.0426       0.0723       0.0574       0.0535        0.092       0.0727        0.695      0.00724\n",
            "     65    18      0.00743      0.00742     5.38e-06       0.0502       0.0667       0.0403       0.0698       0.0551       0.0519       0.0891       0.0705        0.143      0.00149\n",
            "     65    19      0.00818      0.00815     2.68e-05       0.0524       0.0699       0.0405       0.0763       0.0584       0.0514       0.0967       0.0741        0.365       0.0038\n",
            "     65    20      0.00825      0.00824     1.35e-05       0.0533       0.0702       0.0426       0.0748       0.0587       0.0535       0.0952       0.0744        0.257      0.00268\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     65     1      0.00807      0.00806     2.61e-06       0.0522       0.0695       0.0411       0.0746       0.0578       0.0531        0.094       0.0736          0.1      0.00104\n",
            "     65     2      0.00806      0.00806     1.05e-06       0.0516       0.0694       0.0403       0.0743       0.0573       0.0523       0.0948       0.0736       0.0544     0.000567\n",
            "     65     3      0.00756      0.00756      1.4e-06       0.0502       0.0672       0.0393        0.072       0.0557       0.0504       0.0921       0.0713       0.0704     0.000733\n",
            "     65     4      0.00786      0.00785     1.37e-06       0.0509       0.0686       0.0397       0.0732       0.0565       0.0511       0.0943       0.0727       0.0778     0.000811\n",
            "     65     5      0.00731      0.00731     1.23e-06       0.0499       0.0662       0.0398         0.07       0.0549       0.0517       0.0882         0.07       0.0571     0.000595\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              65  203.743    0.005      0.00839     2.89e-05      0.00842       0.0541       0.0709       0.0448       0.0727       0.0587        0.057       0.0926       0.0748        0.302      0.00314\n",
            "! Validation         65  203.743    0.005      0.00777     1.53e-06      0.00777        0.051       0.0682         0.04       0.0728       0.0564       0.0517       0.0927       0.0722        0.072      0.00075\n",
            "Wall time: 203.74426542599997\n",
            "! Best model       65    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     66     1      0.00826      0.00826     3.72e-06       0.0552       0.0703       0.0472       0.0712       0.0592       0.0595        0.088       0.0738        0.138      0.00144\n",
            "     66     2      0.00693      0.00693     1.23e-06       0.0485       0.0644       0.0388       0.0679       0.0534       0.0488       0.0876       0.0682       0.0723     0.000753\n",
            "     66     3      0.00834      0.00831        3e-05       0.0545       0.0705       0.0447       0.0741       0.0594       0.0564       0.0925       0.0744        0.404      0.00421\n",
            "     66     4      0.00673      0.00673      1.7e-06       0.0479       0.0635       0.0379       0.0678       0.0529       0.0493       0.0851       0.0672        0.082     0.000854\n",
            "     66     5      0.00757      0.00755     2.86e-05       0.0509       0.0672         0.04       0.0727       0.0564        0.051       0.0913       0.0712        0.387      0.00403\n",
            "     66     6      0.00767      0.00767     3.63e-06       0.0519       0.0677       0.0405       0.0745       0.0575       0.0519       0.0915       0.0717        0.109      0.00113\n",
            "     66     7      0.00795      0.00794     1.37e-05       0.0516       0.0689       0.0403       0.0743       0.0573       0.0513       0.0948       0.0731        0.272      0.00284\n",
            "     66     8      0.00762      0.00761     1.29e-05       0.0517       0.0675       0.0431       0.0688        0.056       0.0556       0.0865        0.071        0.254      0.00265\n",
            "     66     9      0.00801        0.008     1.17e-05       0.0532       0.0692       0.0454        0.069       0.0572       0.0575        0.088       0.0728        0.252      0.00263\n",
            "     66    10      0.00681       0.0068     4.81e-06       0.0484       0.0638        0.038       0.0691       0.0535       0.0481       0.0871       0.0676        0.131      0.00137\n",
            "     66    11      0.00824      0.00816     7.71e-05        0.053       0.0699       0.0414       0.0762       0.0588       0.0522        0.096       0.0741        0.643       0.0067\n",
            "     66    12      0.00745      0.00745     1.31e-06        0.051       0.0668       0.0409        0.071        0.056       0.0524       0.0887       0.0706       0.0756     0.000787\n",
            "     66    13      0.00889      0.00883     5.98e-05       0.0564       0.0727       0.0471       0.0752       0.0611       0.0598       0.0933       0.0766         0.57      0.00594\n",
            "     66    14      0.00687      0.00686     1.38e-05       0.0482       0.0641       0.0381       0.0685       0.0533       0.0488       0.0869       0.0678        0.274      0.00286\n",
            "     66    15      0.00922      0.00919     3.03e-05       0.0558       0.0741       0.0458       0.0758       0.0608        0.059       0.0976       0.0783        0.408      0.00425\n",
            "     66    16      0.00685      0.00685     1.98e-06       0.0482        0.064       0.0381       0.0684       0.0532       0.0485       0.0871       0.0678       0.0947     0.000987\n",
            "     66    17      0.00781      0.00779     1.27e-05       0.0516       0.0683        0.041        0.073        0.057       0.0526        0.092       0.0723         0.26      0.00271\n",
            "     66    18      0.00723      0.00722     6.09e-06       0.0475       0.0657       0.0366       0.0691       0.0529       0.0474       0.0921       0.0697        0.176      0.00183\n",
            "     66    19        0.008      0.00796     3.63e-05        0.051        0.069       0.0401       0.0729       0.0565       0.0509       0.0955       0.0732        0.444      0.00463\n",
            "     66    20      0.00774      0.00773     1.29e-05       0.0514        0.068       0.0426       0.0689       0.0557       0.0545       0.0891       0.0718        0.259       0.0027\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     66     1      0.00801      0.00801     2.71e-06        0.052       0.0692       0.0409       0.0744       0.0576       0.0529       0.0937       0.0733        0.101      0.00105\n",
            "     66     2      0.00801      0.00801     1.15e-06       0.0514       0.0692       0.0401       0.0741       0.0571       0.0521       0.0946       0.0733       0.0571     0.000595\n",
            "     66     3      0.00752      0.00752     1.35e-06       0.0501       0.0671       0.0392       0.0719       0.0555       0.0502        0.092       0.0711       0.0692     0.000721\n",
            "     66     4       0.0078       0.0078     1.41e-06       0.0507       0.0683       0.0396        0.073       0.0563       0.0508        0.094       0.0724       0.0782     0.000815\n",
            "     66     5      0.00726      0.00726     1.22e-06       0.0497       0.0659       0.0396       0.0698       0.0547       0.0514        0.088       0.0697       0.0561     0.000584\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              66  206.813    0.005      0.00769     1.82e-05      0.00771       0.0514       0.0678       0.0414       0.0714       0.0564       0.0529       0.0906       0.0718        0.265      0.00276\n",
            "! Validation         66  206.813    0.005      0.00772     1.57e-06      0.00772       0.0508        0.068       0.0398       0.0726       0.0562       0.0515       0.0925        0.072       0.0723     0.000753\n",
            "Wall time: 206.81353766299992\n",
            "! Best model       66    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     67     1      0.00779      0.00773     5.77e-05       0.0519        0.068       0.0428       0.0703       0.0565       0.0545       0.0892       0.0718        0.557       0.0058\n",
            "     67     2      0.00729      0.00728     1.65e-06       0.0487        0.066       0.0375       0.0711       0.0543       0.0485       0.0916         0.07       0.0807      0.00084\n",
            "     67     3      0.00713      0.00712     1.32e-05       0.0501       0.0653       0.0412        0.068       0.0546        0.051        0.087        0.069        0.262      0.00273\n",
            "     67     4      0.00781      0.00779     2.22e-05       0.0513       0.0683       0.0395        0.075       0.0573       0.0507       0.0941       0.0724        0.344      0.00358\n",
            "     67     5      0.00931      0.00926     5.07e-05       0.0564       0.0744       0.0451        0.079       0.0621        0.057        0.101       0.0788        0.527      0.00549\n",
            "     67     6      0.00734      0.00734     7.65e-06       0.0496       0.0663       0.0383       0.0722       0.0553       0.0496       0.0908       0.0702        0.202       0.0021\n",
            "     67     7       0.0092      0.00914     6.37e-05       0.0551       0.0739       0.0429       0.0795       0.0612       0.0552        0.102       0.0784        0.582      0.00606\n",
            "     67     8       0.0114       0.0114      2.8e-06       0.0639       0.0825       0.0571       0.0773       0.0672       0.0722          0.1       0.0861        0.115       0.0012\n",
            "     67     9       0.0133       0.0132     4.67e-05       0.0708        0.089       0.0655       0.0814       0.0735       0.0809        0.103       0.0921        0.499       0.0052\n",
            "     67    10        0.011        0.011     6.55e-05        0.063        0.081       0.0528       0.0834       0.0681       0.0659        0.105       0.0853        0.599      0.00624\n",
            "     67    11      0.00619      0.00611     8.45e-05       0.0456       0.0605       0.0354        0.066       0.0507       0.0443        0.084       0.0641         0.68      0.00709\n",
            "     67    12      0.00907      0.00898     9.45e-05       0.0557       0.0733        0.044       0.0789       0.0615       0.0559       0.0994       0.0776         0.72       0.0075\n",
            "     67    13       0.0118       0.0117     9.71e-05       0.0646       0.0835       0.0531       0.0874       0.0703       0.0666         0.11       0.0882        0.729      0.00759\n",
            "     67    14      0.00854      0.00853     6.62e-06       0.0563       0.0715       0.0517       0.0655       0.0586        0.065       0.0829        0.074        0.167      0.00174\n",
            "     67    15      0.00763      0.00761     1.96e-05       0.0512       0.0675       0.0396       0.0742       0.0569       0.0507       0.0923       0.0715        0.314      0.00327\n",
            "     67    16         0.01         0.01      9.4e-07       0.0616       0.0775       0.0572       0.0706       0.0639       0.0708       0.0894       0.0801       0.0662      0.00069\n",
            "     67    17       0.0111       0.0111     2.49e-06       0.0654       0.0816       0.0597       0.0768       0.0682       0.0737       0.0955       0.0846        0.109      0.00114\n",
            "     67    18      0.00899      0.00897     2.05e-05       0.0549       0.0733       0.0423       0.0802       0.0612       0.0546        0.101       0.0777        0.333      0.00347\n",
            "     67    19      0.00782      0.00782      4.2e-06        0.052       0.0684       0.0431       0.0698       0.0564       0.0553       0.0891       0.0722        0.138      0.00144\n",
            "     67    20       0.0105       0.0105     1.77e-05       0.0595       0.0792       0.0454       0.0877       0.0666       0.0573        0.111        0.084        0.306      0.00319\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     67     1      0.00796      0.00796     2.61e-06       0.0519        0.069       0.0407       0.0742       0.0574       0.0526       0.0935       0.0731       0.0988      0.00103\n",
            "     67     2      0.00796      0.00796     1.19e-06       0.0513        0.069       0.0399        0.074       0.0569       0.0519       0.0944       0.0731       0.0578     0.000602\n",
            "     67     3      0.00747      0.00747     1.41e-06       0.0499       0.0669        0.039       0.0717       0.0553         0.05       0.0918       0.0709       0.0699     0.000728\n",
            "     67     4      0.00775      0.00775     1.37e-06       0.0505       0.0681       0.0393       0.0728       0.0561       0.0505       0.0939       0.0722       0.0768       0.0008\n",
            "     67     5      0.00722      0.00721      1.2e-06       0.0495       0.0657       0.0394       0.0697       0.0545       0.0512       0.0878       0.0695       0.0557      0.00058\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              67  209.868    0.005      0.00913      3.4e-05      0.00916       0.0564       0.0739       0.0467       0.0757       0.0612       0.0598       0.0962        0.078        0.367      0.00382\n",
            "! Validation         67  209.868    0.005      0.00767     1.56e-06      0.00767       0.0506       0.0678       0.0397       0.0725       0.0561       0.0512       0.0923       0.0718       0.0718     0.000748\n",
            "Wall time: 209.8688512079999\n",
            "! Best model       67    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     68     1       0.0106       0.0105     1.81e-05       0.0607       0.0794       0.0495       0.0831       0.0663       0.0628        0.105       0.0839        0.309      0.00322\n",
            "     68     2      0.00868      0.00867     8.23e-06       0.0554        0.072       0.0457        0.075       0.0603       0.0582       0.0938        0.076        0.193      0.00201\n",
            "     68     3      0.00794      0.00792     2.36e-05       0.0509       0.0688       0.0394        0.074       0.0567       0.0507       0.0953        0.073        0.342      0.00357\n",
            "     68     4       0.0135       0.0135     3.45e-05       0.0717       0.0899       0.0681       0.0788       0.0734       0.0838        0.101       0.0924        0.432       0.0045\n",
            "     68     5       0.0154       0.0154     2.33e-06       0.0766       0.0959       0.0674        0.095       0.0812       0.0822        0.119          0.1        0.101      0.00105\n",
            "     68     6       0.0101         0.01     3.16e-05       0.0595       0.0775        0.048       0.0824       0.0652       0.0597        0.104       0.0821        0.413       0.0043\n",
            "     68     7      0.00616      0.00615     1.62e-05       0.0461       0.0607       0.0364       0.0655        0.051        0.046       0.0825       0.0643        0.292      0.00304\n",
            "     68     8      0.00953      0.00944     8.82e-05       0.0563       0.0752       0.0453       0.0781       0.0617       0.0571        0.102       0.0796        0.695      0.00724\n",
            "     68     9      0.00942       0.0094     1.67e-05       0.0574        0.075       0.0492       0.0737       0.0615       0.0624       0.0953       0.0789          0.3      0.00312\n",
            "     68    10      0.00729      0.00728      1.7e-05       0.0496        0.066       0.0404        0.068       0.0542       0.0521       0.0874       0.0697        0.297      0.00309\n",
            "     68    11      0.00774      0.00774      1.4e-06       0.0524        0.068       0.0428       0.0716       0.0572       0.0542       0.0896       0.0719       0.0822     0.000857\n",
            "     68    12      0.00839      0.00839     1.25e-06       0.0538       0.0709       0.0442       0.0731       0.0587       0.0567        0.093       0.0748       0.0705     0.000734\n",
            "     68    13      0.00781      0.00781     2.41e-06       0.0522       0.0684        0.041       0.0745       0.0577       0.0519       0.0928       0.0724        0.101      0.00105\n",
            "     68    14      0.00794      0.00794     5.81e-06       0.0531       0.0689       0.0436       0.0722       0.0579       0.0556       0.0898       0.0727         0.15      0.00156\n",
            "     68    15       0.0099      0.00988     1.77e-05       0.0583       0.0769       0.0466       0.0817       0.0641       0.0594        0.103       0.0814        0.311      0.00324\n",
            "     68    16       0.0079      0.00789     3.01e-06       0.0519       0.0687       0.0411       0.0735       0.0573       0.0518       0.0939       0.0728        0.121      0.00126\n",
            "     68    17      0.00732       0.0073     1.84e-05       0.0495       0.0661        0.038       0.0725       0.0552       0.0488       0.0913       0.0701        0.297       0.0031\n",
            "     68    18      0.00769      0.00769     4.94e-07       0.0507       0.0678       0.0403       0.0714       0.0559       0.0517        0.092       0.0718       0.0422     0.000439\n",
            "     68    19      0.00747      0.00746      8.3e-06       0.0507       0.0668       0.0416       0.0687       0.0552       0.0536       0.0874       0.0705        0.169      0.00176\n",
            "     68    20      0.00642       0.0064     1.72e-05       0.0469       0.0619       0.0371       0.0667       0.0519       0.0471       0.0841       0.0656        0.302      0.00314\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     68     1      0.00791      0.00791     2.68e-06       0.0517       0.0688       0.0405       0.0741       0.0573       0.0524       0.0934       0.0729        0.101      0.00105\n",
            "     68     2      0.00792      0.00792     1.16e-06       0.0511       0.0689       0.0398       0.0738       0.0568       0.0517       0.0942        0.073       0.0565     0.000589\n",
            "     68     3      0.00744      0.00744      1.4e-06       0.0498       0.0667       0.0388       0.0716       0.0552       0.0498       0.0917       0.0707       0.0715     0.000745\n",
            "     68     4      0.00771      0.00771     1.39e-06       0.0503       0.0679       0.0392       0.0727       0.0559       0.0503       0.0937        0.072       0.0771     0.000803\n",
            "     68     5      0.00718      0.00718     1.16e-06       0.0493       0.0655       0.0392       0.0696       0.0544        0.051       0.0877       0.0693       0.0552     0.000575\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              68  212.924    0.005      0.00884     1.66e-05      0.00886       0.0552       0.0727       0.0453        0.075       0.0601       0.0581       0.0955       0.0768        0.251      0.00261\n",
            "! Validation         68  212.924    0.005      0.00763     1.56e-06      0.00763       0.0504       0.0676       0.0395       0.0724       0.0559        0.051       0.0922       0.0716       0.0722     0.000752\n",
            "Wall time: 212.924528346\n",
            "! Best model       68    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     69     1      0.00689      0.00688     6.29e-06       0.0488       0.0642       0.0396       0.0672       0.0534       0.0508       0.0848       0.0678        0.178      0.00186\n",
            "     69     2      0.00686      0.00685      5.3e-06       0.0478       0.0641       0.0388       0.0657       0.0523       0.0497       0.0858       0.0678        0.164      0.00171\n",
            "     69     3      0.00689      0.00689     5.09e-06        0.048       0.0642       0.0374       0.0693       0.0534       0.0476       0.0886       0.0681        0.161      0.00167\n",
            "     69     4      0.00803      0.00803     8.35e-06       0.0519       0.0693       0.0397       0.0762        0.058       0.0506       0.0964       0.0735        0.209      0.00218\n",
            "     69     5      0.00708      0.00707     3.96e-06       0.0491       0.0651       0.0388       0.0696       0.0542       0.0503       0.0874       0.0688        0.127      0.00132\n",
            "     69     6      0.00808      0.00806     1.62e-05       0.0533       0.0695       0.0436       0.0727       0.0581       0.0563       0.0902       0.0732        0.289      0.00301\n",
            "     69     7        0.008      0.00799     3.25e-06       0.0524       0.0692       0.0415       0.0742       0.0578       0.0533       0.0931       0.0732         0.12      0.00125\n",
            "     69     8      0.00609      0.00607     1.95e-05       0.0457       0.0603        0.036       0.0652       0.0506        0.045       0.0827       0.0639        0.326      0.00339\n",
            "     69     9      0.00748      0.00748     4.41e-07       0.0511       0.0669       0.0405       0.0724       0.0565       0.0508       0.0909       0.0709       0.0408     0.000425\n",
            "     69    10      0.00758      0.00756     1.66e-05        0.051       0.0673       0.0399       0.0732       0.0565       0.0499       0.0927       0.0713          0.3      0.00312\n",
            "     69    11      0.00759      0.00759     5.32e-06       0.0508       0.0674       0.0402       0.0721       0.0561        0.051       0.0918       0.0714        0.155      0.00161\n",
            "     69    12      0.00642      0.00641     1.08e-05       0.0469       0.0619       0.0375       0.0656       0.0515       0.0471       0.0841       0.0656        0.241      0.00251\n",
            "     69    13      0.00836      0.00835     4.67e-06       0.0548       0.0707       0.0463       0.0719       0.0591       0.0579        0.091       0.0745        0.149      0.00156\n",
            "     69    14       0.0096      0.00959     7.16e-06       0.0587       0.0758       0.0512       0.0736       0.0624       0.0646       0.0942       0.0794        0.189      0.00197\n",
            "     69    15       0.0073      0.00728     1.87e-05       0.0506        0.066       0.0423       0.0671       0.0547       0.0534       0.0858       0.0696        0.318      0.00331\n",
            "     69    16      0.00725      0.00722     3.11e-05       0.0502       0.0657       0.0411       0.0686       0.0548       0.0519        0.087       0.0695        0.409      0.00426\n",
            "     69    17       0.0112       0.0112     4.36e-05       0.0632       0.0817       0.0523       0.0851       0.0687        0.066        0.106       0.0862        0.485      0.00505\n",
            "     69    18       0.0139       0.0139     2.08e-05       0.0713       0.0911       0.0601       0.0939        0.077       0.0756        0.116       0.0958        0.326       0.0034\n",
            "     69    19      0.00968      0.00968     1.33e-06       0.0583       0.0761       0.0488       0.0772        0.063        0.063       0.0972       0.0801       0.0713     0.000743\n",
            "     69    20      0.00743      0.00743      1.2e-06       0.0501       0.0667       0.0395       0.0712       0.0554       0.0501       0.0912       0.0707       0.0713     0.000743\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     69     1      0.00788      0.00787     2.61e-06       0.0515       0.0686       0.0403       0.0739       0.0571       0.0522       0.0932       0.0727       0.0987      0.00103\n",
            "     69     2      0.00788      0.00788     1.17e-06        0.051       0.0687       0.0396       0.0737       0.0567       0.0515       0.0941       0.0728       0.0564     0.000588\n",
            "     69     3       0.0074       0.0074     1.42e-06       0.0496       0.0666       0.0387       0.0715       0.0551       0.0496       0.0915       0.0705       0.0706     0.000735\n",
            "     69     4      0.00767      0.00767      1.4e-06       0.0502       0.0677        0.039       0.0725       0.0558       0.0501       0.0935       0.0718       0.0771     0.000803\n",
            "     69     5      0.00714      0.00713     1.16e-06       0.0492       0.0653       0.0391       0.0694       0.0542       0.0507       0.0875       0.0691       0.0557      0.00058\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              69  215.979    0.005      0.00807     1.15e-05      0.00809       0.0527       0.0695       0.0428       0.0726       0.0577       0.0548       0.0922       0.0735        0.216      0.00225\n",
            "! Validation         69  215.979    0.005      0.00759     1.55e-06      0.00759       0.0503       0.0674       0.0393       0.0722       0.0558       0.0508        0.092       0.0714       0.0717     0.000747\n",
            "Wall time: 215.97925697699998\n",
            "! Best model       69    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     70     1       0.0103       0.0103     3.86e-05        0.062       0.0785       0.0567       0.0726       0.0647       0.0704       0.0925       0.0815        0.458      0.00477\n",
            "     70     2      0.00951      0.00951     2.64e-06       0.0585       0.0754       0.0507       0.0741       0.0624       0.0641       0.0941       0.0791         0.11      0.00114\n",
            "     70     3      0.00706      0.00704     1.94e-05       0.0489       0.0649       0.0408       0.0652        0.053       0.0527       0.0842       0.0684        0.316      0.00329\n",
            "     70     4      0.00875      0.00875     2.14e-06       0.0562       0.0724       0.0489       0.0708       0.0598       0.0616       0.0901       0.0759       0.0941     0.000981\n",
            "     70     5       0.0077       0.0077     1.51e-06       0.0504       0.0679       0.0391       0.0731       0.0561       0.0504       0.0935        0.072       0.0701      0.00073\n",
            "     70     6      0.00722      0.00718     3.46e-05       0.0504       0.0656       0.0415       0.0682       0.0548       0.0521       0.0865       0.0693        0.426      0.00443\n",
            "     70     7      0.00968      0.00968     1.71e-06       0.0575       0.0761       0.0457        0.081       0.0634        0.058        0.103       0.0806       0.0934     0.000972\n",
            "     70     8      0.00877      0.00869     8.36e-05       0.0539       0.0721       0.0426       0.0764       0.0595       0.0543       0.0985       0.0764        0.677      0.00705\n",
            "     70     9       0.0067       0.0067     8.28e-07       0.0479       0.0633       0.0376       0.0685       0.0531       0.0477       0.0864       0.0671       0.0586      0.00061\n",
            "     70    10      0.00927      0.00918     9.62e-05       0.0554       0.0741       0.0441       0.0781       0.0611       0.0566          0.1       0.0785        0.727      0.00758\n",
            "     70    11       0.0078      0.00779     7.23e-06       0.0523       0.0683       0.0431       0.0705       0.0568       0.0546       0.0895       0.0721        0.185      0.00193\n",
            "     70    12      0.00631      0.00628     3.21e-05       0.0472       0.0613        0.038       0.0657       0.0518        0.048       0.0816       0.0648        0.413      0.00431\n",
            "     70    13      0.00787      0.00785      2.7e-05       0.0521       0.0685       0.0421       0.0722       0.0572       0.0537       0.0913       0.0725        0.381      0.00397\n",
            "     70    14      0.00806      0.00806      9.1e-07       0.0534       0.0694       0.0445       0.0714       0.0579       0.0563       0.0901       0.0732       0.0557      0.00058\n",
            "     70    15      0.00687      0.00685     1.33e-05       0.0487       0.0641       0.0391       0.0681       0.0536       0.0497       0.0858       0.0678        0.256      0.00267\n",
            "     70    16      0.00889      0.00888      6.6e-06       0.0568       0.0729       0.0505       0.0695         0.06       0.0637       0.0885       0.0761        0.189      0.00197\n",
            "     70    17      0.00857      0.00857     8.93e-07       0.0541       0.0716       0.0439       0.0747       0.0593       0.0562       0.0951       0.0757       0.0594     0.000618\n",
            "     70    18      0.00794      0.00794     2.75e-06       0.0524       0.0689       0.0413       0.0746        0.058       0.0519       0.0942        0.073        0.085     0.000885\n",
            "     70    19       0.0069      0.00689     8.77e-06       0.0488       0.0642        0.039       0.0684       0.0537       0.0492       0.0868        0.068        0.204      0.00213\n",
            "     70    20      0.00771       0.0077     7.48e-06       0.0514       0.0679       0.0398       0.0747       0.0572         0.05        0.094        0.072        0.194      0.00202\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     70     1      0.00783      0.00783     2.67e-06       0.0514       0.0685       0.0402       0.0737        0.057        0.052        0.093       0.0725       0.0994      0.00104\n",
            "     70     2      0.00784      0.00784     1.24e-06       0.0509       0.0685       0.0395       0.0736       0.0565       0.0513       0.0939       0.0726       0.0578     0.000602\n",
            "     70     3      0.00737      0.00737     1.38e-06       0.0495       0.0664       0.0385       0.0714        0.055       0.0494       0.0914       0.0704       0.0689     0.000718\n",
            "     70     4      0.00763      0.00763     1.39e-06         0.05       0.0676       0.0389       0.0724       0.0556       0.0499       0.0933       0.0716       0.0759      0.00079\n",
            "     70     5      0.00709      0.00709     1.15e-06        0.049       0.0652       0.0389       0.0693       0.0541       0.0505       0.0873       0.0689       0.0542     0.000565\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              70  219.040    0.005      0.00808     1.94e-05       0.0081       0.0529       0.0695       0.0434       0.0719       0.0577       0.0554       0.0915       0.0734        0.253      0.00263\n",
            "! Validation         70  219.040    0.005      0.00755     1.56e-06      0.00755       0.0502       0.0672       0.0392       0.0721       0.0556       0.0506       0.0918       0.0712       0.0712     0.000742\n",
            "Wall time: 219.04057181899998\n",
            "! Best model       70    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     71     1       0.0081      0.00809     9.83e-06        0.052       0.0696       0.0405       0.0751       0.0578       0.0527       0.0947       0.0737        0.218      0.00227\n",
            "     71     2      0.00859      0.00859     5.56e-07       0.0553       0.0717       0.0468       0.0724       0.0596       0.0589       0.0921       0.0755       0.0447     0.000466\n",
            "     71     3      0.00752      0.00751     5.28e-07        0.051       0.0671       0.0404       0.0723       0.0563        0.051       0.0911        0.071       0.0484     0.000505\n",
            "     71     4       0.0086      0.00859     7.87e-06       0.0549       0.0717       0.0433       0.0782       0.0608        0.054       0.0979        0.076        0.196      0.00205\n",
            "     71     5      0.00712       0.0071     1.72e-05       0.0485       0.0652       0.0386       0.0681       0.0534         0.05        0.088        0.069        0.304      0.00317\n",
            "     71     6      0.00918      0.00914     3.59e-05       0.0558        0.074       0.0446       0.0781       0.0614       0.0567       0.0999       0.0783         0.44      0.00458\n",
            "     71     7      0.00637      0.00634      2.8e-05       0.0459       0.0616       0.0361       0.0655       0.0508       0.0458       0.0848       0.0653        0.392      0.00409\n",
            "     71     8      0.00741      0.00734     6.55e-05       0.0508       0.0663       0.0407        0.071       0.0558       0.0518       0.0885       0.0701        0.598      0.00623\n",
            "     71     9      0.00743       0.0074     2.42e-05       0.0488       0.0666        0.037       0.0723       0.0546       0.0486       0.0925       0.0706        0.356      0.00371\n",
            "     71    10      0.00672      0.00664     7.78e-05        0.049        0.063       0.0405       0.0661       0.0533       0.0505       0.0826       0.0665        0.649      0.00676\n",
            "     71    11      0.00687      0.00687     5.44e-07       0.0482       0.0641       0.0375       0.0696       0.0536       0.0477       0.0883        0.068       0.0504     0.000525\n",
            "     71    12      0.00787      0.00776     0.000106       0.0518       0.0682       0.0416       0.0722       0.0569       0.0533       0.0908       0.0721        0.763      0.00795\n",
            "     71    13      0.00771      0.00767     3.72e-05       0.0502       0.0678       0.0393       0.0721       0.0557       0.0503       0.0933       0.0718        0.448      0.00467\n",
            "     71    14      0.00712      0.00702     9.64e-05       0.0489       0.0648       0.0389       0.0687       0.0538       0.0502        0.087       0.0686        0.721      0.00751\n",
            "     71    15      0.00724       0.0072     4.76e-05       0.0495       0.0656       0.0382       0.0722       0.0552        0.048       0.0912       0.0696        0.509      0.00531\n",
            "     71    16      0.00726      0.00725     1.64e-05       0.0488       0.0659       0.0385       0.0696        0.054       0.0488       0.0908       0.0698        0.296      0.00308\n",
            "     71    17      0.00809      0.00807     2.46e-05       0.0538       0.0695       0.0456       0.0702       0.0579       0.0576       0.0887       0.0731        0.361      0.00376\n",
            "     71    18      0.00723      0.00723     3.65e-06       0.0505       0.0658       0.0419       0.0678       0.0548       0.0532       0.0856       0.0694        0.122      0.00127\n",
            "     71    19      0.00703      0.00701     1.25e-05       0.0482       0.0648       0.0378       0.0689       0.0534       0.0483        0.089       0.0687         0.25      0.00261\n",
            "     71    20      0.00659      0.00658      1.4e-05        0.048       0.0627       0.0393       0.0656       0.0524       0.0502       0.0823       0.0662        0.253      0.00263\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     71     1      0.00779      0.00779     2.63e-06       0.0512       0.0683       0.0401       0.0735       0.0568       0.0518       0.0928       0.0723       0.0986      0.00103\n",
            "     71     2       0.0078      0.00779     1.26e-06       0.0507       0.0683       0.0393       0.0735       0.0564       0.0511       0.0937       0.0724       0.0574     0.000598\n",
            "     71     3      0.00734      0.00734     1.41e-06       0.0494       0.0663       0.0384       0.0714       0.0549       0.0492       0.0913       0.0702       0.0699     0.000728\n",
            "     71     4      0.00758      0.00758     1.46e-06       0.0499       0.0674       0.0387       0.0722       0.0555       0.0497       0.0931       0.0714        0.078     0.000813\n",
            "     71     5      0.00705      0.00705      1.2e-06       0.0489       0.0649       0.0387       0.0691       0.0539       0.0503       0.0871       0.0687        0.055     0.000573\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              71  222.099    0.005      0.00747     3.13e-05       0.0075       0.0505       0.0669       0.0404       0.0708       0.0556       0.0515       0.0901       0.0708        0.351      0.00366\n",
            "! Validation         71  222.099    0.005      0.00751     1.59e-06      0.00751         0.05        0.067       0.0391       0.0719       0.0555       0.0504       0.0916        0.071       0.0718     0.000748\n",
            "Wall time: 222.09985429899996\n",
            "! Best model       71    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     72     1      0.00672      0.00671     8.75e-06       0.0474       0.0634       0.0374       0.0674       0.0524       0.0485       0.0857       0.0671        0.201       0.0021\n",
            "     72     2      0.00656      0.00656     1.92e-06       0.0467       0.0627       0.0369       0.0662       0.0516       0.0475       0.0853       0.0664       0.0883      0.00092\n",
            "     72     3       0.0067      0.00669     6.61e-06        0.048       0.0633       0.0368       0.0704       0.0536       0.0459       0.0883       0.0671        0.166      0.00173\n",
            "     72     4      0.00734      0.00734     1.82e-06       0.0497       0.0663       0.0394       0.0702       0.0548       0.0513       0.0889       0.0701       0.0926     0.000964\n",
            "     72     5      0.00672      0.00672     3.45e-06       0.0481       0.0634       0.0376       0.0689       0.0533       0.0476       0.0868       0.0672        0.119      0.00124\n",
            "     72     6      0.00834      0.00834     8.95e-07       0.0534       0.0707       0.0418       0.0766       0.0592       0.0534       0.0963       0.0749       0.0652      0.00068\n",
            "     72     7      0.00712      0.00711     1.25e-05       0.0498       0.0652       0.0399       0.0698       0.0548       0.0503       0.0878       0.0691        0.246      0.00257\n",
            "     72     8      0.00686      0.00685     1.19e-05        0.049        0.064       0.0401       0.0667       0.0534       0.0501       0.0853       0.0677        0.223      0.00233\n",
            "     72     9      0.00734      0.00734     4.97e-06       0.0483       0.0663       0.0363       0.0725       0.0544       0.0472       0.0934       0.0703        0.154       0.0016\n",
            "     72    10      0.00758      0.00758     8.67e-06       0.0509       0.0673       0.0402       0.0724       0.0563       0.0517       0.0909       0.0713        0.211       0.0022\n",
            "     72    11      0.00724      0.00724     6.34e-06       0.0491       0.0658       0.0384       0.0706       0.0545        0.049       0.0905       0.0697        0.183       0.0019\n",
            "     72    12      0.00701      0.00698     2.69e-05       0.0477       0.0647        0.037       0.0692       0.0531       0.0469       0.0902       0.0686        0.383      0.00399\n",
            "     72    13      0.00621      0.00621     4.07e-07       0.0465        0.061       0.0368       0.0659       0.0513       0.0462        0.083       0.0646       0.0439     0.000458\n",
            "     72    14      0.00681       0.0068     1.65e-05       0.0478       0.0638       0.0371       0.0693       0.0532       0.0474       0.0878       0.0676        0.297       0.0031\n",
            "     72    15      0.00814      0.00813        9e-06       0.0525       0.0697       0.0409       0.0758       0.0584       0.0521       0.0958       0.0739        0.218      0.00227\n",
            "     72    16      0.00794       0.0079     3.76e-05        0.052       0.0688        0.042       0.0719        0.057       0.0538       0.0916       0.0727        0.451       0.0047\n",
            "     72    17      0.00722      0.00722     2.46e-06       0.0501       0.0657       0.0396        0.071       0.0553       0.0512       0.0879       0.0695       0.0979      0.00102\n",
            "     72    18      0.00638      0.00637     5.64e-06       0.0465       0.0618       0.0363       0.0669       0.0516       0.0456       0.0853       0.0655        0.171      0.00178\n",
            "     72    19      0.00665      0.00664     2.04e-06       0.0478       0.0631       0.0384       0.0666       0.0525       0.0484       0.0851       0.0668       0.0812     0.000846\n",
            "     72    20       0.0073       0.0073     7.79e-07       0.0495       0.0661       0.0386       0.0711       0.0549       0.0499       0.0902         0.07       0.0605     0.000631\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     72     1      0.00774      0.00774     2.61e-06        0.051       0.0681       0.0399       0.0733       0.0566       0.0516       0.0925       0.0721       0.0973      0.00101\n",
            "     72     2      0.00774      0.00774     1.29e-06       0.0506       0.0681       0.0392       0.0733       0.0563       0.0508       0.0935       0.0721        0.059     0.000614\n",
            "     72     3       0.0073       0.0073     1.43e-06       0.0492       0.0661       0.0383       0.0712       0.0547        0.049       0.0911       0.0701       0.0697     0.000726\n",
            "     72     4      0.00754      0.00754     1.46e-06       0.0498       0.0672       0.0386        0.072       0.0553       0.0495       0.0929       0.0712       0.0765     0.000797\n",
            "     72     5      0.00701      0.00701     1.18e-06       0.0487       0.0648       0.0386        0.069       0.0538       0.0501       0.0869       0.0685       0.0551     0.000574\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              72  225.156    0.005       0.0071     8.46e-06      0.00711        0.049       0.0652       0.0386         0.07       0.0543       0.0493       0.0889       0.0691        0.178      0.00185\n",
            "! Validation         72  225.156    0.005      0.00746      1.6e-06      0.00747       0.0499       0.0668       0.0389       0.0718       0.0553       0.0502       0.0914       0.0708       0.0715     0.000745\n",
            "Wall time: 225.1567227139999\n",
            "! Best model       72    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     73     1      0.00704      0.00704     3.82e-06       0.0485       0.0649       0.0381       0.0692       0.0537       0.0481       0.0895       0.0688        0.131      0.00137\n",
            "     73     2      0.00678      0.00678     1.25e-06       0.0482       0.0637       0.0389       0.0669       0.0529       0.0498       0.0849       0.0674       0.0754     0.000785\n",
            "     73     3      0.00835      0.00834     1.52e-05        0.053       0.0706       0.0417       0.0758       0.0587        0.053       0.0968       0.0749        0.279      0.00291\n",
            "     73     4      0.00878      0.00877     8.54e-06       0.0549       0.0724       0.0438        0.077       0.0604       0.0549       0.0985       0.0767        0.196      0.00204\n",
            "     73     5      0.00765      0.00765     2.26e-06       0.0507       0.0676       0.0413       0.0696       0.0555       0.0538       0.0891       0.0714       0.0986      0.00103\n",
            "     73     6      0.00787      0.00787     8.01e-06       0.0519       0.0686       0.0414       0.0727       0.0571       0.0528       0.0925       0.0726        0.191      0.00199\n",
            "     73     7      0.00925      0.00925     1.08e-06       0.0578       0.0744       0.0474       0.0786        0.063       0.0593       0.0978       0.0786       0.0619     0.000645\n",
            "     73     8      0.00993      0.00993     2.82e-06       0.0596       0.0771       0.0506       0.0775       0.0641       0.0638       0.0984       0.0811        0.116      0.00121\n",
            "     73     9       0.0086      0.00857      3.5e-05       0.0536       0.0716       0.0428        0.075       0.0589       0.0553       0.0963       0.0758        0.437      0.00455\n",
            "     73    10      0.00662      0.00662     5.93e-07       0.0472        0.063       0.0369       0.0678       0.0523       0.0474        0.086       0.0667       0.0457     0.000476\n",
            "     73    11      0.00912      0.00909     3.14e-05        0.057       0.0737       0.0457       0.0795       0.0626        0.056          0.1       0.0781        0.414      0.00432\n",
            "     73    12      0.00886      0.00886     1.04e-06        0.057       0.0728       0.0469       0.0772       0.0621       0.0588       0.0948       0.0768       0.0711     0.000741\n",
            "     73    13      0.00696      0.00695     1.67e-05       0.0486       0.0645       0.0403       0.0654       0.0528       0.0513        0.085       0.0681        0.283      0.00294\n",
            "     73    14      0.00741      0.00741     7.94e-07       0.0503       0.0666       0.0391       0.0727       0.0559       0.0495       0.0917       0.0706       0.0588     0.000612\n",
            "     73    15      0.00913      0.00906     7.15e-05       0.0568       0.0736       0.0477       0.0749       0.0613       0.0605       0.0946       0.0776        0.628      0.00654\n",
            "     73    16      0.00955      0.00955     1.58e-06       0.0585       0.0756       0.0482       0.0793       0.0637       0.0602       0.0995       0.0799       0.0754     0.000785\n",
            "     73    17      0.00706      0.00706     8.39e-07       0.0486        0.065       0.0386       0.0685       0.0536       0.0493       0.0883       0.0688       0.0527     0.000549\n",
            "     73    18      0.00766      0.00764     2.05e-05       0.0514       0.0676       0.0389       0.0763       0.0576       0.0497       0.0937       0.0717        0.327      0.00341\n",
            "     73    19      0.00977      0.00975     2.04e-05       0.0589       0.0764       0.0488        0.079       0.0639       0.0613       0.0999       0.0806        0.329      0.00342\n",
            "     73    20       0.0109       0.0109     3.79e-05       0.0629       0.0807       0.0543       0.0801       0.0672       0.0675        0.102       0.0847        0.452       0.0047\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     73     1       0.0077      0.00769     2.61e-06       0.0509       0.0679       0.0398       0.0732       0.0565       0.0514       0.0924       0.0719       0.0977      0.00102\n",
            "     73     2       0.0077       0.0077      1.2e-06       0.0504       0.0679        0.039       0.0732       0.0561       0.0506       0.0933       0.0719       0.0571     0.000595\n",
            "     73     3      0.00726      0.00726      1.4e-06       0.0491       0.0659       0.0381       0.0711       0.0546       0.0488        0.091       0.0699       0.0687     0.000715\n",
            "     73     4       0.0075      0.00749     1.47e-06       0.0496        0.067       0.0384       0.0719       0.0552       0.0493       0.0927        0.071       0.0777      0.00081\n",
            "     73     5      0.00696      0.00696     1.17e-06       0.0486       0.0646       0.0384       0.0688       0.0536       0.0499       0.0868       0.0683       0.0551     0.000574\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              73  228.209    0.005      0.00835      1.4e-05      0.00837       0.0538       0.0707       0.0436       0.0741       0.0589       0.0554       0.0941       0.0748        0.216      0.00225\n",
            "! Validation         73  228.209    0.005      0.00742     1.57e-06      0.00742       0.0497       0.0666       0.0387       0.0716       0.0552         0.05       0.0913       0.0706       0.0713     0.000742\n",
            "Wall time: 228.21013737399994\n",
            "! Best model       73    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     74     1      0.00741       0.0074     9.13e-06       0.0499       0.0666       0.0393        0.071       0.0552       0.0508       0.0901       0.0705        0.203      0.00211\n",
            "     74     2      0.00726      0.00726     6.69e-06       0.0501       0.0659       0.0387       0.0728       0.0558       0.0491       0.0906       0.0699        0.179      0.00187\n",
            "     74     3      0.00816      0.00816     9.07e-07       0.0528       0.0699       0.0433       0.0717       0.0575       0.0552       0.0925       0.0739       0.0588     0.000612\n",
            "     74     4      0.00743      0.00742     1.03e-05         0.05       0.0667       0.0381        0.074        0.056       0.0483       0.0931       0.0707        0.233      0.00242\n",
            "     74     5      0.00749      0.00742     6.14e-05       0.0497       0.0667       0.0383       0.0726       0.0554       0.0495       0.0918       0.0707        0.578      0.00602\n",
            "     74     6       0.0075       0.0075     3.32e-06       0.0499        0.067       0.0389       0.0718       0.0554       0.0501       0.0919        0.071        0.111      0.00116\n",
            "     74     7       0.0085      0.00835     0.000152       0.0542       0.0707        0.044       0.0746       0.0593       0.0552       0.0943       0.0748        0.912       0.0095\n",
            "     74     8      0.00799      0.00796     3.04e-05       0.0516        0.069       0.0422       0.0705       0.0564       0.0552       0.0906       0.0729        0.406      0.00423\n",
            "     74     9      0.00723      0.00723     7.79e-06       0.0499       0.0658       0.0396       0.0706       0.0551       0.0502        0.089       0.0696        0.194      0.00202\n",
            "     74    10      0.00804        0.008     4.48e-05       0.0525       0.0692       0.0428       0.0719       0.0573       0.0554       0.0907        0.073        0.491      0.00511\n",
            "     74    11      0.00777      0.00776     5.41e-06       0.0521       0.0682       0.0433       0.0696       0.0565       0.0546       0.0892       0.0719         0.15      0.00156\n",
            "     74    12       0.0073      0.00727     2.82e-05       0.0498        0.066       0.0393       0.0708       0.0551       0.0498       0.0899       0.0699        0.391      0.00407\n",
            "     74    13      0.00712      0.00712     1.89e-06       0.0496       0.0653       0.0399       0.0689       0.0544       0.0507       0.0873        0.069       0.0891     0.000928\n",
            "     74    14      0.00798      0.00791     6.82e-05       0.0515       0.0688       0.0387       0.0773        0.058       0.0498       0.0961        0.073        0.611      0.00637\n",
            "     74    15        0.009      0.00899     1.14e-05       0.0563       0.0733       0.0459       0.0771       0.0615       0.0574       0.0977       0.0776        0.245      0.00255\n",
            "     74    16      0.00701      0.00695     5.72e-05       0.0495       0.0645        0.041       0.0664       0.0537       0.0519       0.0842        0.068         0.56      0.00583\n",
            "     74    17      0.00646      0.00644      1.8e-05       0.0461       0.0621       0.0362       0.0659       0.0511        0.047       0.0845       0.0658        0.309      0.00321\n",
            "     74    18      0.00693      0.00693     1.86e-06       0.0475       0.0644       0.0368       0.0688       0.0528       0.0484       0.0881       0.0682        0.091     0.000948\n",
            "     74    19      0.00728      0.00726      2.4e-05       0.0497       0.0659       0.0393       0.0706       0.0549       0.0508       0.0887       0.0698         0.35      0.00365\n",
            "     74    20      0.00676      0.00672     4.72e-05       0.0488       0.0634        0.039       0.0684       0.0537       0.0488       0.0855       0.0671        0.502      0.00522\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     74     1      0.00765      0.00765     2.57e-06       0.0507       0.0677       0.0396        0.073       0.0563       0.0512       0.0922       0.0717       0.0962        0.001\n",
            "     74     2      0.00766      0.00766     1.29e-06       0.0502       0.0677       0.0388        0.073       0.0559       0.0504       0.0931       0.0717       0.0593     0.000617\n",
            "     74     3      0.00722      0.00722     1.39e-06        0.049       0.0657       0.0379        0.071       0.0545       0.0486       0.0908       0.0697       0.0683     0.000711\n",
            "     74     4      0.00745      0.00745     1.44e-06       0.0494       0.0668       0.0383       0.0718        0.055        0.049       0.0925       0.0708       0.0769     0.000801\n",
            "     74     5      0.00692      0.00692     1.14e-06       0.0484       0.0644       0.0383       0.0687       0.0535       0.0496       0.0866       0.0681       0.0546     0.000569\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              74  231.256    0.005       0.0075     2.95e-05      0.00753       0.0506        0.067       0.0402       0.0713       0.0557       0.0515       0.0904       0.0709        0.333      0.00347\n",
            "! Validation         74  231.256    0.005      0.00738     1.57e-06      0.00738       0.0496       0.0665       0.0386       0.0715        0.055       0.0498       0.0911       0.0704        0.071      0.00074\n",
            "Wall time: 231.25702426599992\n",
            "! Best model       74    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     75     1      0.00702      0.00702      5.9e-07       0.0504       0.0648       0.0411        0.069       0.0551       0.0513       0.0858       0.0685       0.0502     0.000523\n",
            "     75     2      0.00718      0.00714     3.57e-05       0.0488       0.0654       0.0394       0.0676       0.0535       0.0507       0.0876       0.0692         0.44      0.00459\n",
            "     75     3      0.00799      0.00799     2.96e-06       0.0509       0.0692       0.0399        0.073       0.0564       0.0526       0.0939       0.0732        0.118      0.00123\n",
            "     75     4      0.00865      0.00864     1.05e-05       0.0559       0.0719       0.0486       0.0705       0.0595        0.061       0.0898       0.0754        0.232      0.00241\n",
            "     75     5      0.00633      0.00633     3.47e-07       0.0472       0.0616       0.0391       0.0634       0.0513        0.049        0.081        0.065       0.0303     0.000315\n",
            "     75     6      0.00789      0.00787     2.21e-05       0.0518       0.0686       0.0427         0.07       0.0564        0.056       0.0886       0.0723         0.33      0.00344\n",
            "     75     7      0.00874      0.00873     1.13e-05       0.0569       0.0723        0.049       0.0729       0.0609       0.0611       0.0905       0.0758        0.238      0.00248\n",
            "     75     8      0.00912      0.00911     1.76e-05       0.0557       0.0738       0.0454       0.0764       0.0609        0.057       0.0992       0.0781         0.31      0.00323\n",
            "     75     9      0.00733       0.0073      2.8e-05       0.0503       0.0661       0.0392       0.0727       0.0559       0.0493       0.0908         0.07        0.387      0.00403\n",
            "     75    10      0.00717      0.00715     1.86e-05       0.0491       0.0654       0.0391       0.0691       0.0541       0.0494       0.0892       0.0693        0.315      0.00328\n",
            "     75    11      0.00923      0.00923     1.28e-06        0.056       0.0743       0.0455        0.077       0.0613        0.058       0.0992       0.0786       0.0627     0.000653\n",
            "     75    12      0.00911       0.0091     8.13e-06       0.0568       0.0738        0.048       0.0744       0.0612        0.061       0.0944       0.0777        0.195      0.00203\n",
            "     75    13      0.00712      0.00712     8.56e-06       0.0491       0.0653       0.0399       0.0675       0.0537       0.0513       0.0867        0.069         0.21      0.00219\n",
            "     75    14      0.00799      0.00798     1.55e-05       0.0525       0.0691       0.0411       0.0752       0.0582       0.0524        0.094       0.0732        0.283      0.00295\n",
            "     75    15      0.00914      0.00914     3.18e-07       0.0578        0.074       0.0494       0.0745        0.062       0.0616        0.094       0.0778       0.0367     0.000382\n",
            "     75    16      0.00913      0.00913     2.65e-06       0.0576       0.0739       0.0493       0.0743       0.0618       0.0608       0.0949       0.0778       0.0992      0.00103\n",
            "     75    17      0.00729      0.00728     1.53e-05         0.05        0.066       0.0393       0.0714       0.0554       0.0495       0.0904       0.0699        0.289      0.00301\n",
            "     75    18      0.00696      0.00694     1.71e-05       0.0485       0.0644       0.0386       0.0685       0.0535       0.0497       0.0867       0.0682        0.274      0.00285\n",
            "     75    19      0.00963      0.00961     1.54e-05       0.0568       0.0758       0.0429       0.0846       0.0637       0.0552        0.106       0.0804        0.285      0.00297\n",
            "     75    20      0.00804      0.00803     3.72e-06       0.0526       0.0693       0.0426       0.0727       0.0576       0.0545       0.0921       0.0733        0.121      0.00127\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     75     1      0.00762      0.00762     2.65e-06       0.0506       0.0675       0.0395       0.0729       0.0562        0.051       0.0921       0.0715       0.0975      0.00102\n",
            "     75     2      0.00762      0.00762     1.18e-06       0.0501       0.0675       0.0387       0.0729       0.0558       0.0502       0.0929       0.0716       0.0562     0.000585\n",
            "     75     3      0.00719      0.00719     1.41e-06       0.0488       0.0656       0.0378       0.0709       0.0544       0.0484       0.0907       0.0695       0.0702     0.000731\n",
            "     75     4      0.00742      0.00741     1.44e-06       0.0493       0.0666       0.0381       0.0717       0.0549       0.0488       0.0924       0.0706       0.0768       0.0008\n",
            "     75     5      0.00688      0.00688     1.11e-06       0.0483       0.0642       0.0381       0.0686       0.0533       0.0494       0.0864       0.0679       0.0527     0.000549\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              75  234.308    0.005      0.00804     1.18e-05      0.00805       0.0527       0.0694        0.043       0.0722       0.0576       0.0548       0.0919       0.0733        0.215      0.00224\n",
            "! Validation         75  234.308    0.005      0.00734     1.56e-06      0.00735       0.0494       0.0663       0.0384       0.0714       0.0549       0.0496       0.0909       0.0703       0.0707     0.000736\n",
            "Wall time: 234.30897168299998\n",
            "! Best model       75    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     76     1      0.00663      0.00663     1.93e-06       0.0474        0.063       0.0376       0.0669       0.0523       0.0485       0.0848       0.0667       0.0881     0.000918\n",
            "     76     2      0.00704      0.00704     2.42e-06       0.0488       0.0649       0.0378        0.071       0.0544       0.0477       0.0899       0.0688        0.104      0.00108\n",
            "     76     3      0.00863      0.00863     8.58e-07       0.0552       0.0719       0.0462       0.0733       0.0598        0.057       0.0949       0.0759       0.0545     0.000568\n",
            "     76     4       0.0101       0.0101     2.14e-05       0.0598       0.0777       0.0481       0.0832       0.0656       0.0605        0.104       0.0822        0.331      0.00345\n",
            "     76     5      0.00827      0.00825     1.57e-05       0.0536       0.0703       0.0433       0.0744       0.0588        0.055       0.0936       0.0743        0.287      0.00299\n",
            "     76     6      0.00652       0.0065     1.32e-05       0.0469       0.0624       0.0357       0.0695       0.0526       0.0453        0.087       0.0662        0.264      0.00275\n",
            "     76     7      0.00762      0.00761        4e-06       0.0509       0.0675         0.04       0.0726       0.0563       0.0508       0.0923       0.0715        0.142      0.00148\n",
            "     76     8      0.00681      0.00681     3.22e-06       0.0479       0.0638       0.0377       0.0682       0.0529       0.0481       0.0871       0.0676        0.128      0.00133\n",
            "     76     9      0.00596      0.00595     4.64e-06       0.0449       0.0597        0.033       0.0687       0.0508       0.0419       0.0847       0.0633        0.154      0.00161\n",
            "     76    10       0.0072       0.0072     5.67e-06       0.0493       0.0656       0.0382       0.0716       0.0549       0.0491         0.09       0.0696        0.162      0.00169\n",
            "     76    11      0.00718      0.00717     8.56e-06       0.0493       0.0655       0.0398       0.0683        0.054       0.0511       0.0875       0.0693        0.205      0.00214\n",
            "     76    12      0.00697      0.00696     2.25e-06       0.0494       0.0646       0.0389       0.0704       0.0547       0.0488        0.088       0.0684       0.0959     0.000999\n",
            "     76    13      0.00703      0.00702     1.05e-05       0.0493       0.0648       0.0391       0.0697       0.0544       0.0489       0.0884       0.0687        0.236      0.00246\n",
            "     76    14      0.00743      0.00743     2.19e-06       0.0504       0.0667       0.0407       0.0699       0.0553       0.0514       0.0897       0.0706        0.093     0.000968\n",
            "     76    15      0.00625      0.00624      1.2e-05       0.0449       0.0611       0.0345       0.0656       0.0501       0.0451       0.0845       0.0648        0.246      0.00256\n",
            "     76    16      0.00788      0.00788     2.17e-06        0.051       0.0687       0.0397       0.0738       0.0567       0.0511       0.0945       0.0728       0.0729     0.000759\n",
            "     76    17       0.0076      0.00756     4.57e-05       0.0506       0.0673       0.0406       0.0706       0.0556       0.0521       0.0902       0.0712        0.497      0.00518\n",
            "     76    18        0.007      0.00697     2.61e-05       0.0489       0.0646       0.0391       0.0685       0.0538       0.0495       0.0873       0.0684        0.373      0.00388\n",
            "     76    19      0.00723      0.00721     1.32e-05       0.0487       0.0657       0.0386       0.0688       0.0537       0.0505       0.0886       0.0696        0.256      0.00266\n",
            "     76    20      0.00692      0.00691     7.49e-06       0.0486       0.0643        0.039       0.0677       0.0533       0.0496       0.0866       0.0681        0.193      0.00201\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     76     1      0.00758      0.00758     2.56e-06       0.0505       0.0673       0.0393       0.0727        0.056       0.0508       0.0919       0.0713       0.0961        0.001\n",
            "     76     2      0.00759      0.00759     1.23e-06         0.05       0.0674       0.0386       0.0728       0.0557       0.0501       0.0928       0.0714       0.0568     0.000592\n",
            "     76     3      0.00716      0.00716     1.42e-06       0.0487       0.0654       0.0376       0.0708       0.0542       0.0482       0.0906       0.0694       0.0705     0.000734\n",
            "     76     4      0.00738      0.00737     1.46e-06       0.0492       0.0664        0.038       0.0715       0.0548       0.0486       0.0922       0.0704        0.078     0.000813\n",
            "     76     5      0.00684      0.00684     1.12e-06       0.0481        0.064        0.038       0.0684       0.0532       0.0492       0.0863       0.0677       0.0527     0.000549\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              76  237.371    0.005       0.0073     1.02e-05      0.00731       0.0498       0.0661       0.0394       0.0706        0.055       0.0503       0.0898         0.07        0.199      0.00207\n",
            "! Validation         76  237.371    0.005      0.00731     1.56e-06      0.00731       0.0493       0.0661       0.0383       0.0713       0.0548       0.0494       0.0908       0.0701       0.0708     0.000738\n",
            "Wall time: 237.3716097859999\n",
            "! Best model       76    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     77     1       0.0063       0.0063      1.3e-06       0.0463       0.0614       0.0354        0.068       0.0517       0.0443        0.086       0.0651       0.0693     0.000722\n",
            "     77     2      0.00776      0.00776     3.39e-06        0.051       0.0681       0.0401       0.0729       0.0565       0.0511       0.0934       0.0722        0.125       0.0013\n",
            "     77     3      0.00645      0.00644     5.04e-06       0.0468       0.0621       0.0384       0.0637       0.0511       0.0486       0.0827       0.0656        0.149      0.00155\n",
            "     77     4      0.00615      0.00615     7.72e-07        0.045       0.0607       0.0349       0.0653       0.0501       0.0443       0.0844       0.0643       0.0635     0.000661\n",
            "     77     5      0.00755      0.00753     1.93e-05         0.05       0.0671       0.0403       0.0693       0.0548       0.0516       0.0905        0.071        0.316      0.00329\n",
            "     77     6      0.00751      0.00749     1.16e-05       0.0502        0.067        0.038       0.0744       0.0562       0.0488       0.0932        0.071        0.247      0.00258\n",
            "     77     7      0.00633      0.00632     1.01e-05       0.0461       0.0615       0.0358       0.0667       0.0513       0.0456       0.0847       0.0652        0.231       0.0024\n",
            "     77     8      0.00695      0.00694     2.96e-06       0.0489       0.0645        0.038       0.0705       0.0543       0.0485       0.0882       0.0683        0.126      0.00131\n",
            "     77     9      0.00783      0.00782     5.65e-06       0.0521       0.0684        0.041       0.0742       0.0576       0.0518       0.0932       0.0725        0.163       0.0017\n",
            "     77    10      0.00828      0.00827     5.95e-06       0.0527       0.0704       0.0404       0.0773       0.0589       0.0512        0.098       0.0746        0.174      0.00181\n",
            "     77    11      0.00766      0.00765     4.28e-06       0.0512       0.0677       0.0418         0.07       0.0559       0.0538       0.0891       0.0715        0.139      0.00145\n",
            "     77    12      0.00705      0.00705     4.68e-06       0.0496       0.0649       0.0394         0.07       0.0547       0.0498       0.0877       0.0688        0.141      0.00147\n",
            "     77    13      0.00631      0.00629      2.1e-05       0.0461       0.0613        0.036       0.0664       0.0512       0.0468       0.0831        0.065        0.324      0.00338\n",
            "     77    14      0.00626      0.00625     1.36e-05       0.0465       0.0611        0.037       0.0656       0.0513        0.047       0.0825       0.0647        0.247      0.00258\n",
            "     77    15      0.00747      0.00746        1e-05       0.0502       0.0668       0.0404       0.0697        0.055       0.0522       0.0892       0.0707        0.211      0.00219\n",
            "     77    16      0.00728      0.00727      2.2e-06       0.0507        0.066       0.0419       0.0684       0.0551       0.0527       0.0867       0.0697       0.0926     0.000964\n",
            "     77    17      0.00642      0.00642     6.62e-06       0.0479        0.062       0.0383        0.067       0.0527       0.0481       0.0831       0.0656        0.176      0.00183\n",
            "     77    18      0.00613      0.00613     1.58e-06       0.0461       0.0606       0.0375       0.0632       0.0504       0.0467       0.0815       0.0641       0.0879     0.000916\n",
            "     77    19      0.00807      0.00806     3.17e-06       0.0528       0.0695       0.0431       0.0722       0.0577       0.0554       0.0914       0.0734        0.116      0.00121\n",
            "     77    20       0.0072       0.0072     3.41e-06       0.0491       0.0656       0.0365       0.0744       0.0555       0.0462        0.093       0.0696        0.133      0.00139\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     77     1      0.00753      0.00753     2.49e-06       0.0503       0.0671       0.0392       0.0725       0.0559       0.0506       0.0916       0.0711       0.0946     0.000986\n",
            "     77     2      0.00754      0.00754     1.23e-06       0.0499       0.0672       0.0384       0.0727       0.0556       0.0498       0.0926       0.0712       0.0572     0.000596\n",
            "     77     3      0.00713      0.00712     1.38e-06       0.0486       0.0653       0.0375       0.0707       0.0541        0.048       0.0905       0.0692       0.0692     0.000721\n",
            "     77     4      0.00734      0.00734     1.49e-06        0.049       0.0663       0.0379       0.0714       0.0546       0.0485       0.0921       0.0703       0.0786     0.000819\n",
            "     77     5      0.00681       0.0068     1.11e-06        0.048       0.0638       0.0379       0.0683       0.0531        0.049       0.0861       0.0676       0.0523     0.000545\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              77  240.419    0.005      0.00704     6.83e-06      0.00705        0.049       0.0649       0.0387       0.0695       0.0541       0.0493       0.0882       0.0688        0.167      0.00174\n",
            "! Validation         77  240.419    0.005      0.00727     1.54e-06      0.00727       0.0492        0.066       0.0382       0.0711       0.0546       0.0492       0.0906       0.0699       0.0704     0.000733\n",
            "Wall time: 240.419259345\n",
            "! Best model       77    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     78     1      0.00657      0.00656     5.21e-06       0.0474       0.0627       0.0378       0.0667       0.0523       0.0475       0.0852       0.0664        0.123      0.00128\n",
            "     78     2       0.0083      0.00829     1.48e-05       0.0541       0.0704       0.0436       0.0753       0.0594       0.0541        0.095       0.0746         0.28      0.00292\n",
            "     78     3      0.00854      0.00853     1.34e-05       0.0546       0.0714        0.044       0.0759       0.0599       0.0557       0.0954       0.0756        0.272      0.00283\n",
            "     78     4      0.00743      0.00743     4.01e-07        0.051       0.0667       0.0424       0.0683       0.0554        0.054       0.0867       0.0703       0.0398     0.000415\n",
            "     78     5      0.00683      0.00682     8.97e-06       0.0479       0.0639       0.0374       0.0691       0.0532       0.0477       0.0878       0.0677        0.217      0.00226\n",
            "     78     6      0.00856      0.00856     5.32e-07       0.0552       0.0716       0.0475       0.0708       0.0591       0.0595       0.0911       0.0753       0.0369     0.000385\n",
            "     78     7      0.00764      0.00764     2.05e-06       0.0517       0.0676       0.0417       0.0716       0.0566       0.0532       0.0898       0.0715        0.101      0.00105\n",
            "     78     8      0.00803      0.00803     5.48e-07       0.0524       0.0693       0.0424       0.0724       0.0574       0.0543       0.0923       0.0733       0.0469     0.000488\n",
            "     78     9      0.00683      0.00683     1.52e-06       0.0487       0.0639       0.0403       0.0656       0.0529       0.0512       0.0838       0.0675       0.0559     0.000582\n",
            "     78    10      0.00668      0.00668     7.81e-07       0.0478       0.0632       0.0374       0.0686        0.053       0.0473       0.0866        0.067       0.0598     0.000623\n",
            "     78    11      0.00616      0.00616     3.33e-06       0.0448       0.0607       0.0341        0.066       0.0501       0.0428       0.0859       0.0644        0.126      0.00131\n",
            "     78    12      0.00773      0.00773     2.64e-06       0.0507        0.068       0.0389       0.0742       0.0566         0.05       0.0942       0.0721       0.0893      0.00093\n",
            "     78    13      0.00804        0.008     3.85e-05       0.0522       0.0692       0.0421       0.0724       0.0573       0.0544       0.0919       0.0731        0.454      0.00473\n",
            "     78    14      0.00716      0.00715     1.03e-05       0.0494       0.0654       0.0397       0.0689       0.0543       0.0502       0.0884       0.0693         0.23      0.00239\n",
            "     78    15      0.00699      0.00691     7.37e-05       0.0482       0.0643       0.0369       0.0706       0.0538       0.0463       0.0902       0.0682        0.635      0.00662\n",
            "     78    16      0.00887      0.00886     1.18e-05       0.0553       0.0728       0.0484        0.069       0.0587       0.0622       0.0904       0.0763        0.229      0.00238\n",
            "     78    17      0.00751      0.00748     3.79e-05       0.0507       0.0669       0.0406        0.071       0.0558       0.0524        0.089       0.0707        0.451       0.0047\n",
            "     78    18      0.00663      0.00661     1.51e-05       0.0479       0.0629       0.0386       0.0666       0.0526       0.0484       0.0848       0.0666        0.278       0.0029\n",
            "     78    19      0.00718      0.00714     4.03e-05       0.0509       0.0654       0.0418       0.0693       0.0555       0.0526       0.0854        0.069        0.466      0.00486\n",
            "     78    20      0.00921      0.00916     4.72e-05       0.0556        0.074       0.0435       0.0798       0.0617       0.0551        0.102       0.0785        0.502      0.00523\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     78     1      0.00749      0.00749      2.6e-06       0.0502        0.067       0.0391       0.0724       0.0557       0.0504       0.0915       0.0709       0.0963        0.001\n",
            "     78     2      0.00749      0.00749     1.19e-06       0.0497        0.067       0.0383       0.0726       0.0554       0.0496       0.0924        0.071       0.0562     0.000586\n",
            "     78     3      0.00709      0.00709     1.42e-06       0.0484       0.0651       0.0374       0.0706        0.054       0.0478       0.0903       0.0691       0.0703     0.000732\n",
            "     78     4       0.0073       0.0073     1.48e-06       0.0489       0.0661       0.0377       0.0712       0.0545       0.0483       0.0919       0.0701       0.0772     0.000805\n",
            "     78     5      0.00677      0.00677     1.04e-06       0.0479       0.0637       0.0377       0.0682       0.0529       0.0488       0.0859       0.0674       0.0514     0.000535\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              78  243.462    0.005      0.00753     1.64e-05      0.00755       0.0508       0.0671       0.0409       0.0706       0.0558       0.0521       0.0899        0.071        0.235      0.00244\n",
            "! Validation         78  243.462    0.005      0.00723     1.55e-06      0.00723        0.049       0.0658        0.038        0.071       0.0545        0.049       0.0904       0.0697       0.0703     0.000732\n",
            "Wall time: 243.46328405199995\n",
            "! Best model       78    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     79     1      0.00878      0.00871     6.54e-05       0.0543       0.0722       0.0447       0.0737       0.0592       0.0565       0.0962       0.0764        0.599      0.00624\n",
            "     79     2      0.00735      0.00733     1.86e-05       0.0498       0.0662       0.0389       0.0717       0.0553       0.0492       0.0913       0.0702        0.317       0.0033\n",
            "     79     3      0.00769      0.00764     4.24e-05       0.0503       0.0676       0.0381       0.0748       0.0564       0.0489       0.0945       0.0717        0.477      0.00496\n",
            "     79     4      0.00821      0.00821     2.49e-06       0.0543       0.0701       0.0455        0.072       0.0587       0.0572       0.0906       0.0739       0.0963        0.001\n",
            "     79     5      0.00746      0.00744     2.11e-05       0.0517       0.0667       0.0427       0.0696       0.0562       0.0538        0.087       0.0704        0.327       0.0034\n",
            "     79     6      0.00751       0.0075     1.03e-05       0.0504        0.067       0.0393       0.0725       0.0559       0.0495       0.0925        0.071        0.233      0.00243\n",
            "     79     7      0.00709      0.00705     3.66e-05       0.0495        0.065       0.0407        0.067       0.0538       0.0526       0.0845       0.0685        0.438      0.00456\n",
            "     79     8      0.00719      0.00715     3.37e-05       0.0484       0.0654       0.0373       0.0706        0.054       0.0477       0.0911       0.0694        0.425      0.00443\n",
            "     79     9      0.00734      0.00729     5.17e-05       0.0488       0.0661       0.0372       0.0718       0.0545       0.0487       0.0913         0.07        0.531      0.00553\n",
            "     79    10      0.00674      0.00667     6.41e-05       0.0476       0.0632       0.0371       0.0686       0.0529        0.047        0.087        0.067        0.591      0.00615\n",
            "     79    11      0.00767      0.00764     3.67e-05       0.0508       0.0676       0.0397       0.0732       0.0564       0.0511       0.0921       0.0716        0.444      0.00462\n",
            "     79    12      0.00855      0.00852     2.34e-05       0.0561       0.0714         0.05       0.0684       0.0592       0.0621       0.0872       0.0746        0.356      0.00371\n",
            "     79    13      0.00774      0.00774     1.21e-06       0.0531        0.068       0.0466       0.0661       0.0563       0.0582       0.0843       0.0713       0.0543     0.000566\n",
            "     79    14       0.0068      0.00678     2.01e-05        0.048       0.0637       0.0367       0.0707       0.0537       0.0467       0.0883       0.0675        0.325      0.00339\n",
            "     79    15      0.00853      0.00853     6.71e-07       0.0552       0.0714       0.0468        0.072       0.0594       0.0589       0.0915       0.0752       0.0537     0.000559\n",
            "     79    16      0.00801      0.00797     4.05e-05       0.0518       0.0691       0.0419       0.0716       0.0567       0.0528       0.0935       0.0731         0.47      0.00489\n",
            "     79    17      0.00683      0.00683        1e-06       0.0472       0.0639       0.0362       0.0692       0.0527       0.0471       0.0885       0.0678       0.0729     0.000759\n",
            "     79    18      0.00841      0.00835     6.18e-05       0.0544       0.0707       0.0436        0.076       0.0598       0.0553       0.0942       0.0747        0.579      0.00603\n",
            "     79    19      0.00913      0.00913     2.26e-06       0.0569       0.0739       0.0475       0.0758       0.0616       0.0592       0.0969        0.078          0.1      0.00104\n",
            "     79    20      0.00718      0.00718     5.71e-06       0.0501       0.0655       0.0411       0.0681       0.0546       0.0519       0.0866       0.0693        0.163      0.00169\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     79     1      0.00745      0.00745     2.64e-06         0.05       0.0668       0.0389       0.0722       0.0556       0.0502       0.0913       0.0708       0.0969      0.00101\n",
            "     79     2      0.00745      0.00745     1.26e-06       0.0496       0.0668       0.0382       0.0724       0.0553       0.0494       0.0922       0.0708       0.0577     0.000601\n",
            "     79     3      0.00706      0.00706     1.42e-06       0.0483        0.065       0.0372       0.0705       0.0539       0.0476       0.0902       0.0689       0.0709     0.000739\n",
            "     79     4      0.00727      0.00727     1.47e-06       0.0488       0.0659       0.0376       0.0712       0.0544       0.0481       0.0918       0.0699        0.077     0.000802\n",
            "     79     5      0.00673      0.00673     1.09e-06       0.0477       0.0635       0.0376       0.0681       0.0528       0.0486       0.0858       0.0672       0.0526     0.000548\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              79  246.531    0.005      0.00768      2.7e-05      0.00771       0.0514       0.0678       0.0416       0.0712       0.0564       0.0529       0.0905       0.0717        0.333      0.00346\n",
            "! Validation         79  246.531    0.005      0.00719     1.58e-06      0.00719       0.0489       0.0656       0.0379       0.0709       0.0544       0.0488       0.0903       0.0695        0.071      0.00074\n",
            "Wall time: 246.532162556\n",
            "! Best model       79    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     80     1      0.00652       0.0065     1.99e-05       0.0471       0.0624       0.0368       0.0676       0.0522       0.0463       0.0859       0.0661        0.327       0.0034\n",
            "     80     2      0.00777      0.00776     9.12e-06       0.0517       0.0681       0.0404       0.0741       0.0573       0.0512       0.0932       0.0722        0.198      0.00207\n",
            "     80     3      0.00846      0.00844     1.85e-05       0.0543       0.0711        0.045       0.0729        0.059       0.0568       0.0933        0.075        0.309      0.00321\n",
            "     80     4      0.00672      0.00672     2.63e-07       0.0486       0.0634       0.0392       0.0674       0.0533       0.0495       0.0846       0.0671       0.0301     0.000313\n",
            "     80     5      0.00668      0.00668     2.33e-06       0.0479       0.0632       0.0384       0.0669       0.0527       0.0482       0.0856       0.0669        0.106      0.00111\n",
            "     80     6      0.00577      0.00577     2.22e-06       0.0438       0.0588       0.0336       0.0642       0.0489       0.0434       0.0812       0.0623        0.104      0.00109\n",
            "     80     7      0.00646      0.00646     2.45e-06       0.0473       0.0622       0.0371       0.0677       0.0524       0.0468        0.085       0.0659        0.106       0.0011\n",
            "     80     8      0.00664      0.00664     9.74e-07       0.0467        0.063       0.0361       0.0679        0.052       0.0473       0.0863       0.0668       0.0678     0.000706\n",
            "     80     9      0.00669      0.00669     1.44e-06       0.0472       0.0633       0.0355       0.0706        0.053        0.046       0.0882       0.0671       0.0572     0.000596\n",
            "     80    10      0.00662      0.00662     1.19e-06       0.0472       0.0629       0.0359       0.0699       0.0529       0.0455        0.088       0.0667       0.0795     0.000828\n",
            "     80    11      0.00619      0.00619     4.25e-06       0.0451       0.0609       0.0335       0.0681       0.0508        0.042        0.087       0.0645         0.14      0.00146\n",
            "     80    12      0.00711      0.00711     3.56e-06       0.0485       0.0652       0.0375       0.0705        0.054       0.0486       0.0897       0.0691        0.114      0.00119\n",
            "     80    13      0.00643      0.00642     3.78e-06       0.0466        0.062       0.0367       0.0664       0.0516       0.0474       0.0839       0.0657        0.108      0.00113\n",
            "     80    14      0.00745      0.00745     3.65e-07       0.0481       0.0668       0.0361       0.0721       0.0541        0.048       0.0936       0.0708       0.0355      0.00037\n",
            "     80    15      0.00778      0.00778     2.03e-06       0.0508       0.0682        0.038       0.0763       0.0572       0.0495       0.0952       0.0724       0.0875     0.000911\n",
            "     80    16      0.00705      0.00704     6.37e-06       0.0496       0.0649       0.0388       0.0712        0.055       0.0486        0.089       0.0688        0.165      0.00172\n",
            "     80    17      0.00639      0.00638     1.07e-05       0.0472       0.0618        0.039       0.0634       0.0512       0.0499       0.0805       0.0652        0.237      0.00246\n",
            "     80    18      0.00754      0.00753     7.47e-06       0.0517       0.0671       0.0436       0.0677       0.0557       0.0556       0.0857       0.0706        0.191      0.00199\n",
            "     80    19      0.00781       0.0078     4.38e-06       0.0498       0.0683       0.0365       0.0764       0.0564       0.0467       0.0982       0.0725        0.152      0.00158\n",
            "     80    20      0.00887      0.00883     3.93e-05        0.055       0.0727       0.0433       0.0784       0.0609        0.055        0.099        0.077         0.46       0.0048\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     80     1      0.00741      0.00741     2.57e-06       0.0499       0.0666       0.0388        0.072       0.0554         0.05       0.0911       0.0706       0.0951     0.000991\n",
            "     80     2      0.00742      0.00742      1.2e-06       0.0495       0.0666        0.038       0.0723       0.0552       0.0492        0.092       0.0706       0.0552     0.000575\n",
            "     80     3      0.00703      0.00703     1.43e-06       0.0482       0.0649       0.0371       0.0704       0.0538       0.0475       0.0901       0.0688       0.0714     0.000744\n",
            "     80     4      0.00723      0.00723      1.5e-06       0.0486       0.0658       0.0375        0.071       0.0542       0.0479       0.0916       0.0698       0.0791     0.000824\n",
            "     80     5       0.0067       0.0067     1.05e-06       0.0476       0.0633       0.0374        0.068       0.0527       0.0484       0.0857       0.0671       0.0524     0.000546\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              80  249.592    0.005      0.00704     7.03e-06      0.00705       0.0487       0.0649       0.0381         0.07        0.054       0.0488       0.0888       0.0688        0.154       0.0016\n",
            "! Validation         80  249.592    0.005      0.00716     1.55e-06      0.00716       0.0488       0.0654       0.0378       0.0707       0.0543       0.0486       0.0901       0.0694       0.0706     0.000736\n",
            "Wall time: 249.592972744\n",
            "! Best model       80    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     81     1      0.00722      0.00722     5.58e-07       0.0503       0.0657       0.0407       0.0695       0.0551       0.0517       0.0873       0.0695       0.0541     0.000564\n",
            "     81     2      0.00644      0.00644     3.63e-06       0.0476       0.0621       0.0377       0.0674       0.0526       0.0473       0.0841       0.0657        0.126      0.00131\n",
            "     81     3      0.00631       0.0063     2.15e-06       0.0468       0.0614        0.038       0.0644       0.0512       0.0481       0.0818        0.065       0.0824     0.000859\n",
            "     81     4      0.00631       0.0063     3.63e-06       0.0474       0.0614       0.0393       0.0636       0.0514       0.0494       0.0802       0.0648        0.117      0.00121\n",
            "     81     5      0.00669      0.00668     4.97e-06       0.0474       0.0632       0.0356       0.0711       0.0533       0.0448       0.0894       0.0671        0.151      0.00157\n",
            "     81     6      0.00699      0.00699     3.59e-07       0.0497       0.0647       0.0405       0.0681       0.0543       0.0503       0.0866       0.0684       0.0443     0.000462\n",
            "     81     7      0.00729      0.00728     1.02e-05       0.0492        0.066       0.0389       0.0699       0.0544       0.0502       0.0896       0.0699        0.233      0.00242\n",
            "     81     8      0.00634      0.00633     8.82e-06       0.0466       0.0615       0.0354        0.069       0.0522       0.0452       0.0853       0.0653        0.215      0.00224\n",
            "     81     9      0.00588      0.00587     1.46e-05       0.0445       0.0592       0.0351       0.0633       0.0492       0.0444       0.0811       0.0628        0.279      0.00291\n",
            "     81    10      0.00803      0.00803     3.84e-06       0.0515       0.0693       0.0404       0.0739       0.0571       0.0524       0.0945       0.0734        0.143      0.00149\n",
            "     81    11      0.00837      0.00837     1.91e-06       0.0535       0.0708       0.0435       0.0736       0.0585       0.0562       0.0933       0.0748       0.0883      0.00092\n",
            "     81    12      0.00666      0.00665     1.87e-06       0.0473       0.0631       0.0377       0.0665       0.0521       0.0478       0.0859       0.0669       0.0766     0.000798\n",
            "     81    13      0.00668      0.00668     3.55e-06       0.0481       0.0632       0.0396       0.0651       0.0523       0.0498       0.0838       0.0668        0.125      0.00131\n",
            "     81    14      0.00733      0.00731      1.9e-05       0.0512       0.0661       0.0412       0.0711       0.0562       0.0512       0.0888         0.07        0.318      0.00332\n",
            "     81    15      0.00871      0.00871      1.1e-06       0.0549       0.0722       0.0428       0.0791        0.061       0.0544       0.0986       0.0765       0.0729     0.000759\n",
            "     81    16      0.00932      0.00925     7.79e-05       0.0571       0.0744       0.0461        0.079       0.0625       0.0575       0.0999       0.0787         0.65      0.00677\n",
            "     81    17      0.00834      0.00834     6.03e-06       0.0543       0.0706       0.0442       0.0744       0.0593        0.056       0.0933       0.0746        0.171      0.00178\n",
            "     81    18      0.00715      0.00712     2.79e-05       0.0492       0.0653       0.0392       0.0693       0.0542       0.0499       0.0883       0.0691        0.387      0.00403\n",
            "     81    19      0.00811      0.00811      2.6e-06       0.0528       0.0697       0.0426       0.0732       0.0579       0.0544       0.0929       0.0737        0.105      0.00109\n",
            "     81    20      0.00875      0.00872     2.29e-05       0.0558       0.0723        0.046       0.0754       0.0607        0.058       0.0946       0.0763        0.342      0.00356\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     81     1      0.00738      0.00738     2.57e-06       0.0497       0.0664       0.0387       0.0719       0.0553       0.0498        0.091       0.0704       0.0948     0.000988\n",
            "     81     2      0.00737      0.00737     1.14e-06       0.0493       0.0664       0.0379       0.0722        0.055        0.049       0.0918       0.0704       0.0541     0.000564\n",
            "     81     3        0.007        0.007     1.38e-06       0.0481       0.0647        0.037       0.0703       0.0536       0.0473       0.0899       0.0686       0.0708     0.000738\n",
            "     81     4      0.00719      0.00719      1.5e-06       0.0485       0.0656       0.0373       0.0709       0.0541       0.0477       0.0915       0.0696       0.0787      0.00082\n",
            "     81     5      0.00667      0.00667     1.04e-06       0.0475       0.0632       0.0373       0.0678       0.0526       0.0482       0.0856       0.0669       0.0502     0.000523\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              81  252.643    0.005      0.00733     1.09e-05      0.00735       0.0503       0.0663       0.0402       0.0704       0.0553       0.0511       0.0891       0.0701        0.189      0.00197\n",
            "! Validation         81  252.643    0.005      0.00712     1.53e-06      0.00712       0.0486       0.0653       0.0376       0.0706       0.0541       0.0484         0.09       0.0692       0.0697     0.000726\n",
            "Wall time: 252.64327033799998\n",
            "! Best model       81    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     82     1      0.00938      0.00936     2.11e-05       0.0578       0.0748       0.0483       0.0768       0.0626       0.0606       0.0973       0.0789        0.337      0.00351\n",
            "     82     2      0.00878      0.00871     7.41e-05        0.053       0.0722       0.0413       0.0766       0.0589       0.0535       0.0996       0.0765        0.639      0.00666\n",
            "     82     3      0.00619      0.00619     3.22e-06        0.046       0.0608       0.0353       0.0675       0.0514       0.0447       0.0843       0.0645        0.113      0.00118\n",
            "     82     4      0.00697      0.00695      2.3e-05       0.0485       0.0645       0.0378       0.0699       0.0539       0.0482       0.0885       0.0683        0.349      0.00364\n",
            "     82     5      0.00712      0.00712     8.62e-07       0.0492       0.0653       0.0403       0.0669       0.0536       0.0523       0.0856       0.0689       0.0639     0.000665\n",
            "     82     6      0.00661      0.00661     1.57e-06       0.0477       0.0629       0.0373       0.0686       0.0529        0.047       0.0864       0.0667       0.0814     0.000848\n",
            "     82     7      0.00629      0.00629     9.22e-07       0.0455       0.0614       0.0355       0.0657       0.0506       0.0462       0.0838        0.065       0.0605     0.000631\n",
            "     82     8      0.00698      0.00698     2.69e-06       0.0484       0.0646       0.0387       0.0677       0.0532         0.05       0.0868       0.0684        0.091     0.000948\n",
            "     82     9      0.00798      0.00798     3.59e-07       0.0508       0.0691       0.0397        0.073       0.0563       0.0507       0.0958       0.0733         0.04     0.000417\n",
            "     82    10      0.00646      0.00644     2.24e-05       0.0465       0.0621       0.0351       0.0692       0.0522       0.0446       0.0871       0.0658         0.34      0.00355\n",
            "     82    11      0.00648      0.00648     1.95e-06       0.0464       0.0623       0.0343       0.0706       0.0524       0.0434       0.0887       0.0661       0.0945     0.000985\n",
            "     82    12      0.00788      0.00783      5.2e-05       0.0512       0.0684       0.0398       0.0742        0.057       0.0506       0.0945       0.0725        0.527      0.00549\n",
            "     82    13      0.00683      0.00683     2.94e-06       0.0495       0.0639       0.0412       0.0661       0.0537       0.0519       0.0829       0.0674        0.122      0.00127\n",
            "     82    14      0.00678      0.00676     2.15e-05       0.0487       0.0636       0.0379       0.0701        0.054       0.0471       0.0877       0.0674        0.338      0.00352\n",
            "     82    15      0.00608      0.00608     3.07e-07       0.0452       0.0603        0.035       0.0656       0.0503       0.0447       0.0832       0.0639       0.0297     0.000309\n",
            "     82    16      0.00678      0.00677     1.31e-05       0.0486       0.0636        0.039       0.0678       0.0534       0.0497        0.085       0.0673        0.257      0.00268\n",
            "     82    17      0.00622      0.00621     6.85e-06       0.0463        0.061       0.0367       0.0654       0.0511       0.0471        0.082       0.0645        0.166      0.00173\n",
            "     82    18      0.00655      0.00654     1.26e-05       0.0479       0.0626       0.0388        0.066       0.0524       0.0486       0.0838       0.0662         0.26      0.00271\n",
            "     82    19      0.00803      0.00801     1.37e-05       0.0521       0.0692        0.042       0.0722       0.0571       0.0534       0.0932       0.0733        0.247      0.00257\n",
            "     82    20      0.00872       0.0087     1.38e-05       0.0543       0.0722        0.042       0.0791       0.0605       0.0528          0.1       0.0765        0.275      0.00286\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     82     1      0.00734      0.00734     2.61e-06       0.0496       0.0663       0.0385       0.0718       0.0552       0.0497       0.0908       0.0702       0.0963        0.001\n",
            "     82     2      0.00734      0.00734     1.21e-06       0.0492       0.0663       0.0378       0.0721       0.0549       0.0489       0.0917       0.0703       0.0557      0.00058\n",
            "     82     3      0.00697      0.00697     1.34e-06        0.048       0.0646       0.0368       0.0702       0.0535       0.0471       0.0898       0.0685       0.0684     0.000712\n",
            "     82     4      0.00716      0.00716     1.53e-06       0.0484       0.0654       0.0372       0.0708        0.054       0.0475       0.0913       0.0694       0.0786     0.000819\n",
            "     82     5      0.00664      0.00664        1e-06       0.0473        0.063       0.0372       0.0677       0.0524        0.048       0.0854       0.0667       0.0496     0.000517\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              82  255.684    0.005      0.00714     1.44e-05      0.00716       0.0492       0.0654       0.0388         0.07       0.0544       0.0495        0.089       0.0693        0.222      0.00231\n",
            "! Validation         82  255.684    0.005      0.00709     1.54e-06      0.00709       0.0485       0.0651       0.0375       0.0705        0.054       0.0482       0.0898        0.069       0.0697     0.000726\n",
            "Wall time: 255.68515327699993\n",
            "! Best model       82    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     83     1        0.007        0.007     4.73e-07       0.0479       0.0647       0.0376       0.0684        0.053       0.0479       0.0893       0.0686       0.0475     0.000494\n",
            "     83     2      0.00695      0.00694     9.04e-06       0.0491       0.0645        0.038       0.0714       0.0547       0.0473       0.0894       0.0684        0.216      0.00225\n",
            "     83     3       0.0083      0.00826     3.42e-05       0.0552       0.0703       0.0465       0.0726       0.0596       0.0582       0.0898        0.074         0.43      0.00448\n",
            "     83     4       0.0102       0.0102     4.81e-06       0.0626       0.0781       0.0585       0.0708       0.0647       0.0725       0.0882       0.0803        0.138      0.00143\n",
            "     83     5        0.008        0.008     1.26e-06       0.0534       0.0692       0.0462        0.068       0.0571       0.0583       0.0869       0.0726       0.0637     0.000663\n",
            "     83     6      0.00795      0.00789     6.14e-05       0.0518       0.0687       0.0405       0.0745       0.0575        0.052       0.0936       0.0728        0.581      0.00605\n",
            "     83     7      0.00699      0.00698     5.03e-06       0.0487       0.0646       0.0376        0.071       0.0543       0.0482       0.0888       0.0685        0.149      0.00156\n",
            "     83     8      0.00801      0.00799     1.54e-05       0.0523       0.0692       0.0403       0.0764       0.0583       0.0503       0.0964       0.0734        0.288        0.003\n",
            "     83     9      0.00788      0.00787     5.34e-06       0.0518       0.0687        0.042       0.0714       0.0567       0.0534       0.0918       0.0726         0.15      0.00156\n",
            "     83    10      0.00759      0.00758      5.3e-06       0.0511       0.0674       0.0413       0.0707        0.056       0.0519       0.0907       0.0713        0.166      0.00173\n",
            "     83    11      0.00706      0.00703     2.68e-05       0.0496       0.0649       0.0392       0.0704       0.0548       0.0486       0.0888       0.0687        0.378      0.00394\n",
            "     83    12      0.00976      0.00976     1.18e-06       0.0593       0.0764       0.0514       0.0751       0.0632       0.0639       0.0967       0.0803       0.0738     0.000769\n",
            "     83    13       0.0103       0.0103     1.53e-06       0.0608       0.0786       0.0515       0.0795       0.0655       0.0647        0.101       0.0828       0.0701      0.00073\n",
            "     83    14       0.0087      0.00867     3.45e-05       0.0538        0.072       0.0417        0.078       0.0598       0.0544       0.0982       0.0763        0.432       0.0045\n",
            "     83    15      0.00696      0.00695     9.93e-06       0.0481       0.0645       0.0379       0.0685       0.0532       0.0478       0.0889       0.0683        0.233      0.00243\n",
            "     83    16      0.00674      0.00674      1.5e-06       0.0485       0.0635       0.0388       0.0679       0.0533       0.0484       0.0861       0.0673       0.0803     0.000836\n",
            "     83    17      0.00804      0.00804     1.12e-06        0.053       0.0694        0.043        0.073        0.058       0.0548       0.0919       0.0733       0.0609     0.000635\n",
            "     83    18       0.0071      0.00705     4.85e-05       0.0508        0.065       0.0441       0.0642       0.0542        0.055       0.0813       0.0682        0.511      0.00533\n",
            "     83    19      0.00651      0.00649     1.82e-05       0.0468       0.0623       0.0362        0.068       0.0521       0.0466       0.0855        0.066        0.309      0.00322\n",
            "     83    20      0.00788      0.00785     3.62e-05       0.0526       0.0685        0.042       0.0737       0.0578       0.0536       0.0913       0.0725        0.444      0.00463\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     83     1      0.00731      0.00731     2.57e-06       0.0495       0.0661       0.0384       0.0717       0.0551       0.0495       0.0907       0.0701       0.0946     0.000986\n",
            "     83     2      0.00731      0.00731     1.18e-06       0.0491       0.0661       0.0377       0.0719       0.0548       0.0487       0.0915       0.0701       0.0551     0.000574\n",
            "     83     3      0.00694      0.00694     1.39e-06       0.0478       0.0644       0.0367       0.0701       0.0534        0.047       0.0897       0.0683       0.0696     0.000725\n",
            "     83     4      0.00713      0.00713     1.56e-06       0.0483       0.0653        0.037       0.0707       0.0539       0.0473       0.0912       0.0693       0.0793     0.000826\n",
            "     83     5      0.00661       0.0066     1.04e-06       0.0472       0.0629        0.037       0.0676       0.0523       0.0478       0.0853       0.0666       0.0516     0.000537\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              83  258.737    0.005      0.00788     1.61e-05       0.0079       0.0524       0.0687       0.0427       0.0717       0.0572       0.0543       0.0908       0.0726        0.241      0.00251\n",
            "! Validation         83  258.737    0.005      0.00706     1.55e-06      0.00706       0.0484        0.065       0.0374       0.0704       0.0539       0.0481       0.0897       0.0689         0.07      0.00073\n",
            "Wall time: 258.7379766759999\n",
            "! Best model       83    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     84     1      0.00744      0.00741     2.57e-05       0.0502       0.0666       0.0407       0.0693        0.055       0.0517       0.0892       0.0705         0.37      0.00386\n",
            "     84     2      0.00705      0.00705     1.67e-06       0.0482       0.0649       0.0375       0.0696       0.0535       0.0482       0.0895       0.0688        0.082     0.000854\n",
            "     84     3      0.00681      0.00679     2.47e-05       0.0486       0.0637       0.0397       0.0662        0.053       0.0503       0.0845       0.0674        0.364      0.00379\n",
            "     84     4      0.00717      0.00717     2.18e-07       0.0496       0.0655       0.0392       0.0702       0.0547       0.0498        0.089       0.0694       0.0281     0.000293\n",
            "     84     5      0.00854      0.00848     5.72e-05       0.0527       0.0713       0.0395       0.0791       0.0593       0.0512          0.1       0.0756        0.548      0.00571\n",
            "     84     6      0.00757      0.00755     1.93e-05       0.0522       0.0672        0.044       0.0685       0.0562       0.0555        0.086       0.0707        0.322      0.00335\n",
            "     84     7      0.00706      0.00702     3.84e-05       0.0486       0.0648       0.0378       0.0702        0.054       0.0482       0.0892       0.0687        0.453      0.00471\n",
            "     84     8       0.0061      0.00609     8.12e-06       0.0448       0.0604       0.0346       0.0651       0.0498       0.0444       0.0836        0.064         0.19      0.00198\n",
            "     84     9      0.00693       0.0069     3.58e-05       0.0495       0.0643       0.0419       0.0646       0.0532       0.0521       0.0834       0.0678        0.442      0.00461\n",
            "     84    10      0.00739      0.00739     1.53e-06       0.0504       0.0665       0.0394       0.0724       0.0559       0.0499        0.091       0.0705       0.0783     0.000816\n",
            "     84    11      0.00724      0.00718     6.07e-05       0.0496       0.0656       0.0388       0.0711        0.055       0.0492       0.0898       0.0695        0.577      0.00601\n",
            "     84    12       0.0057      0.00569     4.38e-06       0.0445       0.0584       0.0354       0.0626        0.049       0.0447       0.0789       0.0618        0.133      0.00138\n",
            "     84    13      0.00812      0.00807     4.64e-05       0.0525       0.0695       0.0414       0.0749       0.0581       0.0519       0.0954       0.0737        0.492      0.00512\n",
            "     84    14      0.00854      0.00853     5.38e-06       0.0561       0.0715       0.0471        0.074       0.0605       0.0586        0.092       0.0753        0.158      0.00165\n",
            "     84    15      0.00737      0.00737     2.67e-06       0.0516       0.0664       0.0428       0.0693       0.0561       0.0539       0.0861         0.07        0.111      0.00116\n",
            "     84    16      0.00654      0.00651     3.07e-05       0.0467       0.0624       0.0361        0.068       0.0521       0.0467       0.0856       0.0661        0.403      0.00419\n",
            "     84    17      0.00592      0.00592     3.06e-06       0.0448       0.0595       0.0347       0.0651       0.0499       0.0435       0.0827       0.0631        0.121      0.00126\n",
            "     84    18       0.0074      0.00738     2.85e-05       0.0498       0.0664        0.039       0.0713       0.0551       0.0501       0.0907       0.0704        0.385      0.00401\n",
            "     84    19      0.00739      0.00738     1.35e-05       0.0494       0.0665       0.0399       0.0686       0.0542        0.051       0.0897       0.0704        0.269      0.00281\n",
            "     84    20      0.00719      0.00718     3.44e-06        0.049       0.0656       0.0384       0.0703       0.0543       0.0492       0.0898       0.0695        0.131      0.00137\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     84     1      0.00728      0.00728     2.52e-06       0.0494        0.066       0.0383       0.0716       0.0549       0.0493       0.0905       0.0699       0.0938     0.000977\n",
            "     84     2      0.00727      0.00727     1.18e-06        0.049        0.066       0.0375       0.0718       0.0547       0.0485       0.0913       0.0699       0.0552     0.000575\n",
            "     84     3      0.00691      0.00691     1.38e-06       0.0477       0.0643       0.0366         0.07       0.0533       0.0468       0.0896       0.0682       0.0703     0.000732\n",
            "     84     4       0.0071       0.0071     1.53e-06       0.0482       0.0652       0.0369       0.0706       0.0538       0.0472       0.0911       0.0691       0.0791     0.000824\n",
            "     84     5      0.00657      0.00657     9.85e-07       0.0471       0.0627       0.0369       0.0675       0.0522       0.0477       0.0852       0.0664       0.0493     0.000514\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              84  261.802    0.005      0.00715     2.06e-05      0.00717       0.0494       0.0654       0.0394       0.0695       0.0545       0.0501       0.0884       0.0693        0.283      0.00295\n",
            "! Validation         84  261.802    0.005      0.00703     1.52e-06      0.00703       0.0483       0.0648       0.0372       0.0703       0.0538       0.0479       0.0896       0.0687       0.0695     0.000724\n",
            "Wall time: 261.80260048499997\n",
            "! Best model       84    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     85     1      0.00635      0.00635     1.02e-06       0.0462       0.0617       0.0356       0.0675       0.0515        0.046       0.0847       0.0654       0.0703     0.000732\n",
            "     85     2      0.00691      0.00689     1.61e-05       0.0481       0.0642       0.0381       0.0683       0.0532       0.0489       0.0872        0.068        0.279      0.00291\n",
            "     85     3      0.00841      0.00839     1.55e-05       0.0533       0.0709       0.0423       0.0754       0.0588       0.0543       0.0958        0.075        0.282      0.00294\n",
            "     85     4      0.00663      0.00656     6.85e-05       0.0473       0.0627       0.0369       0.0679       0.0524       0.0469       0.0859       0.0664        0.613      0.00638\n",
            "     85     5      0.00581      0.00581     3.74e-06       0.0438        0.059       0.0336       0.0641       0.0488       0.0424       0.0827       0.0625        0.137      0.00143\n",
            "     85     6      0.00663      0.00661     1.29e-05       0.0467       0.0629        0.036       0.0682       0.0521       0.0452       0.0882       0.0667        0.262      0.00273\n",
            "     85     7      0.00723      0.00722     5.27e-06       0.0494       0.0658       0.0376       0.0729       0.0553       0.0471       0.0924       0.0697        0.169      0.00176\n",
            "     85     8      0.00652      0.00652      2.2e-06       0.0469       0.0625       0.0364       0.0677       0.0521       0.0464        0.086       0.0662          0.1      0.00105\n",
            "     85     9      0.00592      0.00591     1.34e-05       0.0445       0.0595       0.0351       0.0634       0.0492       0.0455       0.0805        0.063        0.269       0.0028\n",
            "     85    10      0.00634      0.00633     1.48e-05       0.0468       0.0615       0.0371       0.0663       0.0517       0.0473        0.083       0.0651        0.265      0.00276\n",
            "     85    11      0.00729      0.00728     1.41e-05       0.0499        0.066         0.04       0.0696       0.0548       0.0506       0.0891       0.0699        0.275      0.00286\n",
            "     85    12      0.00602        0.006     1.21e-05       0.0454       0.0599       0.0354       0.0653       0.0504       0.0444       0.0826       0.0635        0.244      0.00254\n",
            "     85    13      0.00677      0.00677     6.48e-07       0.0471       0.0636       0.0367        0.068       0.0523       0.0473       0.0876       0.0675       0.0437     0.000456\n",
            "     85    14      0.00675      0.00674     1.29e-05       0.0472       0.0635       0.0365       0.0687       0.0526       0.0478       0.0868       0.0673         0.26      0.00271\n",
            "     85    15      0.00669      0.00667     1.36e-05       0.0482       0.0632       0.0386       0.0675        0.053       0.0485       0.0853       0.0669        0.258      0.00269\n",
            "     85    16      0.00645      0.00643     2.12e-05       0.0469        0.062       0.0364       0.0678       0.0521        0.046       0.0855       0.0658        0.339      0.00353\n",
            "     85    17      0.00684      0.00683     1.54e-05       0.0481       0.0639       0.0379       0.0686       0.0533       0.0482       0.0872       0.0677        0.285      0.00297\n",
            "     85    18      0.00704      0.00704     5.35e-07       0.0487       0.0649       0.0373       0.0717       0.0545       0.0477         0.09       0.0688       0.0469     0.000488\n",
            "     85    19      0.00666      0.00663     2.74e-05       0.0476        0.063       0.0372       0.0684       0.0528        0.048       0.0855       0.0667        0.382      0.00398\n",
            "     85    20      0.00708      0.00706      2.8e-05       0.0491        0.065       0.0374       0.0725       0.0549       0.0477       0.0901       0.0689        0.381      0.00397\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     85     1      0.00724      0.00724     2.51e-06       0.0493       0.0658       0.0382       0.0715       0.0548       0.0492       0.0904       0.0698       0.0932      0.00097\n",
            "     85     2      0.00724      0.00724     1.13e-06       0.0488       0.0658       0.0374       0.0717       0.0546       0.0484       0.0912       0.0698       0.0538     0.000561\n",
            "     85     3      0.00688      0.00688     1.39e-06       0.0476       0.0642       0.0365         0.07       0.0532       0.0466       0.0894        0.068       0.0711     0.000741\n",
            "     85     4      0.00707      0.00706     1.57e-06        0.048        0.065       0.0368       0.0705       0.0536        0.047       0.0909        0.069       0.0807      0.00084\n",
            "     85     5      0.00655      0.00655     9.43e-07        0.047       0.0626       0.0368       0.0674       0.0521       0.0475       0.0851       0.0663       0.0484     0.000505\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              85  264.864    0.005       0.0067      1.5e-05      0.00672       0.0476       0.0633       0.0371       0.0685       0.0528       0.0474       0.0869       0.0671        0.248      0.00258\n",
            "! Validation         85  264.864    0.005      0.00699     1.51e-06        0.007       0.0481       0.0647       0.0371       0.0702       0.0537       0.0477       0.0894       0.0686       0.0694     0.000723\n",
            "Wall time: 264.864371013\n",
            "! Best model       85    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     86     1      0.00612      0.00608     3.52e-05       0.0454       0.0603       0.0348       0.0668       0.0508       0.0436       0.0844        0.064        0.438      0.00457\n",
            "     86     2      0.00653      0.00646      6.9e-05       0.0461       0.0622       0.0346        0.069       0.0518        0.044       0.0878       0.0659        0.613      0.00639\n",
            "     86     3      0.00747      0.00746     1.71e-05         0.05       0.0668       0.0398       0.0706       0.0552       0.0513       0.0901       0.0707        0.305      0.00318\n",
            "     86     4      0.00699      0.00694      4.8e-05       0.0488       0.0645       0.0389       0.0685       0.0537       0.0493       0.0872       0.0683        0.509       0.0053\n",
            "     86     5       0.0071       0.0071     1.88e-06       0.0495       0.0652       0.0385       0.0715        0.055       0.0488       0.0894       0.0691       0.0777      0.00081\n",
            "     86     6      0.00738      0.00734     3.67e-05       0.0488       0.0663       0.0374       0.0716       0.0545       0.0482       0.0924       0.0703        0.447      0.00465\n",
            "     86     7      0.00677      0.00676     7.76e-06       0.0476       0.0636       0.0375        0.068       0.0527       0.0485       0.0862       0.0674        0.193      0.00201\n",
            "     86     8      0.00567      0.00564      3.1e-05       0.0439       0.0581        0.033       0.0656       0.0493       0.0413        0.082       0.0616        0.407      0.00424\n",
            "     86     9      0.00662      0.00659     2.79e-05       0.0478       0.0628       0.0379       0.0675       0.0527        0.048        0.085       0.0665        0.383      0.00399\n",
            "     86    10      0.00637      0.00631     5.82e-05       0.0462       0.0615        0.036       0.0667       0.0513       0.0462       0.0841       0.0651        0.564      0.00588\n",
            "     86    11        0.006      0.00599     4.33e-06       0.0458       0.0599       0.0359       0.0656       0.0507       0.0452       0.0817       0.0634        0.137      0.00143\n",
            "     86    12      0.00668      0.00665     2.75e-05       0.0479       0.0631       0.0385       0.0665       0.0525       0.0491       0.0844       0.0667        0.386      0.00402\n",
            "     86    13      0.00633      0.00629     3.39e-05       0.0463       0.0614       0.0364       0.0659       0.0512        0.046       0.0841        0.065         0.43      0.00448\n",
            "     86    14      0.00652      0.00652     1.44e-06       0.0475       0.0625       0.0385       0.0655        0.052        0.048       0.0842       0.0661       0.0725     0.000755\n",
            "     86    15      0.00687      0.00685     2.49e-05        0.048        0.064       0.0365        0.071       0.0537       0.0469       0.0888       0.0679        0.367      0.00382\n",
            "     86    16       0.0077      0.00769     1.11e-06       0.0513       0.0679       0.0399       0.0741        0.057       0.0509       0.0929       0.0719       0.0711     0.000741\n",
            "     86    17      0.00823      0.00822     9.15e-06       0.0528       0.0701       0.0416       0.0752       0.0584       0.0538       0.0947       0.0743        0.213      0.00222\n",
            "     86    18      0.00841      0.00841      1.8e-06       0.0549        0.071       0.0489       0.0668       0.0579       0.0611       0.0874       0.0742        0.091     0.000948\n",
            "     86    19      0.00777      0.00777     9.67e-07       0.0516       0.0682       0.0418       0.0711       0.0564       0.0541         0.09        0.072       0.0572     0.000596\n",
            "     86    20      0.00764      0.00762     2.62e-05       0.0508       0.0675       0.0402       0.0721       0.0562       0.0502        0.093       0.0716        0.365       0.0038\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     86     1      0.00721      0.00721     2.53e-06       0.0491       0.0657        0.038       0.0713       0.0547        0.049       0.0902       0.0696       0.0944     0.000984\n",
            "     86     2      0.00721      0.00721     1.11e-06       0.0487       0.0657       0.0373       0.0716       0.0544       0.0482       0.0911       0.0696       0.0531     0.000553\n",
            "     86     3      0.00685      0.00685     1.39e-06       0.0475        0.064       0.0363       0.0699       0.0531       0.0465       0.0894       0.0679       0.0707     0.000736\n",
            "     86     4      0.00703      0.00703     1.57e-06       0.0479       0.0649       0.0367       0.0704       0.0535       0.0468       0.0908       0.0688       0.0798     0.000831\n",
            "     86     5      0.00652      0.00652     9.88e-07       0.0469       0.0625       0.0367       0.0673        0.052       0.0473        0.085       0.0662       0.0497     0.000518\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              86  267.918    0.005      0.00694     2.32e-05      0.00696       0.0485       0.0644       0.0383        0.069       0.0537       0.0489       0.0876       0.0682        0.306      0.00319\n",
            "! Validation         86  267.918    0.005      0.00696     1.52e-06      0.00697        0.048       0.0646        0.037       0.0701       0.0535       0.0476       0.0893       0.0684       0.0696     0.000724\n",
            "Wall time: 267.91859404499996\n",
            "! Best model       86    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     87     1      0.00604      0.00604      7.3e-07       0.0455       0.0601       0.0366       0.0634         0.05       0.0469       0.0803       0.0636       0.0574     0.000598\n",
            "     87     2       0.0077      0.00769      5.6e-06         0.05       0.0678        0.038       0.0739        0.056       0.0481       0.0959        0.072        0.171      0.00179\n",
            "     87     3      0.00743      0.00741     1.44e-05       0.0499       0.0666       0.0399       0.0698       0.0548       0.0513       0.0897       0.0705        0.281      0.00292\n",
            "     87     4      0.00908      0.00907      3.8e-06       0.0571       0.0737       0.0511       0.0693       0.0602        0.065       0.0885       0.0768        0.136      0.00142\n",
            "     87     5      0.00732      0.00726     5.18e-05        0.051       0.0659       0.0423       0.0684       0.0554       0.0525       0.0867       0.0696        0.532      0.00554\n",
            "     87     6      0.00672      0.00672      4.4e-06       0.0482       0.0634       0.0377       0.0692       0.0535       0.0469       0.0875       0.0672        0.141      0.00147\n",
            "     87     7      0.00676      0.00667      8.3e-05       0.0483       0.0632       0.0374       0.0701       0.0538       0.0466       0.0874        0.067        0.667      0.00695\n",
            "     87     8      0.00744      0.00742     1.78e-05       0.0513       0.0667       0.0402       0.0735       0.0568       0.0513       0.0898       0.0706          0.3      0.00313\n",
            "     87     9      0.00756      0.00752      4.1e-05       0.0503       0.0671       0.0385       0.0738       0.0562       0.0489       0.0934       0.0711        0.476      0.00495\n",
            "     87    10      0.00725      0.00724     1.09e-05       0.0498       0.0658       0.0393       0.0708        0.055         0.05       0.0894       0.0697        0.234      0.00244\n",
            "     87    11      0.00792      0.00792     1.58e-06       0.0526       0.0689       0.0447       0.0685       0.0566       0.0567       0.0883       0.0725       0.0834     0.000869\n",
            "     87    12      0.00741       0.0074     1.55e-05       0.0503       0.0665       0.0387       0.0735       0.0561       0.0489       0.0921       0.0705        0.291      0.00304\n",
            "     87    13      0.00882      0.00881     9.55e-06       0.0542       0.0726         0.04       0.0827       0.0613       0.0507        0.103        0.077        0.223      0.00233\n",
            "     87    14      0.00703        0.007     2.65e-05       0.0482       0.0647       0.0385       0.0676        0.053       0.0495       0.0876       0.0686        0.372      0.00387\n",
            "     87    15      0.00739      0.00737     1.51e-05        0.051       0.0664       0.0408       0.0713       0.0561       0.0509       0.0898       0.0703        0.265      0.00276\n",
            "     87    16      0.00603        0.006        3e-05       0.0449       0.0599        0.034       0.0665       0.0503       0.0431       0.0839       0.0635        0.401      0.00417\n",
            "     87    17      0.00679      0.00677     1.34e-05       0.0482       0.0637       0.0393       0.0659       0.0526       0.0511       0.0833       0.0672        0.264      0.00275\n",
            "     87    18      0.00749      0.00749     2.79e-06         0.05        0.067       0.0395       0.0711       0.0553       0.0518       0.0899       0.0709        0.093     0.000968\n",
            "     87    19      0.00654       0.0065     4.37e-05       0.0463       0.0624       0.0361       0.0666       0.0513       0.0471       0.0851       0.0661        0.481      0.00501\n",
            "     87    20      0.00623      0.00623      1.1e-06       0.0458       0.0611       0.0358       0.0658       0.0508       0.0459       0.0836       0.0647       0.0652      0.00068\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     87     1      0.00717      0.00717     2.46e-06        0.049       0.0655       0.0379       0.0712       0.0546       0.0488       0.0901       0.0694       0.0924     0.000962\n",
            "     87     2      0.00718      0.00717     1.19e-06       0.0486       0.0655       0.0372       0.0715       0.0543        0.048       0.0909       0.0695       0.0551     0.000574\n",
            "     87     3      0.00683      0.00683     1.34e-06       0.0474       0.0639       0.0362       0.0698        0.053       0.0463       0.0893       0.0678       0.0691      0.00072\n",
            "     87     4        0.007        0.007     1.52e-06       0.0478       0.0647       0.0366       0.0702       0.0534       0.0466       0.0907       0.0686       0.0771     0.000804\n",
            "     87     5      0.00648      0.00648     1.01e-06       0.0467       0.0623       0.0365       0.0672       0.0518       0.0471       0.0848        0.066       0.0494     0.000515\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              87  270.971    0.005      0.00723     1.96e-05      0.00725       0.0496       0.0658       0.0394       0.0701       0.0547       0.0504       0.0889       0.0696        0.277      0.00288\n",
            "! Validation         87  270.971    0.005      0.00693      1.5e-06      0.00693       0.0479       0.0644       0.0369         0.07       0.0534       0.0474       0.0892       0.0683       0.0686     0.000715\n",
            "Wall time: 270.97176415599995\n",
            "! Best model       87    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     88     1      0.00728      0.00724     3.98e-05       0.0496       0.0658       0.0389        0.071        0.055       0.0499       0.0896       0.0697        0.468      0.00488\n",
            "     88     2      0.00762      0.00762     1.19e-06       0.0511       0.0675        0.041       0.0713       0.0561       0.0515       0.0916       0.0715        0.068     0.000708\n",
            "     88     3       0.0064      0.00639     8.88e-06       0.0474       0.0618       0.0381       0.0659        0.052       0.0485       0.0823       0.0654        0.213      0.00222\n",
            "     88     4      0.00578      0.00577      1.3e-05        0.044       0.0587       0.0346       0.0628       0.0487       0.0441       0.0804       0.0622        0.264      0.00275\n",
            "     88     5      0.00811       0.0081     4.21e-06       0.0537       0.0696       0.0452       0.0706       0.0579       0.0575       0.0891       0.0733        0.132      0.00137\n",
            "     88     6      0.00816      0.00811      4.4e-05       0.0532       0.0697        0.044       0.0715       0.0577       0.0559       0.0912       0.0735         0.49       0.0051\n",
            "     88     7      0.00663      0.00662     2.61e-06       0.0469        0.063       0.0348       0.0712        0.053       0.0437       0.0899       0.0668        0.106       0.0011\n",
            "     88     8      0.00615      0.00612     2.73e-05       0.0456       0.0605       0.0367       0.0634         0.05       0.0464       0.0818       0.0641        0.384        0.004\n",
            "     88     9      0.00813      0.00812     1.22e-06       0.0527       0.0697       0.0407       0.0768       0.0587       0.0517       0.0962       0.0739       0.0662      0.00069\n",
            "     88    10      0.00857      0.00857      1.3e-06       0.0554       0.0716       0.0439       0.0784       0.0611       0.0538        0.098       0.0759       0.0795     0.000828\n",
            "     88    11      0.00849      0.00848     1.33e-05       0.0556       0.0712       0.0476       0.0717       0.0597       0.0595       0.0902       0.0749        0.248      0.00258\n",
            "     88    12      0.00655      0.00655     1.88e-06       0.0471       0.0626       0.0389       0.0637       0.0513       0.0501       0.0821       0.0661       0.0867     0.000903\n",
            "     88    13      0.00681       0.0068     8.59e-06       0.0476       0.0638       0.0358       0.0713       0.0535       0.0458       0.0895       0.0677        0.216      0.00225\n",
            "     88    14      0.00701      0.00701      1.6e-06       0.0491       0.0648       0.0382        0.071       0.0546       0.0483        0.089       0.0686       0.0773     0.000806\n",
            "     88    15      0.00923      0.00923     7.11e-07       0.0569       0.0743       0.0467       0.0772       0.0619       0.0583       0.0989       0.0786       0.0455     0.000474\n",
            "     88    16      0.00968      0.00965     2.79e-05       0.0571        0.076       0.0462       0.0788       0.0625       0.0596        0.101       0.0804        0.383      0.00399\n",
            "     88    17      0.00703      0.00698     4.69e-05         0.05       0.0646       0.0423       0.0653       0.0538       0.0529       0.0832       0.0681        0.505      0.00526\n",
            "     88    18      0.00604      0.00604     5.99e-06       0.0454       0.0601       0.0351       0.0659       0.0505       0.0444       0.0831       0.0637        0.175      0.00182\n",
            "     88    19      0.00683      0.00681     2.54e-05        0.048       0.0638       0.0388       0.0665       0.0527       0.0503       0.0847       0.0675        0.372      0.00388\n",
            "     88    20      0.00863      0.00863     2.14e-06       0.0558       0.0719       0.0475       0.0725         0.06       0.0598       0.0914       0.0756       0.0916     0.000954\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     88     1      0.00714      0.00714     2.51e-06       0.0489       0.0654       0.0378       0.0711       0.0544       0.0486       0.0899       0.0693        0.093     0.000968\n",
            "     88     2      0.00714      0.00714     1.13e-06       0.0485       0.0654        0.037       0.0714       0.0542       0.0479       0.0908       0.0693       0.0543     0.000566\n",
            "     88     3      0.00679      0.00679     1.38e-06       0.0473       0.0638        0.036       0.0697       0.0529       0.0461       0.0891       0.0676       0.0708     0.000738\n",
            "     88     4      0.00697      0.00697     1.61e-06       0.0477       0.0646       0.0364       0.0702       0.0533       0.0464       0.0906       0.0685       0.0805     0.000838\n",
            "     88     5      0.00645      0.00645     9.44e-07       0.0466       0.0621       0.0364       0.0671       0.0517       0.0469       0.0847       0.0658       0.0484     0.000505\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              88  274.031    0.005      0.00744     1.39e-05      0.00746       0.0506       0.0667       0.0407       0.0703       0.0555       0.0519       0.0894       0.0706        0.223      0.00233\n",
            "! Validation         88  274.031    0.005       0.0069     1.52e-06       0.0069       0.0478       0.0643       0.0367       0.0699       0.0533       0.0472       0.0891       0.0681       0.0694     0.000723\n",
            "Wall time: 274.0320987519999\n",
            "! Best model       88    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     89     1      0.00879      0.00879     8.87e-07       0.0561       0.0725       0.0477       0.0729       0.0603       0.0593       0.0936       0.0764       0.0607     0.000633\n",
            "     89     2      0.00655      0.00654     3.83e-06       0.0469       0.0626        0.037       0.0667       0.0518       0.0466        0.086       0.0663        0.141      0.00147\n",
            "     89     3       0.0068       0.0068     7.94e-07       0.0479       0.0638       0.0377       0.0683        0.053       0.0483       0.0869       0.0676       0.0465     0.000484\n",
            "     89     4      0.00537      0.00536     2.62e-06        0.043       0.0567       0.0336       0.0617       0.0477       0.0421        0.078       0.0601          0.1      0.00104\n",
            "     89     5      0.00615      0.00615     5.11e-06       0.0461       0.0606       0.0359       0.0664       0.0511       0.0453       0.0833       0.0643        0.152      0.00159\n",
            "     89     6      0.00642      0.00642     3.56e-06       0.0461        0.062       0.0353       0.0676       0.0514       0.0452       0.0863       0.0657        0.131      0.00137\n",
            "     89     7      0.00692      0.00691     1.03e-05       0.0488       0.0643       0.0383         0.07       0.0541       0.0483       0.0879       0.0681        0.213      0.00222\n",
            "     89     8      0.00758      0.00758     8.69e-07       0.0499       0.0673       0.0402       0.0693       0.0547       0.0519       0.0907       0.0713       0.0607     0.000633\n",
            "     89     9      0.00656      0.00656     1.66e-06        0.048       0.0627       0.0392       0.0655       0.0524       0.0497       0.0827       0.0662       0.0859     0.000895\n",
            "     89    10      0.00733      0.00731     1.88e-05       0.0497       0.0661       0.0392       0.0706       0.0549       0.0498       0.0903       0.0701        0.322      0.00335\n",
            "     89    11      0.00686      0.00686     1.25e-06       0.0483       0.0641       0.0368       0.0713        0.054       0.0468       0.0891       0.0679        0.076     0.000791\n",
            "     89    12       0.0082      0.00818     2.61e-05       0.0526       0.0699       0.0425       0.0726       0.0576       0.0537       0.0944       0.0741        0.373      0.00389\n",
            "     89    13      0.00714      0.00714     5.73e-07       0.0505       0.0654        0.042       0.0673       0.0547       0.0526       0.0855        0.069       0.0469     0.000488\n",
            "     89    14        0.007        0.007     2.76e-06       0.0483       0.0647       0.0362       0.0725       0.0544       0.0466       0.0907       0.0686        0.106       0.0011\n",
            "     89    15      0.00674      0.00672     1.85e-05       0.0479       0.0634       0.0379       0.0678       0.0529        0.049       0.0853       0.0671        0.306      0.00319\n",
            "     89    16      0.00718      0.00717     3.22e-06       0.0494       0.0655       0.0398       0.0687       0.0542       0.0511       0.0876       0.0693        0.122      0.00127\n",
            "     89    17      0.00712       0.0071     2.35e-05        0.048       0.0652       0.0351        0.074       0.0545       0.0445       0.0937       0.0691        0.357      0.00372\n",
            "     89    18      0.00669      0.00669     6.25e-07       0.0476       0.0633       0.0365       0.0696       0.0531       0.0461       0.0881       0.0671        0.049     0.000511\n",
            "     89    19      0.00597      0.00596     4.06e-06       0.0445       0.0597        0.034       0.0655       0.0497       0.0436       0.0831       0.0633        0.113      0.00118\n",
            "     89    20      0.00637      0.00637     5.12e-06       0.0463       0.0617       0.0358       0.0673       0.0516        0.046       0.0849       0.0654        0.151      0.00157\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     89     1      0.00711      0.00711     2.44e-06       0.0488       0.0652       0.0377        0.071       0.0543       0.0485       0.0898       0.0691       0.0908     0.000946\n",
            "     89     2      0.00711      0.00711     1.08e-06       0.0484       0.0652       0.0369       0.0713       0.0541       0.0477       0.0906       0.0692       0.0514     0.000535\n",
            "     89     3      0.00677      0.00677     1.36e-06       0.0472       0.0636       0.0359       0.0697       0.0528        0.046        0.089       0.0675       0.0695     0.000724\n",
            "     89     4      0.00694      0.00694     1.53e-06       0.0476       0.0644       0.0363       0.0701       0.0532       0.0463       0.0904       0.0684        0.079     0.000823\n",
            "     89     5      0.00642      0.00642     9.57e-07       0.0465        0.062       0.0363        0.067       0.0516       0.0468       0.0846       0.0657       0.0492     0.000513\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              89  277.098    0.005      0.00688      6.7e-06      0.00689       0.0483       0.0642        0.038       0.0688       0.0534       0.0485       0.0875        0.068        0.151      0.00157\n",
            "! Validation         89  277.098    0.005      0.00687     1.47e-06      0.00687       0.0477       0.0641       0.0366       0.0698       0.0532       0.0471       0.0889        0.068        0.068     0.000708\n",
            "Wall time: 277.0985135789999\n",
            "! Best model       89    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     90     1      0.00706      0.00705     6.74e-06       0.0477        0.065       0.0363       0.0705       0.0534       0.0475       0.0904       0.0689        0.187      0.00194\n",
            "     90     2      0.00669      0.00669     1.21e-06       0.0483       0.0633       0.0374       0.0702       0.0538       0.0472       0.0869       0.0671       0.0697     0.000726\n",
            "     90     3       0.0072       0.0072     4.31e-06       0.0491       0.0656       0.0387       0.0699       0.0543        0.049       0.0901       0.0696        0.102      0.00106\n",
            "     90     4      0.00706      0.00706     2.05e-06       0.0487        0.065         0.04       0.0662       0.0531       0.0511       0.0863       0.0687        0.104      0.00108\n",
            "     90     5       0.0066      0.00659     1.23e-05       0.0459       0.0628       0.0355       0.0668       0.0511       0.0462       0.0869       0.0666        0.257      0.00268\n",
            "     90     6      0.00604      0.00604     1.42e-06       0.0457       0.0601       0.0346       0.0679       0.0513       0.0436       0.0839       0.0638       0.0645     0.000671\n",
            "     90     7      0.00629      0.00628     1.69e-06       0.0462       0.0613       0.0364       0.0659       0.0512       0.0463       0.0836        0.065       0.0928     0.000966\n",
            "     90     8      0.00655      0.00655     5.14e-07       0.0477       0.0626       0.0367       0.0696       0.0532       0.0461       0.0866       0.0664       0.0516     0.000537\n",
            "     90     9      0.00603      0.00603     1.86e-06       0.0453       0.0601       0.0348       0.0664       0.0506        0.044       0.0834       0.0637       0.0963        0.001\n",
            "     90    10      0.00628      0.00628     4.31e-06       0.0458       0.0613       0.0352       0.0671       0.0512       0.0448       0.0852        0.065        0.143      0.00149\n",
            "     90    11      0.00754      0.00754     3.85e-06       0.0498       0.0672       0.0378       0.0738       0.0558       0.0484        0.094       0.0712        0.139      0.00144\n",
            "     90    12      0.00751      0.00747     3.78e-05        0.051       0.0669       0.0419       0.0693       0.0556       0.0522       0.0892       0.0707        0.452       0.0047\n",
            "     90    13      0.00748      0.00747     2.51e-06       0.0511       0.0669       0.0422       0.0689       0.0556       0.0535       0.0878       0.0706       0.0789     0.000822\n",
            "     90    14      0.00677      0.00676     6.42e-06       0.0478       0.0636       0.0366         0.07       0.0533       0.0464       0.0885       0.0674         0.18      0.00187\n",
            "     90    15       0.0069      0.00688     1.87e-05       0.0474       0.0642       0.0371       0.0682       0.0526       0.0482       0.0878        0.068        0.319      0.00332\n",
            "     90    16      0.00574      0.00573     1.05e-06       0.0448       0.0586       0.0354       0.0634       0.0494        0.044       0.0802       0.0621       0.0629     0.000655\n",
            "     90    17      0.00711      0.00706     4.87e-05        0.049        0.065       0.0389       0.0691        0.054       0.0501       0.0875       0.0688        0.511      0.00532\n",
            "     90    18      0.00606      0.00606     1.06e-06       0.0445       0.0602       0.0338       0.0659       0.0498       0.0431       0.0847       0.0639       0.0658     0.000686\n",
            "     90    19      0.00695      0.00691     3.58e-05       0.0491       0.0643       0.0391       0.0692       0.0542        0.049       0.0872       0.0681        0.444      0.00462\n",
            "     90    20      0.00606      0.00605     4.96e-06       0.0447       0.0602       0.0336       0.0668       0.0502       0.0436        0.084       0.0638        0.158      0.00165\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     90     1      0.00708      0.00708     2.48e-06       0.0487       0.0651       0.0376       0.0709       0.0542       0.0483       0.0896        0.069       0.0916     0.000954\n",
            "     90     2      0.00708      0.00707     1.12e-06       0.0483       0.0651       0.0368       0.0712        0.054       0.0475       0.0905        0.069       0.0529     0.000551\n",
            "     90     3      0.00674      0.00674     1.37e-06       0.0471       0.0635       0.0358       0.0696       0.0527       0.0458       0.0889       0.0674       0.0704     0.000733\n",
            "     90     4      0.00691      0.00691     1.55e-06       0.0475       0.0643       0.0362       0.0699       0.0531       0.0461       0.0903       0.0682       0.0789     0.000822\n",
            "     90     5       0.0064       0.0064     9.45e-07       0.0464       0.0619       0.0361       0.0669       0.0515       0.0466       0.0845       0.0656       0.0458     0.000477\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              90  280.154    0.005      0.00669     9.86e-06       0.0067       0.0475       0.0633       0.0371       0.0683       0.0527       0.0473       0.0868        0.067        0.179      0.00186\n",
            "! Validation         90  280.154    0.005      0.00684     1.49e-06      0.00684       0.0476        0.064       0.0365       0.0697       0.0531       0.0469       0.0888       0.0678       0.0679     0.000708\n",
            "Wall time: 280.15463160599995\n",
            "! Best model       90    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     91     1      0.00794       0.0079     3.18e-05       0.0525       0.0688        0.044       0.0693       0.0567       0.0564       0.0884       0.0724        0.406      0.00423\n",
            "     91     2      0.00644      0.00643     3.44e-06       0.0464        0.062       0.0356       0.0681       0.0518       0.0446        0.087       0.0658        0.125       0.0013\n",
            "     91     3      0.00932      0.00928     3.85e-05        0.057       0.0745       0.0444       0.0823       0.0634       0.0553        0.103        0.079        0.452      0.00471\n",
            "     91     4       0.0088      0.00879     1.23e-05        0.056       0.0725       0.0467       0.0747       0.0607       0.0592       0.0936       0.0764         0.25      0.00261\n",
            "     91     5      0.00752      0.00752      2.1e-06       0.0525       0.0671       0.0464       0.0648       0.0556       0.0574       0.0831       0.0703       0.0867     0.000903\n",
            "     91     6      0.00619      0.00619     1.02e-06       0.0457       0.0609       0.0353       0.0666       0.0509       0.0451       0.0839       0.0645       0.0732     0.000763\n",
            "     91     7      0.00626      0.00624     1.45e-05       0.0462       0.0611       0.0367       0.0654        0.051       0.0464       0.0831       0.0647        0.264      0.00275\n",
            "     91     8      0.00617      0.00617     6.16e-07       0.0455       0.0608       0.0335       0.0695       0.0515       0.0426       0.0863       0.0645       0.0385     0.000401\n",
            "     91     9      0.00652      0.00651     9.64e-06       0.0468       0.0624       0.0371       0.0663       0.0517       0.0469       0.0854       0.0661        0.225      0.00235\n",
            "     91    10      0.00646      0.00645     5.43e-06       0.0465       0.0621       0.0357       0.0681       0.0519       0.0449       0.0869       0.0659        0.172      0.00179\n",
            "     91    11       0.0076      0.00759     7.01e-06       0.0498       0.0674       0.0371       0.0752       0.0561       0.0479       0.0951       0.0715        0.194      0.00202\n",
            "     91    12      0.00641      0.00641     2.93e-06       0.0465       0.0619       0.0368       0.0659       0.0513        0.046       0.0853       0.0656        0.112      0.00117\n",
            "     91    13      0.00765      0.00765     2.57e-06        0.052       0.0677       0.0444       0.0673       0.0558        0.056       0.0864       0.0712        0.112      0.00117\n",
            "     91    14      0.00623      0.00622      6.8e-06       0.0464        0.061        0.037       0.0652       0.0511       0.0473       0.0818       0.0646        0.184      0.00191\n",
            "     91    15      0.00606      0.00606     1.09e-06        0.045       0.0602       0.0341       0.0667       0.0504       0.0432       0.0845       0.0639        0.067     0.000698\n",
            "     91    16       0.0068      0.00677     2.39e-05       0.0478       0.0637        0.038       0.0673       0.0527        0.048       0.0869       0.0675        0.359      0.00374\n",
            "     91    17      0.00779      0.00777     1.56e-05       0.0504       0.0682       0.0387       0.0737       0.0562        0.051       0.0936       0.0723         0.29      0.00302\n",
            "     91    18      0.00638      0.00637     1.49e-05       0.0444       0.0617       0.0335       0.0662       0.0498       0.0435       0.0875       0.0655        0.285      0.00296\n",
            "     91    19      0.00674      0.00673        1e-05       0.0476       0.0635       0.0357       0.0714       0.0535       0.0449       0.0897       0.0673        0.219      0.00228\n",
            "     91    20       0.0086      0.00854     5.71e-05       0.0552       0.0715       0.0457       0.0743         0.06       0.0576       0.0933       0.0754        0.558      0.00582\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     91     1      0.00705      0.00705     2.47e-06       0.0486        0.065       0.0374       0.0708       0.0541       0.0482       0.0895       0.0689       0.0925     0.000963\n",
            "     91     2      0.00706      0.00706     1.07e-06       0.0482        0.065       0.0367       0.0711       0.0539       0.0474       0.0904       0.0689       0.0512     0.000533\n",
            "     91     3      0.00672      0.00672     1.38e-06        0.047       0.0634       0.0357       0.0695       0.0526       0.0457       0.0888       0.0672       0.0712     0.000742\n",
            "     91     4      0.00688      0.00688     1.54e-06       0.0473       0.0642       0.0361       0.0698        0.053        0.046       0.0901       0.0681       0.0799     0.000832\n",
            "     91     5      0.00637      0.00637     9.16e-07       0.0463       0.0617        0.036       0.0668       0.0514       0.0464       0.0844       0.0654       0.0459     0.000478\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              91  283.202    0.005      0.00708     1.31e-05      0.00709        0.049       0.0651       0.0388       0.0694       0.0541       0.0495       0.0884       0.0689        0.224      0.00233\n",
            "! Validation         91  283.202    0.005      0.00681     1.48e-06      0.00682       0.0475       0.0639       0.0364       0.0696        0.053       0.0468       0.0887       0.0677       0.0681      0.00071\n",
            "Wall time: 283.203124732\n",
            "! Best model       91    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     92     1      0.00935      0.00935        4e-06       0.0587       0.0748        0.053       0.0702       0.0616       0.0652        0.091       0.0781        0.134       0.0014\n",
            "     92     2      0.00671      0.00666     4.63e-05       0.0478       0.0631       0.0387        0.066       0.0523       0.0485       0.0852       0.0668        0.502      0.00523\n",
            "     92     3      0.00622      0.00621     1.12e-05       0.0466        0.061        0.038       0.0638       0.0509       0.0475       0.0815       0.0645        0.231      0.00241\n",
            "     92     4      0.00851      0.00847     4.27e-05       0.0549       0.0712       0.0454        0.074       0.0597       0.0573       0.0929       0.0751        0.483      0.00503\n",
            "     92     5      0.00964      0.00949     0.000145       0.0579       0.0754       0.0469       0.0799       0.0634       0.0588        0.101       0.0797         0.89      0.00928\n",
            "     92     6      0.00868      0.00862     6.02e-05       0.0544       0.0718       0.0438       0.0755       0.0596       0.0547       0.0975       0.0761        0.574      0.00598\n",
            "     92     7      0.00644      0.00634       0.0001       0.0466       0.0616       0.0373       0.0652       0.0513       0.0475       0.0828       0.0652         0.74       0.0077\n",
            "     92     8      0.00691       0.0069      3.1e-06       0.0482       0.0643       0.0376       0.0694       0.0535       0.0471       0.0893       0.0682        0.127      0.00133\n",
            "     92     9      0.00649      0.00647     1.78e-05       0.0479       0.0622       0.0401       0.0636       0.0518       0.0506       0.0805       0.0656        0.309      0.00322\n",
            "     92    10      0.00747      0.00745      1.9e-05       0.0509       0.0668       0.0434        0.066       0.0547       0.0549       0.0858       0.0703        0.322      0.00336\n",
            "     92    11      0.00629      0.00628     1.07e-05       0.0458       0.0613       0.0365       0.0642       0.0504       0.0461       0.0838        0.065        0.233      0.00242\n",
            "     92    12      0.00657      0.00656     7.46e-06       0.0482       0.0627       0.0388        0.067       0.0529       0.0491       0.0834       0.0663        0.178      0.00185\n",
            "     92    13       0.0069       0.0069     6.26e-07       0.0477       0.0643       0.0371       0.0688        0.053       0.0483       0.0879       0.0681       0.0498     0.000519\n",
            "     92    14      0.00724      0.00723     3.56e-06       0.0496       0.0658       0.0389        0.071        0.055       0.0494       0.0901       0.0697        0.137      0.00143\n",
            "     92    15      0.00587      0.00587     1.22e-06       0.0446       0.0593       0.0337       0.0664       0.0501       0.0428        0.083       0.0629        0.067     0.000698\n",
            "     92    16      0.00742      0.00742     1.43e-06       0.0508       0.0667       0.0393       0.0737       0.0565       0.0492       0.0921       0.0707       0.0707     0.000736\n",
            "     92    17      0.00683      0.00682     1.57e-05       0.0478       0.0639       0.0355       0.0725        0.054       0.0453       0.0902       0.0677        0.289      0.00302\n",
            "     92    18      0.00781      0.00781     2.97e-06       0.0527       0.0684       0.0432       0.0716       0.0574       0.0548       0.0896       0.0722        0.108      0.00113\n",
            "     92    19      0.00669      0.00667     1.49e-05       0.0475       0.0632       0.0371       0.0683       0.0527       0.0467       0.0873        0.067        0.277      0.00288\n",
            "     92    20      0.00709      0.00708     5.04e-06       0.0498       0.0651       0.0397       0.0702       0.0549       0.0497       0.0881       0.0689        0.158      0.00165\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     92     1      0.00703      0.00703     2.47e-06       0.0485       0.0649       0.0373       0.0707        0.054       0.0481       0.0894       0.0688       0.0917     0.000955\n",
            "     92     2      0.00703      0.00703     1.04e-06       0.0481       0.0648       0.0366        0.071       0.0538       0.0473       0.0903       0.0688        0.051     0.000531\n",
            "     92     3      0.00669      0.00669     1.36e-06       0.0469       0.0633       0.0356       0.0695       0.0525       0.0455       0.0887       0.0671       0.0708     0.000738\n",
            "     92     4      0.00685      0.00685     1.54e-06       0.0473        0.064        0.036       0.0698       0.0529       0.0458         0.09       0.0679       0.0784     0.000817\n",
            "     92     5      0.00634      0.00634        9e-07       0.0462       0.0616       0.0359       0.0667       0.0513       0.0463       0.0843       0.0653       0.0454     0.000473\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              92  286.252    0.005      0.00723     2.56e-05      0.00726       0.0499       0.0658       0.0402       0.0694       0.0548       0.0509       0.0883       0.0696        0.294      0.00306\n",
            "! Validation         92  286.252    0.005      0.00679     1.46e-06      0.00679       0.0474       0.0637       0.0363       0.0695       0.0529       0.0466       0.0886       0.0676       0.0675     0.000703\n",
            "Wall time: 286.252568709\n",
            "! Best model       92    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     93     1      0.00661       0.0066     1.16e-05       0.0469       0.0628       0.0361       0.0684       0.0523       0.0464       0.0869       0.0666         0.24       0.0025\n",
            "     93     2      0.00794      0.00794     7.79e-07        0.055       0.0689       0.0498       0.0654       0.0576       0.0611       0.0825       0.0718        0.052     0.000541\n",
            "     93     3      0.00818      0.00817     4.99e-06       0.0536       0.0699       0.0445       0.0718       0.0581        0.057       0.0904       0.0737        0.163       0.0017\n",
            "     93     4      0.00755      0.00754     3.53e-06       0.0504       0.0672       0.0389       0.0732       0.0561       0.0489       0.0936       0.0713        0.136      0.00142\n",
            "     93     5      0.00645      0.00642     2.72e-05       0.0459        0.062        0.034       0.0697       0.0519       0.0436       0.0879       0.0658         0.38      0.00396\n",
            "     93     6      0.00696      0.00693     3.14e-05       0.0482       0.0644       0.0367        0.071       0.0539       0.0465       0.0901       0.0683        0.412      0.00429\n",
            "     93     7      0.00754      0.00744     9.34e-05       0.0509       0.0667       0.0421       0.0685       0.0553       0.0538        0.087       0.0704        0.715      0.00745\n",
            "     93     8      0.00716      0.00714     1.92e-05       0.0487       0.0654       0.0397       0.0669       0.0533       0.0504       0.0879       0.0692        0.315      0.00328\n",
            "     93     9      0.00676      0.00673     2.69e-05       0.0478       0.0635       0.0363        0.071       0.0536       0.0461       0.0886       0.0673        0.371      0.00387\n",
            "     93    10      0.00718      0.00714      4.7e-05       0.0491       0.0654       0.0407       0.0658       0.0533       0.0527       0.0852        0.069        0.505      0.00526\n",
            "     93    11      0.00746      0.00746      1.5e-06       0.0506       0.0668       0.0394       0.0729       0.0562       0.0499       0.0918       0.0708       0.0689     0.000718\n",
            "     93    12      0.00747      0.00733     0.000132       0.0499       0.0663       0.0386       0.0724       0.0555        0.048       0.0925       0.0703        0.849      0.00884\n",
            "     93    13      0.00574      0.00574     7.27e-06       0.0443       0.0586       0.0344       0.0641       0.0493       0.0438       0.0804       0.0621        0.191      0.00199\n",
            "     93    14      0.00761      0.00755      6.1e-05       0.0505       0.0672       0.0394       0.0728       0.0561       0.0504       0.0921       0.0712        0.578      0.00602\n",
            "     93    15      0.00726      0.00726     2.64e-06       0.0491       0.0659       0.0385       0.0705       0.0545       0.0493       0.0904       0.0698        0.114      0.00119\n",
            "     93    16      0.00685      0.00684     8.18e-06       0.0486        0.064         0.04       0.0657       0.0529       0.0506       0.0847       0.0676        0.204      0.00213\n",
            "     93    17      0.00544      0.00543      5.1e-06        0.043        0.057       0.0344       0.0602       0.0473       0.0443       0.0764       0.0603        0.125       0.0013\n",
            "     93    18      0.00741      0.00741     5.28e-06       0.0515       0.0666       0.0411       0.0723       0.0567       0.0512       0.0897       0.0705        0.158      0.00165\n",
            "     93    19       0.0104       0.0104     1.97e-05       0.0597       0.0787       0.0484       0.0823       0.0654       0.0615        0.105       0.0832        0.325      0.00338\n",
            "     93    20      0.00948      0.00936     0.000113        0.058       0.0749       0.0481       0.0779        0.063         0.06        0.098        0.079        0.787      0.00819\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     93     1        0.007        0.007     2.48e-06       0.0484       0.0647       0.0372       0.0706       0.0539       0.0479       0.0893       0.0686       0.0938     0.000977\n",
            "     93     2        0.007        0.007     9.79e-07        0.048       0.0647       0.0365        0.071       0.0537       0.0471       0.0902       0.0686       0.0473     0.000492\n",
            "     93     3      0.00667      0.00667     1.37e-06       0.0468       0.0632       0.0355       0.0694       0.0524       0.0454       0.0886        0.067       0.0722     0.000752\n",
            "     93     4      0.00683      0.00682     1.64e-06       0.0471       0.0639       0.0359       0.0697       0.0528       0.0457       0.0899       0.0678       0.0833     0.000868\n",
            "     93     5      0.00633      0.00632     8.82e-07       0.0461       0.0615       0.0358       0.0666       0.0512       0.0461       0.0843       0.0652       0.0471      0.00049\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              93  289.300    0.005      0.00734     3.11e-05      0.00737       0.0501       0.0663       0.0401       0.0701       0.0551       0.0511       0.0893       0.0702        0.334      0.00348\n",
            "! Validation         93  289.300    0.005      0.00676     1.47e-06      0.00677       0.0473       0.0636       0.0362       0.0695       0.0528       0.0465       0.0885       0.0675       0.0687     0.000716\n",
            "Wall time: 289.30109944899993\n",
            "! Best model       93    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     94     1        0.007      0.00698     2.26e-05       0.0488       0.0646       0.0389       0.0685       0.0537       0.0496       0.0872       0.0684        0.346       0.0036\n",
            "     94     2      0.00692      0.00688     4.23e-05       0.0478       0.0642       0.0364       0.0706       0.0535       0.0471        0.089        0.068         0.48        0.005\n",
            "     94     3      0.00743      0.00738     4.39e-05        0.051       0.0665        0.041       0.0709       0.0559       0.0512       0.0895       0.0704         0.49      0.00511\n",
            "     94     4      0.00824      0.00824     2.43e-06       0.0556       0.0702       0.0482       0.0705       0.0593       0.0591       0.0883       0.0737       0.0857     0.000893\n",
            "     94     5      0.00853      0.00847     5.98e-05       0.0548       0.0712       0.0449       0.0746       0.0597       0.0564        0.094       0.0752        0.569      0.00592\n",
            "     94     6      0.00649      0.00649     4.82e-07        0.046       0.0623       0.0359        0.066        0.051        0.046       0.0861       0.0661       0.0453     0.000472\n",
            "     94     7      0.00658      0.00657     1.34e-05       0.0471       0.0627       0.0365       0.0683       0.0524       0.0473       0.0855       0.0664        0.262      0.00273\n",
            "     94     8      0.00833      0.00833      5.4e-06       0.0539       0.0706       0.0426       0.0764       0.0595       0.0529       0.0967       0.0748        0.166      0.00173\n",
            "     94     9      0.00884      0.00883     1.27e-05       0.0558       0.0727       0.0462       0.0749       0.0606       0.0582       0.0953       0.0767        0.249      0.00259\n",
            "     94    10      0.00694      0.00693     7.48e-06       0.0485       0.0644       0.0394       0.0668       0.0531       0.0508       0.0854       0.0681        0.199      0.00208\n",
            "     94    11      0.00647      0.00647     1.21e-06        0.047       0.0622       0.0354         0.07       0.0527       0.0447       0.0873        0.066       0.0541     0.000564\n",
            "     94    12      0.00759      0.00757     1.78e-05       0.0512       0.0673       0.0434       0.0667       0.0551       0.0554       0.0863       0.0709         0.29      0.00302\n",
            "     94    13      0.00766      0.00765     7.44e-06       0.0518       0.0677       0.0417       0.0721       0.0569       0.0527       0.0904       0.0716        0.167      0.00174\n",
            "     94    14       0.0057      0.00569     7.28e-06       0.0433       0.0584        0.033       0.0639       0.0485       0.0419       0.0819       0.0619        0.185      0.00192\n",
            "     94    15      0.00669      0.00669     1.03e-06       0.0484       0.0633       0.0394       0.0665        0.053       0.0495       0.0843       0.0669       0.0717     0.000747\n",
            "     94    16      0.00796      0.00796     8.48e-07       0.0522        0.069       0.0424       0.0717       0.0571       0.0538       0.0922        0.073       0.0572     0.000596\n",
            "     94    17      0.00754      0.00751        3e-05       0.0494        0.067       0.0374       0.0734       0.0554       0.0483       0.0938       0.0711        0.403       0.0042\n",
            "     94    18      0.00614      0.00614     1.14e-06       0.0456       0.0606       0.0347       0.0672        0.051       0.0438       0.0849       0.0643       0.0607     0.000633\n",
            "     94    19      0.00587      0.00586     9.95e-06       0.0451       0.0592       0.0361       0.0633       0.0497       0.0451       0.0803       0.0627        0.229      0.00239\n",
            "     94    20      0.00618      0.00618     1.56e-06       0.0456       0.0608       0.0355       0.0659       0.0507       0.0454       0.0834       0.0644       0.0854     0.000889\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     94     1      0.00698      0.00698     2.51e-06       0.0483       0.0646       0.0371       0.0705       0.0538       0.0478       0.0892       0.0685       0.0926     0.000964\n",
            "     94     2      0.00698      0.00698     1.08e-06       0.0479       0.0646       0.0364       0.0709       0.0536        0.047       0.0901       0.0685       0.0526     0.000548\n",
            "     94     3      0.00665      0.00665     1.34e-06       0.0467       0.0631       0.0354       0.0693       0.0524       0.0453       0.0885       0.0669       0.0695     0.000724\n",
            "     94     4       0.0068       0.0068     1.58e-06        0.047       0.0638       0.0358       0.0696       0.0527       0.0456       0.0898       0.0677       0.0792     0.000825\n",
            "     94     5       0.0063       0.0063     8.97e-07        0.046       0.0614       0.0357       0.0665       0.0511        0.046       0.0842       0.0651       0.0465     0.000484\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              94  292.368    0.005      0.00714     1.44e-05      0.00715       0.0494       0.0654       0.0394       0.0694       0.0544       0.0502       0.0882       0.0692        0.225      0.00234\n",
            "! Validation         94  292.368    0.005      0.00674     1.48e-06      0.00674       0.0472       0.0635       0.0361       0.0694       0.0527       0.0463       0.0884       0.0674       0.0681     0.000709\n",
            "Wall time: 292.3685706309999\n",
            "! Best model       94    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     95     1       0.0064       0.0064     8.03e-07       0.0464       0.0619       0.0357       0.0678       0.0517       0.0456       0.0857       0.0656       0.0529     0.000551\n",
            "     95     2       0.0067      0.00669     1.14e-05       0.0487       0.0633       0.0392       0.0676       0.0534       0.0491       0.0848       0.0669        0.213      0.00222\n",
            "     95     3      0.00605      0.00605     4.23e-07       0.0458       0.0602       0.0372       0.0628         0.05       0.0467       0.0806       0.0637       0.0451      0.00047\n",
            "     95     4      0.00691      0.00689     2.14e-05       0.0475       0.0642       0.0363       0.0699       0.0531       0.0473       0.0888       0.0681        0.343      0.00357\n",
            "     95     5      0.00608      0.00604     3.39e-05       0.0451       0.0601       0.0348       0.0657       0.0503       0.0439       0.0836       0.0638         0.43      0.00448\n",
            "     95     6      0.00739      0.00739     1.39e-06        0.049       0.0665       0.0363       0.0745       0.0554       0.0468       0.0943       0.0705        0.074     0.000771\n",
            "     95     7      0.00721      0.00705     0.000164       0.0498        0.065       0.0408       0.0678       0.0543       0.0516       0.0857       0.0686        0.948      0.00988\n",
            "     95     8      0.00657      0.00656     6.38e-06       0.0477       0.0627       0.0373       0.0685       0.0529       0.0473       0.0855       0.0664        0.186      0.00193\n",
            "     95     9      0.00672      0.00663     9.17e-05       0.0467        0.063       0.0352       0.0696       0.0524       0.0445        0.089       0.0668        0.711       0.0074\n",
            "     95    10      0.00833       0.0083     2.15e-05       0.0546       0.0705       0.0481       0.0678       0.0579       0.0618       0.0853       0.0735        0.321      0.00334\n",
            "     95    11      0.00795      0.00794     8.64e-06       0.0528       0.0689        0.042       0.0744       0.0582       0.0529        0.093        0.073        0.216      0.00225\n",
            "     95    12      0.00583       0.0058     3.09e-05       0.0443       0.0589       0.0334       0.0662       0.0498       0.0421       0.0829       0.0625        0.401      0.00418\n",
            "     95    13      0.00613      0.00611     2.09e-05       0.0452       0.0605       0.0365       0.0628       0.0496       0.0468       0.0812        0.064        0.333      0.00347\n",
            "     95    14      0.00782      0.00776     5.83e-05       0.0505       0.0682       0.0378       0.0758       0.0568       0.0488       0.0958       0.0723        0.565      0.00588\n",
            "     95    15      0.00695       0.0069     5.58e-05       0.0487       0.0643        0.038       0.0699        0.054       0.0485       0.0877       0.0681        0.553      0.00576\n",
            "     95    16      0.00765      0.00756     8.77e-05       0.0507       0.0673         0.04       0.0721       0.0561       0.0513       0.0912       0.0713        0.693      0.00722\n",
            "     95    17      0.00608      0.00607     7.53e-06       0.0447       0.0603       0.0345       0.0653       0.0499       0.0445       0.0832       0.0639        0.199      0.00207\n",
            "     95    18      0.00704      0.00695     8.64e-05        0.049       0.0645       0.0397       0.0678       0.0537       0.0505        0.086       0.0682        0.686      0.00715\n",
            "     95    19      0.00706      0.00704     2.56e-05       0.0473       0.0649       0.0361       0.0696       0.0528       0.0466       0.0911       0.0688        0.371      0.00387\n",
            "     95    20      0.00626       0.0062     5.93e-05       0.0457       0.0609       0.0353       0.0666       0.0509       0.0452        0.084       0.0646        0.568      0.00591\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     95     1      0.00696      0.00695      2.5e-06       0.0482       0.0645       0.0371       0.0704       0.0537       0.0477       0.0891       0.0684       0.0935     0.000974\n",
            "     95     2      0.00696      0.00695     1.01e-06       0.0478       0.0645       0.0363       0.0708       0.0536       0.0469         0.09       0.0684        0.049     0.000511\n",
            "     95     3      0.00662      0.00662     1.34e-06       0.0466        0.063       0.0353       0.0693       0.0523       0.0451       0.0884       0.0668       0.0707     0.000736\n",
            "     95     4      0.00678      0.00678     1.58e-06       0.0469       0.0637       0.0357       0.0695       0.0526       0.0454       0.0897       0.0676        0.081     0.000843\n",
            "     95     5      0.00628      0.00628     9.08e-07       0.0459       0.0613       0.0356       0.0665        0.051       0.0458       0.0841        0.065       0.0461      0.00048\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              95  295.426    0.005      0.00682     3.97e-05      0.00686        0.048       0.0639       0.0377       0.0686       0.0532       0.0483       0.0871       0.0677        0.395      0.00412\n",
            "! Validation         95  295.426    0.005      0.00672     1.47e-06      0.00672       0.0471       0.0634        0.036       0.0693       0.0526       0.0462       0.0883       0.0672        0.068     0.000709\n",
            "Wall time: 295.427157125\n",
            "! Best model       95    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     96     1      0.00695      0.00688      7.2e-05       0.0489       0.0642       0.0386       0.0696       0.0541        0.048        0.088        0.068        0.627      0.00653\n",
            "     96     2       0.0087      0.00865     4.46e-05       0.0536        0.072       0.0432       0.0744       0.0588       0.0547       0.0977       0.0762        0.493      0.00513\n",
            "     96     3      0.00839      0.00839     3.79e-06       0.0552       0.0708       0.0453       0.0748       0.0601       0.0567       0.0928       0.0748        0.142      0.00148\n",
            "     96     4      0.00668      0.00668     5.32e-07       0.0473       0.0632       0.0373       0.0673       0.0523       0.0474       0.0866        0.067       0.0467     0.000486\n",
            "     96     5      0.00642      0.00642     1.83e-06       0.0457        0.062       0.0354       0.0661       0.0508        0.046       0.0854       0.0657       0.0891     0.000928\n",
            "     96     6      0.00747      0.00746     7.16e-06       0.0512       0.0668        0.042       0.0695       0.0558       0.0542       0.0867       0.0705        0.189      0.00197\n",
            "     96     7       0.0082      0.00818     2.21e-05       0.0525         0.07        0.042       0.0734       0.0577       0.0538       0.0944       0.0741        0.347      0.00361\n",
            "     96     8      0.00761      0.00761      1.5e-06       0.0507       0.0675       0.0415       0.0692       0.0553       0.0525       0.0903       0.0714       0.0842     0.000877\n",
            "     96     9      0.00662      0.00661     2.17e-06       0.0474       0.0629       0.0386        0.065       0.0518       0.0494       0.0837       0.0665       0.0982      0.00102\n",
            "     96    10      0.00629      0.00628     3.63e-06        0.045       0.0613       0.0333       0.0682       0.0508       0.0429       0.0872        0.065         0.13      0.00136\n",
            "     96    11      0.00683      0.00683     6.18e-06       0.0483       0.0639       0.0382       0.0684       0.0533        0.049       0.0864       0.0677         0.17      0.00177\n",
            "     96    12      0.00544      0.00543      5.8e-06        0.043        0.057       0.0334       0.0622       0.0478       0.0422       0.0787       0.0605        0.155      0.00161\n",
            "     96    13      0.00599      0.00598     1.01e-05       0.0451       0.0598       0.0353       0.0648       0.0501       0.0441       0.0828       0.0634        0.214      0.00223\n",
            "     96    14      0.00587      0.00586     1.49e-05       0.0435       0.0592       0.0321       0.0663       0.0492       0.0407       0.0849       0.0628        0.286      0.00298\n",
            "     96    15      0.00606      0.00605     1.04e-05       0.0452       0.0602       0.0342       0.0672       0.0507       0.0433       0.0844       0.0638        0.225      0.00234\n",
            "     96    16      0.00526      0.00521     4.92e-05       0.0418       0.0559        0.033       0.0594       0.0462        0.042       0.0764       0.0592        0.519       0.0054\n",
            "     96    17      0.00639      0.00639     1.52e-06       0.0466       0.0618       0.0371       0.0657       0.0514       0.0477       0.0832       0.0654       0.0838     0.000873\n",
            "     96    18      0.00581      0.00575     6.26e-05       0.0444       0.0587       0.0339       0.0654       0.0496       0.0427       0.0817       0.0622        0.586       0.0061\n",
            "     96    19      0.00573      0.00572     1.16e-05       0.0439       0.0585       0.0337       0.0644        0.049       0.0437       0.0803        0.062        0.249      0.00259\n",
            "     96    20       0.0064      0.00636     3.95e-05       0.0463       0.0617       0.0346       0.0697       0.0522       0.0436       0.0873       0.0655        0.458      0.00477\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     96     1      0.00693      0.00693     2.47e-06       0.0481       0.0644        0.037       0.0703       0.0536       0.0476        0.089       0.0683       0.0927     0.000965\n",
            "     96     2      0.00693      0.00693     9.75e-07       0.0477       0.0644       0.0362       0.0708       0.0535       0.0468       0.0898       0.0683       0.0475     0.000494\n",
            "     96     3       0.0066       0.0066     1.35e-06       0.0465       0.0629       0.0352       0.0692       0.0522        0.045       0.0883       0.0667       0.0712     0.000742\n",
            "     96     4      0.00676      0.00675     1.52e-06       0.0469       0.0636       0.0356       0.0694       0.0525       0.0453       0.0896       0.0674       0.0794     0.000827\n",
            "     96     5      0.00626      0.00626     8.84e-07       0.0458       0.0612       0.0355       0.0664        0.051       0.0457        0.084       0.0649       0.0463     0.000482\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              96  298.493    0.005      0.00664     1.85e-05      0.00666       0.0473        0.063       0.0371       0.0676       0.0523       0.0475       0.0861       0.0668         0.26       0.0027\n",
            "! Validation         96  298.493    0.005      0.00669     1.44e-06       0.0067        0.047       0.0633       0.0359       0.0692       0.0526       0.0461       0.0882       0.0671       0.0674     0.000702\n",
            "Wall time: 298.49358263499994\n",
            "! Best model       96    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     97     1      0.00651       0.0065     1.66e-05       0.0463       0.0624       0.0354       0.0682       0.0518       0.0453       0.0869       0.0661        0.298       0.0031\n",
            "     97     2      0.00749      0.00747     2.38e-05       0.0509       0.0669       0.0415       0.0696       0.0556       0.0527       0.0887       0.0707         0.35      0.00365\n",
            "     97     3      0.00627      0.00627      5.4e-06       0.0464       0.0613       0.0372       0.0649       0.0511       0.0471       0.0826       0.0648        0.168      0.00175\n",
            "     97     4      0.00637      0.00635     1.22e-05       0.0449       0.0617       0.0339       0.0668       0.0504       0.0434       0.0874       0.0654        0.257      0.00268\n",
            "     97     5       0.0065      0.00647     3.18e-05       0.0466       0.0622       0.0365        0.067       0.0517       0.0473       0.0846       0.0659        0.413       0.0043\n",
            "     97     6      0.00587      0.00585     1.91e-05        0.045       0.0592       0.0351        0.065         0.05       0.0438       0.0817       0.0627        0.314      0.00327\n",
            "     97     7      0.00649      0.00639     9.61e-05        0.045       0.0618       0.0342       0.0666       0.0504       0.0443       0.0869       0.0656        0.723      0.00754\n",
            "     97     8      0.00609      0.00608     1.17e-05       0.0453       0.0603       0.0354       0.0652       0.0503       0.0453       0.0825       0.0639        0.252      0.00263\n",
            "     97     9      0.00646      0.00633     0.000125       0.0461       0.0616       0.0366       0.0653       0.0509       0.0466       0.0838       0.0652        0.827      0.00861\n",
            "     97    10      0.00686      0.00686     2.45e-06       0.0479       0.0641       0.0357       0.0723        0.054        0.046         0.09        0.068       0.0949     0.000989\n",
            "     97    11      0.00775      0.00771     3.77e-05       0.0522       0.0679       0.0431       0.0705       0.0568       0.0549       0.0885       0.0717        0.447      0.00466\n",
            "     97    12      0.00757      0.00749     7.99e-05        0.052       0.0669       0.0437       0.0686       0.0562       0.0549       0.0861       0.0705        0.661      0.00689\n",
            "     97    13      0.00709      0.00707     1.61e-05       0.0483        0.065       0.0388       0.0673        0.053       0.0506        0.087       0.0688        0.291      0.00304\n",
            "     97    14      0.00553      0.00551     1.37e-05       0.0426       0.0574        0.033       0.0617       0.0474       0.0419       0.0799       0.0609        0.271      0.00282\n",
            "     97    15      0.00745      0.00744     9.93e-07       0.0484       0.0668       0.0362       0.0727       0.0544       0.0475       0.0941       0.0708        0.058     0.000604\n",
            "     97    16       0.0078       0.0078     7.18e-06       0.0525       0.0683       0.0416       0.0742       0.0579       0.0516       0.0931       0.0724        0.193      0.00201\n",
            "     97    17      0.00846      0.00845     8.17e-06       0.0547       0.0711       0.0464       0.0714       0.0589       0.0584       0.0914       0.0749        0.192        0.002\n",
            "     97    18      0.00746      0.00745     1.42e-05       0.0499       0.0668       0.0387       0.0722       0.0554       0.0501       0.0914       0.0708        0.278       0.0029\n",
            "     97    19      0.00597      0.00597     1.38e-06       0.0456       0.0598       0.0365       0.0638       0.0501       0.0463       0.0802       0.0632       0.0824     0.000859\n",
            "     97    20      0.00631      0.00631     2.41e-06       0.0461       0.0614       0.0367       0.0649       0.0508       0.0466       0.0835       0.0651        0.104      0.00108\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     97     1      0.00691      0.00691     2.44e-06        0.048       0.0643       0.0369       0.0702       0.0535       0.0474       0.0889       0.0682       0.0925     0.000963\n",
            "     97     2      0.00691       0.0069     1.04e-06       0.0476       0.0643       0.0361       0.0707       0.0534       0.0466       0.0897       0.0682         0.05     0.000521\n",
            "     97     3      0.00658      0.00658     1.33e-06       0.0465       0.0628       0.0351       0.0692       0.0521       0.0449       0.0882       0.0666       0.0707     0.000736\n",
            "     97     4      0.00674      0.00673     1.57e-06       0.0468       0.0635       0.0355       0.0694       0.0524       0.0452       0.0895       0.0673       0.0804     0.000837\n",
            "     97     5      0.00624      0.00624     8.57e-07       0.0457       0.0611       0.0354       0.0663       0.0509       0.0456       0.0839       0.0648       0.0445     0.000464\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              97  301.564    0.005      0.00679     2.63e-05      0.00681       0.0478       0.0637       0.0378       0.0679       0.0529       0.0484       0.0866       0.0675        0.314      0.00327\n",
            "! Validation         97  301.564    0.005      0.00667     1.45e-06      0.00667       0.0469       0.0632       0.0358       0.0692       0.0525        0.046       0.0881        0.067       0.0676     0.000704\n",
            "Wall time: 301.5652118009999\n",
            "! Best model       97    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     98     1        0.007      0.00697     2.74e-05        0.049       0.0646       0.0384       0.0701       0.0542       0.0484       0.0885       0.0684         0.38      0.00396\n",
            "     98     2        0.007      0.00699     3.92e-06       0.0492       0.0647       0.0379       0.0719       0.0549        0.048       0.0891       0.0686        0.112      0.00117\n",
            "     98     3      0.00582      0.00581     1.34e-05       0.0432        0.059       0.0319       0.0657       0.0488       0.0402       0.0848       0.0625        0.268      0.00279\n",
            "     98     4      0.00726      0.00726     4.39e-07       0.0483       0.0659       0.0356       0.0736       0.0546       0.0461       0.0937       0.0699        0.041     0.000427\n",
            "     98     5      0.00698      0.00696     2.24e-05       0.0475       0.0645        0.036       0.0705       0.0532       0.0461       0.0908       0.0684        0.351      0.00366\n",
            "     98     6      0.00578      0.00578     3.59e-06       0.0442       0.0588       0.0327       0.0671       0.0499       0.0415       0.0833       0.0624        0.122      0.00127\n",
            "     98     7      0.00624      0.00622     1.56e-05       0.0455        0.061       0.0348       0.0668       0.0508       0.0448       0.0847       0.0647        0.291      0.00303\n",
            "     98     8      0.00577      0.00577     2.78e-06       0.0443       0.0588       0.0349        0.063        0.049       0.0445         0.08       0.0622        0.112      0.00117\n",
            "     98     9      0.00686      0.00686     2.45e-06       0.0484       0.0641       0.0386       0.0681       0.0534       0.0488       0.0869       0.0678        0.103      0.00107\n",
            "     98    10      0.00717      0.00713     3.63e-05       0.0491       0.0653       0.0387         0.07       0.0543       0.0499       0.0885       0.0692        0.444      0.00462\n",
            "     98    11      0.00718      0.00717     6.31e-06       0.0491       0.0655       0.0375       0.0725        0.055       0.0481       0.0909       0.0695         0.18      0.00188\n",
            "     98    12      0.00634      0.00634     5.73e-06       0.0467       0.0616       0.0376       0.0649       0.0512       0.0473       0.0831       0.0652        0.173       0.0018\n",
            "     98    13      0.00581      0.00581     4.36e-06       0.0445        0.059       0.0338       0.0658       0.0498       0.0428       0.0823       0.0625         0.14      0.00146\n",
            "     98    14       0.0062       0.0062     4.73e-06       0.0452       0.0609       0.0353        0.065       0.0501       0.0445       0.0847       0.0646        0.152      0.00158\n",
            "     98    15      0.00583      0.00583     9.33e-07        0.044        0.059        0.034       0.0639        0.049       0.0429       0.0823       0.0626       0.0682      0.00071\n",
            "     98    16      0.00496      0.00496     1.55e-06       0.0416       0.0545       0.0332       0.0584       0.0458       0.0418       0.0736       0.0577       0.0754     0.000785\n",
            "     98    17      0.00721      0.00721     3.48e-07       0.0484       0.0657       0.0369       0.0713       0.0541       0.0477       0.0916       0.0697       0.0387     0.000403\n",
            "     98    18      0.00642       0.0064     1.86e-05       0.0471       0.0619       0.0365       0.0682       0.0523       0.0454       0.0858       0.0656        0.318      0.00332\n",
            "     98    19       0.0063       0.0063     1.12e-06       0.0463       0.0614       0.0367       0.0654        0.051       0.0474       0.0826        0.065       0.0664     0.000692\n",
            "     98    20      0.00573      0.00572     9.44e-06       0.0434       0.0585       0.0334       0.0633       0.0484       0.0438       0.0802        0.062        0.225      0.00235\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     98     1      0.00688      0.00688     2.39e-06       0.0479       0.0642       0.0368       0.0701       0.0535       0.0473       0.0888        0.068       0.0899     0.000937\n",
            "     98     2      0.00688      0.00688     1.11e-06       0.0476       0.0642        0.036       0.0706       0.0533       0.0465       0.0896        0.068       0.0513     0.000534\n",
            "     98     3      0.00656      0.00656     1.41e-06       0.0464       0.0627        0.035       0.0691        0.052       0.0448       0.0881       0.0664       0.0719     0.000749\n",
            "     98     4      0.00671      0.00671     1.54e-06       0.0467       0.0634       0.0354       0.0693       0.0523        0.045       0.0894       0.0672       0.0785     0.000818\n",
            "     98     5      0.00622      0.00622     8.48e-07       0.0456        0.061       0.0353       0.0663       0.0508       0.0455       0.0839       0.0647       0.0453     0.000472\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              98  304.632    0.005      0.00638     9.07e-06      0.00639       0.0462       0.0618       0.0357       0.0673       0.0515       0.0456       0.0855       0.0655        0.183      0.00191\n",
            "! Validation         98  304.632    0.005      0.00665     1.46e-06      0.00665       0.0468       0.0631       0.0357       0.0691       0.0524       0.0458        0.088       0.0669       0.0674     0.000702\n",
            "Wall time: 304.632624599\n",
            "! Best model       98    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     99     1      0.00634      0.00633      1.3e-05        0.046       0.0615       0.0356       0.0669       0.0512       0.0453       0.0852       0.0652        0.256      0.00267\n",
            "     99     2      0.00599      0.00597      1.1e-05       0.0442       0.0598       0.0349       0.0628       0.0489       0.0454       0.0813       0.0633        0.218      0.00227\n",
            "     99     3      0.00641       0.0064     5.17e-06        0.046       0.0619       0.0342       0.0698        0.052       0.0442       0.0871       0.0656        0.162      0.00168\n",
            "     99     4      0.00864      0.00859     4.79e-05       0.0546       0.0717       0.0449       0.0741       0.0595       0.0568       0.0948       0.0758        0.512      0.00534\n",
            "     99     5      0.00912      0.00911     2.75e-06       0.0566       0.0739       0.0459       0.0779       0.0619       0.0574       0.0988       0.0781        0.107      0.00111\n",
            "     99     6      0.00862      0.00862     6.44e-07       0.0564       0.0718       0.0462       0.0769       0.0615       0.0563       0.0955       0.0759       0.0484     0.000505\n",
            "     99     7      0.00806      0.00803     3.16e-05       0.0535       0.0693       0.0443       0.0718       0.0581       0.0569       0.0891        0.073        0.414      0.00431\n",
            "     99     8      0.00605      0.00605     2.81e-06       0.0445       0.0602       0.0349       0.0636       0.0493       0.0442       0.0834       0.0638        0.116      0.00121\n",
            "     99     9       0.0066       0.0066      2.6e-06       0.0468       0.0629       0.0354       0.0696       0.0525       0.0452       0.0881       0.0667        0.101      0.00106\n",
            "     99    10        0.007      0.00699     6.38e-06       0.0484       0.0647       0.0392       0.0669        0.053       0.0508        0.086       0.0684        0.184      0.00192\n",
            "     99    11      0.00712       0.0071     1.74e-05       0.0498       0.0652       0.0421       0.0651       0.0536       0.0537       0.0836       0.0686        0.301      0.00314\n",
            "     99    12      0.00568      0.00568     4.55e-06       0.0435       0.0583       0.0338       0.0628       0.0483       0.0429       0.0807       0.0618         0.15      0.00156\n",
            "     99    13      0.00585      0.00585     9.27e-07        0.044       0.0592       0.0324       0.0672       0.0498       0.0413       0.0842       0.0628       0.0572     0.000596\n",
            "     99    14      0.00694      0.00694     1.01e-06       0.0467       0.0645       0.0354       0.0692       0.0523       0.0467         0.09       0.0684       0.0643     0.000669\n",
            "     99    15       0.0064      0.00638     1.95e-05       0.0462       0.0618       0.0355       0.0675       0.0515       0.0449       0.0862       0.0656        0.317       0.0033\n",
            "     99    16      0.00619      0.00618     6.47e-06       0.0452       0.0608       0.0332       0.0694       0.0513       0.0422       0.0868       0.0645        0.187      0.00195\n",
            "     99    17      0.00657      0.00654     2.48e-05       0.0477       0.0626       0.0384       0.0664       0.0524       0.0482       0.0842       0.0662        0.365       0.0038\n",
            "     99    18      0.00755      0.00755     8.19e-07       0.0526       0.0672       0.0443       0.0693       0.0568       0.0553       0.0862       0.0708       0.0508     0.000529\n",
            "     99    19      0.00821      0.00821     2.52e-06        0.053       0.0701       0.0427       0.0736       0.0581        0.055       0.0933       0.0741       0.0957     0.000997\n",
            "     99    20      0.00628      0.00628     2.18e-06       0.0456       0.0613       0.0348       0.0672        0.051       0.0445       0.0856        0.065       0.0953     0.000993\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     99     1      0.00686      0.00685     2.51e-06       0.0478        0.064       0.0367         0.07       0.0534       0.0472       0.0886       0.0679       0.0929     0.000967\n",
            "     99     2      0.00686      0.00686     9.77e-07       0.0475       0.0641       0.0359       0.0706       0.0532       0.0464       0.0895       0.0679       0.0493     0.000514\n",
            "     99     3      0.00654      0.00654     1.36e-06       0.0463       0.0626       0.0349       0.0691        0.052       0.0446       0.0881       0.0664       0.0712     0.000742\n",
            "     99     4      0.00669      0.00669     1.59e-06       0.0466       0.0633       0.0353       0.0693       0.0523       0.0449       0.0893       0.0671       0.0804     0.000837\n",
            "     99     5      0.00619      0.00619     8.69e-07       0.0455       0.0609       0.0352       0.0662       0.0507       0.0453       0.0837       0.0645       0.0463     0.000482\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              99  307.735    0.005      0.00697     1.02e-05      0.00698       0.0486       0.0646       0.0384       0.0689       0.0536       0.0492       0.0876       0.0684         0.19      0.00198\n",
            "! Validation         99  307.735    0.005      0.00663     1.46e-06      0.00663       0.0467        0.063       0.0356        0.069       0.0523       0.0457       0.0879       0.0668        0.068     0.000708\n",
            "Wall time: 307.7361249669999\n",
            "! Best model       99    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    100     1      0.00575      0.00575     2.88e-06       0.0442       0.0587       0.0339       0.0647       0.0493        0.042       0.0824       0.0622       0.0914     0.000952\n",
            "    100     2      0.00585      0.00584     6.19e-06       0.0445       0.0591       0.0345       0.0645       0.0495       0.0444       0.0809       0.0627        0.172      0.00179\n",
            "    100     3      0.00614      0.00613     6.39e-06       0.0449       0.0606        0.034       0.0668       0.0504       0.0442       0.0843       0.0642        0.175      0.00182\n",
            "    100     4      0.00541      0.00539     1.43e-05       0.0431       0.0568       0.0341        0.061       0.0476       0.0427       0.0777       0.0602        0.275      0.00286\n",
            "    100     5      0.00612      0.00611     7.14e-06       0.0445       0.0605       0.0334       0.0666         0.05       0.0422        0.086       0.0641        0.191      0.00199\n",
            "    100     6      0.00636      0.00636      5.2e-07       0.0444       0.0617       0.0333       0.0666         0.05        0.043       0.0879       0.0654       0.0437     0.000456\n",
            "    100     7      0.00645      0.00645     1.26e-06       0.0466       0.0621       0.0369       0.0659       0.0514       0.0476        0.084       0.0658       0.0709     0.000739\n",
            "    100     8      0.00663      0.00662     1.12e-05       0.0483        0.063       0.0403       0.0643       0.0523       0.0512       0.0815       0.0664        0.237      0.00247\n",
            "    100     9      0.00644      0.00644     1.41e-06       0.0475       0.0621       0.0384       0.0657       0.0521       0.0481       0.0832       0.0657       0.0814     0.000848\n",
            "    100    10      0.00577      0.00577     1.39e-06       0.0442       0.0588       0.0333       0.0661       0.0497       0.0423       0.0824       0.0623        0.068     0.000708\n",
            "    100    11      0.00703      0.00702     2.54e-06       0.0488       0.0648       0.0394       0.0676       0.0535       0.0501       0.0872       0.0686        0.102      0.00106\n",
            "    100    12      0.00658      0.00657     1.05e-06        0.047       0.0627       0.0363       0.0685       0.0524       0.0455       0.0876       0.0665       0.0619     0.000645\n",
            "    100    13      0.00745      0.00743     2.46e-05       0.0494       0.0667       0.0372       0.0738       0.0555       0.0477       0.0938       0.0707        0.347      0.00362\n",
            "    100    14      0.00614      0.00614     7.12e-07       0.0455       0.0606       0.0343       0.0679       0.0511       0.0434       0.0852       0.0643       0.0541     0.000564\n",
            "    100    15      0.00553      0.00552     1.47e-05       0.0437       0.0575       0.0341       0.0628       0.0484       0.0435       0.0783       0.0609        0.272      0.00284\n",
            "    100    16      0.00693      0.00693      1.7e-06       0.0483       0.0644       0.0383       0.0682       0.0533       0.0498       0.0865       0.0682       0.0908     0.000946\n",
            "    100    17      0.00652      0.00651     9.19e-06       0.0486       0.0624       0.0388        0.068       0.0534       0.0485       0.0836        0.066        0.217      0.00226\n",
            "    100    18      0.00654      0.00654     2.09e-06       0.0468       0.0626       0.0351       0.0703       0.0527        0.045       0.0877       0.0663       0.0707     0.000736\n",
            "    100    19      0.00617      0.00616     5.97e-06       0.0449       0.0607        0.035       0.0647       0.0499       0.0446       0.0842       0.0644        0.177      0.00185\n",
            "    100    20      0.00662      0.00661        4e-06       0.0464       0.0629       0.0338       0.0716       0.0527       0.0437       0.0898       0.0667        0.136      0.00141\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    100     1      0.00683      0.00683     2.44e-06       0.0477       0.0639       0.0366       0.0699       0.0533        0.047       0.0885       0.0678       0.0921     0.000959\n",
            "    100     2      0.00683      0.00683     1.02e-06       0.0474       0.0639       0.0358       0.0705       0.0532       0.0462       0.0894       0.0678       0.0492     0.000513\n",
            "    100     3      0.00652      0.00652     1.36e-06       0.0462       0.0625       0.0348        0.069       0.0519       0.0445        0.088       0.0663       0.0699     0.000728\n",
            "    100     4      0.00666      0.00666     1.62e-06       0.0465       0.0631       0.0352       0.0692       0.0522       0.0448       0.0892        0.067        0.081     0.000843\n",
            "    100     5      0.00617      0.00617     8.48e-07       0.0454       0.0608       0.0351       0.0661       0.0506       0.0452       0.0836       0.0644       0.0457     0.000476\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             100  310.793    0.005      0.00631     5.96e-06      0.00632       0.0461       0.0615       0.0357       0.0668       0.0512       0.0456       0.0848       0.0652        0.147      0.00153\n",
            "! Validation        100  310.793    0.005       0.0066     1.46e-06       0.0066       0.0466       0.0629       0.0355       0.0689       0.0522       0.0456       0.0878       0.0667       0.0676     0.000704\n",
            "Wall time: 310.7934055239999\n",
            "! Best model      100    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    101     1      0.00603      0.00603      4.2e-07       0.0451       0.0601       0.0343       0.0666       0.0505       0.0431       0.0844       0.0637       0.0359     0.000374\n",
            "    101     2      0.00635      0.00634      7.8e-06       0.0456       0.0616       0.0332       0.0703       0.0517       0.0424       0.0882       0.0653        0.192        0.002\n",
            "    101     3      0.00515      0.00514     2.86e-06       0.0422       0.0555       0.0319       0.0627       0.0473         0.04       0.0777       0.0588       0.0754     0.000785\n",
            "    101     4      0.00612      0.00611     1.03e-05       0.0457       0.0605       0.0345       0.0683       0.0514       0.0436       0.0846       0.0641        0.233      0.00243\n",
            "    101     5      0.00725      0.00724     1.02e-06       0.0505       0.0658       0.0427       0.0661       0.0544       0.0537       0.0851       0.0694       0.0686     0.000714\n",
            "    101     6      0.00793      0.00793     2.25e-06       0.0531       0.0689       0.0451       0.0689        0.057       0.0568       0.0882       0.0725        0.101      0.00105\n",
            "    101     7      0.00667      0.00665     2.56e-05       0.0476       0.0631       0.0367       0.0694       0.0531        0.046       0.0878       0.0669        0.371      0.00387\n",
            "    101     8       0.0059       0.0059     5.87e-07       0.0436       0.0594       0.0334       0.0641       0.0487        0.043        0.083        0.063       0.0549     0.000572\n",
            "    101     9      0.00676      0.00673     3.27e-05       0.0483       0.0634        0.038        0.069       0.0535        0.048       0.0864       0.0672        0.421      0.00439\n",
            "    101    10      0.00729      0.00728     4.99e-06       0.0501        0.066       0.0413       0.0677       0.0545       0.0529       0.0864       0.0697        0.162      0.00168\n",
            "    101    11      0.00568      0.00568     1.49e-06       0.0441       0.0583        0.034       0.0644       0.0492       0.0431       0.0805       0.0618       0.0812     0.000846\n",
            "    101    12      0.00626      0.00625     1.24e-06       0.0457       0.0612       0.0342       0.0687       0.0514        0.043       0.0868       0.0649       0.0699     0.000728\n",
            "    101    13      0.00579      0.00577     1.05e-05       0.0442       0.0588       0.0344       0.0638       0.0491       0.0443       0.0803       0.0623        0.237      0.00247\n",
            "    101    14      0.00732      0.00731     5.89e-07       0.0492       0.0662       0.0377       0.0722       0.0549        0.049       0.0913       0.0701        0.042     0.000437\n",
            "    101    15      0.00592      0.00591     1.37e-05       0.0453       0.0595       0.0342       0.0676       0.0509       0.0426       0.0835       0.0631        0.265      0.00276\n",
            "    101    16      0.00608      0.00608     9.22e-07       0.0452       0.0603        0.035       0.0654       0.0502       0.0452       0.0826       0.0639       0.0652      0.00068\n",
            "    101    17      0.00592      0.00592     4.02e-06       0.0439       0.0595       0.0328       0.0663       0.0495       0.0419       0.0844       0.0631        0.133      0.00139\n",
            "    101    18      0.00645      0.00645     1.13e-06       0.0462       0.0621       0.0343       0.0702       0.0522       0.0441       0.0877       0.0659       0.0711     0.000741\n",
            "    101    19      0.00609      0.00609      3.2e-06       0.0448       0.0604       0.0356       0.0631       0.0494       0.0456       0.0823        0.064       0.0943     0.000983\n",
            "    101    20      0.00717      0.00715     1.79e-05        0.049       0.0654        0.039        0.069        0.054       0.0496        0.089       0.0693        0.298       0.0031\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    101     1       0.0068       0.0068     2.35e-06       0.0476       0.0638       0.0365       0.0698       0.0532       0.0469       0.0884       0.0677       0.0903     0.000941\n",
            "    101     2      0.00681      0.00681     9.86e-07       0.0473       0.0638       0.0358       0.0704       0.0531       0.0461       0.0893       0.0677       0.0476     0.000495\n",
            "    101     3       0.0065       0.0065     1.32e-06       0.0461       0.0624       0.0347       0.0689       0.0518       0.0444       0.0879       0.0662       0.0705     0.000734\n",
            "    101     4      0.00664      0.00664     1.63e-06       0.0464        0.063       0.0351       0.0691       0.0521       0.0446       0.0891       0.0669       0.0815     0.000849\n",
            "    101     5      0.00615      0.00615     8.76e-07       0.0454       0.0607        0.035        0.066       0.0505        0.045       0.0836       0.0643       0.0453     0.000472\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             101  313.836    0.005       0.0064     7.16e-06      0.00641       0.0465       0.0619       0.0361       0.0672       0.0516       0.0461       0.0851       0.0656        0.154       0.0016\n",
            "! Validation        101  313.836    0.005      0.00658     1.43e-06      0.00658       0.0466       0.0628       0.0354       0.0689       0.0521       0.0454       0.0877       0.0666       0.0671     0.000698\n",
            "Wall time: 313.83694623300005\n",
            "! Best model      101    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    102     1      0.00582      0.00581     2.37e-06       0.0443        0.059       0.0331       0.0669         0.05       0.0423       0.0828       0.0626          0.1      0.00105\n",
            "    102     2      0.00694      0.00693     1.03e-05        0.049       0.0644       0.0395       0.0678       0.0537       0.0508       0.0853        0.068        0.238      0.00248\n",
            "    102     3       0.0076       0.0076     1.96e-06       0.0513       0.0675       0.0412       0.0714       0.0563        0.052       0.0908       0.0714       0.0939     0.000979\n",
            "    102     4        0.008        0.008     1.88e-06       0.0524       0.0692       0.0412       0.0748        0.058       0.0526        0.094       0.0733       0.0738     0.000769\n",
            "    102     5      0.00773      0.00772     8.17e-06       0.0521        0.068       0.0415       0.0735       0.0575       0.0518       0.0922        0.072        0.202      0.00211\n",
            "    102     6      0.00646      0.00645     1.08e-05       0.0472       0.0621       0.0393       0.0629       0.0511       0.0497       0.0815       0.0656        0.228      0.00238\n",
            "    102     7      0.00671      0.00671     4.73e-06       0.0483       0.0634       0.0386       0.0678       0.0532       0.0491        0.085       0.0671        0.154       0.0016\n",
            "    102     8      0.00587      0.00584      2.2e-05       0.0445       0.0591       0.0349       0.0639       0.0494       0.0438       0.0816       0.0627        0.342      0.00356\n",
            "    102     9      0.00834      0.00833     1.78e-06       0.0545       0.0706       0.0452       0.0732       0.0592       0.0568       0.0923       0.0745       0.0781     0.000814\n",
            "    102    10      0.00848      0.00847     8.41e-06       0.0545       0.0712       0.0427       0.0781       0.0604       0.0538       0.0971       0.0754        0.208      0.00217\n",
            "    102    11      0.00785      0.00783     2.59e-05       0.0524       0.0685        0.042       0.0731       0.0575       0.0535       0.0913       0.0724        0.369      0.00384\n",
            "    102    12      0.00638      0.00634     4.19e-05        0.047       0.0616       0.0387       0.0635       0.0511       0.0485       0.0817       0.0651        0.475      0.00495\n",
            "    102    13      0.00572      0.00569     2.52e-05       0.0443       0.0584       0.0354       0.0621       0.0488       0.0449       0.0787       0.0618        0.369      0.00384\n",
            "    102    14      0.00665      0.00659     5.79e-05       0.0475       0.0628       0.0364       0.0696        0.053       0.0465       0.0866       0.0666        0.557       0.0058\n",
            "    102    15      0.00742      0.00737     4.66e-05       0.0513       0.0664       0.0422       0.0694       0.0558       0.0524        0.088       0.0702        0.502      0.00523\n",
            "    102    16       0.0079       0.0079     8.98e-07       0.0518       0.0688       0.0406       0.0742       0.0574       0.0513       0.0945       0.0729       0.0568     0.000592\n",
            "    102    17      0.00654      0.00636     0.000183       0.0467       0.0617       0.0378       0.0645       0.0512       0.0476        0.083       0.0653            1       0.0105\n",
            "    102    18      0.00594      0.00592     1.38e-05       0.0436       0.0595       0.0331       0.0647       0.0489       0.0422       0.0841       0.0632        0.266      0.00277\n",
            "    102    19      0.00756      0.00745     0.000108         0.05       0.0668       0.0383       0.0733       0.0558       0.0486       0.0931       0.0708        0.772      0.00804\n",
            "    102    20       0.0092       0.0092     2.23e-06       0.0575       0.0742       0.0498        0.073       0.0614       0.0633       0.0922       0.0777       0.0979      0.00102\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    102     1      0.00678      0.00678     2.31e-06       0.0475       0.0637       0.0364       0.0697       0.0531       0.0468       0.0883       0.0675       0.0886     0.000923\n",
            "    102     2      0.00678      0.00678     1.04e-06       0.0472       0.0637       0.0356       0.0703        0.053        0.046       0.0891       0.0675       0.0501     0.000522\n",
            "    102     3      0.00648      0.00648      1.3e-06        0.046       0.0623       0.0346       0.0688       0.0517       0.0442       0.0878        0.066       0.0696     0.000725\n",
            "    102     4      0.00662      0.00662     1.59e-06       0.0463       0.0629        0.035        0.069        0.052       0.0445        0.089       0.0668       0.0802     0.000835\n",
            "    102     5      0.00612      0.00612     8.81e-07       0.0452       0.0605       0.0349        0.066       0.0504       0.0449       0.0835       0.0642        0.046     0.000479\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             102  316.897    0.005      0.00713     2.89e-05      0.00716       0.0495       0.0653       0.0396       0.0694       0.0545       0.0503       0.0879       0.0691        0.309      0.00322\n",
            "! Validation        102  316.897    0.005      0.00655     1.42e-06      0.00656       0.0465       0.0626       0.0353       0.0688        0.052       0.0453       0.0876       0.0664       0.0669     0.000697\n",
            "Wall time: 316.8975061949999\n",
            "! Best model      102    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    103     1      0.00817      0.00816        5e-06       0.0546       0.0699        0.047       0.0699       0.0584       0.0583       0.0886       0.0735         0.14      0.00146\n",
            "    103     2      0.00656      0.00655     3.95e-06       0.0468       0.0626       0.0348       0.0709       0.0528       0.0441       0.0888       0.0664        0.139      0.00144\n",
            "    103     3      0.00691      0.00689     2.52e-05       0.0481       0.0642       0.0367       0.0707       0.0537       0.0468       0.0894       0.0681        0.367      0.00382\n",
            "    103     4      0.00642      0.00641     4.73e-06       0.0465       0.0619        0.036       0.0674       0.0517       0.0457       0.0856       0.0657        0.136      0.00142\n",
            "    103     5      0.00543      0.00536     7.02e-05       0.0435       0.0567       0.0347       0.0612       0.0479       0.0434       0.0766         0.06        0.615       0.0064\n",
            "    103     6      0.00593      0.00592     5.65e-06       0.0443       0.0595       0.0335       0.0659       0.0497       0.0427       0.0835       0.0631        0.162      0.00169\n",
            "    103     7      0.00752      0.00747     5.44e-05       0.0504       0.0669        0.038       0.0753       0.0566       0.0479        0.094       0.0709        0.546      0.00568\n",
            "    103     8      0.00774      0.00774     3.11e-07       0.0518       0.0681       0.0413        0.073       0.0571       0.0522        0.092       0.0721       0.0334     0.000348\n",
            "    103     9      0.00813      0.00812     1.83e-05       0.0551       0.0697       0.0506        0.064       0.0573       0.0622       0.0826       0.0724        0.317       0.0033\n",
            "    103    10      0.00692      0.00691     8.92e-06       0.0482       0.0643        0.039       0.0667       0.0528       0.0501       0.0859        0.068         0.22      0.00229\n",
            "    103    11       0.0068      0.00679     1.06e-05       0.0484       0.0637       0.0364       0.0723       0.0543       0.0456       0.0896       0.0676        0.232      0.00242\n",
            "    103    12      0.00605      0.00605     2.93e-06       0.0457       0.0602       0.0369       0.0634       0.0502       0.0475       0.0797       0.0636        0.111      0.00115\n",
            "    103    13      0.00811      0.00808     2.17e-05        0.052       0.0696       0.0389       0.0782       0.0586       0.0488       0.0987       0.0738        0.338      0.00352\n",
            "    103    14       0.0067       0.0067     7.41e-06       0.0468       0.0633       0.0363       0.0679       0.0521       0.0469       0.0874       0.0671        0.185      0.00193\n",
            "    103    15      0.00627      0.00626      7.3e-06       0.0471       0.0612       0.0381       0.0651       0.0516       0.0471       0.0825       0.0648        0.188      0.00195\n",
            "    103    16      0.00687      0.00686     3.93e-06       0.0486       0.0641       0.0376       0.0706       0.0541       0.0468       0.0891        0.068         0.14      0.00146\n",
            "    103    17      0.00859      0.00859     5.05e-07       0.0574       0.0717        0.052       0.0682       0.0601       0.0636       0.0858       0.0747       0.0443     0.000462\n",
            "    103    18      0.00919      0.00918     4.78e-06       0.0587       0.0741        0.053       0.0701       0.0615       0.0654        0.089       0.0772        0.147      0.00154\n",
            "    103    19      0.00955      0.00953     1.15e-05       0.0573       0.0755       0.0443       0.0835       0.0639        0.056        0.104       0.0801        0.241      0.00251\n",
            "    103    20      0.00731       0.0073     1.11e-05       0.0502       0.0661        0.039       0.0726       0.0558       0.0483       0.0919       0.0701        0.244      0.00255\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    103     1      0.00676      0.00675     2.34e-06       0.0474       0.0636       0.0363       0.0696        0.053       0.0466       0.0882       0.0674       0.0898     0.000936\n",
            "    103     2      0.00676      0.00675     9.98e-07       0.0471       0.0636       0.0356       0.0702       0.0529       0.0459        0.089       0.0674       0.0479     0.000499\n",
            "    103     3      0.00646      0.00646      1.3e-06       0.0459       0.0622       0.0345       0.0687       0.0516       0.0441       0.0877       0.0659       0.0692     0.000721\n",
            "    103     4       0.0066       0.0066     1.57e-06       0.0462       0.0628       0.0349        0.069       0.0519       0.0444       0.0889       0.0666       0.0806     0.000839\n",
            "    103     5      0.00611      0.00611     8.17e-07       0.0452       0.0605       0.0348       0.0659       0.0504       0.0448       0.0834       0.0641       0.0437     0.000456\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             103  319.954    0.005      0.00724     1.39e-05      0.00726       0.0501       0.0658       0.0402       0.0698        0.055       0.0509       0.0885       0.0697        0.227      0.00237\n",
            "! Validation        103  319.954    0.005      0.00653     1.41e-06      0.00654       0.0464       0.0625       0.0352       0.0687        0.052       0.0452       0.0875       0.0663       0.0663      0.00069\n",
            "Wall time: 319.95479630299985\n",
            "! Best model      103    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    104     1      0.00525      0.00525     3.12e-06       0.0423        0.056       0.0315        0.064       0.0478       0.0398       0.0791       0.0594        0.121      0.00126\n",
            "    104     2      0.00726      0.00724     1.96e-05       0.0485       0.0658       0.0357       0.0742       0.0549       0.0459       0.0937       0.0698        0.324      0.00338\n",
            "    104     3      0.00773      0.00772     1.04e-05       0.0537        0.068       0.0475       0.0662       0.0569       0.0593       0.0826        0.071        0.223      0.00232\n",
            "    104     4       0.0092       0.0092     2.11e-06       0.0574       0.0742       0.0498       0.0727       0.0612       0.0632       0.0923       0.0778       0.0742     0.000773\n",
            "    104     5      0.00782       0.0078     1.64e-05       0.0517       0.0683       0.0417       0.0715       0.0566       0.0528       0.0918       0.0723        0.286      0.00298\n",
            "    104     6      0.00558      0.00556     1.83e-05       0.0429       0.0577       0.0323        0.064       0.0482       0.0408       0.0816       0.0612        0.313      0.00326\n",
            "    104     7      0.00563      0.00562     7.57e-06       0.0433        0.058       0.0336       0.0626       0.0481       0.0423       0.0807       0.0615        0.203      0.00211\n",
            "    104     8      0.00679      0.00677     1.89e-05       0.0479       0.0636       0.0384        0.067       0.0527       0.0481       0.0867       0.0674        0.321      0.00335\n",
            "    104     9      0.00732      0.00732      2.8e-07       0.0507       0.0662        0.042       0.0681        0.055       0.0529       0.0868       0.0699       0.0352     0.000366\n",
            "    104    10      0.00636      0.00636     4.48e-06       0.0475       0.0617       0.0377       0.0669       0.0523       0.0469       0.0838       0.0653        0.145      0.00151\n",
            "    104    11      0.00675      0.00674     6.41e-06       0.0486       0.0635        0.038       0.0697       0.0539       0.0488       0.0857       0.0673        0.166      0.00173\n",
            "    104    12      0.00731       0.0073     3.58e-06       0.0513       0.0661       0.0442       0.0655       0.0549       0.0556       0.0833       0.0694        0.138      0.00143\n",
            "    104    13      0.00743      0.00742     1.78e-06       0.0501       0.0667       0.0393       0.0718       0.0555         0.05       0.0913       0.0706       0.0764     0.000795\n",
            "    104    14      0.00666      0.00663     3.49e-05       0.0469        0.063       0.0352       0.0701       0.0527       0.0447       0.0889       0.0668        0.434      0.00452\n",
            "    104    15      0.00603      0.00603      3.7e-06       0.0441       0.0601       0.0347        0.063       0.0489       0.0451       0.0822       0.0637        0.108      0.00113\n",
            "    104    16      0.00607      0.00605     2.28e-05       0.0455       0.0602       0.0362       0.0641       0.0501        0.046       0.0815       0.0637        0.343      0.00357\n",
            "    104    17      0.00773      0.00773     1.17e-06       0.0526        0.068       0.0435       0.0708       0.0572       0.0548       0.0887       0.0718        0.066     0.000688\n",
            "    104    18      0.00724      0.00724     2.98e-07       0.0513       0.0658       0.0429        0.068       0.0555       0.0534       0.0854       0.0694       0.0352     0.000366\n",
            "    104    19      0.00716      0.00716     1.29e-06        0.048       0.0654        0.035       0.0742       0.0546       0.0445       0.0943       0.0694       0.0676     0.000704\n",
            "    104    20      0.00778      0.00778     6.99e-07       0.0528       0.0682       0.0459       0.0667       0.0563       0.0574       0.0858       0.0716       0.0584     0.000608\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    104     1      0.00674      0.00673      2.3e-06       0.0473       0.0635       0.0362       0.0696       0.0529       0.0465       0.0881       0.0673       0.0907     0.000945\n",
            "    104     2      0.00674      0.00674     9.79e-07        0.047       0.0635       0.0355       0.0702       0.0528       0.0458       0.0889       0.0673       0.0479     0.000499\n",
            "    104     3      0.00644      0.00644     1.34e-06       0.0459       0.0621       0.0345       0.0687       0.0516       0.0441       0.0877       0.0659       0.0699     0.000728\n",
            "    104     4      0.00658      0.00658     1.61e-06       0.0462       0.0628       0.0348       0.0689       0.0518       0.0443       0.0888       0.0666       0.0817     0.000851\n",
            "    104     5      0.00609      0.00609     8.46e-07       0.0451       0.0604       0.0347       0.0659       0.0503       0.0446       0.0834        0.064       0.0444     0.000463\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             104  323.000    0.005      0.00695      8.9e-06      0.00695       0.0489       0.0645       0.0393       0.0681       0.0537         0.05       0.0864       0.0682        0.177      0.00184\n",
            "! Validation        104  323.000    0.005      0.00652     1.41e-06      0.00652       0.0463       0.0625       0.0351       0.0686       0.0519       0.0451       0.0874       0.0662        0.067     0.000697\n",
            "Wall time: 323.0008767129999\n",
            "! Best model      104    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    105     1      0.00718      0.00718     1.26e-06         0.05       0.0655       0.0417       0.0667       0.0542       0.0533       0.0849       0.0691       0.0775     0.000808\n",
            "    105     2      0.00697      0.00696     1.35e-05       0.0475       0.0645       0.0344       0.0738       0.0541       0.0445       0.0924       0.0685        0.271      0.00283\n",
            "    105     3      0.00696      0.00696     3.87e-07       0.0485       0.0646       0.0386       0.0683       0.0534       0.0498       0.0868       0.0683       0.0328     0.000342\n",
            "    105     4       0.0063      0.00627     2.89e-05        0.047       0.0613       0.0379       0.0652       0.0516        0.047       0.0827       0.0649        0.396      0.00412\n",
            "    105     5      0.00617      0.00617     1.49e-06       0.0458       0.0608       0.0359       0.0654       0.0507       0.0464       0.0823       0.0644       0.0768       0.0008\n",
            "    105     6      0.00607      0.00606     1.54e-05       0.0449       0.0602       0.0344       0.0657       0.0501       0.0431       0.0846       0.0639        0.285      0.00297\n",
            "    105     7      0.00634      0.00634     6.89e-07       0.0461       0.0616        0.035       0.0685       0.0517       0.0444       0.0863       0.0653       0.0516     0.000537\n",
            "    105     8        0.007        0.007     4.17e-06       0.0496       0.0647       0.0413       0.0661       0.0537       0.0521       0.0845       0.0683        0.133      0.00138\n",
            "    105     9      0.00608      0.00607     2.65e-06       0.0443       0.0603        0.032       0.0689       0.0505        0.041       0.0869       0.0639        0.117      0.00121\n",
            "    105    10      0.00681       0.0068      8.2e-06       0.0502       0.0638       0.0422       0.0663       0.0542       0.0526       0.0817       0.0672        0.206      0.00215\n",
            "    105    11      0.00832       0.0083     1.89e-05       0.0547       0.0705       0.0458       0.0726       0.0592       0.0577       0.0908       0.0742        0.313      0.00326\n",
            "    105    12      0.00849      0.00847     2.34e-05       0.0545       0.0712       0.0435       0.0765         0.06       0.0537       0.0971       0.0754        0.356       0.0037\n",
            "    105    13      0.00644      0.00639     5.03e-05       0.0458       0.0619       0.0357       0.0658       0.0508       0.0457       0.0855       0.0656        0.521      0.00543\n",
            "    105    14      0.00655      0.00649     5.97e-05       0.0461       0.0623       0.0343       0.0697        0.052       0.0435       0.0887       0.0661        0.569      0.00593\n",
            "    105    15      0.00771      0.00766     5.04e-05        0.051       0.0677       0.0419       0.0692       0.0556       0.0539       0.0891       0.0715        0.522      0.00544\n",
            "    105    16      0.00828      0.00828     3.77e-07       0.0553       0.0704       0.0499        0.066       0.0579        0.062       0.0848       0.0734       0.0305     0.000317\n",
            "    105    17      0.00778      0.00777     2.75e-06       0.0512       0.0682       0.0397        0.074       0.0569       0.0503       0.0944       0.0723        0.117      0.00122\n",
            "    105    18      0.00668      0.00668     1.03e-06       0.0476       0.0632       0.0375       0.0677       0.0526       0.0468       0.0872        0.067       0.0666     0.000694\n",
            "    105    19      0.00579      0.00576     2.72e-05       0.0437       0.0587        0.034       0.0631       0.0486       0.0434        0.081       0.0622        0.379      0.00395\n",
            "    105    20      0.00799      0.00797     2.73e-05       0.0522        0.069       0.0401       0.0764       0.0582       0.0508       0.0957       0.0732        0.374      0.00389\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    105     1      0.00672      0.00672     2.27e-06       0.0473       0.0634       0.0362       0.0695       0.0528       0.0464        0.088       0.0672       0.0888     0.000925\n",
            "    105     2      0.00672      0.00672     1.02e-06        0.047       0.0634       0.0354       0.0701       0.0528       0.0457       0.0889       0.0673       0.0479     0.000499\n",
            "    105     3      0.00643      0.00643     1.39e-06       0.0458        0.062       0.0344       0.0687       0.0516        0.044       0.0876       0.0658       0.0715     0.000745\n",
            "    105     4      0.00657      0.00657      1.6e-06       0.0461       0.0627       0.0347       0.0689       0.0518       0.0442       0.0888       0.0665       0.0804     0.000837\n",
            "    105     5      0.00607      0.00607     8.28e-07        0.045       0.0603       0.0346       0.0658       0.0502       0.0445       0.0833       0.0639        0.045     0.000469\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             105  326.054    0.005      0.00698     1.69e-05        0.007       0.0488       0.0646       0.0388       0.0688       0.0538       0.0494       0.0875       0.0684        0.245      0.00255\n",
            "! Validation        105  326.054    0.005       0.0065     1.42e-06       0.0065       0.0462       0.0624       0.0351       0.0686       0.0518        0.045       0.0873       0.0662       0.0667     0.000695\n",
            "Wall time: 326.0548104599999\n",
            "! Best model      105    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    106     1      0.00682      0.00668     0.000136       0.0479       0.0632        0.038       0.0677       0.0529       0.0481       0.0858        0.067        0.862      0.00898\n",
            "    106     2       0.0073      0.00725     5.09e-05       0.0513       0.0659       0.0441       0.0656       0.0549       0.0549       0.0835       0.0692        0.528       0.0055\n",
            "    106     3      0.00627      0.00624     2.75e-05       0.0462       0.0611       0.0353       0.0678       0.0516       0.0457       0.0838       0.0648        0.377      0.00393\n",
            "    106     4       0.0065      0.00635     0.000154       0.0463       0.0616       0.0373       0.0642       0.0507       0.0472       0.0834       0.0653        0.921      0.00959\n",
            "    106     5      0.00607      0.00607     5.13e-07       0.0445       0.0603       0.0337       0.0661       0.0499       0.0433       0.0845       0.0639        0.042     0.000437\n",
            "    106     6      0.00716      0.00704     0.000123       0.0481       0.0649       0.0364       0.0717        0.054       0.0473       0.0904       0.0688         0.82      0.00854\n",
            "    106     7      0.00714      0.00714     3.32e-06       0.0494       0.0654       0.0398       0.0688       0.0543         0.05       0.0884       0.0692        0.113      0.00118\n",
            "    106     8      0.00639      0.00638     1.23e-05       0.0473       0.0618       0.0384       0.0651       0.0517       0.0482       0.0825       0.0654        0.251      0.00261\n",
            "    106     9      0.00552      0.00552     5.89e-07        0.043       0.0575       0.0325       0.0639       0.0482       0.0405       0.0814       0.0609       0.0492     0.000513\n",
            "    106    10      0.00648      0.00647     5.71e-06       0.0472       0.0622       0.0384       0.0647       0.0516       0.0484       0.0834       0.0659        0.156      0.00163\n",
            "    106    11      0.00502      0.00502     3.88e-07       0.0409       0.0548       0.0309        0.061        0.046       0.0394       0.0768       0.0581        0.034     0.000354\n",
            "    106    12      0.00711      0.00708     3.16e-05       0.0499       0.0651       0.0403       0.0691       0.0547       0.0509       0.0868       0.0689        0.417      0.00434\n",
            "    106    13      0.00655      0.00655     2.21e-06       0.0474       0.0626       0.0396        0.063       0.0513       0.0499       0.0823       0.0661        0.109      0.00113\n",
            "    106    14      0.00771      0.00764     7.41e-05       0.0511       0.0676       0.0399       0.0735       0.0567       0.0501       0.0933       0.0717        0.637      0.00664\n",
            "    106    15      0.00642      0.00642     3.57e-06       0.0461        0.062       0.0342       0.0699       0.0521       0.0433       0.0882       0.0657         0.13      0.00136\n",
            "    106    16      0.00655      0.00653     2.71e-05       0.0469       0.0625       0.0368       0.0673        0.052        0.047       0.0855       0.0662        0.382      0.00398\n",
            "    106    17      0.00648      0.00648     6.78e-06       0.0452       0.0623       0.0327       0.0702       0.0514       0.0418       0.0902        0.066        0.191      0.00199\n",
            "    106    18      0.00639      0.00638     9.66e-06       0.0459       0.0618       0.0356       0.0665        0.051       0.0463       0.0847       0.0655          0.2      0.00208\n",
            "    106    19      0.00584      0.00584     7.87e-07       0.0448       0.0591       0.0351       0.0642       0.0496       0.0443        0.081       0.0626       0.0596     0.000621\n",
            "    106    20      0.00664      0.00664     7.77e-07       0.0474       0.0631       0.0361         0.07        0.053       0.0456       0.0882       0.0669       0.0502     0.000523\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    106     1       0.0067       0.0067      2.3e-06       0.0472       0.0633       0.0361       0.0694       0.0528       0.0464       0.0879       0.0671       0.0894     0.000931\n",
            "    106     2       0.0067       0.0067     9.69e-07       0.0469       0.0633       0.0354       0.0701       0.0527       0.0456       0.0888       0.0672       0.0469     0.000488\n",
            "    106     3      0.00642      0.00642     1.37e-06       0.0458        0.062       0.0344       0.0687       0.0515       0.0439       0.0876       0.0657       0.0719     0.000749\n",
            "    106     4      0.00655      0.00655     1.58e-06       0.0461       0.0626       0.0347       0.0688       0.0518       0.0441       0.0887       0.0664       0.0802     0.000835\n",
            "    106     5      0.00606      0.00606     8.02e-07       0.0449       0.0602       0.0345       0.0657       0.0501       0.0444       0.0832       0.0638       0.0446     0.000465\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             106  329.120    0.005      0.00649     3.36e-05      0.00652       0.0468       0.0623       0.0367        0.067       0.0519       0.0467       0.0853        0.066        0.316       0.0033\n",
            "! Validation        106  329.120    0.005      0.00649      1.4e-06      0.00649       0.0462       0.0623        0.035       0.0686       0.0518       0.0449       0.0873       0.0661       0.0666     0.000694\n",
            "Wall time: 329.120874867\n",
            "! Best model      106    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    107     1      0.00575      0.00574     4.68e-06       0.0441       0.0586       0.0346       0.0633       0.0489       0.0438       0.0805       0.0621         0.15      0.00156\n",
            "    107     2       0.0058       0.0058     4.38e-06        0.045       0.0589       0.0358       0.0634       0.0496       0.0452       0.0795       0.0624         0.14      0.00146\n",
            "    107     3      0.00648      0.00647     1.37e-05       0.0458       0.0622       0.0354       0.0668       0.0511       0.0461       0.0858       0.0659        0.268      0.00279\n",
            "    107     4      0.00623      0.00622     3.67e-06       0.0451        0.061       0.0346       0.0662       0.0504       0.0443       0.0852       0.0647        0.126      0.00131\n",
            "    107     5      0.00598      0.00597      9.7e-06       0.0433       0.0598       0.0323       0.0652       0.0488       0.0417        0.085       0.0634        0.223      0.00232\n",
            "    107     6      0.00616      0.00615     1.66e-05       0.0453       0.0607       0.0347       0.0666       0.0507       0.0442       0.0844       0.0643        0.296      0.00308\n",
            "    107     7      0.00618      0.00617     3.24e-06        0.046       0.0608       0.0351       0.0677       0.0514       0.0441       0.0848       0.0645        0.127      0.00132\n",
            "    107     8      0.00695      0.00689     5.58e-05       0.0483       0.0642       0.0378       0.0692       0.0535       0.0486       0.0875        0.068        0.552      0.00575\n",
            "    107     9      0.00591      0.00591     3.59e-06       0.0445       0.0595       0.0333        0.067       0.0501       0.0425       0.0836       0.0631        0.107      0.00112\n",
            "    107    10      0.00608      0.00606     2.82e-05       0.0451       0.0602       0.0335       0.0683       0.0509       0.0426       0.0851       0.0639        0.386      0.00402\n",
            "    107    11      0.00608      0.00605     3.12e-05       0.0455       0.0602       0.0353       0.0659       0.0506       0.0454        0.082       0.0637        0.411      0.00428\n",
            "    107    12      0.00637      0.00636     2.45e-06       0.0459       0.0617       0.0347       0.0683       0.0515       0.0444       0.0865       0.0655        0.105       0.0011\n",
            "    107    13      0.00611      0.00607     4.09e-05       0.0447       0.0603       0.0336       0.0669       0.0503       0.0423       0.0856       0.0639        0.468      0.00488\n",
            "    107    14      0.00578      0.00574     4.24e-05       0.0442       0.0586       0.0334       0.0657       0.0495       0.0428       0.0815       0.0622        0.482      0.00502\n",
            "    107    15      0.00585      0.00583     1.29e-05       0.0446       0.0591       0.0355       0.0627       0.0491       0.0449       0.0803       0.0626        0.263      0.00274\n",
            "    107    16      0.00618      0.00617     5.94e-06       0.0451       0.0608       0.0342        0.067       0.0506       0.0443       0.0845       0.0644        0.173       0.0018\n",
            "    107    17      0.00589      0.00587     1.59e-05       0.0441       0.0593       0.0345       0.0635        0.049       0.0436       0.0821       0.0628        0.296      0.00308\n",
            "    107    18      0.00591      0.00591     2.96e-06       0.0438       0.0595       0.0332        0.065       0.0491       0.0426       0.0836       0.0631        0.104      0.00108\n",
            "    107    19      0.00694      0.00691     2.71e-05       0.0485       0.0643       0.0385       0.0686       0.0535        0.049       0.0872       0.0681        0.385      0.00401\n",
            "    107    20      0.00711      0.00711     6.15e-07       0.0506       0.0652       0.0411       0.0695       0.0553       0.0515       0.0864       0.0689       0.0451      0.00047\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    107     1      0.00668      0.00668     2.29e-06       0.0471       0.0632        0.036       0.0694       0.0527       0.0462       0.0878        0.067       0.0891     0.000928\n",
            "    107     2      0.00668      0.00668     9.94e-07       0.0468       0.0632       0.0353         0.07       0.0526       0.0454       0.0887       0.0671        0.047     0.000489\n",
            "    107     3       0.0064       0.0064     1.34e-06       0.0457       0.0619       0.0343       0.0686       0.0514       0.0438       0.0875       0.0656       0.0697     0.000726\n",
            "    107     4      0.00653      0.00653     1.61e-06        0.046       0.0625       0.0346       0.0687       0.0517        0.044       0.0886       0.0663       0.0808     0.000841\n",
            "    107     5      0.00604      0.00604     7.92e-07       0.0449       0.0601       0.0344       0.0657       0.0501       0.0443       0.0832       0.0637        0.044     0.000459\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             107  332.181    0.005      0.00617     1.63e-05      0.00619       0.0455       0.0608       0.0351       0.0663       0.0507       0.0448       0.0841       0.0644        0.255      0.00266\n",
            "! Validation        107  332.181    0.005      0.00647      1.4e-06      0.00647       0.0461       0.0622       0.0349       0.0685       0.0517       0.0448       0.0872        0.066       0.0661     0.000689\n",
            "Wall time: 332.18150672900003\n",
            "! Best model      107    0.006\n",
            "! Stop training: Early stopping: validation_loss has not reduced for 50 epochs\n",
            "Wall time: 332.2088144579999\n",
            "Cumulative wall time: 332.2088144579999\n"
          ]
        }
      ],
      "source": [
        "!rm -rf ./results\n",
        "!nequip-train /content/nequip/configs/my-full-example.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJitSZgLYNNF"
      },
      "source": [
        "### Deploy the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo_kIpYV00as"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/deploy.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VoeGtlA02KQ"
      },
      "source": [
        "We now convert the model to a potential file. This makes it independent of NequIP and we can use it any downstream application, such as LAMMPS. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLlagzVhrVGz",
        "outputId": "ff161c64-7d5a-40a2-a5ae-a9a60c993398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 5.3M\n",
            "-rw------- 1 root root 227K Jul  7 13:26 config.yaml\n",
            "-rw-r--r-- 1 root root  489 Jul  7 13:26 metrics_initialization.csv\n",
            "-rw-r--r-- 1 root root 517K Jul  7 13:31 metrics_batch_train.csv\n",
            "-rw-r--r-- 1 root root 131K Jul  7 13:31 metrics_batch_val.csv\n",
            "-rw-r--r-- 1 root root  45K Jul  7 13:31 metrics_epoch.csv\n",
            "-rw------- 1 root root 657K Jul  7 13:31 best_model.pth\n",
            "-rw-r--r-- 1 root root 593K Jul  7 13:31 log\n",
            "-rw------- 1 root root 2.5M Jul  7 13:31 trainer.pth\n",
            "-rw------- 1 root root 657K Jul  7 13:31 last_model.pth\n"
          ]
        }
      ],
      "source": [
        "! ls -lrth results/water/example-run-water"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJmFAbBzez3P",
        "outputId": "3dd2ed5d-7a8f-4b6a-9a4e-1780ae0db7a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: nequip-deploy [-h] [--verbose VERBOSE] {info,build} ...\n",
            "\n",
            "Create and view information about deployed NequIP potentials.\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help         show this help message and exit\n",
            "  --verbose VERBOSE  log level\n",
            "\n",
            "commands:\n",
            "  {info,build}\n",
            "    info             Get information from a deployed model file\n",
            "    build            Build a deployment model\n"
          ]
        }
      ],
      "source": [
        "! nequip-deploy -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3NJJgtDIDNc",
        "outputId": "9f374d59-b463-47ca-f2e0-33575c6775e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Loading best_model from training session...\n",
            "INFO:root:Compiled & optimized model.\n"
          ]
        }
      ],
      "source": [
        "!nequip-deploy build --train-dir /content/results/water/example-run-water water-deploy.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXpcE3oP0LyD"
      },
      "source": [
        "## Evaluate Test Error on all remaining frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wRKKCZ2PRl3"
      },
      "source": [
        "Before running inference, we'd like to know how well the model is doing on a hold-out test set. We run the nequip-evaluate command to compute the test error on all data that we didn't use for training or validation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB54WSrN0PaS",
        "outputId": "326ede9c-e6c8-41ca-fed7-3738bb0a47ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "WARNING: please note that models running on CUDA are usually nondeterministc and that this manifests in the final test errors; for a _more_ deterministic result, please use `--device cpu`\n",
            "Loading model... \n",
            "loaded model from training session\n",
            "Loading original dataset...\n",
            "Loaded dataset specified in config.yaml.\n",
            "Using origial training dataset (10000 frames) minus training (100 frames) and validation frames (50 frames), yielding a test set size of 9850 frames.\n",
            "Starting...\n",
            "  0% 0/9850 [00:00<?, ?it/s]\n",
            "\u001b[A\n",
            "  1% 50/9850 [00:00<01:14, 131.22it/s]\n",
            "  1% 100/9850 [00:01<02:07, 76.69it/s]\n",
            "  2% 150/9850 [00:02<03:16, 49.37it/s]\n",
            "  2% 200/9850 [00:04<03:46, 42.65it/s]\n",
            "  3% 250/9850 [00:04<02:40, 59.72it/s]\n",
            "  3% 300/9850 [00:04<02:01, 78.64it/s]\n",
            "  4% 350/9850 [00:04<01:36, 98.56it/s]\n",
            "  4% 400/9850 [00:05<01:20, 117.93it/s]\n",
            "  5% 450/9850 [00:05<01:09, 136.14it/s]\n",
            "  5% 500/9850 [00:05<01:01, 151.81it/s]\n",
            "  6% 550/9850 [00:05<00:56, 164.41it/s]\n",
            "  6% 600/9850 [00:06<00:53, 174.46it/s]\n",
            "  7% 650/9850 [00:06<00:50, 182.24it/s]\n",
            "  7% 700/9850 [00:06<00:48, 188.00it/s]\n",
            "  8% 750/9850 [00:06<00:47, 192.16it/s]\n",
            "  8% 800/9850 [00:07<00:46, 195.55it/s]\n",
            "  9% 850/9850 [00:07<00:45, 198.31it/s]\n",
            "  9% 900/9850 [00:07<00:44, 200.57it/s]\n",
            " 10% 950/9850 [00:07<00:43, 202.59it/s]\n",
            " 10% 1000/9850 [00:07<00:43, 203.51it/s]\n",
            " 11% 1050/9850 [00:08<00:43, 204.23it/s]\n",
            " 11% 1100/9850 [00:08<00:42, 204.38it/s]\n",
            " 12% 1150/9850 [00:08<00:42, 204.99it/s]\n",
            " 12% 1200/9850 [00:08<00:41, 206.22it/s]\n",
            " 13% 1250/9850 [00:09<00:41, 207.25it/s]\n",
            " 13% 1300/9850 [00:09<00:41, 207.86it/s]\n",
            " 14% 1350/9850 [00:09<00:40, 208.11it/s]\n",
            " 14% 1400/9850 [00:09<00:40, 207.97it/s]\n",
            " 15% 1450/9850 [00:10<00:40, 207.86it/s]\n",
            " 15% 1500/9850 [00:10<00:40, 208.32it/s]\n",
            " 16% 1550/9850 [00:10<00:39, 208.22it/s]\n",
            " 16% 1600/9850 [00:10<00:39, 208.10it/s]\n",
            " 17% 1650/9850 [00:11<00:39, 208.29it/s]\n",
            " 17% 1700/9850 [00:11<00:39, 207.43it/s]\n",
            " 18% 1750/9850 [00:11<00:39, 206.85it/s]\n",
            " 18% 1800/9850 [00:11<00:38, 206.76it/s]\n",
            " 19% 1850/9850 [00:12<00:38, 206.39it/s]\n",
            " 19% 1900/9850 [00:12<00:38, 206.46it/s]\n",
            " 20% 1950/9850 [00:12<00:38, 206.61it/s]\n",
            " 20% 2000/9850 [00:12<00:38, 206.15it/s]\n",
            " 21% 2050/9850 [00:13<00:37, 206.36it/s]\n",
            " 21% 2100/9850 [00:13<00:37, 206.57it/s]\n",
            " 22% 2150/9850 [00:13<00:37, 206.85it/s]\n",
            " 22% 2200/9850 [00:13<00:37, 206.73it/s]\n",
            " 23% 2250/9850 [00:14<00:36, 207.52it/s]\n",
            " 23% 2300/9850 [00:14<00:36, 207.62it/s]\n",
            " 24% 2350/9850 [00:14<00:36, 207.63it/s]\n",
            " 24% 2400/9850 [00:14<00:35, 208.18it/s]\n",
            " 25% 2450/9850 [00:14<00:35, 208.30it/s]\n",
            " 25% 2500/9850 [00:15<00:35, 208.60it/s]\n",
            " 26% 2550/9850 [00:15<00:35, 208.11it/s]\n",
            " 26% 2600/9850 [00:15<00:35, 207.13it/s]\n",
            " 27% 2650/9850 [00:15<00:34, 207.30it/s]\n",
            " 27% 2700/9850 [00:16<00:34, 207.19it/s]\n",
            " 28% 2750/9850 [00:16<00:34, 207.82it/s]\n",
            " 28% 2800/9850 [00:16<00:33, 208.19it/s]\n",
            " 29% 2850/9850 [00:16<00:33, 207.96it/s]\n",
            " 29% 2900/9850 [00:17<00:33, 207.74it/s]\n",
            " 30% 2950/9850 [00:17<00:33, 206.58it/s]\n",
            " 30% 3000/9850 [00:17<00:33, 207.35it/s]\n",
            " 31% 3050/9850 [00:17<00:32, 208.10it/s]\n",
            " 31% 3100/9850 [00:18<00:32, 208.47it/s]\n",
            " 32% 3150/9850 [00:18<00:31, 209.65it/s]\n",
            " 32% 3200/9850 [00:18<00:31, 209.18it/s]\n",
            " 33% 3250/9850 [00:18<00:31, 209.86it/s]\n",
            " 34% 3300/9850 [00:19<00:31, 209.38it/s]\n",
            " 34% 3350/9850 [00:19<00:31, 207.51it/s]\n",
            " 35% 3400/9850 [00:19<00:31, 208.03it/s]\n",
            " 35% 3450/9850 [00:19<00:30, 208.47it/s]\n",
            " 36% 3500/9850 [00:20<00:30, 208.48it/s]\n",
            " 36% 3550/9850 [00:20<00:30, 208.36it/s]\n",
            " 37% 3600/9850 [00:20<00:30, 208.09it/s]\n",
            " 37% 3650/9850 [00:20<00:29, 207.17it/s]\n",
            " 38% 3700/9850 [00:20<00:29, 206.82it/s]\n",
            " 38% 3750/9850 [00:21<00:29, 206.52it/s]\n",
            " 39% 3800/9850 [00:21<00:29, 206.35it/s]\n",
            " 39% 3850/9850 [00:21<00:29, 206.24it/s]\n",
            " 40% 3900/9850 [00:21<00:28, 206.10it/s]\n",
            " 40% 3950/9850 [00:22<00:28, 206.39it/s]\n",
            " 41% 4000/9850 [00:22<00:28, 206.19it/s]\n",
            " 41% 4050/9850 [00:22<00:28, 205.93it/s]\n",
            " 42% 4100/9850 [00:22<00:28, 205.19it/s]\n",
            " 42% 4150/9850 [00:23<00:27, 205.06it/s]\n",
            " 43% 4200/9850 [00:23<00:27, 204.96it/s]\n",
            " 43% 4250/9850 [00:23<00:27, 204.75it/s]\n",
            " 44% 4300/9850 [00:23<00:27, 205.12it/s]\n",
            " 44% 4350/9850 [00:24<00:26, 205.46it/s]\n",
            " 45% 4400/9850 [00:24<00:26, 205.47it/s]\n",
            " 45% 4450/9850 [00:24<00:26, 205.02it/s]\n",
            " 46% 4500/9850 [00:24<00:26, 205.43it/s]\n",
            " 46% 4550/9850 [00:25<00:25, 205.51it/s]\n",
            " 47% 4600/9850 [00:25<00:25, 205.88it/s]\n",
            " 47% 4650/9850 [00:25<00:25, 205.59it/s]\n",
            " 48% 4700/9850 [00:25<00:25, 205.80it/s]\n",
            " 48% 4750/9850 [00:26<00:24, 205.66it/s]\n",
            " 49% 4800/9850 [00:26<00:24, 204.59it/s]\n",
            " 49% 4850/9850 [00:26<00:24, 204.57it/s]\n",
            " 50% 4900/9850 [00:26<00:24, 204.43it/s]\n",
            " 50% 4950/9850 [00:27<00:23, 204.38it/s]\n",
            " 51% 5000/9850 [00:27<00:23, 204.86it/s]\n",
            " 51% 5050/9850 [00:27<00:23, 204.83it/s]\n",
            " 52% 5100/9850 [00:27<00:23, 204.93it/s]\n",
            " 52% 5150/9850 [00:28<00:22, 204.84it/s]\n",
            " 53% 5200/9850 [00:28<00:22, 205.10it/s]\n",
            " 53% 5250/9850 [00:28<00:22, 204.89it/s]\n",
            " 54% 5300/9850 [00:28<00:22, 204.20it/s]\n",
            " 54% 5350/9850 [00:29<00:22, 204.25it/s]\n",
            " 55% 5400/9850 [00:29<00:21, 202.84it/s]\n",
            " 55% 5450/9850 [00:29<00:21, 202.65it/s]\n",
            " 56% 5500/9850 [00:29<00:21, 201.73it/s]\n",
            " 56% 5550/9850 [00:30<00:21, 201.68it/s]\n",
            " 57% 5600/9850 [00:30<00:21, 201.90it/s]\n",
            " 57% 5650/9850 [00:30<00:20, 202.15it/s]\n",
            " 58% 5700/9850 [00:30<00:20, 201.80it/s]\n",
            " 58% 5750/9850 [00:31<00:20, 201.83it/s]\n",
            " 59% 5800/9850 [00:31<00:20, 201.96it/s]\n",
            " 59% 5850/9850 [00:31<00:19, 201.31it/s]\n",
            " 60% 5900/9850 [00:31<00:19, 202.23it/s]\n",
            " 60% 5950/9850 [00:32<00:19, 202.45it/s]\n",
            " 61% 6000/9850 [00:32<00:19, 201.79it/s]\n",
            " 61% 6050/9850 [00:32<00:18, 201.78it/s]\n",
            " 62% 6100/9850 [00:32<00:18, 201.95it/s]\n",
            " 62% 6150/9850 [00:33<00:18, 201.64it/s]\n",
            " 63% 6200/9850 [00:33<00:18, 202.01it/s]\n",
            " 63% 6250/9850 [00:33<00:17, 201.82it/s]\n",
            " 64% 6300/9850 [00:33<00:17, 202.42it/s]\n",
            " 64% 6350/9850 [00:33<00:17, 202.63it/s]\n",
            " 65% 6400/9850 [00:34<00:16, 203.39it/s]\n",
            " 65% 6450/9850 [00:34<00:16, 203.45it/s]\n",
            " 66% 6500/9850 [00:34<00:16, 203.54it/s]\n",
            " 66% 6550/9850 [00:34<00:16, 203.59it/s]\n",
            " 67% 6600/9850 [00:35<00:16, 203.05it/s]\n",
            " 68% 6650/9850 [00:35<00:15, 202.72it/s]\n",
            " 68% 6700/9850 [00:35<00:15, 202.36it/s]\n",
            " 69% 6750/9850 [00:35<00:15, 202.36it/s]\n",
            " 69% 6800/9850 [00:36<00:15, 201.81it/s]\n",
            " 70% 6850/9850 [00:36<00:14, 202.24it/s]\n",
            " 70% 6900/9850 [00:36<00:14, 202.65it/s]\n",
            " 71% 6950/9850 [00:36<00:14, 203.31it/s]\n",
            " 71% 7000/9850 [00:37<00:13, 203.87it/s]\n",
            " 72% 7050/9850 [00:37<00:13, 204.10it/s]\n",
            " 72% 7100/9850 [00:37<00:13, 203.94it/s]\n",
            " 73% 7150/9850 [00:37<00:13, 203.00it/s]\n",
            " 73% 7200/9850 [00:38<00:13, 202.67it/s]\n",
            " 74% 7250/9850 [00:38<00:12, 202.42it/s]\n",
            " 74% 7300/9850 [00:38<00:12, 202.76it/s]\n",
            " 75% 7350/9850 [00:38<00:12, 202.87it/s]\n",
            " 75% 7400/9850 [00:39<00:12, 203.18it/s]\n",
            " 76% 7450/9850 [00:39<00:11, 203.91it/s]\n",
            " 76% 7500/9850 [00:39<00:11, 203.77it/s]\n",
            " 77% 7550/9850 [00:39<00:11, 202.80it/s]\n",
            " 77% 7600/9850 [00:40<00:11, 203.60it/s]\n",
            " 78% 7650/9850 [00:40<00:10, 204.20it/s]\n",
            " 78% 7700/9850 [00:40<00:10, 203.96it/s]\n",
            " 79% 7750/9850 [00:40<00:10, 204.25it/s]\n",
            " 79% 7800/9850 [00:41<00:10, 204.12it/s]\n",
            " 80% 7850/9850 [00:41<00:09, 204.42it/s]\n",
            " 80% 7900/9850 [00:41<00:09, 204.63it/s]\n",
            " 81% 7950/9850 [00:41<00:09, 204.48it/s]\n",
            " 81% 8000/9850 [00:42<00:09, 202.97it/s]\n",
            " 82% 8050/9850 [00:42<00:08, 202.34it/s]\n",
            " 82% 8100/9850 [00:42<00:08, 203.07it/s]\n",
            " 83% 8150/9850 [00:42<00:08, 202.83it/s]\n",
            " 83% 8200/9850 [00:43<00:08, 203.38it/s]\n",
            " 84% 8250/9850 [00:43<00:07, 204.20it/s]\n",
            " 84% 8300/9850 [00:43<00:07, 204.52it/s]\n",
            " 85% 8350/9850 [00:43<00:07, 204.87it/s]\n",
            " 85% 8400/9850 [00:44<00:07, 203.89it/s]\n",
            " 86% 8450/9850 [00:44<00:06, 203.03it/s]\n",
            " 86% 8500/9850 [00:44<00:06, 202.63it/s]\n",
            " 87% 8550/9850 [00:44<00:06, 203.29it/s]\n",
            " 87% 8600/9850 [00:45<00:06, 202.69it/s]\n",
            " 88% 8650/9850 [00:45<00:05, 202.70it/s]\n",
            " 88% 8700/9850 [00:45<00:05, 203.26it/s]\n",
            " 89% 8750/9850 [00:45<00:05, 203.76it/s]\n",
            " 89% 8800/9850 [00:46<00:05, 203.74it/s]\n",
            " 90% 8850/9850 [00:46<00:04, 203.66it/s]\n",
            " 90% 8900/9850 [00:46<00:04, 203.76it/s]\n",
            " 91% 8950/9850 [00:46<00:04, 203.34it/s]\n",
            " 91% 9000/9850 [00:47<00:04, 203.50it/s]\n",
            " 92% 9050/9850 [00:47<00:03, 203.35it/s]\n",
            " 92% 9100/9850 [00:47<00:03, 203.43it/s]\n",
            " 93% 9150/9850 [00:47<00:03, 203.19it/s]\n",
            " 93% 9200/9850 [00:48<00:03, 203.18it/s]\n",
            " 94% 9250/9850 [00:48<00:02, 203.25it/s]\n",
            " 94% 9300/9850 [00:48<00:02, 203.77it/s]\n",
            " 95% 9350/9850 [00:48<00:02, 204.28it/s]\n",
            " 95% 9400/9850 [00:48<00:02, 203.77it/s]\n",
            " 96% 9450/9850 [00:49<00:01, 202.32it/s]\n",
            " 96% 9500/9850 [00:49<00:01, 196.03it/s]\n",
            " 97% 9550/9850 [00:49<00:01, 197.50it/s]\n",
            " 97% 9600/9850 [00:50<00:01, 198.51it/s]\n",
            " 98% 9650/9850 [00:50<00:01, 199.55it/s]\n",
            " 98% 9700/9850 [00:50<00:00, 199.34it/s]\n",
            " 99% 9750/9850 [00:50<00:00, 199.47it/s]\n",
            " 99% 9800/9850 [00:51<00:00, 199.31it/s]\n",
            "100% 9850/9850 [00:51<00:00, 192.13it/s]\n",
            "\n",
            "\n",
            "--- Final result: ---\n",
            "               f_mae =  0.046357           \n",
            "              f_rmse =  0.062583           \n",
            "             H_f_mae =  0.035021           \n",
            "             O_f_mae =  0.069030           \n",
            "         psavg_f_mae =  0.052025           \n",
            "            H_f_rmse =  0.045198           \n",
            "            O_f_rmse =  0.087545           \n",
            "        psavg_f_rmse =  0.066372           \n",
            "               e_mae =  0.059472           \n",
            "             e/N_mae =  0.000619           \n",
            "               f_mae =  0.046357           \n",
            "              f_rmse =  0.062583           \n",
            "             H_f_mae =  0.035021           \n",
            "             O_f_mae =  0.069030           \n",
            "         psavg_f_mae =  0.052025           \n",
            "            H_f_rmse =  0.045198           \n",
            "            O_f_rmse =  0.087545           \n",
            "        psavg_f_rmse =  0.066372           \n",
            "               e_mae =  0.059472           \n",
            "             e/N_mae =  0.000619           \n"
          ]
        }
      ],
      "source": [
        "!nequip-evaluate --train-dir results/water/example-run-water --batch-size 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQHrMMnsPaJO"
      },
      "source": [
        "Again, energy errors of < 1meV/atom (converted from kcal/mol to eV), and force errors of ~45 meV/A 🎉"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4r5FBXaum9n"
      },
      "source": [
        "# LAMMPS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qIYIYyr1B4O"
      },
      "source": [
        "We are now in a position to run MD with our potential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UirNBTlJ1BNZ"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/run.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQs0ijPhvAGb"
      },
      "source": [
        "Set up a simple LAMMPS input file\n",
        "\n",
        "CAUTION: the reference data here are in eV for the energies and eV/A for the forces. The NequIP model will therefore also be predicting outputs in these units. We are therefore using `units metal` in LAMMPS (see [docs](https://docs.lammps.org/units.html)). Time units are also in`ps`)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We now run MD in the NVT ensemble for 10 ps (20,000 steps)"
      ],
      "metadata": {
        "id": "3AXv_y_JD1TJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W090KfMsd2Do"
      },
      "outputs": [],
      "source": [
        "lammps_input_md = \"\"\"\n",
        "units           metal\n",
        "boundary        p p p\n",
        "atom_style      atomic\n",
        "thermo 1\n",
        "newton off\n",
        "read_data structure.data\n",
        "\n",
        "neighbor        1.0 bin\n",
        "neigh_modify    every 10 delay 0 check no\n",
        "\n",
        "pair_style\tnequip\n",
        "pair_coeff\t* * ../water-deploy.pth H O\n",
        "mass            1 1.00794\n",
        "mass            2 15.9994\n",
        "\n",
        "velocity        all create 300.0 23456789\n",
        "timestep        0.0005\n",
        "fix             1 all nvt temp 300.0 300.0 $(100.0*dt)\n",
        "\n",
        "#print log every X steps\n",
        "thermo          100\n",
        "thermo_style    custom step pe ke etotal temp press vol\n",
        "\n",
        "#print trajectory in xyz every X time units\n",
        "dump              1 all xyz 20 water.xyz \n",
        "dump_modify       1 element H O\n",
        "\n",
        "# dump            2 all custom 1 dump_frc.lamppstrj id type element fx fy fz\n",
        "# dump_modify     2 element O H\n",
        "# dump            3 all custom 1 dump.lammpstrj id type element x y z\n",
        "# dump_modify     3 element O H\n",
        "\n",
        "run             20000\n",
        "\"\"\"\n",
        "\n",
        "!mkdir lammps_run\n",
        "with open(\"lammps_run/water_md.in\", \"w\") as f:\n",
        "    f.write(lammps_input_md)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mu4kbiXOt1pI"
      },
      "outputs": [],
      "source": [
        "! cp /content/water-deploy.pth /content/lammps_run/."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvWZCw1zvjRc"
      },
      "source": [
        "We specify the initial water configuration by reading the extxyz file and writing to structure.data file easily parsed by lammps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXeHb2ZPvIbU"
      },
      "outputs": [],
      "source": [
        "wat_pos_frc_trj = read('/content/nequip/data/AIMD_data/wat_pos_frc-10k.extxyz')\n",
        "write(\"/content/lammps_run/structure.data\", wat_pos_frc_trj,format='lammps-data')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDuyueY11YBF"
      },
      "source": [
        "### Run the LAMMPS command: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG1LE98LukSO",
        "outputId": "5dd8028d-706a-43c7-e45a-66f877d83556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "units           metal\n",
            "boundary        p p p\n",
            "atom_style      atomic\n",
            "thermo 1\n",
            "newton off\n",
            "read_data structure.data\n",
            "\n",
            "neighbor        1.0 bin\n",
            "neigh_modify    every 10 delay 0 check no\n",
            "\n",
            "pair_style\tnequip\n",
            "pair_coeff\t* * ../water-deploy.pth H O\n",
            "mass            1 1.00794\n",
            "mass            2 15.9994\n",
            "\n",
            "velocity        all create 300.0 23456789\n",
            "timestep        0.0005\n",
            "fix             1 all nvt temp 300.0 300.0 $(100.0*dt)\n",
            "\n",
            "#print log every X steps\n",
            "thermo          100\n",
            "thermo_style    custom step pe ke etotal temp press vol\n",
            "\n",
            "#print trajectory in xyz every X time units\n",
            "dump              1 all xyz 20 water.xyz \n",
            "dump_modify       1 element H O\n",
            "\n",
            "# dump            2 all custom 1 dump_frc.lamppstrj id type element fx fy fz\n",
            "# dump_modify     2 element O H\n",
            "# dump            3 all custom 1 dump.lammpstrj id type element x y z\n",
            "# dump_modify     3 element O H\n",
            "\n",
            "run             20000\n",
            "/content/lammps_run/structure.data (written by ASE) \n",
            "\n",
            "96 \t atoms \n",
            "2  atom types\n",
            "0.0      9.8499999999999996  xlo xhi\n",
            "0.0      9.8499999999999996  ylo yhi\n",
            "0.0      9.8499999999999996  zlo zhi\n",
            "\n",
            "\n",
            "Atoms \n",
            "\n",
            "     1   1      42.886169670000001   -0.055681660000000001      38.329161120000002\n",
            "     2   1      34.202588720000001              -0.6185484      37.365568080000003\n",
            "     3   1      30.080392589999999     -2.0124176500000002      36.480796079999998\n",
            "     4   1      28.705791179999999     -2.6880392799999999      36.602098329999997\n",
            "     5   1             36.24794267    -0.51634849000000005      34.492359610000001\n",
            "     6   1      37.696472460000003   -0.041087279999999997      35.014073510000003\n",
            "     7   1      27.760669979999999      7.4854206000000003      33.927691950000003\n",
            "     8   1      28.816099959999999              6.49857774             34.21636084\n",
            "     9   1      37.157637200000003      9.0188280800000005      31.926581209999998\n",
            "    10   1      38.606381650000003      9.5820079600000003      32.343597279999997\n",
            "    11   1      34.303195930000001      2.2195014400000002      45.988045190000001\n",
            "    12   1      33.244413940000001              1.30253325      46.469842720000003\n",
            "    13   1      38.728617479999997     -5.0541897699999998      26.074396839999999\n",
            "    14   1      38.348392150000002     -6.2832846900000003      26.986725310000001\n",
            "    15   1      32.864252090000001              3.20606327               30.897116\n",
            "    16   1      31.290408809999999      3.0871834699999998             30.62739775\n",
            "    17   1      33.751986969999997             -3.13832624      39.672760769999996\n",
            "    18   1      34.664297990000001             -3.66438591      38.646602719999997\n",
            "    19   1      42.717321439999999      5.1246883700000003      32.588340119999998\n",
            "    20   1             41.56274552      5.5893544000000004      33.417490280000003\n",
            "    21   1      32.428380089999997      9.1182520999999994             30.54776786\n",
            "    22   1              32.6432407             10.77068308             30.48427787\n",
            "    23   1      31.484867080000001      4.6777144699999997      37.395719499999998\n",
            "    24   1      32.317188299999998     -6.2287496200000003      36.467186439999999\n",
            "    25   1      26.662134099999999      3.1708123800000001      35.682014649999999\n",
            "    26   1             26.52713675              1.60390403      35.488348299999998\n",
            "    27   1      32.023823659999998      16.918208029999999      31.688356989999999\n",
            "    28   1      31.400657970000001      7.0315610800000004      30.239455469999999\n",
            "    29   1             33.52642531     -3.5594808100000002             34.26368308\n",
            "    30   1      34.640485550000001             -3.26538336      35.497148240000001\n",
            "    31   1      40.056437510000002    -0.30543864999999998      29.831207429999999\n",
            "    32   1      39.478446419999997             -1.09483146      38.310114040000002\n",
            "    33   1      39.704076149999999      1.9584631400000001      33.390237599999999\n",
            "    34   1      38.333857080000001              2.69671781             42.92619457\n",
            "    35   1      40.182045549999998     -7.2199289499999999      27.658039049999999\n",
            "    36   1      39.320443179999998     -8.4564252700000004             28.13196589\n",
            "    37   1             36.38769637      8.8117085900000003      38.354536240000002\n",
            "    38   1      36.320563759999999      9.0063075799999996      36.752600139999998\n",
            "    39   1      29.999158309999999     -5.5637817500000004      33.929505059999997\n",
            "    40   1      30.772854550000002     -5.0385870199999996      35.199806719999998\n",
            "    41   1      40.059251779999997      6.3305279499999996      28.257946189999998\n",
            "    42   1      40.239836089999997      5.1745923999999999             29.29629568\n",
            "    43   1             26.33209111      2.4393638599999998      33.565386850000003\n",
            "    44   1      26.960697119999999      1.2711078899999999      32.592388440000001\n",
            "    45   1      34.837269769999999    -0.47227084000000003      30.382436200000001\n",
            "    46   1      35.396881370000003             -1.92684834      30.308183759999999\n",
            "    47   1      32.121760719999997    -0.73334299000000003      36.510438299999997\n",
            "    48   1      32.218084349999998      7.8454304099999996      35.667196779999998\n",
            "    49   1             36.37809987     -4.3048878799999999              36.4539793\n",
            "    50   1      35.811927560000001             -3.00139282      27.034893759999999\n",
            "    51   1      29.645249140000001      1.0652123600000001      35.714365399999998\n",
            "    52   1      30.379465499999998   -0.066814650000000003      34.988246869999998\n",
            "    53   1             34.21493366             -1.65591205      33.887643709999999\n",
            "    54   1      34.784243549999999     -1.0252141100000001      32.503483260000003\n",
            "    55   1      40.464995450000004              1.14678254      31.307350320000001\n",
            "    56   1      41.326246990000001     0.65508034999999998      32.455588290000001\n",
            "    57   1             29.02108599      3.5038194900000001             39.90877029\n",
            "    58   1      29.494542689999999      3.7276637300000002      41.376613800000001\n",
            "    59   1      34.135966400000001     -6.7533422300000003      32.356841029999998\n",
            "    60   1      34.954657009999998     -5.7704242399999996             31.45710669\n",
            "    61   1      33.253235689999997      1.5268048299999999      44.056217189999998\n",
            "    62   1      33.793166939999999     0.50146325999999997      43.059759010000001\n",
            "    63   1      36.820540960000002      2.6214681999999998      40.683400659999997\n",
            "    64   1      37.555270669999999              1.56498329             39.76489351\n",
            "    65   2      43.209908720000001   -0.062845650000000003             47.25931559\n",
            "    66   2      29.394058350000002     -2.3133019500000001      37.140788360000002\n",
            "    67   2      36.741570840000001   -0.083871000000000001      35.259178339999998\n",
            "    68   2             27.94247769      6.7622961500000001      34.564838440000003\n",
            "    69   2      37.681265600000003      9.4216399800000001      32.647864349999999\n",
            "    70   2      33.317129080000001      2.0951401700000001      45.872226509999997\n",
            "    71   2      37.995135589999997      4.3611431200000004             26.55718199\n",
            "    72   2      32.182467090000003      2.6611503399999998      30.457724819999999\n",
            "    73   2      34.653801209999997     -3.4374573100000001      39.588924570000003\n",
            "    74   2      42.292983370000002      5.9471069500000002      32.846099549999998\n",
            "    75   2      32.960469009999997      9.9050313200000009      30.158730670000001\n",
            "    76   2             31.42818866     -5.8338304000000001      36.673874359999999\n",
            "    77   2             26.05637308      2.4973869199999998      35.348687040000002\n",
            "    78   2      32.033492709999997      17.325228939999999      30.811601379999999\n",
            "    79   2      33.825218239999998     -2.9520949600000002      35.022046070000002\n",
            "    80   2      39.456998159999998    -0.30727594000000003      38.934782900000002\n",
            "    81   2             29.48467089      2.8692561099999998      43.006186839999998\n",
            "    82   2      39.286418410000003             -7.62061031      27.627114779999999\n",
            "    83   2      35.879750280000003      8.6515870800000005      37.522173430000002\n",
            "    84   2      30.358254389999999     -4.7607656800000004      34.335564570000003\n",
            "    85   2      40.709895690000003      5.8331250199999998      28.755837589999999\n",
            "    86   2      26.717908300000001      2.2415138300000002      32.657729799999998\n",
            "    87   2      35.658925619999998    -0.99689035000000004      30.574953059999999\n",
            "    88   2      31.585160200000001             -1.31218042      35.901110940000002\n",
            "    89   2      35.548938669999998     -3.9056138900000001      26.821449000000001\n",
            "    90   2      29.565661609999999     0.46817945999999999      34.967071199999999\n",
            "    91   2             34.76151282    -0.95696802000000003      33.489136729999998\n",
            "    92   2      40.485340669999999     0.40236209000000001             31.94254162\n",
            "    93   2      29.672828979999998      4.0134825300000001               40.450578\n",
            "    94   2      34.127228619999997     -5.8796882899999998      31.892542299999999\n",
            "    95   2      33.116888420000002      1.2338084899999999      43.112771199999997\n",
            "    96   2      37.199699350000003      2.5049007099999998      39.791712660000002\n"
          ]
        }
      ],
      "source": [
        "! cat /content/lammps_run/water_md.in\n",
        "! cat /content/lammps_run/structure.data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run MD!"
      ],
      "metadata": {
        "id": "plER-wpaFCCT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gurLjNK5upvq",
        "outputId": "a5f378fe-a1b6-47f5-b0f5-58f875ec43d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LAMMPS (29 Sep 2021 - Update 2)\n",
            "OMP_NUM_THREADS environment is not set. Defaulting to 1 thread. (src/comm.cpp:98)\n",
            "  using 1 OpenMP thread(s) per MPI task\n",
            "Reading data file ...\n",
            "  orthogonal box = (0.0000000 0.0000000 0.0000000) to (9.8500000 9.8500000 9.8500000)\n",
            "  1 by 1 by 1 MPI processor grid\n",
            "  reading atoms ...\n",
            "  96 atoms\n",
            "  read_data CPU = 0.001 seconds\n",
            "NEQUIP is using device cuda\n",
            "NequIP Coeff: type 1 is element H\n",
            "NequIP Coeff: type 2 is element O\n",
            "Loading model from ../water-deploy.pth\n",
            "Freezing TorchScript model...\n",
            "Neighbor list info ...\n",
            "  update every 10 steps, delay 0 steps, check no\n",
            "  max neighbors/atom: 2000, page size: 100000\n",
            "  master list distance cutoff = 5\n",
            "  ghost atom cutoff = 5\n",
            "  binsize = 2.5, bins = 4 4 4\n",
            "  1 neighbor lists, perpetual/occasional/extra = 1 0 0\n",
            "  (1) pair nequip, perpetual\n",
            "      attributes: full, newton off\n",
            "      pair build: full/bin/atomonly\n",
            "      stencil: full/bin/3d\n",
            "      bin: standard\n",
            "Setting up Verlet run ...\n",
            "  Unit style    : metal\n",
            "  Current step  : 0\n",
            "  Time step     : 0.0005\n",
            "Per MPI rank memory allocation (min/avg/max) = 3.586 | 3.586 | 3.586 Mbytes\n",
            "Step PotEng KinEng TotEng Temp Press Volume \n",
            "       0   -14985.618    3.6839141   -14981.934          300    4117.3701    955.67162 \n",
            "     100   -14985.151    3.6123014   -14981.539    294.16821    4037.3313    955.67162 \n",
            "     200   -14984.611    3.5535472   -14981.058    289.38355    3971.6639    955.67162 \n",
            "     300   -14984.511    3.7110018     -14980.8    302.20589    4147.6449    955.67162 \n",
            "     400   -14984.444    3.9257399   -14980.519    319.69311    4387.6495    955.67162 \n",
            "     500   -14984.367     4.137223    -14980.23    336.91526    4624.0161    955.67162 \n",
            "     600   -14983.874    3.9032946   -14979.971    317.86527    4362.5632    955.67162 \n",
            "     700    -14983.34    4.0960478   -14979.244    333.56216    4577.9962    955.67162 \n",
            "     800   -14983.479    4.3725246   -14979.107    356.07708    4887.0037    955.67162 \n",
            "     900   -14983.761    3.7055239   -14980.055     301.7598    4141.5225    955.67162 \n",
            "    1000   -14983.903    3.6048911   -14980.298    293.56475     4029.049    955.67162 \n",
            "    1100   -14984.227    3.6791812   -14980.547    299.61458    4112.0803    955.67162 \n",
            "    1200   -14984.437    3.7880495   -14980.648    308.48028    4233.7582    955.67162 \n",
            "    1300   -14984.694    3.6168345   -14981.078    294.53736    4042.3977    955.67162 \n",
            "    1400   -14984.773    3.1276429   -14981.646    254.69998     3495.647    955.67162 \n",
            "    1500   -14985.038    3.5051783   -14981.533    285.44462    3917.6038    955.67162 \n",
            "    1600   -14984.851     3.708229   -14981.142    301.98008    4144.5459    955.67162 \n",
            "    1700   -14984.458    3.3183472    -14981.14    270.23001    3708.7899    955.67162 \n",
            "    1800   -14985.282    3.2084902   -14982.074    261.28379    3586.0069    955.67162 \n",
            "    1900   -14985.322    3.1106514   -14982.212    253.31628    3476.6562    955.67162 \n",
            "    2000   -14985.201    3.6165888   -14981.585    294.51736    4042.1232    955.67162 \n",
            "    2100   -14984.674    3.5768675   -14981.097    291.28265    3997.7282    955.67162 \n",
            "    2200   -14984.893    4.0602299   -14980.832    330.64532    4537.9639    955.67162 \n",
            "    2300   -14984.314    3.5694888   -14980.745    290.68176    3989.4812    955.67162 \n",
            "    2400   -14984.207    3.7436344   -14980.463    304.86333    4184.1171    955.67162 \n",
            "    2500   -14983.907    4.0612948   -14979.846    330.73204     4539.154    955.67162 \n",
            "    2600   -14983.614    4.1371986   -14979.477    336.91327    4623.9888    955.67162 \n",
            "    2700    -14984.18    4.1933672   -14979.986    341.48737    4686.7663    955.67162 \n",
            "    2800   -14984.119    3.4813033   -14980.638    283.50036    3890.9197    955.67162 \n",
            "    2900   -14984.588    3.7650942   -14980.823    306.61091    4208.1019    955.67162 \n",
            "    3000   -14984.778    3.6365723   -14981.142    296.14471     4064.458    955.67162 \n",
            "    3100   -14984.638    3.3714982   -14981.266    274.55837    3768.1948    955.67162 \n",
            "    3200   -14984.385    3.9097427   -14980.475    318.39038    4369.7701    955.67162 \n",
            "    3300   -14984.091    3.8418562   -14980.249    312.86204     4293.896    955.67162 \n",
            "    3400   -14984.108    3.5850721   -14980.523    291.95078    4006.8981    955.67162 \n",
            "    3500   -14983.694    3.1748284    -14980.52    258.54254    3548.3844    955.67162 \n",
            "    3600   -14983.539    3.8151279   -14979.724    310.68541    4264.0227    955.67162 \n",
            "    3700    -14982.85    3.6175191   -14979.232    294.59311    4043.1629    955.67162 \n",
            "    3800   -14983.235    3.8312919   -14979.404    312.00173    4282.0886    955.67162 \n",
            "    3900    -14983.49       3.6309   -14979.859    295.68279    4058.1182    955.67162 \n",
            "    4000   -14984.064     3.633226   -14980.431    295.87221     4060.718    955.67162 \n",
            "    4100   -14984.006    3.5336374   -14980.472    287.76219    3949.4115    955.67162 \n",
            "    4200    -14983.72     3.412396   -14980.307    277.88889    3813.9046    955.67162 \n",
            "    4300   -14983.724    3.7951491   -14979.928    309.05844    4241.6932    955.67162 \n",
            "    4400   -14983.815    3.9808959   -14979.835    324.18474    4449.2952    955.67162 \n",
            "    4500    -14983.99    3.9118102   -14980.078    318.55875    4372.0808    955.67162 \n",
            "    4600   -14984.398    3.6068441   -14980.792     293.7238    4031.2319    955.67162 \n",
            "    4700   -14984.654    3.3842144    -14981.27    275.59392    3782.4072    955.67162 \n",
            "    4800   -14984.178    3.1457864   -14981.032     256.1775    3515.9253    955.67162 \n",
            "    4900   -14983.957    3.5339544   -14980.423    287.78801    3949.7658    955.67162 \n",
            "    5000   -14983.932     4.013536   -14979.918     326.8428    4485.7759    955.67162 \n",
            "    5100   -14983.426    3.9105288   -14979.515    318.45439    4370.6486    955.67162 \n",
            "    5200   -14984.077    4.6058767   -14979.471    375.08013    5147.8124    955.67162 \n",
            "    5300    -14984.13    3.8317838   -14980.298    312.04178    4282.6384    955.67162 \n",
            "    5400   -14983.747    3.1333291   -14980.614    255.16304    3502.0022    955.67162 \n",
            "    5500    -14983.84    3.3531963   -14980.487    273.06795    3747.7394    955.67162 \n",
            "    5600   -14983.542    3.5151229   -14980.027    286.25446    3928.7185    955.67162 \n",
            "    5700   -14984.261    4.5382772   -14979.722    369.57516     5072.259    955.67162 \n",
            "    5800   -14984.139    3.4106695   -14980.728    277.74829     3811.975    955.67162 \n",
            "    5900    -14984.47    3.4924498   -14980.977    284.40808    3903.3777    955.67162 \n",
            "    6000     -14983.9    3.2783411   -14980.622    266.97211    3664.0766    955.67162 \n",
            "    6100   -14984.136     3.937238   -14980.199    320.62946    4400.5005    955.67162 \n",
            "    6200   -14983.611     3.446242   -14980.165    280.64514     3851.733    955.67162 \n",
            "    6300   -14983.598    3.5412353   -14980.056    288.38093    3957.9034    955.67162 \n",
            "    6400    -14983.71    4.0030253   -14979.707    325.98686    4474.0284    955.67162 \n",
            "    6500   -14983.797     4.183531   -14979.613    340.68636    4675.7728    955.67162 \n",
            "    6600   -14983.897    3.6801816   -14980.217    299.69604    4113.1984    955.67162 \n",
            "    6700   -14983.471    2.9536719   -14980.517    240.53264    3301.2063    955.67162 \n",
            "    6800   -14983.851    3.6007251    -14980.25    293.22549    4024.3929    955.67162 \n",
            "    6900   -14983.536    3.6282154   -14979.908    295.46416    4055.1177    955.67162 \n",
            "    7000   -14983.736    3.6564543    -14980.08    297.76381    4086.6793    955.67162 \n",
            "    7100   -14984.597    4.1973725   -14980.399    341.81354    4691.2429    955.67162 \n",
            "    7200   -14984.201    3.3654741   -14980.836     274.0678    3761.4618    955.67162 \n",
            "    7300   -14984.371    3.5200647   -14980.851    286.65691    3934.2419    955.67162 \n",
            "    7400   -14984.136    4.0099374   -14980.126    326.54974    4481.7538    955.67162 \n",
            "    7500   -14984.108    4.2338977   -14979.875    344.78798    4732.0657    955.67162 \n",
            "    7600   -14984.371    4.0759773   -14980.295    331.92772    4555.5642    955.67162 \n",
            "    7700   -14984.597    3.9870208    -14980.61    324.68353    4456.1408    955.67162 \n",
            "    7800   -14984.445    3.6008256   -14980.844    293.23368    4024.5052    955.67162 \n",
            "    7900   -14984.539    3.5612069   -14980.978    290.00732    3980.2249    955.67162 \n",
            "    8000   -14985.107    3.8569383    -14981.25    314.09024    4310.7526    955.67162 \n",
            "    8100   -14984.737      2.99284   -14981.744    243.72229    3344.9829    955.67162 \n",
            "    8200   -14985.284    3.5869891   -14981.697     292.1069    4009.0407    955.67162 \n",
            "    8300   -14984.884    3.8705304   -14981.013    315.19712     4325.944    955.67162 \n",
            "    8400   -14984.539    4.0354774   -14980.504     328.6296     4510.299    955.67162 \n",
            "    8500   -14984.312    3.8115179   -14980.501    310.39143     4259.988    955.67162 \n",
            "    8600   -14983.785    3.3094283   -14980.476     269.5037    3698.8216    955.67162 \n",
            "    8700    -14984.15    3.8321405   -14980.318    312.07083     4283.037    955.67162 \n",
            "    8800   -14984.381     3.728424   -14980.652    303.62466     4167.117    955.67162 \n",
            "    8900   -14984.407    3.3817675   -14981.025    275.39465    3779.6723    955.67162 \n",
            "    9000   -14984.467    3.6121057   -14980.855    294.15227    4037.1126    955.67162 \n",
            "    9100   -14984.158    4.0389999   -14980.119    328.91645    4514.2359    955.67162 \n",
            "    9200   -14983.779    3.7776593   -14980.002    307.63415    4222.1455    955.67162 \n",
            "    9300   -14984.219    4.1292227    -14980.09    336.26376    4615.0744    955.67162 \n",
            "    9400   -14984.332    4.0111464   -14980.321     326.6482    4483.1051    955.67162 \n",
            "    9500   -14984.371    3.7700914   -14980.601    307.01786    4213.6872    955.67162 \n",
            "    9600   -14984.382    3.6977494   -14980.684    301.12667    4132.8332    955.67162 \n",
            "    9700   -14984.059    3.2921139   -14980.766     268.0937    3679.4699    955.67162 \n",
            "    9800   -14983.794    3.3988855   -14980.395    276.78866    3798.8045    955.67162 \n",
            "    9900   -14983.152     3.406059   -14979.746    277.37283     3806.822    955.67162 \n",
            "   10000   -14983.732    4.0812617   -14979.651    332.35805    4561.4703    955.67162 \n",
            "   10100   -14984.111     4.028895   -14980.082    328.09356     4502.942    955.67162 \n",
            "   10200   -14984.174    3.6533813    -14980.52    297.51356    4083.2447    955.67162 \n",
            "   10300   -14983.874    3.2110269   -14980.663    261.49037    3588.8421    955.67162 \n",
            "   10400    -14983.86    3.2651626   -14980.595    265.89892    3649.3475    955.67162 \n",
            "   10500    -14984.39    4.1681549   -14980.221    339.43421    4658.5875    955.67162 \n",
            "   10600   -14983.479    4.1247469   -14979.355    335.89927     4610.072    955.67162 \n",
            "   10700   -14983.362    4.1358105   -14979.226    336.80024    4622.4374    955.67162 \n",
            "   10800   -14983.758    4.0398096   -14979.718    328.98239    4515.1408    955.67162 \n",
            "   10900    -14983.42    3.4736809   -14979.946    282.87963    3882.4004    955.67162 \n",
            "   11000   -14983.683    3.5978168   -14980.085    292.98866    4021.1424    955.67162 \n",
            "   11100   -14984.141    4.0876136   -14980.053    332.87531    4568.5695    955.67162 \n",
            "   11200   -14984.398    3.5949002   -14980.804    292.75114    4017.8827    955.67162 \n",
            "   11300   -14984.752     3.521445   -14981.231    286.76931    3935.7846    955.67162 \n",
            "   11400   -14984.602    3.5532309   -14981.048     289.3578    3971.3104    955.67162 \n",
            "   11500   -14984.125    3.6209016   -14980.504    294.86856    4046.9434    955.67162 \n",
            "   11600   -14984.254    4.0003907   -14980.254     325.7723    4471.0838    955.67162 \n",
            "   11700   -14983.697    3.3792948   -14980.318    275.19329    3776.9087    955.67162 \n",
            "   11800   -14983.802    3.6477051   -14980.154    297.05131    4076.9006    955.67162 \n",
            "   11900   -14983.802    4.0302244   -14979.772    328.20182    4504.4279    955.67162 \n",
            "   12000   -14984.045    4.3100986   -14979.735    350.99341    4817.2325    955.67162 \n",
            "   12100   -14983.937    3.7206268   -14980.216     302.9897    4158.4025    955.67162 \n",
            "   12200    -14983.95     3.501762   -14980.448    285.16642    3913.7856    955.67162 \n",
            "   12300   -14983.906    3.6752934   -14980.231    299.29797    4107.7351    955.67162 \n",
            "   12400   -14983.998    3.9023661   -14980.096    317.78966    4361.5255    955.67162 \n",
            "   12500    -14984.48    3.9444079   -14980.536    321.21334     4408.514    955.67162 \n",
            "   12600   -14984.465    3.5004902   -14980.964    285.06285    3912.3642    955.67162 \n",
            "   12700   -14984.014    3.3182607   -14980.695    270.22297    3708.6933    955.67162 \n",
            "   12800   -14983.808      3.56594   -14980.242    290.39276    3985.5149    955.67162 \n",
            "   12900   -14983.779    3.7826153   -14979.997    308.03774    4227.6846    955.67162 \n",
            "   13000   -14983.812    3.8067742   -14980.006    310.00512    4254.6861    955.67162 \n",
            "   13100   -14984.047    3.7429679   -14980.304    304.80905    4183.3722    955.67162 \n",
            "   13200   -14984.672    3.7245968   -14980.947      303.313    4162.8396    955.67162 \n",
            "   13300   -14984.699     3.420319   -14981.279     278.5341    3822.7599    955.67162 \n",
            "   13400   -14984.585    3.3828471   -14981.202    275.48257     3780.879    955.67162 \n",
            "   13500   -14984.592    3.6994655   -14980.892    301.26643    4134.7513    955.67162 \n",
            "   13600   -14984.421    3.6492384   -14980.772    297.17618    4078.6144    955.67162 \n",
            "   13700   -14984.454    3.6185392   -14980.836    294.67619    4044.3031    955.67162 \n",
            "   13800   -14984.249    3.3259076   -14980.923    270.84569    3717.2399    955.67162 \n",
            "   13900   -14984.798    4.0195323   -14980.778     327.3311    4492.4777    955.67162 \n",
            "   14000   -14984.603    3.7164719   -14980.886    302.65134    4153.7587    955.67162 \n",
            "   14100     -14985.1    3.5972223   -14981.502    292.94024     4020.478    955.67162 \n",
            "   14200   -14984.932    3.1159039   -14981.816    253.74402    3482.5267    955.67162 \n",
            "   14300   -14984.884     3.441049   -14981.443    280.22224     3845.929    955.67162 \n",
            "   14400   -14984.578    3.6618655   -14980.916    298.20447    4092.7272    955.67162 \n",
            "   14500   -14984.809    3.8769558   -14980.932    315.72037    4333.1254    955.67162 \n",
            "   14600   -14985.299    4.2769193   -14981.022    348.29144    4780.1493    955.67162 \n",
            "   14700    -14984.84    3.4509571   -14981.389    281.02912     3857.003    955.67162 \n",
            "   14800   -14984.961    3.1890321   -14981.772    259.69922    3564.2593    955.67162 \n",
            "   14900   -14985.009    3.3745878   -14981.634    274.80997    3771.6478    955.67162 \n",
            "   15000   -14984.434    3.5945067   -14980.839     292.7191    4017.4428    955.67162 \n",
            "   15100   -14984.395    3.6246498    -14980.77     295.1738    4051.1327    955.67162 \n",
            "   15200   -14985.092    3.6858336   -14981.406    300.15631    4119.5154    955.67162 \n",
            "   15300   -14985.058    3.3898018   -14981.668    276.04893     3788.652    955.67162 \n",
            "   15400   -14984.785    3.3742522   -14981.411    274.78264    3771.2727    955.67162 \n",
            "   15500   -14984.222    3.3152588   -14980.906     269.9785    3705.3381    955.67162 \n",
            "   15600    -14984.58    3.8942797   -14980.686    317.13115    4352.4877    955.67162 \n",
            "   15700   -14984.781    3.8553767   -14980.926    313.96307    4309.0072    955.67162 \n",
            "   15800   -14984.765    3.3759656   -14981.389    274.92218    3773.1878    955.67162 \n",
            "   15900   -14985.299    3.7631296   -14981.536    306.45092    4205.9062    955.67162 \n",
            "   16000   -14985.063    3.5895583   -14981.474    292.31612    4011.9122    955.67162 \n",
            "   16100    -14984.89    3.4792038    -14981.41    283.32939    3888.5732    955.67162 \n",
            "   16200   -14985.268    4.0110423   -14981.257    326.63972    4482.9888    955.67162 \n",
            "   16300   -14984.828     3.605331   -14981.223    293.60057    4029.5408    955.67162 \n",
            "   16400   -14984.922      4.02158     -14980.9    327.49786    4494.7663    955.67162 \n",
            "   16500   -14984.155    3.6093887   -14980.546    293.93101    4034.0758    955.67162 \n",
            "   16600   -14984.063    3.6917737   -14980.372    300.64004    4126.1544    955.67162 \n",
            "   16700   -14984.605    3.7787323   -14980.827    307.72153    4223.3447    955.67162 \n",
            "   16800   -14985.053    3.5876867   -14981.465    292.16371    4009.8204    955.67162 \n",
            "   16900   -14984.997    3.4169475    -14981.58    278.25954    3818.9917    955.67162 \n",
            "   17000   -14985.275    3.7082379   -14981.567    301.98081    4144.5558    955.67162 \n",
            "   17100   -14985.169    3.6161963   -14981.553    294.48539    4041.6844    955.67162 \n",
            "   17200    -14985.21    3.6900166    -14981.52    300.49695    4124.1906    955.67162 \n",
            "   17300   -14984.409    3.4052765   -14981.004    277.30911    3805.9474    955.67162 \n",
            "   17400   -14984.677    4.0211698   -14980.656    327.46445    4494.3078    955.67162 \n",
            "   17500   -14985.083    4.2513112   -14980.832    346.20605    4751.5281    955.67162 \n",
            "   17600    -14985.39    3.8157228   -14981.574    310.73386    4264.6877    955.67162 \n",
            "   17700   -14985.405     3.776256   -14981.629    307.51987    4220.5771    955.67162 \n",
            "   17800    -14984.98    3.5610585   -14981.419    289.99524     3980.059    955.67162 \n",
            "   17900   -14984.788    3.8159633   -14980.972    310.75344    4264.9564    955.67162 \n",
            "   18000   -14984.604    4.3371906   -14980.267    353.19965    4847.5123    955.67162 \n",
            "   18100   -14984.299    4.5938029   -14979.705     374.0969     5134.318    955.67162 \n",
            "   18200   -14984.563    4.0577261   -14980.506    330.44142    4535.1654    955.67162 \n",
            "   18300   -14984.505     3.663662   -14980.841    298.35077    4094.7351    955.67162 \n",
            "   18400   -14984.729     3.390231   -14981.338    276.08388    3789.1317    955.67162 \n",
            "   18500   -14985.989    4.2732434   -14981.716     347.9921    4776.0409    955.67162 \n",
            "   18600   -14985.453    3.3071526   -14982.146    269.31838    3696.2781    955.67162 \n",
            "   18700   -14985.433    3.3088671   -14982.124      269.458    3698.1943    955.67162 \n",
            "   18800   -14984.923    3.7719263   -14981.151    307.16728    4215.7379    955.67162 \n",
            "   18900   -14984.407    3.5499872   -14980.857    289.09365    3967.6851    955.67162 \n",
            "   19000   -14984.655    3.5216283   -14981.134    286.78423    3935.9894    955.67162 \n",
            "   19100   -14985.177     3.743797   -14981.433    304.87657    4184.2989    955.67162 \n",
            "   19200   -14984.628    3.0431458   -14981.585    247.81895    3401.2078    955.67162 \n",
            "   19300   -14984.737    3.6421315   -14981.095    296.59742    4070.6712    955.67162 \n",
            "   19400   -14984.826     4.417714   -14980.408    359.75708    4937.5102    955.67162 \n",
            "   19500   -14984.769    4.3517261   -14980.417    354.38335     4863.758    955.67162 \n",
            "   19600   -14984.558    3.7774896    -14980.78    307.62033    4221.9558    955.67162 \n",
            "   19700   -14984.865    3.6337389   -14981.231    295.91398    4061.2912    955.67162 \n",
            "   19800   -14984.973    3.5687558   -14981.404    290.62207    3988.6621    955.67162 \n",
            "   19900   -14984.658    3.3610281   -14981.297    273.70574    3756.4927    955.67162 \n",
            "   20000   -14984.914    3.7817173   -14981.132    307.96461    4226.6809    955.67162 \n",
            "Loop time of 530.169 on 1 procs for 20000 steps with 96 atoms\n",
            "\n",
            "Performance: 1.630 ns/day, 14.727 hours/ns, 37.724 timesteps/s\n",
            "99.8% CPU use with 1 MPI tasks x 1 OpenMP threads\n",
            "\n",
            "MPI task timing breakdown:\n",
            "Section |  min time  |  avg time  |  max time  |%varavg| %total\n",
            "---------------------------------------------------------------\n",
            "Pair    | 529.16     | 529.16     | 529.16     |   0.0 | 99.81\n",
            "Neigh   | 0.63533    | 0.63533    | 0.63533    |   0.0 |  0.12\n",
            "Comm    | 0.081109   | 0.081109   | 0.081109   |   0.0 |  0.02\n",
            "Output  | 0.11466    | 0.11466    | 0.11466    |   0.0 |  0.02\n",
            "Modify  | 0.14976    | 0.14976    | 0.14976    |   0.0 |  0.03\n",
            "Other   |            | 0.02858    |            |       |  0.01\n",
            "\n",
            "Nlocal:        96.0000 ave          96 max          96 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "Nghost:        696.000 ave         696 max         696 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "Neighs:         0.00000 ave           0 max           0 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "FullNghs:      5044.00 ave        5044 max        5044 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "\n",
            "Total # of neighbors = 5044\n",
            "Ave neighs/atom = 52.541667\n",
            "Neighbor list builds = 2000\n",
            "Dangerous builds not checked\n",
            "Total wall time: 0:08:53\n",
            "[f7ca8b0ac7de:03250] *** Process received signal ***\n",
            "[f7ca8b0ac7de:03250] Signal: Segmentation fault (11)\n",
            "[f7ca8b0ac7de:03250] Signal code: Address not mapped (1)\n",
            "[f7ca8b0ac7de:03250] Failing at address: 0x7fb6d976220d\n",
            "[f7ca8b0ac7de:03250] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fb6dca16980]\n",
            "[f7ca8b0ac7de:03250] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7fb6dc655775]\n",
            "[f7ca8b0ac7de:03250] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7fb6dcec0e44]\n",
            "[f7ca8b0ac7de:03250] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7fb6dc656605]\n",
            "[f7ca8b0ac7de:03250] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7fb6dcebecb3]\n",
            "[f7ca8b0ac7de:03250] *** End of error message ***\n"
          ]
        }
      ],
      "source": [
        "!cd /content/lammps_run/ && ../lammps/build/lmp -in water_md.in"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the trajectory with ase and ngl"
      ],
      "metadata": {
        "id": "ETDEkUHc39u4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGEqNBu2fmKQ"
      },
      "outputs": [],
      "source": [
        "from ase.visualize import view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYxvyvyK2ctM"
      },
      "outputs": [],
      "source": [
        "wat_traj = read(\"/content/lammps_run/water.xyz\",index='::10')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G_nFd7dgWhc"
      },
      "outputs": [],
      "source": [
        "for i in range(len(wat_traj)):\n",
        "  wat_traj[i].cell = cell_vec_abc\n",
        "  wat_traj[i].pbc = np.array([True,True,True])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZLtkZ7bdszJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517,
          "referenced_widgets": [
            "47a25a9dba7942978aa05a745d615496",
            "e9a07ba1e9aa459f8973715dabbd2a87",
            "58ff1b0607084dad9b9c4c8e44b40152",
            "592569e635984eb4a6b9d82cd338a0af",
            "9eb9887d38684f13ad5c87072e844d36",
            "c1a493ae0e6b49979c17cef26b0d56fb",
            "dea23020ca834ed78fdcc89180ab22ca",
            "1b384234293845a792b50bd221531a67",
            "65ce5c450f734b0c9235ea86b9e211d5",
            "738c48653a7648e2bfeb0109e8adc758",
            "adad0778f13f41028b1bcb4e5a9108ca",
            "33221e01100a41bd950007e21eae5ddf",
            "ee2961a019c84c4bb4dd6ea7e9a8d137",
            "5c12ad74630e4a168d0c5698575da8c7",
            "d78f32a11efd4011a65d791151c1e774",
            "f318ac22f8714c159134c3ded8d8745a",
            "62c5524f48ae4991973eb6247e87d894",
            "6aca778059ab476cab3c176ee695f37b",
            "7d3c120301504fa9b5c990350f04ad33",
            "98dcd3152cbe484faea4dd71da6c00be",
            "650d3ad217db41a5bc5e05fcac2f7d4d",
            "73d242132293462c8db1eed7a9c330b4",
            "ddb95deb0b7f4853bdf45b4c0f5e81de",
            "5a8ac9a2fb9944f8bc72b7ecd578621a",
            "18eb97f42d524a7ab6d8a042632571e6",
            "c998bbe8b4f845af8a9c8fc06b88c646",
            "0402fdc13c1c480d8505092db2b347f9",
            "956c51f31fc24578b109ebbbd97e41ed",
            "64d43db436cc44fa8a365f89232f6f10"
          ]
        },
        "id": "-P5s2IbvfocL",
        "outputId": "9824c7d5-989e-4a60-b218-dcaf1bc69b9f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(NGLWidget(max_frame=100), VBox(children=(Dropdown(description='Show', options=('All', 'O', 'H')…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47a25a9dba7942978aa05a745d615496"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/a8874ba6619b6106/manager.min.js"
                }
              }
            }
          }
        }
      ],
      "source": [
        "view(wat_traj, viewer='ngl')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate radial distribution function with MDAnalysis"
      ],
      "metadata": {
        "id": "1CMwyeFr4BAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install MDAnalysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOo4fyaB8jAi",
        "outputId": "842105f9-86f0-4953-d691-2fa7a619c2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting MDAnalysis\n",
            "  Downloading MDAnalysis-2.1.0.tar.gz (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (1.4.1)\n",
            "Collecting biopython>=1.71\n",
            "  Downloading biopython-1.79-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 67.2 MB/s \n",
            "\u001b[?25hCollecting mmtf-python>=1.0.0\n",
            "  Downloading mmtf_python-1.1.3-py2.py3-none-any.whl (25 kB)\n",
            "Collecting gsd>=1.4.0\n",
            "  Downloading gsd-2.5.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 77.4 MB/s \n",
            "\u001b[?25hCollecting GridDataFormats>=0.4.0\n",
            "  Downloading GridDataFormats-0.7.0-py2.py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 80.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (3.2.2)\n",
            "Requirement already satisfied: networkx>=1.0 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (2.6.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (21.3)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (3.1.0)\n",
            "Requirement already satisfied: tqdm>=4.43.0 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (4.64.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GridDataFormats>=0.4.0->MDAnalysis) (1.15.0)\n",
            "Collecting mrcfile\n",
            "  Downloading mrcfile-1.4.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (1.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.1->MDAnalysis) (4.1.1)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from mmtf-python>=1.0.0->MDAnalysis) (1.0.4)\n",
            "Building wheels for collected packages: MDAnalysis\n",
            "  Building wheel for MDAnalysis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for MDAnalysis: filename=MDAnalysis-2.1.0-cp37-cp37m-linux_x86_64.whl size=4652236 sha256=744445583e89a093ac12f79e61a7b04687dd0a218f2679adbb0f1dcd9dec3f15\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/dd/6b/9d51e7216a401b71949467a123e3b2dffba11256346f7f7bda\n",
            "Successfully built MDAnalysis\n",
            "Installing collected packages: mrcfile, mmtf-python, gsd, GridDataFormats, biopython, MDAnalysis\n",
            "Successfully installed GridDataFormats-0.7.0 MDAnalysis-2.1.0 biopython-1.79 gsd-2.5.3 mmtf-python-1.1.3 mrcfile-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import MDAnalysis as mda\n",
        "from MDAnalysis.analysis import rdf"
      ],
      "metadata": {
        "id": "RV4F1NHf9WUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = mda.coordinates.XYZ.XYZReader(\"/content/lammps_run/water.xyz\")\n",
        "topology = mda.topology.XYZParser.XYZParser(\"/content/lammps_run/water.xyz\")\n",
        "\n",
        "u = mda.Universe(\"/content/lammps_run/water.xyz\")\n",
        "\n",
        "u.dimensions = [cell_vec_abc[0], cell_vec_abc[1],cell_vec_abc[2], 90., 90., 90. ]"
      ],
      "metadata": {
        "id": "LtqT9im4AC15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "O_at = u.select_atoms('name O')\n",
        "H_at = u.select_atoms('name H')\n",
        "\n",
        "Ordf = rdf.InterRDF(O_at, O_at,\n",
        "                    nbins=75,  # default\n",
        "                    range=(0.00001, 4.9),  # distance in angstroms\n",
        "                   )\n",
        "Ordf.run()\n",
        "\n",
        "OHrdf = rdf.InterRDF(O_at, H_at,\n",
        "                    nbins=75,  # default\n",
        "                    range=(0.00001, 4.9),  # distance in angstroms\n",
        "                   )\n",
        "OHrdf.run()\n",
        "\n",
        "HHrdf = rdf.InterRDF(H_at, H_at,\n",
        "                    nbins=75,  # default\n",
        "                    range=(0.00001, 4.9),  # distance in angstroms\n",
        "                   )\n",
        "HHrdf.run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkoMQR8ZA0Sc",
        "outputId": "2cf9cdf2-ef49-4f1f-8e06-ef449052f624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/MDAnalysis/coordinates/base.py:892: UserWarning: Reader has no dt information, set to 1.0 ps\n",
            "  warnings.warn(\"Reader has no dt information, set to 1.0 ps\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MDAnalysis.analysis.rdf.InterRDF at 0x7efe1c7a9ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(Ordf.bins, Ordf.rdf)\n",
        "plt.xlabel('Radius (angstrom)')\n",
        "plt.ylabel('Radial distribution')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "Og07Lts5C1q_",
        "outputId": "38c9faaf-658c-4633-ea66-c3be15855b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:417: DeprecationWarning: The `bins` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.bins` instead\n",
            "  warnings.warn(wmsg, DeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:425: DeprecationWarning: The `rdf` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.rdf` instead\n",
            "  warnings.warn(wmsg, DeprecationWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Radial distribution')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(OHrdf.bins, OHrdf.rdf)\n",
        "plt.xlabel('Radius (angstrom)')\n",
        "plt.ylabel('Radial distribution')"
      ],
      "metadata": {
        "id": "w8dxSzw_L_3z",
        "outputId": "83392ade-3ec2-4124-8553-cf3fb393f226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:417: DeprecationWarning: The `bins` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.bins` instead\n",
            "  warnings.warn(wmsg, DeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:425: DeprecationWarning: The `rdf` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.rdf` instead\n",
            "  warnings.warn(wmsg, DeprecationWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Radial distribution')"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(HHrdf.bins, HHrdf.rdf)\n",
        "plt.xlabel('Radius (angstrom)')\n",
        "plt.ylabel('Radial distribution')"
      ],
      "metadata": {
        "id": "YeT-d_udMAE0",
        "outputId": "d0fafc7b-e65a-481e-a070-ad06e985a68d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:417: DeprecationWarning: The `bins` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.bins` instead\n",
            "  warnings.warn(wmsg, DeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:425: DeprecationWarning: The `rdf` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.rdf` instead\n",
            "  warnings.warn(wmsg, DeprecationWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Radial distribution')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "my-short-nequip-tutorial.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "c9be9acec9edbd902b751bf46a8fbd7b71bbc5f0438c72d3ebaee4bffeb5e5e4"
    },
    "kernelspec": {
      "display_name": "Python 3.7.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cc9becc7b8e24b43b56fdb84be6d3f8a": {
          "model_module": "nglview-js-widgets",
          "model_name": "ColormakerRegistryModel",
          "model_module_version": "3.0.1",
          "state": {
            "_dom_classes": [],
            "_model_module": "nglview-js-widgets",
            "_model_module_version": "3.0.1",
            "_model_name": "ColormakerRegistryModel",
            "_msg_ar": [],
            "_msg_q": [],
            "_ready": false,
            "_view_count": null,
            "_view_module": "nglview-js-widgets",
            "_view_module_version": "3.0.1",
            "_view_name": "ColormakerRegistryView",
            "layout": "IPY_MODEL_acbf9169bb724c0eb59f017743bd244f"
          }
        },
        "47a25a9dba7942978aa05a745d615496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9a07ba1e9aa459f8973715dabbd2a87",
              "IPY_MODEL_58ff1b0607084dad9b9c4c8e44b40152"
            ],
            "layout": "IPY_MODEL_592569e635984eb4a6b9d82cd338a0af"
          }
        },
        "e9a07ba1e9aa459f8973715dabbd2a87": {
          "model_module": "nglview-js-widgets",
          "model_name": "NGLModel",
          "model_module_version": "3.0.1",
          "state": {
            "_camera_orientation": [
              18.783115818947113,
              0,
              0,
              0,
              0,
              18.783115818947113,
              0,
              0,
              0,
              0,
              18.783115818947113,
              0,
              -4.905500038526952,
              -4.932000007480383,
              -4.892499856650829,
              1
            ],
            "_camera_str": "orthographic",
            "_dom_classes": [],
            "_gui_theme": null,
            "_ibtn_fullscreen": "IPY_MODEL_7d3c120301504fa9b5c990350f04ad33",
            "_igui": null,
            "_iplayer": "IPY_MODEL_98dcd3152cbe484faea4dd71da6c00be",
            "_model_module": "nglview-js-widgets",
            "_model_module_version": "3.0.1",
            "_model_name": "NGLModel",
            "_ngl_color_dict": {},
            "_ngl_coordinate_resource": {},
            "_ngl_full_stage_parameters": {
              "impostor": true,
              "quality": "medium",
              "workerDefault": true,
              "sampleLevel": 0,
              "backgroundColor": "white",
              "rotateSpeed": 2,
              "zoomSpeed": 1.2,
              "panSpeed": 1,
              "clipNear": 0,
              "clipFar": 100,
              "clipDist": 0,
              "fogNear": 50,
              "fogFar": 100,
              "cameraFov": 40,
              "cameraEyeSep": 0.3,
              "cameraType": "orthographic",
              "lightColor": 14540253,
              "lightIntensity": 1,
              "ambientColor": 14540253,
              "ambientIntensity": 0.2,
              "hoverTimeout": 0,
              "tooltip": true,
              "mousePreset": "default"
            },
            "_ngl_msg_archive": [
              {
                "target": "Stage",
                "type": "call_method",
                "methodName": "loadFile",
                "reconstruc_color_scheme": false,
                "args": [
                  {
                    "type": "blob",
                    "data": "CRYST1    9.850    9.850    9.850  90.00  90.00  90.00 P 1\nMODEL     1\nATOM      1    H MOL     1       3.486   9.794   8.779  1.00  0.00           H  \nATOM      2    H MOL     1       4.653   9.231   7.816  1.00  0.00           H  \nATOM      3    H MOL     1       0.530   7.838   6.931  1.00  0.00           H  \nATOM      4    H MOL     1       9.006   7.162   7.052  1.00  0.00           H  \nATOM      5    H MOL     1       6.698   9.334   4.942  1.00  0.00           H  \nATOM      6    H MOL     1       8.146   9.809   5.464  1.00  0.00           H  \nATOM      7    H MOL     1       8.061   7.485   4.378  1.00  0.00           H  \nATOM      8    H MOL     1       9.116   6.499   4.666  1.00  0.00           H  \nATOM      9    H MOL     1       7.608   9.019   2.377  1.00  0.00           H  \nATOM     10    H MOL     1       9.056   9.582   2.794  1.00  0.00           H  \nATOM     11    H MOL     1       4.753   2.220   6.588  1.00  0.00           H  \nATOM     12    H MOL     1       3.694   1.303   7.070  1.00  0.00           H  \nATOM     13    H MOL     1       9.179   4.796   6.374  1.00  0.00           H  \nATOM     14    H MOL     1       8.798   3.567   7.287  1.00  0.00           H  \nATOM     15    H MOL     1       3.314   3.206   1.347  1.00  0.00           H  \nATOM     16    H MOL     1       1.740   3.087   1.077  1.00  0.00           H  \nATOM     17    H MOL     1       4.202   6.712   0.273  1.00  0.00           H  \nATOM     18    H MOL     1       5.114   6.186   9.097  1.00  0.00           H  \nATOM     19    H MOL     1       3.317   5.125   3.038  1.00  0.00           H  \nATOM     20    H MOL     1       2.163   5.589   3.867  1.00  0.00           H  \nATOM     21    H MOL     1       2.878   9.118   0.998  1.00  0.00           H  \nATOM     22    H MOL     1       3.093   0.921   0.934  1.00  0.00           H  \nATOM     23    H MOL     1       1.935   4.678   7.846  1.00  0.00           H  \nATOM     24    H MOL     1       2.767   3.621   6.917  1.00  0.00           H  \nATOM     25    H MOL     1       6.962   3.171   6.132  1.00  0.00           H  \nATOM     26    H MOL     1       6.827   1.604   5.938  1.00  0.00           H  \nATOM     27    H MOL     1       2.474   7.068   2.138  1.00  0.00           H  \nATOM     28    H MOL     1       1.851   7.032   0.689  1.00  0.00           H  \nATOM     29    H MOL     1       3.976   6.291   4.714  1.00  0.00           H  \nATOM     30    H MOL     1       5.090   6.585   5.947  1.00  0.00           H  \nATOM     31    H MOL     1       0.656   9.545   0.281  1.00  0.00           H  \nATOM     32    H MOL     1       0.078   8.755   8.760  1.00  0.00           H  \nATOM     33    H MOL     1       0.304   1.958   3.840  1.00  0.00           H  \nATOM     34    H MOL     1       8.784   2.697   3.526  1.00  0.00           H  \nATOM     35    H MOL     1       0.782   2.630   7.958  1.00  0.00           H  \nATOM     36    H MOL     1       9.770   1.394   8.432  1.00  0.00           H  \nATOM     37    H MOL     1       6.838   8.812   8.805  1.00  0.00           H  \nATOM     38    H MOL     1       6.771   9.006   7.203  1.00  0.00           H  \nATOM     39    H MOL     1       0.449   4.286   4.380  1.00  0.00           H  \nATOM     40    H MOL     1       1.223   4.811   5.650  1.00  0.00           H  \nATOM     41    H MOL     1       0.659   6.331   8.558  1.00  0.00           H  \nATOM     42    H MOL     1       0.840   5.175   9.596  1.00  0.00           H  \nATOM     43    H MOL     1       6.632   2.439   4.015  1.00  0.00           H  \nATOM     44    H MOL     1       7.261   1.271   3.042  1.00  0.00           H  \nATOM     45    H MOL     1       5.287   9.378   0.832  1.00  0.00           H  \nATOM     46    H MOL     1       5.847   7.923   0.758  1.00  0.00           H  \nATOM     47    H MOL     1       2.572   9.117   6.960  1.00  0.00           H  \nATOM     48    H MOL     1       2.668   7.845   6.117  1.00  0.00           H  \nATOM     49    H MOL     1       6.828   5.545   6.904  1.00  0.00           H  \nATOM     50    H MOL     1       6.262   6.849   7.335  1.00  0.00           H  \nATOM     51    H MOL     1       0.095   1.065   6.164  1.00  0.00           H  \nATOM     52    H MOL     1       0.829   9.783   5.438  1.00  0.00           H  \nATOM     53    H MOL     1       4.665   8.194   4.338  1.00  0.00           H  \nATOM     54    H MOL     1       5.234   8.825   2.953  1.00  0.00           H  \nATOM     55    H MOL     1       1.065   1.147   1.757  1.00  0.00           H  \nATOM     56    H MOL     1       1.926   0.655   2.906  1.00  0.00           H  \nATOM     57    H MOL     1       9.321   3.504   0.509  1.00  0.00           H  \nATOM     58    H MOL     1       9.795   3.728   1.977  1.00  0.00           H  \nATOM     59    H MOL     1       4.586   3.097   2.807  1.00  0.00           H  \nATOM     60    H MOL     1       5.405   4.080   1.907  1.00  0.00           H  \nATOM     61    H MOL     1       3.703   1.527   4.656  1.00  0.00           H  \nATOM     62    H MOL     1       4.243   0.501   3.660  1.00  0.00           H  \nATOM     63    H MOL     1       7.271   2.621   1.283  1.00  0.00           H  \nATOM     64    H MOL     1       8.005   1.565   0.365  1.00  0.00           H  \nATOM     65    O MOL     1       3.810   9.787   7.859  1.00  0.00           O  \nATOM     66    O MOL     1       9.694   7.537   7.591  1.00  0.00           O  \nATOM     67    O MOL     1       7.192   9.766   5.709  1.00  0.00           O  \nATOM     68    O MOL     1       8.242   6.762   5.015  1.00  0.00           O  \nATOM     69    O MOL     1       8.131   9.422   3.098  1.00  0.00           O  \nATOM     70    O MOL     1       3.767   2.095   6.472  1.00  0.00           O  \nATOM     71    O MOL     1       8.445   4.361   6.857  1.00  0.00           O  \nATOM     72    O MOL     1       2.632   2.661   0.908  1.00  0.00           O  \nATOM     73    O MOL     1       5.104   6.413   0.189  1.00  0.00           O  \nATOM     74    O MOL     1       2.893   5.947   3.296  1.00  0.00           O  \nATOM     75    O MOL     1       3.410   0.055   0.609  1.00  0.00           O  \nATOM     76    O MOL     1       1.878   4.016   7.124  1.00  0.00           O  \nATOM     77    O MOL     1       6.356   2.497   5.799  1.00  0.00           O  \nATOM     78    O MOL     1       2.483   7.475   1.262  1.00  0.00           O  \nATOM     79    O MOL     1       4.275   6.898   5.472  1.00  0.00           O  \nATOM     80    O MOL     1       0.057   9.543   9.385  1.00  0.00           O  \nATOM     81    O MOL     1       9.785   2.869   3.606  1.00  0.00           O  \nATOM     82    O MOL     1       9.736   2.229   7.927  1.00  0.00           O  \nATOM     83    O MOL     1       6.330   8.652   7.972  1.00  0.00           O  \nATOM     84    O MOL     1       0.808   5.089   4.786  1.00  0.00           O  \nATOM     85    O MOL     1       1.310   5.833   9.056  1.00  0.00           O  \nATOM     86    O MOL     1       7.018   2.242   3.108  1.00  0.00           O  \nATOM     87    O MOL     1       6.109   8.853   1.025  1.00  0.00           O  \nATOM     88    O MOL     1       2.035   8.538   6.351  1.00  0.00           O  \nATOM     89    O MOL     1       5.999   5.944   7.121  1.00  0.00           O  \nATOM     90    O MOL     1       0.016   0.468   5.417  1.00  0.00           O  \nATOM     91    O MOL     1       5.212   8.893   3.939  1.00  0.00           O  \nATOM     92    O MOL     1       1.085   0.402   2.393  1.00  0.00           O  \nATOM     93    O MOL     1       0.123   4.013   1.051  1.00  0.00           O  \nATOM     94    O MOL     1       4.577   3.970   2.343  1.00  0.00           O  \nATOM     95    O MOL     1       3.567   1.234   3.713  1.00  0.00           O  \nATOM     96    O MOL     1       7.650   2.505   0.392  1.00  0.00           O  \nENDMDL\n",
                    "binary": false
                  }
                ],
                "kwargs": {
                  "name": "nglview.adaptor.ASETrajectory",
                  "defaultRepresentation": false,
                  "ext": "pdb"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "setSize",
                "reconstruc_color_scheme": false,
                "args": [
                  "500px",
                  "500px"
                ],
                "kwargs": {}
              },
              {
                "component_index": 0,
                "target": "compList",
                "type": "call_method",
                "methodName": "addRepresentation",
                "reconstruc_color_scheme": false,
                "args": [
                  "unitcell"
                ],
                "kwargs": {
                  "sele": "all"
                }
              },
              {
                "component_index": 0,
                "target": "compList",
                "type": "call_method",
                "methodName": "addRepresentation",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill"
                ],
                "kwargs": {
                  "sele": "all"
                }
              },
              {
                "target": "Stage",
                "type": "call_method",
                "methodName": "setParameters",
                "reconstruc_color_scheme": false,
                "args": [],
                "kwargs": {
                  "cameraType": "orthographic"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "setParameters",
                "reconstruc_color_scheme": false,
                "args": [
                  {
                    "clipDist": 0
                  }
                ],
                "kwargs": {}
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.5,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.5,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.49999999999999994,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.49999999999999994,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              }
            ],
            "_ngl_original_stage_parameters": {
              "impostor": true,
              "quality": "medium",
              "workerDefault": true,
              "sampleLevel": 0,
              "backgroundColor": "white",
              "rotateSpeed": 2,
              "zoomSpeed": 1.2,
              "panSpeed": 1,
              "clipNear": 0,
              "clipFar": 100,
              "clipDist": 10,
              "fogNear": 50,
              "fogFar": 100,
              "cameraFov": 40,
              "cameraEyeSep": 0.3,
              "cameraType": "perspective",
              "lightColor": 14540253,
              "lightIntensity": 1,
              "ambientColor": 14540253,
              "ambientIntensity": 0.2,
              "hoverTimeout": 0,
              "tooltip": true,
              "mousePreset": "default"
            },
            "_ngl_repr_dict": {
              "0": {
                "0": {
                  "type": "unitcell",
                  "params": {
                    "lazy": false,
                    "visible": true,
                    "quality": "medium",
                    "radiusSize": 0.04924999783811635,
                    "sphereDetail": 1,
                    "radialSegments": 10,
                    "disableImpostor": false,
                    "radiusType": "vdw",
                    "radiusData": {},
                    "radiusScale": 1,
                    "defaultAssembly": "",
                    "clipNear": 0,
                    "clipRadius": 0,
                    "clipCenter": {
                      "x": 0,
                      "y": 0,
                      "z": 0
                    },
                    "flatShaded": false,
                    "opacity": 1,
                    "depthWrite": true,
                    "side": "double",
                    "wireframe": false,
                    "colorScheme": "element",
                    "colorScale": "",
                    "colorReverse": false,
                    "colorValue": "orange",
                    "colorMode": "hcl",
                    "roughness": 0.4,
                    "metalness": 0,
                    "diffuse": 16777215,
                    "diffuseInterior": false,
                    "useInteriorColor": true,
                    "interiorColor": 2236962,
                    "interiorDarkening": 0,
                    "matrix": {
                      "elements": [
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1
                      ]
                    },
                    "disablePicking": false,
                    "sele": "all"
                  }
                },
                "1": {
                  "type": "unitcell",
                  "params": {
                    "lazy": false,
                    "visible": true,
                    "quality": "medium",
                    "radiusSize": 0.04924999783811635,
                    "sphereDetail": 1,
                    "radialSegments": 10,
                    "disableImpostor": false,
                    "radiusType": "vdw",
                    "radiusData": {},
                    "radiusScale": 1,
                    "defaultAssembly": "",
                    "clipNear": 0,
                    "clipRadius": 0,
                    "clipCenter": {
                      "x": 0,
                      "y": 0,
                      "z": 0
                    },
                    "flatShaded": false,
                    "opacity": 1,
                    "depthWrite": true,
                    "side": "double",
                    "wireframe": false,
                    "colorScheme": "element",
                    "colorScale": "",
                    "colorReverse": false,
                    "colorValue": "orange",
                    "colorMode": "hcl",
                    "roughness": 0.4,
                    "metalness": 0,
                    "diffuse": 16777215,
                    "diffuseInterior": false,
                    "useInteriorColor": true,
                    "interiorColor": 2236962,
                    "interiorDarkening": 0,
                    "matrix": {
                      "elements": [
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1
                      ]
                    },
                    "disablePicking": false,
                    "sele": "all"
                  }
                },
                "2": {
                  "type": "spacefill",
                  "params": {
                    "lazy": false,
                    "visible": true,
                    "quality": "medium",
                    "sphereDetail": 1,
                    "disableImpostor": false,
                    "radiusType": "covalent",
                    "radiusData": {},
                    "radiusSize": 1,
                    "radiusScale": 0.5,
                    "assembly": "default",
                    "defaultAssembly": "",
                    "clipNear": 0,
                    "clipRadius": 0,
                    "clipCenter": {
                      "x": 0,
                      "y": 0,
                      "z": 0
                    },
                    "flatShaded": false,
                    "opacity": 1,
                    "depthWrite": true,
                    "side": "double",
                    "wireframe": false,
                    "colorScheme": "element",
                    "colorScale": "rainbow",
                    "colorReverse": false,
                    "colorValue": 9474192,
                    "colorMode": "hcl",
                    "roughness": 0.4,
                    "metalness": 0,
                    "diffuse": 16777215,
                    "diffuseInterior": false,
                    "useInteriorColor": true,
                    "interiorColor": 2236962,
                    "interiorDarkening": 0,
                    "matrix": {
                      "elements": [
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1
                      ]
                    },
                    "disablePicking": false,
                    "sele": "all"
                  }
                },
                "3": {
                  "type": "spacefill",
                  "params": {
                    "lazy": false,
                    "visible": true,
                    "quality": "medium",
                    "sphereDetail": 1,
                    "disableImpostor": false,
                    "radiusType": "covalent",
                    "radiusData": {},
                    "radiusSize": 1,
                    "radiusScale": 0.5,
                    "assembly": "default",
                    "defaultAssembly": "",
                    "clipNear": 0,
                    "clipRadius": 0,
                    "clipCenter": {
                      "x": 0,
                      "y": 0,
                      "z": 0
                    },
                    "flatShaded": false,
                    "opacity": 1,
                    "depthWrite": true,
                    "side": "double",
                    "wireframe": false,
                    "colorScheme": "element",
                    "colorScale": "rainbow",
                    "colorReverse": false,
                    "colorValue": 9474192,
                    "colorMode": "hcl",
                    "roughness": 0.4,
                    "metalness": 0,
                    "diffuse": 16777215,
                    "diffuseInterior": false,
                    "useInteriorColor": true,
                    "interiorColor": 2236962,
                    "interiorDarkening": 0,
                    "matrix": {
                      "elements": [
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1
                      ]
                    },
                    "disablePicking": false,
                    "sele": "all"
                  }
                }
              },
              "1": {}
            },
            "_ngl_serialize": false,
            "_ngl_version": "2.0.0-dev.36",
            "_ngl_view_id": [
              "13999E7B-9148-49E7-92E2-46201717671E"
            ],
            "_player_dict": {},
            "_scene_position": {},
            "_scene_rotation": {},
            "_synced_model_ids": [],
            "_synced_repr_model_ids": [],
            "_view_count": null,
            "_view_height": "",
            "_view_module": "nglview-js-widgets",
            "_view_module_version": "3.0.1",
            "_view_name": "NGLView",
            "_view_width": "",
            "background": "white",
            "frame": 0,
            "gui_style": null,
            "layout": "IPY_MODEL_6aca778059ab476cab3c176ee695f37b",
            "max_frame": 100,
            "n_components": 2,
            "picked": {}
          }
        },
        "58ff1b0607084dad9b9c4c8e44b40152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9eb9887d38684f13ad5c87072e844d36",
              "IPY_MODEL_c1a493ae0e6b49979c17cef26b0d56fb",
              "IPY_MODEL_dea23020ca834ed78fdcc89180ab22ca",
              "IPY_MODEL_1b384234293845a792b50bd221531a67"
            ],
            "layout": "IPY_MODEL_65ce5c450f734b0c9235ea86b9e211d5"
          }
        },
        "592569e635984eb4a6b9d82cd338a0af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eb9887d38684f13ad5c87072e844d36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "All",
              "O",
              "H"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Show",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_738c48653a7648e2bfeb0109e8adc758",
            "style": "IPY_MODEL_adad0778f13f41028b1bcb4e5a9108ca"
          }
        },
        "c1a493ae0e6b49979c17cef26b0d56fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              " ",
              "picking",
              "random",
              "uniform",
              "atomindex",
              "residueindex",
              "chainindex",
              "modelindex",
              "sstruc",
              "element",
              "resname",
              "bfactor",
              "hydrophobicity",
              "value",
              "volume",
              "occupancy"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Color scheme",
            "description_tooltip": null,
            "disabled": false,
            "index": 9,
            "layout": "IPY_MODEL_33221e01100a41bd950007e21eae5ddf",
            "style": "IPY_MODEL_ee2961a019c84c4bb4dd6ea7e9a8d137"
          }
        },
        "dea23020ca834ed78fdcc89180ab22ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FloatSliderView",
            "continuous_update": true,
            "description": "Ball size",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_5c12ad74630e4a168d0c5698575da8c7",
            "max": 1.5,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": ".2f",
            "step": 0.01,
            "style": "IPY_MODEL_d78f32a11efd4011a65d791151c1e774",
            "value": 0.49999999999999994
          }
        },
        "1b384234293845a792b50bd221531a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_f318ac22f8714c159134c3ded8d8745a",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 1,
            "style": "IPY_MODEL_62c5524f48ae4991973eb6247e87d894",
            "value": 0
          }
        },
        "65ce5c450f734b0c9235ea86b9e211d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "738c48653a7648e2bfeb0109e8adc758": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adad0778f13f41028b1bcb4e5a9108ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33221e01100a41bd950007e21eae5ddf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee2961a019c84c4bb4dd6ea7e9a8d137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c12ad74630e4a168d0c5698575da8c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d78f32a11efd4011a65d791151c1e774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "f318ac22f8714c159134c3ded8d8745a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62c5524f48ae4991973eb6247e87d894": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "6aca778059ab476cab3c176ee695f37b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d3c120301504fa9b5c990350f04ad33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "",
            "disabled": false,
            "icon": "compress",
            "layout": "IPY_MODEL_956c51f31fc24578b109ebbbd97e41ed",
            "style": "IPY_MODEL_64d43db436cc44fa8a365f89232f6f10",
            "tooltip": ""
          }
        },
        "98dcd3152cbe484faea4dd71da6c00be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18eb97f42d524a7ab6d8a042632571e6",
              "IPY_MODEL_c998bbe8b4f845af8a9c8fc06b88c646"
            ],
            "layout": "IPY_MODEL_0402fdc13c1c480d8505092db2b347f9"
          }
        },
        "650d3ad217db41a5bc5e05fcac2f7d4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73d242132293462c8db1eed7a9c330b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddb95deb0b7f4853bdf45b4c0f5e81de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a8ac9a2fb9944f8bc72b7ecd578621a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "18eb97f42d524a7ab6d8a042632571e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PlayModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PlayModel",
            "_playing": false,
            "_repeat": false,
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PlayView",
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "interval": 100,
            "layout": "IPY_MODEL_650d3ad217db41a5bc5e05fcac2f7d4d",
            "max": 100,
            "min": 0,
            "show_repeat": true,
            "step": 1,
            "style": "IPY_MODEL_73d242132293462c8db1eed7a9c330b4",
            "value": 0
          }
        },
        "c998bbe8b4f845af8a9c8fc06b88c646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_ddb95deb0b7f4853bdf45b4c0f5e81de",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 1,
            "style": "IPY_MODEL_5a8ac9a2fb9944f8bc72b7ecd578621a",
            "value": 0
          }
        },
        "0402fdc13c1c480d8505092db2b347f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "956c51f31fc24578b109ebbbd97e41ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "34px"
          }
        },
        "64d43db436cc44fa8a365f89232f6f10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}