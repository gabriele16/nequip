{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFpAi8g9XmUU"
      },
      "source": [
        "# Molecular Dynamics with NequIP \n",
        "\n",
        "### Authors: Simon Batzner, Albert Musaelian, Lixin Sun, Anders Johansson, Boris Kozinsky"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m1W9W9yvuKA"
      },
      "source": [
        "### Tutorial Modified by Gabriele Tocci"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QipwlJ4nvuKA"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabriele16/nequip/blob/main/colab/my-short-nequip-tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7bW4JWmmuyD",
        "outputId": "64dfbc0b-a1d9-45c7-b817-83d9677a1d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.10\n",
            "  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.3 MB/s eta 0:00:37tcmalloc: large alloc 1147494400 bytes == 0x66432000 @  0x7fb984eaa615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10) (4.1.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZOLIFOJZaeZ5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import os\n",
        "\n",
        "data_dir = '/content/nequip/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZpOvFtImsy2",
        "outputId": "30e1f4e8-4d97-4637-9f8f-eed3c537d4be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.0+cu102\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "print(torch. __version__)\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7Uh7nyHnR-w",
        "outputId": "f9576b92-1915-4c85-9598-a0c4ddb7721b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIMyrDOEm2IB",
        "outputId": "257b944c-5081-4660-e4ac-294b3de658cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'lammps': No such file or directory\n",
            "Cloning into 'lammps'...\n",
            "remote: Enumerating objects: 11732, done.\u001b[K\n",
            "remote: Counting objects: 100% (11732/11732), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8605/8605), done.\u001b[K\n",
            "remote: Total 11732 (delta 3940), reused 6322 (delta 2928), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (11732/11732), 110.00 MiB | 20.83 MiB/s, done.\n",
            "Resolving deltas: 100% (3940/3940), done.\n",
            "Note: checking out '7586adbb6a61254125992709ef2fda9134cfca6c'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "Checking out files: 100% (11058/11058), done.\n",
            "Cloning into 'pair_nequip'...\n",
            "remote: Enumerating objects: 418, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 418 (delta 79), reused 85 (delta 65), pack-reused 302\u001b[K\n",
            "Receiving objects: 100% (418/418), 427.38 KiB | 9.09 MiB/s, done.\n",
            "Resolving deltas: 100% (207/207), done.\n",
            "Copying files...\n",
            "Updating CMakeLists.txt...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "!rm -r lammps\n",
        "!git clone -b \"stable_29Sep2021_update2\" --depth 1 \"https://github.com/lammps/lammps.git\"\n",
        "!git clone https://github.com/mir-group/pair_nequip\n",
        "!cd pair_nequip && ./patch_lammps.sh /content/lammps/\n",
        "!cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "100Be8B6m5am"
      },
      "outputs": [],
      "source": [
        "!cp /content/pair_nequip/*.cpp /content/lammps/src/\n",
        "!cp /content/pair_nequip/*.h /content/lammps/src/\n",
        "! sed -i 's/CMAKE_CXX_STANDARD 11/CMAKE_CXX_STANDARD 14/g'  /content/lammps/cmake/CMakeLists.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlgRSmyom9VQ",
        "outputId": "203f4e9e-1312-43ad-ec0c-78457f488e17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.7/dist-packages (2019.0)\n",
            "Collecting mkl-include\n",
            "  Downloading mkl_include-2022.1.0-py2.py3-none-manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: intel-openmp in /usr/local/lib/python3.7/dist-packages (from mkl) (2022.1.0)\n",
            "Installing collected packages: mkl-include\n",
            "Successfully installed mkl-include-2022.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mkl mkl-include"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8vY6rwbnEgK",
        "outputId": "ebc2bc0b-1551-40b2-85bc-02e629cac058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.17.1\") \n",
            "-- Appending /usr/local/cuda/lib64/stubs to CMAKE_LIBRARY_PATH: /usr/local/cuda/lib64/stubs\n",
            "-- Running check for auto-generated files from make-based build system\n",
            "-- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so (found version \"3.1\") \n",
            "-- Found MPI: TRUE (found version \"3.1\")  \n",
            "-- Looking for C++ include omp.h\n",
            "-- Looking for C++ include omp.h - found\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
            "-- Found JPEG: /usr/lib/x86_64-linux-gnu/libjpeg.so (found version \"80\") \n",
            "-- Found PNG: /usr/lib/x86_64-linux-gnu/libpng.so (found version \"1.6.34\") \n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
            "-- Found GZIP: /bin/gzip  \n",
            "-- Found FFMPEG: /usr/bin/ffmpeg  \n",
            "-- Looking for C++ include cmath\n",
            "-- Looking for C++ include cmath - found\n",
            "-- Generating style headers...\n",
            "-- Generating package headers...\n",
            "-- Generating lmpinstalledpkgs.h...\n",
            "-- Could NOT find ClangFormat (missing: ClangFormat_EXECUTABLE) (Required is at least version \"8.0\")\n",
            "-- The following tools and libraries have been found and configured:\n",
            " * Git\n",
            " * MPI\n",
            " * OpenMP\n",
            " * JPEG\n",
            " * PNG\n",
            " * ZLIB\n",
            "\n",
            "-- <<< Build configuration >>>\n",
            "   Operating System: Linux Ubuntu 18.04\n",
            "   Build type:       RelWithDebInfo\n",
            "   Install path:     /root/.local\n",
            "   Generator:        Unix Makefiles using /usr/bin/make\n",
            "-- Enabled packages: <None>\n",
            "-- <<< Compilers and Flags: >>>\n",
            "-- C++ Compiler:     /usr/bin/c++\n",
            "      Type:          GNU\n",
            "      Version:       7.5.0\n",
            "      C++ Flags:     -O2 -g -DNDEBUG\n",
            "      Defines:       LAMMPS_SMALLBIG;LAMMPS_MEMALIGN=64;LAMMPS_OMP_COMPAT=3;LAMMPS_JPEG;LAMMPS_PNG;LAMMPS_GZIP;LAMMPS_FFMPEG\n",
            "-- <<< Linker flags: >>>\n",
            "-- Executable name:  lmp\n",
            "-- Static library flags:    \n",
            "-- <<< MPI flags >>>\n",
            "-- MPI_defines:      MPICH_SKIP_MPICXX;OMPI_SKIP_MPICXX;_MPICC_H\n",
            "-- MPI includes:     /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include;/usr/lib/x86_64-linux-gnu/openmpi/include\n",
            "-- MPI libraries:    /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so;\n",
            "-- Looking for C++ include pthread.h\n",
            "-- Looking for C++ include pthread.h - found\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
            "-- Looking for pthread_create in pthreads\n",
            "-- Looking for pthread_create in pthreads - not found\n",
            "-- Looking for pthread_create in pthread\n",
            "-- Looking for pthread_create in pthread - found\n",
            "-- Found Threads: TRUE  \n",
            "-- Found CUDA: /usr/local/cuda (found version \"11.1\") \n",
            "-- Caffe2: CUDA detected: 11.1\n",
            "-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\n",
            "-- Caffe2: CUDA toolkit directory: /usr/local/cuda\n",
            "-- Caffe2: Header version is: 11.1\n",
            "-- Found CUDNN: /usr/lib/x86_64-linux-gnu/libcudnn.so  \n",
            "-- Found cuDNN: v7.6.5  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)\n",
            "-- /usr/local/cuda/lib64/libnvrtc.so shorthash is 3a20f2b6\n",
            "-- Autodetected CUDA architecture(s):  6.0\n",
            "-- Added CUDA NVCC flags for: -gencode;arch=compute_60,code=sm_60\n",
            "\u001b[33mCMake Warning at /usr/local/lib/python3.7/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\n",
            "  static library kineto_LIBRARY-NOTFOUND not found.\n",
            "Call Stack (most recent call first):\n",
            "  /usr/local/lib/python3.7/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:127 (append_torchlib_if_found)\n",
            "  CMakeLists.txt:922 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch: /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so  \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/lammps/build\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/variable.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/citeme.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/comm.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/compute.h\u001b[0m\n",
            "[  2%] Built target variable.h\n",
            "[  2%] Built target citeme.h\n",
            "[  2%] Built target comm.h\n",
            "[  2%] Built target compute.h\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/domain.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/dihedral.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/error.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/fix.h\u001b[0m\n",
            "[  2%] Built target domain.h\n",
            "[  2%] Built target dihedral.h\n",
            "[  2%] Built target error.h\n",
            "[  2%] Built target fix.h\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/force.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/group.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/improper.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/input.h\u001b[0m\n",
            "[  3%] Built target force.h\n",
            "[  3%] Built target group.h\n",
            "[  3%] Built target improper.h\n",
            "[  3%] Built target input.h\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/info.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/kspace.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lammps.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lattice.h\u001b[0m\n",
            "[  4%] Built target info.h\n",
            "[  4%] Built target kspace.h\n",
            "[  4%] Built target lammps.h\n",
            "[  4%] Built target lattice.h\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lmppython.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lmptype.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/memory.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/library.h\u001b[0m\n",
            "[  6%] Built target lmppython.h\n",
            "[  6%] Built target lmptype.h\n",
            "[  6%] Built target memory.h\n",
            "[  6%] Built target library.h\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/modify.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/neighbor.h\u001b[0m\n",
            "-- Git Directory: /content/lammps/.git\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/angle.h\u001b[0m\n",
            "[  6%] Built target modify.h\n",
            "[  6%] Built target neighbor.h\n",
            "[  6%] Built target angle.h\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/atom.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/neigh_list.h\u001b[0m\n",
            "[  6%] Built target atom.h\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/bond.h\u001b[0m\n",
            "[  6%] Built target neigh_list.h\n",
            "[  6%] Built target bond.h\n",
            "-- Generating lmpgitversion.h...\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/output.h\u001b[0m\n",
            "[  7%] Built target gitversion\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/pair.h\u001b[0m\n",
            "[  7%] Built target output.h\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/pointers.h\u001b[0m\n",
            "[  7%] Built target pair.h\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/region.h\u001b[0m\n",
            "[  7%] Built target pointers.h\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/timer.h\u001b[0m\n",
            "[  8%] Built target region.h\n",
            "[  8%] Built target timer.h\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/universe.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/update.h\u001b[0m\n",
            "[  8%] Built target universe.h\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/utils.h\u001b[0m\n",
            "[  8%] Built target update.h\n",
            "[  8%] Built target utils.h\n",
            "[  8%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_deprecated.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_hybrid.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_zero.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/arg_info.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_map.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_atomic.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_body.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_charge.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_ellipsoid.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_hybrid.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_line.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_sphere.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_tri.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/balance.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/body.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_deprecated.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_hybrid.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_zero.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/change_box.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/citeme.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_brick.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_tiled.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_aggregate_atom.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle_local.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angmom_chunk.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond_local.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centro_atom.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centroid_stress_atom.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_atom.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_spread_atom.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cluster_atom.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cna_atom.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com_chunk.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_coord_atom.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_deprecated.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral_local.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole_chunk.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_displace_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_fragment_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_global_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_group_group.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration_chunk.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_heat_flux.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper_local.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_inertia_chunk.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke_atom.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd_chunk.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_omega_chunk.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_orientorder_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair_local.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe_atom.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pressure.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_atom.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_chunk.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_local.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_rdf.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_chunk.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_region.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_slice.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_stress_atom.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_chunk.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_com.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_deform.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_partial.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_profile.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_ramp.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_region.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_sphere.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_torque_chunk.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vacf.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vcm_chunk.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_atoms.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_bonds.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_box.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_atoms.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_bonds.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/deprecated.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_deprecated.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_hybrid.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_zero.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/displace_atoms.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/domain.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_atom.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_cfg.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_custom.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_deprecated.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_image.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_local.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_movie.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_xyz.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/error.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/finish.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_adapt.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_addforce.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_atom.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_chunk.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_correlate.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo_weight.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_time.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_aveforce.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_balance.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_box_relax.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deform.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deposit.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deprecated.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dt_reset.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dummy.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_efield.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_enforce2d.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_evaporate.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_external.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_gravity.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_group.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_halt.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_heat.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_indent.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_langevin.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_lineforce.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_minimize.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_momentum.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_move.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_neigh_history.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh_sphere.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph_sphere.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt_sphere.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_limit.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_noforce.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_sphere.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sllod.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sphere.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_planeforce.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_press_berendsen.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_print.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_property_atom.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_read_restart.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_recenter.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_respa.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_restrain.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_setforce.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_chunk.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_self.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_force.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_state.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_berendsen.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_rescale.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_thermal_conductivity.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_vector.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_viscous.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_harmonic.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj1043.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj126.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj93.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_morse.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_reflect.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_region.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_format.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_os.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/force.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/gridcomm.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/group.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/hashlittle.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/image.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_group.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_neigh.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_store.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_time.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_var.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_deprecated.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_hybrid.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_zero.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/info.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/input.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/integrate.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/irregular.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace_deprecated.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lammps.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lattice.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/library.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lmppython.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_eigen.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_extra.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_special.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/memory.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_cg.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_fire.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_fire_old.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_hftn.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_linesearch.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_quickmin.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_sd.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/minimize.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/modify.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/molecule.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_page.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_pool_chunk.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_multi.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_standard.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_list.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_request.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neighbor.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_copy.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin_atomonly.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin_ghost.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_multi.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_multi_old.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_nsq.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_nsq_ghost.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_atomonly_newton.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newtoff.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newtoff_ghost.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newton.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newton_tri.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newtoff.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newton.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newton_tri.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newtoff.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newton.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newton_tri.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newtoff.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newtoff_ghost.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newton.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newtoff.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newton.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newton_tri.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_nsq_newtoff.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_nsq_newton.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newtoff.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newton.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newton_tri.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newtoff.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newton.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newton_tri.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newtoff.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newton.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newton_tri.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_nsq_newtoff.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_nsq_newton.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_halffull_newtoff.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_halffull_newton.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_respa.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on_oneside.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_bin_2d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_bin_3d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_ghost_bin_2d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_ghost_bin_3d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_2d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_3d.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_old_2d.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_old_3d.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_2d.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_2d_tri.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_3d.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_3d_tri.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_2d.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_2d_tri.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_3d.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_3d_tri.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_2d.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_2d_tri.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_3d.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_3d_tri.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_all.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_partial.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_template.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_all.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_partial.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_template.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_all.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_partial.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_template.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_all.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_partial.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_template.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/output.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_born.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck_coul_cut.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_cut.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_debye.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_dsf.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_wolf.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_deprecated.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_overlay.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_scaled.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut_coul_cut.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_expand.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_morse.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_nequip.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_soft.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_table.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_yukawa.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zbl.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zero.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/potential_file_reader.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/procmap.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_mars.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_park.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rcb.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_data.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_dump.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_restart.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_native.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_xyz.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_block.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cone.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cylinder.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_deprecated.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_intersect.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_plane.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_prism.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_sphere.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_union.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/replicate.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rerun.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atom_ids.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_mol_ids.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/respa.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/run.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/set.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/special.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/table_file_reader.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tabular_function.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/text_file_reader.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/thermo.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/timer.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tokenizer.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/universe.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/update.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/utils.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/variable.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/lammps/src/variable.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint LAMMPS_NS::Variable::next(int, char**)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/lammps/src/variable.cpp:714:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "         \u001b[01;35m\u001b[Kfread(buf,1,64,fp)\u001b[m\u001b[K;\n",
            "         \u001b[01;35m\u001b[K~~~~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/velocity.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/verlet.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_coeff.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_data.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_dump.cpp.o\u001b[0m\n",
            "[100%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_restart.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX static library liblammps.a\u001b[0m\n",
            "[100%] Built target lammps\n",
            "[100%] \u001b[32mBuilding CXX object CMakeFiles/lmp.dir/content/lammps/src/main.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable lmp\u001b[0m\n",
            "[100%] Built target lmp\n"
          ]
        }
      ],
      "source": [
        "!cd lammps && mkdir -p build && cd build && cmake ../cmake -DCMAKE_PREFIX_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` && make -j4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone --depth 1 \"https://github.com/gabriele16/nequip.git\"\n",
        "! cd nequip && git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoYyUGf70c4H",
        "outputId": "24377250-a3e3-4698-b459-97a8efc5d379"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'nequip' already exists and is not an empty directory.\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (4/4), done.\n",
            "From https://github.com/gabriele16/nequip\n",
            "   577d450..a0a6ef2  main       -> origin/main\n",
            "Updating 577d450..a0a6ef2\n",
            "Fast-forward\n",
            " configs/my-example.yaml      | 153 \u001b[31m-------------------------------------------\u001b[m\n",
            " configs/my-full-example.yaml |   4 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 2 files changed, 2 insertions(+), 155 deletions(-)\n",
            " delete mode 100644 configs/my-example.yaml\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHCh2aC7WfKj",
        "outputId": "f022ec6f-93ab-4ad8-8d68-9415abda3ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nequip'...\n",
            "remote: Enumerating objects: 186, done.\u001b[K\n",
            "remote: Counting objects: 100% (186/186), done.\u001b[K\n",
            "remote: Compressing objects: 100% (180/180), done.\u001b[K\n",
            "remote: Total 186 (delta 8), reused 87 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (186/186), 39.51 MiB | 44.75 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n",
            "._AIMD_data\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n",
            "AIMD_data/\n",
            "AIMD_data/._WATER-frc-10k-1.xyz\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n",
            "AIMD_data/WATER-frc-10k-1.xyz\n",
            "AIMD_data/._celldata.dat\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n",
            "AIMD_data/celldata.dat\n",
            "AIMD_data/._WATER-pos-10k-1.xyz\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n",
            "AIMD_data/WATER-pos-10k-1.xyz\n"
          ]
        }
      ],
      "source": [
        "! tar -xzvf  /content/nequip/data/AIMD_data.tar.gz -C /content/nequip/data/\n",
        "! ls /content/nequip/data/AIMD_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J51CA0Bod1Jv",
        "outputId": "d6dd3ade-038b-41a1-aeba-4f5439d80747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 36.1 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.6.0-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=2ea05ea088edd6c32b57d9f49a439aff5408510fe144cd27a58fe9f14969a1c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.6.0 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.21\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./nequip\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (1.21.6)\n",
            "Collecting ase\n",
            "  Downloading ase-3.22.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (4.64.0)\n",
            "Requirement already satisfied: torch!=1.9.0,<=1.12,>=1.8 in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (1.10.0)\n",
            "Collecting e3nn<0.6.0,>=0.3.5\n",
            "  Downloading e3nn-0.5.0-py3-none-any.whl (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (3.13)\n",
            "Collecting torch-runstats>=0.2.0\n",
            "  Downloading torch_runstats-0.2.0-py3-none-any.whl (8.1 kB)\n",
            "Collecting torch-ema>=0.3.0\n",
            "  Downloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
            "Collecting scikit_learn<=1.0.1\n",
            "  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 87.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (4.1.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.7.1)\n",
            "Collecting opt-einsum-fx>=0.1.4\n",
            "  Downloading opt_einsum_fx-0.1.4-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (21.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (3.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn<=1.0.1->nequip==0.5.5) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn<=1.0.1->nequip==0.5.5) (3.1.0)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from ase->nequip==0.5.5) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (1.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1.0->ase->nequip==0.5.5) (1.15.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.2.1)\n",
            "Building wheels for collected packages: nequip\n",
            "  Building wheel for nequip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nequip: filename=nequip-0.5.5-py3-none-any.whl size=138681 sha256=fee15393833fd41707a34b08b06509571f32373ef3bb62fab716ae2e5ef665be\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vu7pjmlq/wheels/a8/8f/18/b30c4402c2d6ab52853310650b85822afe26aaae5f6d00356b\n",
            "Successfully built nequip\n",
            "Installing collected packages: opt-einsum-fx, torch-runstats, torch-ema, scikit-learn, e3nn, ase, nequip\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "Successfully installed ase-3.22.1 e3nn-0.5.0 nequip-0.5.5 opt-einsum-fx-0.1.4 scikit-learn-1.0.1 torch-ema-0.3 torch-runstats-0.2.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9e9a5ac190>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# install wandb\n",
        "!pip install wandb\n",
        "# install nequip\n",
        "!pip install nequip/\n",
        "# fix colab imports\n",
        "import site\n",
        "site.main()\n",
        "# set to allow anonymous WandB\n",
        "import os\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "import numpy as np\n",
        "from ase.io import read, write\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZixXbCiPcMGg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "24568af79df546058302ee1cc7ff62d5"
          ]
        },
        "outputId": "313c8198-ab8c-4029-c91e-832d8effda72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nglview\n",
            "  Downloading nglview-3.0.3.tar.gz (5.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7 MB 4.5 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jupyterlab-widgets in /usr/local/lib/python3.7/dist-packages (from nglview) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nglview) (1.21.6)\n",
            "Requirement already satisfied: ipywidgets>=7 in /usr/local/lib/python3.7/dist-packages (from nglview) (7.7.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (3.6.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (4.10.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (5.5.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (5.4.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (5.1.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (0.2.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7->nglview) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7->nglview) (5.3.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (57.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (0.7.5)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7->nglview) (2.15.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7->nglview) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7->nglview) (4.10.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (4.1.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (5.7.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (3.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7->nglview) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7->nglview) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (2.11.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7->nglview) (23.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7->nglview) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (2.0.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (5.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.5.1)\n",
            "Building wheels for collected packages: nglview\n",
            "  Building wheel for nglview (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nglview: filename=nglview-3.0.3-py3-none-any.whl size=8057551 sha256=e0fe8fda0334f73bd8328575ddd3c337f0c94d2a14a5b1c72b1d89285c73c6eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/0c/49/c6f79d8edba8fe89752bf20de2d99040bfa57db0548975c5d5\n",
            "Successfully built nglview\n",
            "Installing collected packages: nglview\n",
            "Successfully installed nglview-3.0.3\n",
            "Enabling notebook extension nglview-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/etc/jupyter/nbconfig/notebook.json\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/etc/jupyter/nbconfig/notebook.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24568af79df546058302ee1cc7ff62d5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip3 install nglview\n",
        "!jupyter-nbextension enable nglview --py --sys-prefix\n",
        "import nglview as nv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jDJ9Re0arOb0"
      },
      "outputs": [],
      "source": [
        "def MD_reader_xyz(f, data_dir, no_skip = 0):\n",
        "  filename = os.path.join(data_dir, f)\n",
        "  fo = open(filename, 'r')\n",
        "  natoms_str = fo.read().rsplit(' i = ')[0]\n",
        "  natoms = int(natoms_str.split('\\n')[0])\n",
        "  fo.close()  \n",
        "  fo = open(filename, 'r')\n",
        "  samples = fo.read().split(natoms_str)[1:]\n",
        "  steps = []\n",
        "  xyz = []\n",
        "  temperatures = []\n",
        "  energies = []\n",
        "  for sample in samples[::no_skip]:\n",
        "     entries = sample.split('\\n')[:-1]\n",
        "     energies.append(float(entries[0].split(\"=\")[-1]))\n",
        "     temp = np.array([list(map(float, lv.split()[1:])) for lv in entries[1:]])\n",
        "     xyz.append(temp[:,:])\n",
        "  return natoms_str, np.array(xyz), np.array(energies)     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DziQT_zjMKS4"
      },
      "outputs": [],
      "source": [
        "from ase.build import sort\n",
        "def MD_writer_xyz(positions,forces,cell_vec_abc,energies,\n",
        "                  data_dir,f,  conv_frc = 1.0 , conv_ener = 1.0 ):\n",
        "\n",
        "  filename = os.path.join(data_dir, f)\n",
        "  fo = open(filename, 'w')\n",
        "\n",
        "  for it, frame in enumerate(positions):\n",
        "    natoms = len(frame)\n",
        "    fo.write(\"{:5d}\\n\".format(natoms))\n",
        "    fo.write('Lattice=\"{:.5f} 0.0 0.0 0.0 {:.5f} 0.0 0.0 0.0 {:.5f}\" \\\n",
        "    Properties=\"species:S:1:pos:R:3:forces:R:3\" \\\n",
        "    energy={:.10f} pbc=\"T T T\"\\n'.format(cell_vec_abc[0],cell_vec_abc[1],cell_vec_abc[2],energies[it]*conv_ener)    \n",
        "    )\n",
        "    if it%1000 == 0.0:\n",
        "      print(it)\n",
        "    \n",
        "    sorted_frame = sort(frame)\n",
        "    sorted_forces = sort(forces[it])\n",
        "\n",
        "    fo.write(\"\".join(\"{:8s} {:.8f} {:16.8f} {:16.8f}\\\n",
        "     {:16.8f} {:16.8f} {:16.8f}\\n\".format(sorted_frame[iat].symbol,\n",
        "                                          sorted_frame[iat].position[0],\n",
        "                                          sorted_frame[iat].position[1],\n",
        "                                          sorted_frame[iat].position[2],\n",
        "                                          sorted_forces[iat].position[0]*conv_frc,\n",
        "                                          sorted_forces[iat].position[1]*conv_frc,\n",
        "                                          sorted_forces[iat].position[2]*conv_frc)\n",
        "                                          for iat in range(len(frame))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2U5i3IZqLBI",
        "outputId": "b8b88865-6d33-4cdc-eef3-00114337cc88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.85, 9.85, 9.85])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "def read_cell(f,data_dir):\n",
        "  filename = os.path.join(data_dir,f)\n",
        "  fo = open(filename,'r')\n",
        "  cell_list_abc = fo.read().split('\\n')[:-1]\n",
        "  cell_vec_abc = np.array([list(map(float, lv.split())) for lv in cell_list_abc]).squeeze()\n",
        "  return(cell_vec_abc)\n",
        "\n",
        "cell_vec_abc = read_cell('celldata.dat',data_dir + '/AIMD_data')\n",
        "cell_vec_abc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read positions, energies, forces from of a 32 water molecules box in .xyz format generated with CP2K using the SCAN functional"
      ],
      "metadata": {
        "id": "wUcVzREa7XNR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "X9fcRMYnu5-V"
      },
      "outputs": [],
      "source": [
        "wat_traj = read(data_dir +'/AIMD_data/WATER-pos-10k-1.xyz',index=':')\n",
        "wat_frc = read(data_dir + '/AIMD_data/WATER-frc-10k-1.xyz', index=':')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The reader below is required to get the energies."
      ],
      "metadata": {
        "id": "tFK_vOgL7ul8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jT7B4ryYu82t"
      },
      "outputs": [],
      "source": [
        "natoms, positions, energies = MD_reader_xyz('WATER-pos-10k-1.xyz', data_dir + '/AIMD_data/', no_skip=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The writer below is useful to convert from 2 separate .xyz files for positions and forces to an .extxyz file that can be read by nequip, also including cell and energies"
      ],
      "metadata": {
        "id": "BivsnMaz8NoG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgmmJAU5l5F2",
        "outputId": "66d86a0c-34f9-42ef-82fe-0b0ddf5848b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n",
            "8700\n",
            "8800\n",
            "8900\n",
            "9000\n",
            "9100\n",
            "9200\n",
            "9300\n",
            "9400\n",
            "9500\n",
            "9600\n",
            "9700\n",
            "9800\n",
            "9900\n"
          ]
        }
      ],
      "source": [
        "MD_writer_xyz(wat_traj, wat_frc, cell_vec_abc, energies, data_dir + '/AIMD_data/', 'wat_pos_frc-10k.extxyz',conv_frc = 1.0, conv_ener = 27.211399)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPqnt-SAXyvL"
      },
      "source": [
        "### Turn on GPU\n",
        "\n",
        "Make sure Runtime --> Change runtime type is set to GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HDmxkn3z8_m"
      },
      "source": [
        "## 3 Steps: \n",
        "* Train: using a data set, train the neural network 🧠 \n",
        "* Deploy: convert the Python-based model into a stand-alone potential file for fast execution ⚡\n",
        "* Run: run Molecular Dynamics, Monte Carlo, Structural Minimization, ...  with it in LAMMPS 🏃"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OD71eeDz7dA"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/all.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62aEgq6QYFIn"
      },
      "source": [
        "### Train a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELdBzH_8z4_2"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/train.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KuOIippfVfd"
      },
      "source": [
        "Here, we will train a NequIP potential on the following system\n",
        "\n",
        "* Water\n",
        "* sampled at T=300K from AIMD\n",
        "* Using 1000 training configurations\n",
        "* The units of the reference data are in eV and A."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mgoydrJW5lg0",
        "outputId": "a72e3799-4a21-494a-f4a0-207d43824880"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.5.5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import nequip\n",
        "nequip.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFCszShRk2RP",
        "outputId": "4992b399-bbf4-41e9-a019-4b2782aaa90c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'root': 'results/water', 'run_name': 'example-run-water', 'seed': 123, 'dataset_seed': 456, 'append': True, 'default_dtype': 'float32', 'allow_tf32': False, 'r_max': 4.0, 'num_layers': 4, 'l_max': 1, 'parity': True, 'num_features': 32, 'nonlinearity_type': 'gate', 'resnet': False, 'nonlinearity_scalars': {'e': 'silu', 'o': 'tanh'}, 'nonlinearity_gates': {'e': 'silu', 'o': 'tanh'}, 'num_basis': 8, 'BesselBasis_trainable': True, 'PolynomialCutoff_p': 6, 'invariant_layers': 2, 'invariant_neurons': 64, 'avg_num_neighbors': 'auto', 'use_sc': True, 'dataset': 'ase', 'dataset_file_name': './nequip/data/AIMD_data/wat_pos_frc-10k.extxyz', 'ase_args': {'format': 'extxyz'}, 'include_keys': ['user_label'], 'key_mapping': {'user_label': 'label0'}, 'chemical_symbols': ['H', 'O'], 'verbose': 'info', 'log_batch_freq': 1, 'log_epoch_freq': 1, 'save_checkpoint_freq': -1, 'save_ema_checkpoint_freq': -1, 'n_train': 100, 'n_val': 50, 'learning_rate': 0.005, 'batch_size': 5, 'validation_batch_size': 10, 'max_epochs': 100000, 'train_val_split': 'random', 'shuffle': True, 'metrics_key': 'validation_loss', 'use_ema': True, 'ema_decay': 0.99, 'ema_use_num_updates': True, 'report_init_validation': True, 'early_stopping_patiences': {'validation_loss': 50}, 'early_stopping_delta': {'validation_loss': 0.005}, 'early_stopping_cumulative_delta': False, 'early_stopping_lower_bounds': {'LR': 1e-05}, 'early_stopping_upper_bounds': {'cumulative_wall': 1e+100}, 'loss_coeffs': {'forces': 1, 'total_energy': [1, 'PerAtomMSELoss']}, 'metrics_components': [['forces', 'mae'], ['forces', 'rmse'], ['forces', 'mae', {'PerSpecies': True, 'report_per_component': False}], ['forces', 'rmse', {'PerSpecies': True, 'report_per_component': False}], ['total_energy', 'mae'], ['total_energy', 'mae', {'PerAtom': True}]], 'optimizer_name': 'Adam', 'optimizer_amsgrad': False, 'optimizer_betas': (0.9, 0.999), 'optimizer_eps': 1e-08, 'optimizer_weight_decay': 0, 'max_gradient_norm': None, 'lr_scheduler_name': 'ReduceLROnPlateau', 'lr_scheduler_patience': 100, 'lr_scheduler_factor': 0.5, 'per_species_rescale_scales_trainable': False, 'per_species_rescale_shifts_trainable': False, 'per_species_rescale_shifts': 'dataset_per_atom_total_energy_mean', 'per_species_rescale_scales': 'dataset_forces_rms', 'global_rescale_shift': None, 'global_rescale_scale': 'dataset_forces_rms', 'global_rescale_shift_trainable': False, 'global_rescale_scale_trainable': False}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "from nequip.utils import Config\n",
        "config = Config.from_file('/content/nequip/configs/my-full-example.yaml')\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukSnt_QD5avu",
        "outputId": "c4a939eb-5363-4e98-c232-78b20555c871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch device: cuda\n",
            "Processing dataset...\n",
            "Loaded data: Batch(atomic_numbers=[960000, 1], batch=[960000], cell=[10000, 3, 3], edge_cell_shift=[25556838, 3], edge_index=[2, 25556838], forces=[960000, 3], pbc=[10000, 3], pos=[960000, 3], ptr=[10001], total_energy=[10000, 1])\n",
            "Cached processed data to disk\n",
            "Done!\n",
            "Successfully loaded the data set of type ASEDataset(10000)...\n",
            "Replace string dataset_forces_rms to 0.7736424803733826\n",
            "Replace string dataset_per_atom_total_energy_mean to -156.0919189453125\n",
            "Atomic outputs are scaled by: [H, O: 0.773642], shifted by [H, O: -156.091919].\n",
            "Replace string dataset_forces_rms to 0.7736424803733826\n",
            "Initially outputs are globally scaled by: 0.7736424803733826, total_energy are globally shifted by None.\n",
            "Successfully built the network...\n",
            "Number of weights: 154200\n",
            "! Starting training ...\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      0     1          1.1         1.09       0.0142        0.606        0.807        0.515        0.787        0.651        0.688            1        0.846         8.82       0.0919\n",
            "      0     2         1.02         1.01       0.0135        0.577        0.776        0.491        0.749         0.62        0.662        0.965        0.813         8.61       0.0897\n",
            "      0     3        0.992        0.979       0.0132        0.574        0.765        0.494        0.734        0.614        0.659        0.943        0.801         8.52       0.0888\n",
            "      0     4        0.973         0.96       0.0134        0.564        0.758         0.48        0.733        0.606        0.653        0.933        0.793         8.58       0.0894\n",
            "      0     5        0.969        0.956       0.0131        0.571        0.756        0.491        0.732        0.612         0.65        0.934        0.792          8.5       0.0885\n",
            "\n",
            "\n",
            "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Initial Validation          0    4.666    0.005        0.998       0.0135         1.01        0.578        0.773        0.494        0.747        0.621        0.663        0.956        0.809         8.61       0.0897\n",
            "Wall time: 4.6667078390000825\n",
            "! Best model        0    1.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      1     1        0.886        0.873       0.0135         0.55        0.723        0.481        0.687        0.584        0.633        0.875        0.754          8.6       0.0896\n",
            "      1     2         1.02         1.01      0.00641        0.576        0.778        0.498        0.733        0.616        0.664        0.968        0.816         5.93       0.0618\n",
            "      1     3         1.05         1.05      0.00212        0.586        0.792        0.499        0.759        0.629        0.675        0.986         0.83         3.37       0.0351\n",
            "      1     4         1.01         1.01     0.000403        0.573        0.777        0.492        0.735        0.613        0.672        0.954        0.813          1.4       0.0146\n",
            "      1     5        0.962        0.962     4.99e-05        0.576        0.759        0.499         0.73        0.615        0.655        0.933        0.794         0.48        0.005\n",
            "      1     6        0.956        0.956      2.6e-05        0.573        0.756        0.491        0.737        0.614        0.647        0.938        0.792        0.331      0.00344\n",
            "      1     7        0.904        0.904     6.72e-05        0.553        0.736        0.472        0.715        0.594        0.632        0.908         0.77         0.53      0.00552\n",
            "      1     8        0.891        0.891     3.63e-05         0.53         0.73        0.457        0.677        0.567        0.626        0.903        0.764        0.414      0.00431\n",
            "      1     9        0.826        0.826     0.000137        0.533        0.703        0.457        0.685        0.571        0.597        0.878        0.738        0.814      0.00848\n",
            "      1    10        0.893        0.893     0.000659        0.558        0.731        0.474        0.726          0.6        0.627        0.903        0.765         1.89       0.0197\n",
            "      1    11        0.769        0.767      0.00131        0.505        0.678        0.435        0.646        0.541        0.581        0.839         0.71         2.69        0.028\n",
            "      1    12        0.654        0.651        0.003        0.474        0.624        0.404        0.615        0.509        0.526        0.785        0.655         4.05       0.0422\n",
            "      1    13        0.596        0.591      0.00545        0.458        0.595        0.394        0.586         0.49         0.51        0.735        0.623         5.47        0.057\n",
            "      1    14         0.59        0.581      0.00867        0.445         0.59        0.381        0.574        0.477          0.5        0.737        0.619          6.9       0.0719\n",
            "      1    15        0.528        0.516       0.0123         0.41        0.556        0.343        0.544        0.444        0.463        0.705        0.584         8.23       0.0858\n",
            "      1    16         0.46        0.445       0.0148        0.394        0.516        0.331         0.52        0.425        0.434         0.65        0.542         9.04       0.0942\n",
            "      1    17        0.346        0.333       0.0131        0.345        0.446        0.303        0.428        0.365        0.388        0.545        0.466         8.49       0.0884\n",
            "      1    18        0.335        0.325      0.00993         0.34        0.441        0.302        0.415        0.359        0.386        0.535         0.46         7.38       0.0769\n",
            "      1    19        0.295        0.293      0.00247         0.33        0.419        0.287        0.416        0.351         0.36        0.516        0.438         3.67       0.0382\n",
            "      1    20        0.273        0.272     0.000177        0.313        0.404        0.279        0.381         0.33        0.355        0.487        0.421        0.966       0.0101\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      1     1        0.298        0.298      0.00064        0.326        0.422        0.281        0.415        0.348        0.357        0.529        0.443         1.84       0.0192\n",
            "      1     2        0.273        0.272     0.000525        0.312        0.404        0.272        0.391        0.332        0.348        0.497        0.422         1.65       0.0172\n",
            "      1     3        0.253        0.253     0.000496        0.304        0.389        0.272        0.368         0.32        0.343        0.467        0.405         1.62       0.0169\n",
            "      1     4        0.264        0.263     0.000612        0.311        0.397        0.274        0.385        0.329        0.344        0.487        0.415          1.8       0.0187\n",
            "      1     5        0.265        0.264     0.000581        0.308        0.398        0.266        0.392        0.329        0.338        0.496        0.417         1.77       0.0184\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               1    9.005    0.005        0.707      0.00473        0.712        0.481        0.651        0.414        0.616        0.515        0.558        0.805        0.681         4.03        0.042\n",
            "! Validation          1    9.005    0.005         0.27     0.000571        0.271        0.312        0.402        0.273         0.39        0.332        0.346        0.495        0.421         1.73       0.0181\n",
            "Wall time: 9.006260200000042\n",
            "! Best model        1    0.271\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      2     1         0.27        0.266      0.00439        0.312        0.399        0.277        0.382         0.33         0.35        0.482        0.416         4.87       0.0507\n",
            "      2     2        0.263        0.254      0.00901        0.299         0.39        0.269        0.358        0.313        0.351        0.457        0.404         7.02       0.0731\n",
            "      2     3        0.212          0.2       0.0126        0.265        0.346         0.23        0.335        0.283        0.297        0.426        0.362         8.32       0.0866\n",
            "      2     4        0.207        0.199       0.0088        0.266        0.345        0.233        0.333        0.283        0.298        0.424        0.361         6.94       0.0722\n",
            "      2     5        0.204        0.202      0.00223        0.268        0.347        0.232        0.338        0.285        0.299        0.428        0.364         3.49       0.0364\n",
            "      2     6        0.161        0.161      0.00015        0.239         0.31         0.21        0.299        0.254        0.269        0.379        0.324        0.698      0.00727\n",
            "      2     7        0.167        0.166     0.000341        0.246        0.315        0.213        0.312        0.263        0.272        0.389         0.33         1.21       0.0126\n",
            "      2     8        0.152        0.152     0.000398         0.23        0.301        0.197        0.295        0.246        0.253         0.38        0.316         1.25        0.013\n",
            "      2     9        0.174        0.173     0.000927        0.246        0.322        0.211        0.317        0.264        0.272        0.404        0.338         2.24       0.0233\n",
            "      2    10        0.154        0.154     0.000708        0.236        0.303        0.204          0.3        0.252         0.26        0.375        0.317         1.93       0.0201\n",
            "      2    11        0.146        0.145     0.000941        0.231        0.295        0.204        0.284        0.244        0.256         0.36        0.308         2.23       0.0232\n",
            "      2    12        0.163        0.162     0.000592        0.246        0.312        0.215        0.309        0.262        0.271         0.38        0.325         1.78       0.0186\n",
            "      2    13        0.142        0.141     0.000745        0.226         0.29        0.195        0.289        0.242        0.245        0.364        0.305         1.99       0.0207\n",
            "      2    14         0.13        0.128      0.00154        0.214        0.277        0.184        0.274        0.229        0.233        0.349        0.291         2.88         0.03\n",
            "      2    15        0.138        0.136        0.002         0.22        0.286        0.194        0.273        0.233        0.247        0.351        0.299          3.3       0.0343\n",
            "      2    16        0.147        0.146      0.00121        0.224        0.295        0.189        0.292        0.241        0.242         0.38        0.311         2.54       0.0264\n",
            "      2    17        0.137        0.136     0.000426        0.222        0.286        0.194        0.279        0.236        0.244        0.355          0.3         1.49       0.0155\n",
            "      2    18        0.137        0.137     3.27e-05         0.22        0.286         0.19        0.278        0.234        0.244        0.356          0.3        0.354      0.00368\n",
            "      2    19        0.122        0.122     4.66e-05        0.208         0.27        0.182        0.262        0.222        0.233        0.332        0.282          0.4      0.00417\n",
            "      2    20        0.127        0.126     0.000242        0.211        0.275        0.179        0.274        0.227        0.231        0.347        0.289         1.14       0.0119\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      2     1        0.128        0.128     4.03e-05        0.211        0.277        0.181        0.272        0.226        0.229        0.353        0.291        0.434      0.00453\n",
            "      2     2        0.132        0.132     3.82e-05        0.215        0.281        0.186        0.275         0.23        0.239         0.35        0.294        0.421      0.00438\n",
            "      2     3        0.114        0.114     2.34e-05        0.203        0.261        0.178        0.253        0.216        0.226        0.319        0.272         0.29      0.00303\n",
            "      2     4         0.12         0.12     4.84e-05        0.205        0.268        0.179        0.257        0.218        0.229        0.333        0.281        0.392      0.00409\n",
            "      2     5        0.128        0.127     3.44e-05        0.211        0.276        0.179        0.274        0.227        0.229        0.353        0.291        0.366      0.00381\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               2   12.502    0.005        0.165      0.00237        0.168        0.241        0.314         0.21        0.304        0.257        0.271        0.388        0.329          2.8       0.0292\n",
            "! Validation          2   12.502    0.005        0.124     3.69e-05        0.124        0.209        0.273        0.181        0.266        0.223         0.23        0.342        0.286        0.381      0.00397\n",
            "Wall time: 12.503225468999972\n",
            "! Best model        2    0.124\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      3     1        0.123        0.122      0.00115        0.205        0.271        0.169        0.278        0.223        0.221        0.349        0.285         2.48       0.0258\n",
            "      3     2        0.126        0.124      0.00129        0.209        0.273        0.181        0.265        0.223        0.231        0.341        0.286         2.64       0.0275\n",
            "      3     3        0.128        0.127     0.000888        0.212        0.275        0.186        0.265        0.225        0.239        0.336        0.288         2.16       0.0225\n",
            "      3     4        0.104        0.104      0.00017        0.192        0.249        0.165        0.245        0.205        0.212        0.311        0.261        0.896      0.00933\n",
            "      3     5        0.122        0.122     0.000147        0.204         0.27        0.176        0.261        0.218        0.233        0.332        0.283        0.862      0.00897\n",
            "      3     6       0.0961       0.0959     0.000121        0.184         0.24        0.157        0.238        0.198        0.199        0.305        0.252        0.733      0.00764\n",
            "      3     7        0.104        0.103      0.00029        0.193        0.249        0.166        0.246        0.206        0.211        0.311        0.261         1.15        0.012\n",
            "      3     8        0.114        0.114     0.000186        0.199        0.261        0.171        0.256        0.214        0.221        0.325        0.273        0.974       0.0101\n",
            "      3     9        0.109        0.109     4.67e-05        0.196        0.255        0.169        0.252         0.21         0.22        0.314        0.267        0.334      0.00348\n",
            "      3    10       0.0941       0.0939     0.000194        0.184        0.237        0.162        0.228        0.195        0.205         0.29        0.248         1.01       0.0105\n",
            "      3    11        0.106        0.106     0.000142        0.189        0.252        0.164        0.239        0.201        0.213        0.314        0.264        0.785      0.00818\n",
            "      3    12       0.0905       0.0905      1.8e-05        0.181        0.233        0.157        0.228        0.193        0.199        0.289        0.244         0.25      0.00261\n",
            "      3    13        0.102        0.102     4.55e-05        0.193        0.247        0.169         0.24        0.204        0.214        0.302        0.258        0.487      0.00507\n",
            "      3    14       0.0898       0.0896     0.000191         0.18        0.232         0.16        0.221        0.191        0.205        0.277        0.241        0.886      0.00923\n",
            "      3    15       0.0931        0.093     3.23e-05        0.185        0.236        0.159        0.236        0.197        0.201        0.294        0.247        0.389      0.00405\n",
            "      3    16       0.0853       0.0851     0.000143        0.174        0.226        0.146         0.23        0.188        0.188        0.287        0.237        0.841      0.00876\n",
            "      3    17       0.0794       0.0791     0.000293        0.169        0.218        0.149         0.21        0.179        0.194        0.259        0.226         1.21       0.0126\n",
            "      3    18       0.0847       0.0844     0.000327        0.174        0.225        0.149        0.226        0.187        0.191         0.28        0.236         1.25        0.013\n",
            "      3    19        0.098        0.098     3.51e-05        0.187        0.242        0.161        0.238          0.2        0.206        0.302        0.254        0.382      0.00398\n",
            "      3    20       0.0859       0.0858      8.8e-05        0.175        0.227         0.15        0.226        0.188        0.193        0.282        0.238        0.651      0.00678\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      3     1       0.0955       0.0955     2.48e-05        0.182        0.239        0.155        0.234        0.195        0.199        0.304        0.251        0.297      0.00309\n",
            "      3     2       0.0993       0.0992     3.77e-05        0.186        0.244        0.161        0.238        0.199         0.21          0.3        0.255        0.397      0.00413\n",
            "      3     3       0.0819       0.0819     2.85e-05        0.171        0.221        0.151        0.212        0.182        0.194        0.268        0.231        0.308      0.00321\n",
            "      3     4       0.0904       0.0904     1.67e-05        0.177        0.233        0.154        0.224        0.189        0.199        0.289        0.244        0.283      0.00294\n",
            "      3     5       0.0939       0.0939     1.97e-05        0.181        0.237        0.154        0.235        0.195        0.198        0.301        0.249        0.247      0.00258\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               3   15.993    0.005        0.101      0.00029        0.102        0.189        0.246        0.163        0.241        0.202         0.21        0.306        0.258         1.02       0.0106\n",
            "! Validation          3   15.993    0.005       0.0922     2.55e-05       0.0922         0.18        0.235        0.155        0.228        0.192          0.2        0.292        0.246        0.306      0.00319\n",
            "Wall time: 15.993968591999874\n",
            "! Best model        3    0.092\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      4     1       0.0771        0.077     4.64e-05        0.165        0.215        0.142        0.211        0.177        0.183        0.268        0.225        0.426      0.00443\n",
            "      4     2       0.0834       0.0834     5.59e-05        0.175        0.223         0.15        0.227        0.188        0.187        0.282        0.235        0.492      0.00513\n",
            "      4     3       0.0764       0.0763     8.15e-05        0.169        0.214        0.146        0.215        0.181        0.183        0.265        0.224         0.61      0.00635\n",
            "      4     4       0.0777       0.0775      0.00013        0.166        0.215        0.148        0.202        0.175        0.195        0.252        0.223        0.794      0.00827\n",
            "      4     5       0.0776       0.0775     9.59e-05        0.166        0.215        0.144        0.208        0.176        0.184        0.267        0.226        0.687      0.00716\n",
            "      4     6       0.0768       0.0762     0.000655        0.163        0.214        0.142        0.205        0.174        0.183        0.264        0.223         1.89       0.0197\n",
            "      4     7       0.0787       0.0782     0.000552        0.166        0.216        0.144        0.211        0.177        0.185        0.268        0.226         1.72       0.0179\n",
            "      4     8       0.0756       0.0756     3.21e-05        0.165        0.213        0.143        0.209        0.176        0.183        0.262        0.223        0.362      0.00377\n",
            "      4     9        0.074       0.0738     0.000186        0.161         0.21        0.138        0.207        0.172        0.177        0.265        0.221        0.938      0.00977\n",
            "      4    10       0.0778       0.0773     0.000587        0.166        0.215        0.142        0.215        0.178        0.185        0.265        0.225         1.77       0.0184\n",
            "      4    11       0.0726       0.0725     7.86e-05        0.159        0.208        0.135        0.207        0.171        0.175        0.263        0.219        0.616      0.00642\n",
            "      4    12       0.0609       0.0609     6.43e-05        0.148        0.191        0.128        0.189        0.158        0.164        0.235          0.2        0.546      0.00568\n",
            "      4    13       0.0593       0.0592     6.29e-05        0.146        0.188        0.127        0.185        0.156        0.162        0.231        0.197        0.564      0.00587\n",
            "      4    14       0.0751       0.0751        1e-05        0.163        0.212        0.141        0.207        0.174        0.182        0.262        0.222        0.199      0.00207\n",
            "      4    15       0.0636       0.0634     0.000251        0.152        0.195        0.131        0.195        0.163        0.168         0.24        0.204         1.16       0.0121\n",
            "      4    16        0.078       0.0776      0.00035        0.163        0.216        0.142        0.206        0.174        0.185        0.267        0.226         1.33       0.0138\n",
            "      4    17        0.053        0.053     5.18e-05        0.137        0.178         0.12         0.17        0.145        0.158        0.213        0.185        0.436      0.00454\n",
            "      4    18        0.065       0.0648     0.000272        0.151        0.197         0.13        0.195        0.162        0.168        0.244        0.206         1.17       0.0122\n",
            "      4    19       0.0543       0.0542     0.000138         0.14         0.18        0.124        0.172        0.148        0.158        0.218        0.188        0.865      0.00901\n",
            "      4    20       0.0703       0.0702     9.19e-05         0.16        0.205         0.14        0.201         0.17         0.18        0.248        0.214         0.63      0.00656\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      4     1       0.0703       0.0703     1.69e-05        0.156        0.205        0.135          0.2        0.167        0.174        0.257        0.215        0.258      0.00269\n",
            "      4     2       0.0727       0.0727     2.65e-05        0.159        0.209         0.14        0.199        0.169        0.184        0.251        0.217        0.314      0.00327\n",
            "      4     3       0.0591       0.0591      1.3e-05        0.146        0.188         0.13        0.178        0.154        0.166        0.225        0.196        0.229      0.00238\n",
            "      4     4       0.0644       0.0644     1.15e-05        0.151        0.196        0.131        0.191        0.161        0.168        0.243        0.206        0.205      0.00213\n",
            "      4     5       0.0637       0.0637     1.16e-05        0.149        0.195        0.127        0.192         0.16        0.167        0.243        0.205        0.194      0.00202\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               4   19.524    0.005       0.0712      0.00019       0.0714        0.159        0.206        0.138        0.202         0.17        0.178        0.254        0.216         0.86      0.00896\n",
            "! Validation          4   19.524    0.005        0.066     1.59e-05        0.066        0.152        0.199        0.132        0.192        0.162        0.172        0.244        0.208         0.24       0.0025\n",
            "Wall time: 19.524888186999988\n",
            "! Best model        4    0.066\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      5     1        0.066       0.0657     0.000242        0.151        0.198        0.133        0.186        0.159        0.174         0.24        0.207          1.1       0.0114\n",
            "      5     2        0.068        0.068     5.88e-05        0.155        0.202        0.136        0.193        0.164        0.175        0.246        0.211         0.52      0.00542\n",
            "      5     3       0.0584       0.0581     0.000316        0.142        0.186        0.122         0.18        0.151        0.161         0.23        0.195         1.24       0.0129\n",
            "      5     4        0.055       0.0545     0.000555        0.139        0.181         0.12        0.177        0.148        0.154        0.224        0.189         1.73        0.018\n",
            "      5     5       0.0494       0.0493      0.00014        0.133        0.172        0.117        0.166        0.141        0.149        0.209        0.179        0.838      0.00873\n",
            "      5     6       0.0516       0.0515     8.63e-05        0.136        0.176        0.118        0.173        0.146        0.153        0.214        0.183        0.622      0.00648\n",
            "      5     7       0.0525       0.0522     0.000237        0.138        0.177        0.119        0.178        0.148        0.149        0.222        0.186         1.11       0.0116\n",
            "      5     8       0.0561       0.0557       0.0004        0.142        0.183        0.125        0.175         0.15        0.161         0.22         0.19         1.47       0.0154\n",
            "      5     9       0.0497       0.0496     1.23e-05        0.133        0.172        0.116        0.166        0.141        0.152        0.208         0.18        0.225      0.00235\n",
            "      5    10       0.0543       0.0541     0.000179         0.14         0.18        0.124        0.171        0.148        0.159        0.216        0.187         0.97       0.0101\n",
            "      5    11       0.0636       0.0634     0.000142         0.15        0.195         0.13        0.189        0.159        0.169        0.239        0.204        0.873      0.00909\n",
            "      5    12       0.0526       0.0526     8.21e-06        0.138        0.177        0.122        0.171        0.146        0.157        0.213        0.185        0.161      0.00167\n",
            "      5    13       0.0531       0.0529      0.00026        0.137        0.178        0.122        0.167        0.145        0.158        0.212        0.185         1.17       0.0122\n",
            "      5    14       0.0441       0.0438     0.000331        0.127        0.162        0.112        0.158        0.135        0.142        0.196        0.169         1.35        0.014\n",
            "      5    15       0.0456       0.0454     0.000246        0.127        0.165        0.111         0.16        0.136        0.142        0.203        0.172         1.16       0.0121\n",
            "      5    16       0.0502       0.0502     7.92e-06        0.135        0.173        0.123        0.158        0.141         0.16        0.198        0.179        0.161      0.00168\n",
            "      5    17       0.0464       0.0457     0.000677        0.126        0.165        0.108        0.162        0.135        0.137        0.211        0.174         1.92         0.02\n",
            "      5    18       0.0435       0.0423      0.00118        0.122        0.159        0.108        0.149        0.129        0.142        0.188        0.165         2.55       0.0265\n",
            "      5    19       0.0496       0.0493     0.000295        0.134        0.172        0.117        0.169        0.143        0.151        0.208        0.179         1.26       0.0131\n",
            "      5    20       0.0523       0.0523     1.46e-05        0.135        0.177        0.121        0.163        0.142        0.157        0.211        0.184        0.278       0.0029\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      5     1       0.0568       0.0568     2.64e-05        0.141        0.184        0.123        0.177         0.15        0.159        0.227        0.193        0.315      0.00328\n",
            "      5     2        0.058       0.0579     3.88e-05        0.143        0.186        0.127        0.175        0.151        0.167         0.22        0.193        0.409      0.00426\n",
            "      5     3       0.0471       0.0471     5.04e-05         0.13        0.168        0.117        0.156        0.136        0.151        0.198        0.174         0.47       0.0049\n",
            "      5     4        0.051        0.051     3.31e-05        0.135        0.175        0.117         0.17        0.144        0.151        0.214        0.183         0.37      0.00385\n",
            "      5     5       0.0494       0.0494     4.72e-05        0.132        0.172        0.114        0.167        0.141        0.149         0.21         0.18        0.458      0.00477\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               5   23.026    0.005       0.0528     0.000269       0.0531        0.137        0.178         0.12        0.171        0.145        0.155        0.216        0.186         1.04       0.0108\n",
            "! Validation          5   23.026    0.005       0.0524     3.92e-05       0.0525        0.136        0.177        0.119        0.169        0.144        0.155        0.214        0.185        0.404      0.00421\n",
            "Wall time: 23.027241813000046\n",
            "! Best model        5    0.052\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      6     1       0.0443       0.0442     0.000106        0.125        0.163        0.108         0.16        0.134         0.14        0.201         0.17        0.747      0.00778\n",
            "      6     2       0.0422        0.042     0.000278        0.124        0.158        0.109        0.152        0.131        0.138        0.193        0.165         1.24       0.0129\n",
            "      6     3       0.0457       0.0455     0.000197        0.128        0.165        0.115        0.155        0.135        0.149        0.192        0.171         1.02       0.0107\n",
            "      6     4       0.0522       0.0522     8.08e-06        0.136        0.177        0.121        0.164        0.143        0.157        0.211        0.184        0.207      0.00215\n",
            "      6     5       0.0449       0.0448     7.61e-05        0.127        0.164        0.113        0.156        0.135        0.144        0.197        0.171        0.577      0.00601\n",
            "      6     6       0.0492       0.0492      9.5e-06         0.13        0.172        0.115         0.16        0.137        0.153        0.203        0.178         0.21      0.00219\n",
            "      6     7       0.0431        0.043     1.13e-05        0.126        0.161        0.113        0.151        0.132        0.143        0.191        0.167        0.225      0.00234\n",
            "      6     8       0.0443        0.044     0.000308        0.125        0.162        0.108        0.157        0.133        0.139          0.2         0.17         1.27       0.0133\n",
            "      6     9       0.0378       0.0377     6.51e-05        0.118         0.15        0.103        0.148        0.126        0.132        0.181        0.157        0.596      0.00621\n",
            "      6    10       0.0475       0.0474     1.03e-05        0.129        0.169        0.112        0.162        0.137        0.147        0.205        0.176        0.182       0.0019\n",
            "      6    11       0.0531        0.053     5.86e-05        0.136        0.178        0.119        0.169        0.144        0.157        0.215        0.186        0.452      0.00471\n",
            "      6    12       0.0462       0.0458     0.000342        0.129        0.166        0.115        0.157        0.136        0.146        0.199        0.173         1.36       0.0141\n",
            "      6    13       0.0472       0.0468     0.000329         0.13        0.167        0.115        0.158        0.137        0.149          0.2        0.174         1.29       0.0134\n",
            "      6    14        0.045        0.045     2.35e-05        0.128        0.164        0.114        0.156        0.135        0.147        0.193         0.17         0.32      0.00333\n",
            "      6    15       0.0397       0.0397     9.42e-06        0.118        0.154        0.104        0.146        0.125        0.133        0.189        0.161        0.189      0.00197\n",
            "      6    16       0.0443       0.0442     7.84e-05        0.126        0.163        0.111        0.156        0.134        0.142        0.197         0.17        0.647      0.00674\n",
            "      6    17       0.0428       0.0425     0.000283        0.122         0.16        0.112        0.144        0.128        0.147        0.182        0.164         1.24       0.0129\n",
            "      6    18       0.0357       0.0357     1.28e-05        0.115        0.146        0.102        0.141        0.121        0.129        0.175        0.152        0.229      0.00239\n",
            "      6    19       0.0489       0.0487     0.000193        0.131        0.171        0.116        0.161        0.139        0.151        0.205        0.178         1.01       0.0106\n",
            "      6    20       0.0429       0.0429     1.11e-05        0.125         0.16        0.111        0.153        0.132        0.143         0.19        0.167        0.214      0.00223\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      6     1       0.0488       0.0488     7.92e-06        0.131        0.171        0.115        0.163        0.139        0.149        0.208        0.179        0.174      0.00181\n",
            "      6     2        0.049        0.049      1.6e-05        0.132        0.171        0.118         0.16        0.139        0.154        0.202        0.178        0.247      0.00257\n",
            "      6     3       0.0404       0.0404     1.73e-05        0.121        0.156        0.109        0.144        0.127         0.14        0.182        0.161        0.265      0.00276\n",
            "      6     4       0.0433       0.0433     1.15e-05        0.125        0.161        0.108        0.157        0.133         0.14        0.197        0.168        0.189      0.00197\n",
            "      6     5        0.042        0.042     1.91e-05        0.121        0.158        0.106        0.152        0.129        0.139        0.191        0.165        0.269       0.0028\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               6   26.492    0.005       0.0447     0.000121       0.0449        0.126        0.164        0.112        0.155        0.134        0.145        0.196         0.17        0.661      0.00689\n",
            "! Validation          6   26.492    0.005       0.0447     1.44e-05       0.0447        0.126        0.164        0.111        0.155        0.133        0.144        0.196         0.17        0.229      0.00238\n",
            "Wall time: 26.492954499000007\n",
            "! Best model        6    0.045\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      7     1       0.0443       0.0442     0.000102        0.125        0.163        0.112        0.152        0.132        0.146        0.192        0.169        0.736      0.00767\n",
            "      7     2       0.0428       0.0428     5.53e-05        0.123         0.16        0.107        0.156        0.131        0.139        0.196        0.167        0.528       0.0055\n",
            "      7     3       0.0455       0.0453     0.000154        0.127        0.165         0.11        0.161        0.135        0.141        0.204        0.172        0.919      0.00957\n",
            "      7     4       0.0437       0.0437     5.96e-05        0.125        0.162        0.115        0.146         0.13        0.147        0.187        0.167        0.521      0.00543\n",
            "      7     5       0.0417       0.0414     0.000278         0.12        0.157        0.108        0.145        0.127        0.143        0.183        0.163         1.19       0.0124\n",
            "      7     6       0.0383       0.0377     0.000555        0.118         0.15        0.105        0.142        0.124        0.136        0.176        0.156         1.75       0.0182\n",
            "      7     7       0.0407       0.0406     2.56e-05        0.121        0.156        0.109        0.145        0.127        0.138        0.186        0.162         0.34      0.00355\n",
            "      7     8       0.0394       0.0393     5.61e-06        0.118        0.153        0.105        0.143        0.124        0.137        0.182         0.16        0.134      0.00139\n",
            "      7     9       0.0413       0.0413     1.73e-05         0.12        0.157        0.109        0.143        0.126        0.139        0.188        0.164        0.263      0.00274\n",
            "      7    10       0.0417       0.0415     0.000165        0.122        0.158        0.106        0.154         0.13        0.135        0.195        0.165        0.924      0.00962\n",
            "      7    11       0.0487       0.0484     0.000287        0.131         0.17         0.12        0.153        0.137        0.159        0.191        0.175         1.25       0.0131\n",
            "      7    12       0.0407       0.0407     2.22e-05         0.12        0.156        0.107        0.147        0.127         0.14        0.185        0.162        0.312      0.00326\n",
            "      7    13       0.0411       0.0408     0.000251         0.12        0.156        0.109        0.143        0.126        0.142        0.182        0.162         1.17       0.0122\n",
            "      7    14       0.0446       0.0446     6.65e-06        0.127        0.163        0.114        0.154        0.134        0.146        0.193         0.17        0.154      0.00161\n",
            "      7    15       0.0349       0.0349     5.35e-05        0.112        0.144       0.0978         0.14        0.119        0.125        0.177        0.151        0.528       0.0055\n",
            "      7    16       0.0436       0.0436     7.89e-06        0.124        0.162        0.115        0.143        0.129        0.151         0.18        0.166        0.169      0.00176\n",
            "      7    17       0.0332       0.0331     4.79e-05        0.109        0.141       0.0959        0.135        0.115        0.122        0.172        0.147        0.473      0.00493\n",
            "      7    18       0.0403       0.0402     6.02e-05        0.121        0.155         0.11        0.142        0.126         0.14        0.181        0.161        0.563      0.00586\n",
            "      7    19       0.0379       0.0378     0.000113        0.116         0.15        0.101        0.145        0.123        0.131        0.183        0.157        0.777      0.00809\n",
            "      7    20       0.0397       0.0395     0.000183        0.118        0.154        0.104        0.146        0.125        0.134        0.187         0.16        0.973       0.0101\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      7     1       0.0437       0.0437     5.74e-06        0.124        0.162        0.109        0.155        0.132        0.141        0.197        0.169        0.154      0.00161\n",
            "      7     2        0.043        0.043     1.13e-05        0.124         0.16         0.11        0.151        0.131        0.143         0.19        0.167        0.199      0.00207\n",
            "      7     3       0.0362       0.0362     1.29e-05        0.114        0.147        0.103        0.138         0.12        0.132        0.174        0.153        0.233      0.00243\n",
            "      7     4       0.0385       0.0385     8.15e-06        0.117        0.152        0.102        0.148        0.125        0.132        0.185        0.159        0.155      0.00162\n",
            "      7     5       0.0376       0.0376     1.46e-05        0.115         0.15        0.101        0.143        0.122        0.132        0.181        0.156        0.235      0.00244\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               7   29.987    0.005       0.0411     0.000123       0.0412        0.121        0.157        0.108        0.147        0.127         0.14        0.186        0.163        0.684      0.00712\n",
            "! Validation          7   29.987    0.005       0.0398     1.05e-05       0.0398        0.119        0.154        0.105        0.147        0.126        0.136        0.185        0.161        0.195      0.00203\n",
            "Wall time: 29.987784677000036\n",
            "! Best model        7    0.040\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      8     1       0.0302       0.0302     5.01e-07        0.104        0.134       0.0922        0.127         0.11        0.119        0.161         0.14       0.0451      0.00047\n",
            "      8     2       0.0361       0.0361     4.21e-05        0.112        0.147        0.101        0.134        0.117        0.133        0.172        0.152        0.465      0.00485\n",
            "      8     3       0.0391       0.0391     5.99e-05        0.119        0.153        0.108        0.141        0.124        0.141        0.175        0.158        0.504      0.00525\n",
            "      8     4       0.0396       0.0395     5.86e-05         0.12        0.154        0.106        0.147        0.127        0.137        0.183         0.16        0.547       0.0057\n",
            "      8     5       0.0413       0.0412     5.72e-05         0.12        0.157        0.105         0.15        0.128        0.137         0.19        0.164        0.497      0.00518\n",
            "      8     6       0.0398       0.0397     4.95e-05        0.117        0.154        0.105        0.142        0.123        0.136        0.185        0.161        0.459      0.00478\n",
            "      8     7       0.0334       0.0333     0.000188         0.11        0.141        0.095        0.139        0.117        0.122        0.174        0.148         1.01       0.0105\n",
            "      8     8        0.039       0.0389     5.87e-05        0.116        0.153        0.103        0.142        0.122        0.134        0.184        0.159        0.525      0.00547\n",
            "      8     9       0.0291        0.029     7.06e-05        0.103        0.132       0.0926        0.125        0.109        0.117        0.158        0.137        0.621      0.00646\n",
            "      8    10       0.0376       0.0376     8.02e-06        0.116         0.15        0.104        0.141        0.122        0.134        0.178        0.156         0.12      0.00125\n",
            "      8    11       0.0376       0.0375     0.000189        0.116         0.15        0.103        0.144        0.123        0.131        0.181        0.156        0.997       0.0104\n",
            "      8    12       0.0327       0.0324     0.000349        0.108        0.139       0.0938        0.135        0.114         0.12        0.171        0.146         1.38       0.0144\n",
            "      8    13       0.0296       0.0295     0.000109        0.104        0.133       0.0933        0.124        0.109        0.122        0.153        0.137        0.753      0.00785\n",
            "      8    14       0.0333       0.0333     1.17e-05        0.109        0.141       0.0955        0.135        0.115        0.124         0.17        0.147        0.231      0.00241\n",
            "      8    15       0.0361        0.036     0.000135        0.112        0.147       0.0994        0.136        0.118        0.132        0.173        0.152        0.852      0.00887\n",
            "      8    16       0.0339       0.0336     0.000289         0.11        0.142       0.0984        0.134        0.116        0.128        0.166        0.147         1.26       0.0131\n",
            "      8    17       0.0305       0.0305      4.1e-05        0.106        0.135        0.092        0.133        0.113        0.117        0.166        0.141        0.412       0.0043\n",
            "      8    18       0.0375       0.0374     0.000107        0.115         0.15       0.0992        0.146        0.122        0.129        0.184        0.156        0.763      0.00795\n",
            "      8    19       0.0355       0.0354     9.56e-05        0.112        0.146       0.0989        0.139        0.119        0.129        0.175        0.152        0.708      0.00737\n",
            "      8    20       0.0295       0.0295     4.89e-06        0.104        0.133       0.0906        0.131        0.111        0.115        0.162        0.139         0.13      0.00135\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      8     1       0.0394       0.0394     4.65e-06        0.118        0.154        0.103        0.147        0.125        0.134        0.186         0.16        0.135       0.0014\n",
            "      8     2       0.0382       0.0382     8.04e-06        0.117        0.151        0.104        0.144        0.124        0.135         0.18        0.157        0.162      0.00169\n",
            "      8     3       0.0328       0.0327     9.28e-06        0.109         0.14       0.0975        0.132        0.115        0.125        0.166        0.146          0.2      0.00209\n",
            "      8     4       0.0347       0.0347     5.69e-06        0.111        0.144       0.0969         0.14        0.118        0.126        0.175         0.15        0.118      0.00123\n",
            "      8     5        0.034        0.034     1.03e-05         0.11        0.143       0.0963        0.136        0.116        0.126        0.171        0.149        0.188      0.00196\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               8   33.452    0.005        0.035     9.62e-05       0.0351        0.112        0.145       0.0988        0.137        0.118        0.128        0.173        0.151        0.614      0.00639\n",
            "! Validation          8   33.452    0.005       0.0358     7.59e-06       0.0358        0.113        0.146       0.0997         0.14         0.12        0.129        0.176        0.153        0.161      0.00168\n",
            "Wall time: 33.45256343699998\n",
            "! Best model        8    0.036\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      9     1       0.0371        0.037     0.000153        0.113        0.149          0.1        0.139         0.12        0.131        0.179        0.155        0.885      0.00922\n",
            "      9     2       0.0337       0.0332     0.000567        0.109        0.141       0.0963        0.134        0.115        0.123        0.171        0.147         1.76       0.0183\n",
            "      9     3        0.031       0.0309     0.000145        0.106        0.136       0.0949        0.129        0.112        0.121        0.162        0.141        0.866      0.00902\n",
            "      9     4        0.034       0.0338     0.000211        0.109        0.142          0.1        0.127        0.114        0.132        0.161        0.146         1.07       0.0111\n",
            "      9     5       0.0257       0.0253     0.000391       0.0966        0.123       0.0859        0.118        0.102        0.111        0.145        0.128         1.46       0.0152\n",
            "      9     6       0.0362       0.0361     0.000176        0.114        0.147        0.103        0.138         0.12        0.132        0.173        0.152         0.96         0.01\n",
            "      9     7       0.0319       0.0318     9.99e-05        0.107        0.138       0.0944        0.132        0.113        0.122        0.166        0.144        0.735      0.00766\n",
            "      9     8       0.0346       0.0346     4.76e-06        0.112        0.144       0.0976        0.141        0.119        0.126        0.175         0.15        0.134      0.00139\n",
            "      9     9       0.0312        0.031     0.000218        0.104        0.136       0.0928        0.128         0.11        0.121        0.162        0.142         1.08       0.0113\n",
            "      9    10       0.0322       0.0322     1.42e-05        0.108        0.139       0.0936        0.136        0.115        0.119        0.171        0.145        0.241      0.00251\n",
            "      9    11        0.033        0.033     1.39e-05        0.109        0.141        0.097        0.133        0.115        0.123         0.17        0.147        0.254      0.00264\n",
            "      9    12       0.0354       0.0353     4.76e-05        0.113        0.145          0.1        0.138        0.119        0.129        0.174        0.151        0.502      0.00523\n",
            "      9    13       0.0301       0.0299     0.000233        0.103        0.134       0.0907        0.126        0.109        0.117        0.163         0.14         1.12       0.0116\n",
            "      9    14       0.0348       0.0348     2.58e-05        0.113        0.144        0.104        0.131        0.118        0.132        0.166        0.149        0.351      0.00366\n",
            "      9    15       0.0305       0.0305      6.2e-06        0.104        0.135       0.0932        0.127         0.11        0.119        0.162        0.141        0.143      0.00149\n",
            "      9    16       0.0369       0.0368     4.74e-05        0.114        0.148       0.0991        0.143        0.121        0.129        0.181        0.155        0.504      0.00524\n",
            "      9    17       0.0306       0.0302     0.000393        0.105        0.134       0.0944        0.125         0.11        0.121        0.157        0.139         1.47       0.0153\n",
            "      9    18       0.0338       0.0336     0.000176        0.109        0.142       0.0963        0.135        0.115        0.125        0.171        0.148        0.975       0.0102\n",
            "      9    19       0.0325       0.0325     1.02e-05        0.106        0.139       0.0962        0.126        0.111        0.128         0.16        0.144        0.185      0.00193\n",
            "      9    20       0.0244       0.0244     5.24e-05       0.0944        0.121       0.0813        0.121        0.101        0.103         0.15        0.127        0.529      0.00551\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      9     1       0.0357       0.0357     5.31e-06        0.112        0.146       0.0984         0.14        0.119        0.128        0.178        0.153         0.14      0.00146\n",
            "      9     2       0.0344       0.0344     6.37e-06        0.111        0.143       0.0985        0.137        0.118        0.127        0.171        0.149        0.136      0.00142\n",
            "      9     3       0.0296       0.0296     7.28e-06        0.104        0.133       0.0925        0.126        0.109        0.118        0.159        0.139        0.172      0.00179\n",
            "      9     4       0.0313       0.0313     4.62e-06        0.105        0.137       0.0922        0.132        0.112         0.12        0.166        0.143        0.115      0.00119\n",
            "      9     5       0.0308       0.0308     6.51e-06        0.105        0.136       0.0916        0.131        0.111         0.12        0.164        0.142        0.147      0.00153\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               9   36.942    0.005       0.0323     0.000149       0.0325        0.107        0.139       0.0956        0.131        0.113        0.123        0.166        0.145        0.761      0.00793\n",
            "! Validation          9   36.942    0.005       0.0324     6.02e-06       0.0324        0.107        0.139       0.0946        0.133        0.114        0.123        0.168        0.145        0.142      0.00148\n",
            "Wall time: 36.94289849000006\n",
            "! Best model        9    0.032\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     10     1       0.0302       0.0302     1.29e-05        0.104        0.134       0.0914        0.128         0.11        0.121        0.158        0.139        0.238      0.00248\n",
            "     10     2       0.0305       0.0305     2.47e-05        0.105        0.135       0.0935        0.128        0.111        0.121         0.16         0.14        0.328      0.00341\n",
            "     10     3       0.0313       0.0313      1.3e-05        0.104        0.137       0.0908        0.129         0.11         0.12        0.166        0.143        0.212      0.00221\n",
            "     10     4       0.0314       0.0314     8.94e-06        0.105        0.137       0.0924        0.131        0.112        0.121        0.165        0.143         0.19      0.00198\n",
            "     10     5        0.029       0.0289      3.7e-05        0.104        0.132       0.0911        0.129         0.11        0.116        0.158        0.137        0.417      0.00434\n",
            "     10     6       0.0286       0.0285     0.000101        0.101        0.131       0.0918        0.119        0.105        0.118        0.152        0.135         0.74      0.00771\n",
            "     10     7       0.0252       0.0252     4.56e-06       0.0952        0.123       0.0854        0.115          0.1        0.109        0.146        0.128        0.145      0.00151\n",
            "     10     8       0.0291       0.0291     2.75e-05        0.101        0.132       0.0849        0.133        0.109        0.108         0.17        0.139        0.352      0.00367\n",
            "     10     9       0.0265       0.0264      5.7e-05        0.096        0.126       0.0839         0.12        0.102        0.109        0.154        0.131        0.517      0.00539\n",
            "     10    10         0.03       0.0299      7.6e-05        0.102        0.134       0.0901        0.127        0.108        0.119         0.16        0.139        0.628      0.00654\n",
            "     10    11       0.0288       0.0288     1.01e-05       0.0998        0.131       0.0868        0.126        0.106        0.114         0.16        0.137        0.225      0.00235\n",
            "     10    12       0.0254       0.0253     2.67e-05       0.0951        0.123        0.083        0.119        0.101        0.106        0.151        0.129        0.348      0.00363\n",
            "     10    13       0.0268       0.0268     2.51e-05       0.0985        0.127       0.0874        0.121        0.104        0.112        0.152        0.132        0.361      0.00376\n",
            "     10    14        0.029       0.0289     7.69e-06        0.102        0.132       0.0882        0.129        0.109        0.113        0.162        0.138         0.16      0.00167\n",
            "     10    15       0.0253       0.0253     1.87e-05       0.0963        0.123       0.0825        0.124        0.103        0.104        0.153        0.129        0.299      0.00311\n",
            "     10    16       0.0325       0.0324     2.87e-05        0.107        0.139       0.0925        0.135        0.114        0.122        0.169        0.145         0.38      0.00396\n",
            "     10    17        0.024        0.024     1.92e-05       0.0931         0.12       0.0817        0.116       0.0988        0.103        0.148        0.125        0.276      0.00287\n",
            "     10    18       0.0244       0.0241     0.000288       0.0952         0.12       0.0874        0.111       0.0991        0.111        0.136        0.124         1.26       0.0131\n",
            "     10    19       0.0259       0.0257     0.000132       0.0953        0.124       0.0858        0.114          0.1         0.11        0.149        0.129         0.85      0.00885\n",
            "     10    20       0.0259       0.0258     1.12e-05       0.0951        0.124        0.084        0.117        0.101        0.109        0.151         0.13        0.205      0.00214\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     10     1       0.0324       0.0324     4.91e-06        0.107        0.139       0.0934        0.133        0.113        0.121         0.17        0.145        0.132      0.00138\n",
            "     10     2       0.0309       0.0309     5.33e-06        0.105        0.136       0.0928        0.131        0.112         0.12        0.164        0.142        0.135      0.00141\n",
            "     10     3       0.0268       0.0268     6.75e-06       0.0985        0.127       0.0873        0.121        0.104        0.112        0.153        0.132        0.164      0.00171\n",
            "     10     4       0.0283       0.0283     3.89e-06          0.1         0.13       0.0876        0.126        0.107        0.114        0.158        0.136        0.107      0.00112\n",
            "     10     5       0.0279       0.0279     5.52e-06       0.0996        0.129       0.0869        0.125        0.106        0.113        0.156        0.135        0.137      0.00142\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              10   40.451    0.005       0.0279     4.65e-05        0.028       0.0997        0.129       0.0877        0.124        0.106        0.113        0.156        0.135        0.407      0.00423\n",
            "! Validation         10   40.451    0.005       0.0293     5.28e-06       0.0293        0.102        0.132       0.0896        0.127        0.108        0.116         0.16        0.138        0.135      0.00141\n",
            "Wall time: 40.45187010900008\n",
            "! Best model       10    0.029\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     11     1       0.0268       0.0266     0.000116       0.0954        0.126       0.0816        0.123        0.102        0.106        0.159        0.133        0.778       0.0081\n",
            "     11     2       0.0264       0.0263     0.000103       0.0977        0.125       0.0852        0.123        0.104        0.108        0.155        0.131        0.722      0.00752\n",
            "     11     3       0.0248       0.0248     1.54e-05       0.0943        0.122       0.0825        0.118          0.1        0.106        0.148        0.127        0.238      0.00248\n",
            "     11     4       0.0266       0.0266     1.29e-05       0.0978        0.126        0.088        0.117        0.103        0.113        0.149        0.131        0.256      0.00267\n",
            "     11     5       0.0233       0.0233     5.34e-06       0.0916        0.118        0.081        0.113       0.0969        0.105        0.142        0.123        0.141      0.00147\n",
            "     11     6       0.0257       0.0256     0.000103       0.0959        0.124       0.0832        0.122        0.102        0.107        0.152         0.13        0.744      0.00775\n",
            "     11     7       0.0256       0.0255     0.000118       0.0962        0.124        0.085        0.119        0.102         0.11        0.148        0.129        0.794      0.00827\n",
            "     11     8        0.022        0.022     3.08e-05        0.087        0.115       0.0766        0.108       0.0922          0.1         0.14         0.12          0.4      0.00417\n",
            "     11     9       0.0242       0.0238      0.00042       0.0919        0.119       0.0799        0.116       0.0978        0.103        0.147        0.125         1.52       0.0158\n",
            "     11    10       0.0251       0.0246     0.000466       0.0928        0.121       0.0809        0.117       0.0988        0.106        0.147        0.127         1.59       0.0166\n",
            "     11    11        0.023       0.0229      0.00015       0.0916        0.117       0.0815        0.112       0.0967        0.104        0.139        0.122        0.903       0.0094\n",
            "     11    12       0.0225       0.0223     0.000166       0.0897        0.116       0.0778        0.113       0.0956       0.0994        0.142        0.121        0.938      0.00977\n",
            "     11    13       0.0255       0.0249     0.000653       0.0953        0.122       0.0833        0.119        0.101        0.107        0.147        0.127         1.89       0.0197\n",
            "     11    14       0.0276       0.0274     0.000139       0.0978        0.128       0.0846        0.124        0.104        0.111        0.156        0.134        0.862      0.00897\n",
            "     11    15       0.0231       0.0231     2.26e-05       0.0898        0.118       0.0807        0.108       0.0943        0.105        0.139        0.122        0.348      0.00363\n",
            "     11    16       0.0278       0.0277     6.42e-05       0.0972        0.129       0.0856         0.12        0.103        0.113        0.155        0.134        0.583      0.00607\n",
            "     11    17       0.0278       0.0277     8.89e-05       0.0969        0.129        0.085        0.121        0.103        0.113        0.156        0.134        0.691      0.00719\n",
            "     11    18       0.0268       0.0267     0.000121       0.0966        0.126       0.0831        0.124        0.103        0.109        0.156        0.132        0.808      0.00842\n",
            "     11    19       0.0237       0.0235     0.000165       0.0916        0.119       0.0794        0.116       0.0977        0.101        0.147        0.124        0.945      0.00984\n",
            "     11    20       0.0211       0.0211     1.01e-05       0.0868        0.112       0.0747        0.111       0.0928       0.0966        0.139        0.118         0.16      0.00166\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     11     1       0.0292       0.0292     5.97e-06        0.101        0.132       0.0883        0.127        0.108        0.115        0.162        0.138        0.139      0.00145\n",
            "     11     2       0.0278       0.0278     5.11e-06       0.0996        0.129       0.0872        0.125        0.106        0.113        0.156        0.135        0.147      0.00153\n",
            "     11     3       0.0242       0.0242     6.09e-06       0.0935         0.12       0.0822        0.116       0.0991        0.105        0.146        0.126        0.155      0.00162\n",
            "     11     4       0.0255       0.0255     4.29e-06        0.095        0.124       0.0829        0.119        0.101        0.108         0.15        0.129        0.126      0.00131\n",
            "     11     5       0.0253       0.0253     4.05e-06       0.0949        0.123       0.0824         0.12        0.101        0.107         0.15        0.129        0.122      0.00127\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              11   43.968    0.005       0.0248     0.000148        0.025       0.0937        0.122        0.082        0.117       0.0996        0.106        0.148        0.127        0.766      0.00798\n",
            "! Validation         11   43.968    0.005       0.0264      5.1e-06       0.0264       0.0968        0.126       0.0846        0.121        0.103         0.11        0.153        0.131        0.138      0.00144\n",
            "Wall time: 43.968535212999996\n",
            "! Best model       11    0.026\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     12     1       0.0248       0.0245     0.000266       0.0951        0.121       0.0815        0.122        0.102        0.103        0.151        0.127         1.21       0.0126\n",
            "     12     2       0.0346       0.0339      0.00063        0.112        0.142        0.102        0.131        0.116         0.13        0.164        0.147         1.85       0.0193\n",
            "     12     3       0.0285       0.0285     2.92e-05        0.101        0.131       0.0907        0.123        0.107        0.117        0.154        0.136        0.388      0.00404\n",
            "     12     4         0.02       0.0199     0.000128        0.085        0.109       0.0748        0.105       0.0901       0.0949        0.133        0.114        0.813      0.00847\n",
            "     12     5       0.0297       0.0293     0.000408        0.102        0.132       0.0882        0.129        0.109        0.113        0.164        0.139         1.49       0.0155\n",
            "     12     6       0.0348       0.0344     0.000388        0.112        0.144        0.105        0.127        0.116        0.132        0.163        0.148         1.46       0.0152\n",
            "     12     7       0.0294       0.0294      4.5e-05        0.104        0.133       0.0935        0.124        0.109        0.119        0.157        0.138        0.461      0.00481\n",
            "     12     8       0.0224       0.0222      0.00021       0.0896        0.115       0.0782        0.112       0.0952       0.0995        0.141         0.12         1.07       0.0112\n",
            "     12     9       0.0334       0.0334     9.01e-06        0.111        0.141        0.105        0.123        0.114        0.135        0.154        0.144        0.179      0.00187\n",
            "     12    10       0.0278       0.0278     3.61e-05       0.0999        0.129       0.0854        0.129        0.107        0.109        0.162        0.135        0.434      0.00452\n",
            "     12    11       0.0208       0.0207      0.00012       0.0856        0.111       0.0749        0.107        0.091       0.0957        0.137        0.116        0.806       0.0084\n",
            "     12    12        0.028        0.028     3.36e-05        0.102        0.129       0.0881        0.129        0.108        0.112        0.158        0.135        0.352      0.00367\n",
            "     12    13       0.0273       0.0272     0.000118       0.0967        0.128       0.0826        0.125        0.104        0.108         0.16        0.134        0.771      0.00803\n",
            "     12    14       0.0215       0.0213     0.000151       0.0857        0.113       0.0755        0.106       0.0908       0.0991        0.136        0.118        0.901      0.00939\n",
            "     12    15       0.0217       0.0217     7.97e-05       0.0884        0.114       0.0764        0.112       0.0944       0.0988        0.139        0.119        0.656      0.00684\n",
            "     12    16       0.0207       0.0207     8.23e-06       0.0866        0.111       0.0786        0.103       0.0906        0.101        0.129        0.115        0.208      0.00217\n",
            "     12    17       0.0226       0.0226     7.74e-05       0.0893        0.116       0.0774        0.113       0.0952          0.1        0.143        0.122        0.645      0.00672\n",
            "     12    18       0.0249       0.0247     0.000176       0.0925        0.122       0.0811        0.115       0.0983        0.109        0.144        0.126        0.985       0.0103\n",
            "     12    19       0.0213       0.0213     1.07e-05       0.0873        0.113       0.0753        0.111       0.0933       0.0976        0.139        0.118        0.206      0.00215\n",
            "     12    20       0.0239       0.0239     3.99e-05       0.0911         0.12       0.0802        0.113       0.0965        0.105        0.144        0.125        0.458      0.00477\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     12     1       0.0266       0.0266     7.62e-06       0.0964        0.126       0.0837        0.122        0.103        0.109        0.155        0.132        0.162      0.00169\n",
            "     12     2       0.0254       0.0254     5.33e-06       0.0947        0.123       0.0821         0.12        0.101        0.107        0.151        0.129        0.156      0.00163\n",
            "     12     3       0.0222       0.0221     6.34e-06       0.0891        0.115       0.0777        0.112       0.0948       0.0997        0.141         0.12        0.153      0.00159\n",
            "     12     4       0.0234       0.0234     5.78e-06       0.0908        0.118       0.0788        0.115       0.0967        0.103        0.144        0.123        0.146      0.00152\n",
            "     12     5       0.0231       0.0231     3.59e-06       0.0908        0.118       0.0784        0.116        0.097        0.102        0.145        0.123        0.117      0.00121\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              12   47.485    0.005       0.0258     0.000148       0.0259       0.0958        0.124       0.0847        0.118        0.101         0.11        0.149        0.129        0.768      0.00799\n",
            "! Validation         12   47.485    0.005       0.0241     5.73e-06       0.0241       0.0923         0.12       0.0802        0.117       0.0984        0.104        0.147        0.126        0.147      0.00153\n",
            "Wall time: 47.48560535899992\n",
            "! Best model       12    0.024\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     13     1        0.024        0.024     1.05e-06        0.092         0.12       0.0791        0.118       0.0985        0.104        0.147        0.125       0.0662      0.00069\n",
            "     13     2       0.0206       0.0205     6.28e-05       0.0865        0.111       0.0742        0.111       0.0927       0.0958        0.136        0.116        0.583      0.00607\n",
            "     13     3       0.0239       0.0239      4.2e-05       0.0912         0.12       0.0767         0.12       0.0984        0.101         0.15        0.126        0.454      0.00473\n",
            "     13     4       0.0197       0.0197     8.15e-06       0.0849        0.109       0.0745        0.106       0.0901       0.0943        0.133        0.113        0.196      0.00205\n",
            "     13     5         0.02         0.02     4.98e-05       0.0849        0.109       0.0731        0.108       0.0907        0.093        0.136        0.115        0.515      0.00537\n",
            "     13     6       0.0272        0.027     0.000156          0.1        0.127       0.0921        0.116        0.104        0.117        0.146        0.131        0.916      0.00954\n",
            "     13     7       0.0232       0.0231      5.4e-05       0.0926        0.118       0.0795        0.119       0.0991          0.1        0.146        0.123        0.539      0.00562\n",
            "     13     8       0.0202       0.0202     1.04e-05       0.0837         0.11       0.0731        0.105        0.089       0.0962        0.133        0.115        0.222      0.00231\n",
            "     13     9       0.0249       0.0249     4.55e-05       0.0938        0.122       0.0805         0.12          0.1        0.105        0.151        0.128        0.481      0.00501\n",
            "     13    10       0.0209       0.0207     0.000103       0.0849        0.111       0.0764        0.102       0.0891       0.0999        0.132        0.116        0.742      0.00773\n",
            "     13    11       0.0234       0.0234     1.82e-05       0.0916        0.118       0.0793        0.116       0.0978        0.103        0.145        0.124        0.252      0.00263\n",
            "     13    12       0.0252       0.0251     7.69e-05       0.0957        0.123       0.0856        0.116        0.101         0.11        0.145        0.127        0.649      0.00676\n",
            "     13    13       0.0207       0.0206      5.4e-05       0.0855        0.111       0.0741        0.108       0.0912        0.095        0.138        0.116        0.538      0.00561\n",
            "     13    14       0.0247       0.0247     2.46e-06        0.092        0.122       0.0785        0.119       0.0988        0.106        0.149        0.127        0.113      0.00118\n",
            "     13    15       0.0221       0.0221     6.08e-06       0.0876        0.115       0.0765         0.11       0.0932       0.0989        0.142         0.12        0.178      0.00186\n",
            "     13    16       0.0191        0.019     2.31e-05       0.0827        0.107       0.0717        0.105       0.0882       0.0921        0.131        0.112        0.292      0.00304\n",
            "     13    17       0.0219       0.0219      1.2e-05        0.087        0.114       0.0743        0.113       0.0934        0.098        0.142         0.12        0.225      0.00234\n",
            "     13    18       0.0218       0.0217     5.07e-05       0.0864        0.114       0.0767        0.106       0.0913       0.0992        0.139        0.119        0.501      0.00522\n",
            "     13    19       0.0203       0.0203     1.22e-05       0.0854         0.11       0.0743        0.108       0.0909        0.095        0.135        0.115        0.229      0.00238\n",
            "     13    20       0.0231        0.023      9.2e-05       0.0905        0.117       0.0792        0.113       0.0961        0.103        0.141        0.122        0.707      0.00737\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     13     1       0.0245       0.0244        1e-05       0.0923        0.121       0.0798        0.117       0.0986        0.104         0.15        0.127        0.191      0.00199\n",
            "     13     2       0.0236       0.0236     6.23e-06       0.0908        0.119       0.0781        0.116       0.0971        0.102        0.146        0.124        0.165      0.00172\n",
            "     13     3       0.0205       0.0205     7.18e-06       0.0855        0.111       0.0739        0.109       0.0913       0.0951        0.137        0.116        0.159      0.00166\n",
            "     13     4       0.0216       0.0216     8.06e-06       0.0872        0.114       0.0756         0.11        0.093       0.0987        0.139        0.119        0.174      0.00181\n",
            "     13     5       0.0214       0.0214     4.25e-06       0.0872        0.113       0.0748        0.112       0.0934       0.0967         0.14        0.118        0.124      0.00129\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              13   50.987    0.005       0.0223      4.4e-05       0.0223       0.0889        0.116       0.0775        0.112       0.0947          0.1        0.141        0.121         0.42      0.00438\n",
            "! Validation         13   50.987    0.005       0.0223     7.15e-06       0.0223       0.0886        0.115       0.0764        0.113       0.0947       0.0993        0.142        0.121        0.163      0.00169\n",
            "Wall time: 50.98786583600008\n",
            "! Best model       13    0.022\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     14     1       0.0214       0.0213      8.3e-05       0.0866        0.113        0.075         0.11       0.0924       0.0961        0.141        0.118        0.673      0.00701\n",
            "     14     2       0.0196       0.0195     7.53e-05        0.084        0.108       0.0742        0.103       0.0889       0.0945        0.131        0.113        0.616      0.00642\n",
            "     14     3       0.0184       0.0184     2.39e-06       0.0816        0.105       0.0706        0.104       0.0871       0.0894        0.131         0.11        0.108      0.00112\n",
            "     14     4       0.0242        0.024     0.000163       0.0931         0.12       0.0812        0.117        0.099        0.105        0.146        0.125        0.929      0.00968\n",
            "     14     5       0.0218       0.0216     0.000202       0.0887        0.114       0.0749        0.116       0.0956       0.0963        0.143        0.119         1.03       0.0107\n",
            "     14     6       0.0209       0.0209     6.58e-05       0.0876        0.112       0.0794        0.104       0.0917        0.101        0.131        0.116        0.588      0.00612\n",
            "     14     7       0.0187       0.0187     5.08e-06       0.0814        0.106       0.0689        0.106       0.0876       0.0886        0.134        0.111        0.138      0.00144\n",
            "     14     8       0.0218       0.0217     5.54e-05       0.0878        0.114       0.0782        0.107       0.0926        0.101        0.136        0.119        0.536      0.00558\n",
            "     14     9       0.0254       0.0253     8.44e-05       0.0938        0.123       0.0795        0.122        0.101        0.101        0.158         0.13        0.663      0.00691\n",
            "     14    10       0.0209       0.0208     5.47e-05       0.0839        0.112       0.0725        0.107       0.0896       0.0974        0.135        0.116        0.544      0.00567\n",
            "     14    11       0.0189       0.0189     3.22e-05       0.0817        0.106       0.0696        0.106       0.0878       0.0893        0.134        0.112        0.403       0.0042\n",
            "     14    12       0.0175       0.0175     2.47e-06       0.0795        0.102       0.0693          0.1       0.0847       0.0894        0.124        0.107        0.115       0.0012\n",
            "     14    13       0.0165       0.0164     5.71e-05       0.0758       0.0991        0.065       0.0976       0.0813       0.0852        0.122        0.104        0.555      0.00578\n",
            "     14    14       0.0223       0.0222     3.71e-05       0.0861        0.115       0.0727        0.113       0.0928       0.0981        0.144        0.121        0.446      0.00464\n",
            "     14    15       0.0201       0.0201     1.87e-06       0.0834         0.11       0.0719        0.107       0.0892       0.0941        0.136        0.115       0.0947     0.000987\n",
            "     14    16       0.0177       0.0177     1.33e-05       0.0791        0.103       0.0692       0.0988        0.084         0.09        0.125        0.107        0.242      0.00252\n",
            "     14    17       0.0212       0.0212      3.1e-06       0.0867        0.113       0.0738        0.112       0.0931       0.0956        0.141        0.118         0.11      0.00114\n",
            "     14    18       0.0197       0.0196     6.61e-06       0.0837        0.108       0.0729        0.105       0.0891       0.0937        0.133        0.113        0.173       0.0018\n",
            "     14    19       0.0196       0.0195     3.89e-05       0.0821        0.108       0.0703        0.106        0.088       0.0919        0.135        0.113        0.456      0.00475\n",
            "     14    20       0.0169       0.0169     1.91e-05       0.0785        0.101       0.0675          0.1        0.084        0.085        0.126        0.105        0.308      0.00321\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     14     1       0.0228       0.0227     8.53e-06       0.0889        0.117       0.0764        0.114       0.0952       0.0992        0.145        0.122        0.177      0.00184\n",
            "     14     2        0.022        0.022     5.22e-06       0.0875        0.115       0.0747        0.113       0.0939       0.0981        0.142         0.12        0.158      0.00165\n",
            "     14     3       0.0191       0.0191     6.12e-06       0.0825        0.107       0.0708        0.106       0.0884       0.0911        0.133        0.112        0.149      0.00155\n",
            "     14     4       0.0201       0.0201     6.66e-06        0.084         0.11       0.0725        0.107       0.0897       0.0948        0.135        0.115         0.15      0.00157\n",
            "     14     5         0.02         0.02     3.46e-06       0.0842        0.109       0.0717        0.109       0.0904       0.0925        0.137        0.115        0.109      0.00114\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              14   54.457    0.005       0.0201     5.01e-05       0.0202       0.0843         0.11       0.0728        0.107         0.09       0.0943        0.135        0.115        0.436      0.00455\n",
            "! Validation         14   54.457    0.005       0.0208        6e-06       0.0208       0.0854        0.112       0.0732         0.11       0.0915       0.0952        0.139        0.117        0.149      0.00155\n",
            "Wall time: 54.457844270999885\n",
            "! Best model       14    0.021\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     15     1       0.0193       0.0193     1.54e-05        0.082        0.108       0.0702        0.106       0.0879       0.0923        0.133        0.113        0.274      0.00285\n",
            "     15     2       0.0182       0.0181     0.000114       0.0801        0.104       0.0689        0.103       0.0857        0.089        0.129        0.109        0.777      0.00809\n",
            "     15     3       0.0173       0.0173     2.27e-05       0.0786        0.102       0.0669        0.102       0.0844       0.0854        0.129        0.107        0.333      0.00346\n",
            "     15     4       0.0201       0.0201     9.12e-06       0.0846         0.11       0.0738        0.106       0.0899       0.0949        0.135        0.115        0.191      0.00199\n",
            "     15     5       0.0197       0.0197     2.25e-05       0.0834        0.109       0.0713        0.108       0.0895       0.0921        0.136        0.114        0.328      0.00342\n",
            "     15     6       0.0182       0.0182     1.57e-06       0.0817        0.104       0.0721        0.101       0.0865       0.0912        0.126        0.109       0.0822     0.000857\n",
            "     15     7       0.0202       0.0202     3.26e-06       0.0837         0.11       0.0721        0.107       0.0894       0.0938        0.136        0.115        0.114      0.00119\n",
            "     15     8       0.0194       0.0194     2.65e-06        0.083        0.108       0.0713        0.106       0.0888       0.0915        0.134        0.113        0.117      0.00122\n",
            "     15     9       0.0215       0.0215      1.4e-05       0.0879        0.113       0.0776        0.109        0.093        0.101        0.135        0.118         0.24       0.0025\n",
            "     15    10       0.0215       0.0214     0.000102       0.0866        0.113       0.0773        0.105       0.0913          0.1        0.136        0.118        0.743      0.00774\n",
            "     15    11       0.0197       0.0197     2.51e-05       0.0848        0.109       0.0731        0.108       0.0906       0.0923        0.135        0.114        0.331      0.00345\n",
            "     15    12       0.0221        0.022     6.34e-05        0.088        0.115       0.0772         0.11       0.0934          0.1         0.14         0.12        0.584      0.00608\n",
            "     15    13       0.0268       0.0266     0.000139        0.098        0.126        0.088        0.118        0.103        0.112         0.15        0.131         0.87      0.00907\n",
            "     15    14       0.0189       0.0188      0.00015       0.0809        0.106       0.0705        0.102       0.0861       0.0904        0.132        0.111        0.907      0.00945\n",
            "     15    15       0.0211       0.0211     1.31e-06       0.0854        0.112       0.0727        0.111       0.0918       0.0966        0.138        0.117       0.0717     0.000747\n",
            "     15    16       0.0218       0.0217      0.00013       0.0904        0.114       0.0812        0.109        0.095        0.101        0.135        0.118        0.828      0.00863\n",
            "     15    17       0.0168       0.0167     8.32e-05       0.0774          0.1       0.0659          0.1       0.0832       0.0853        0.124        0.105         0.67      0.00698\n",
            "     15    18       0.0222       0.0222     8.74e-07       0.0867        0.115       0.0738        0.113       0.0932       0.0981        0.143        0.121       0.0521     0.000543\n",
            "     15    19        0.019        0.019     1.37e-05       0.0833        0.107       0.0748          0.1       0.0875       0.0951        0.126        0.111         0.25      0.00261\n",
            "     15    20       0.0198       0.0198     2.16e-05       0.0816        0.109       0.0675         0.11       0.0887       0.0884        0.141        0.115        0.312      0.00325\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     15     1       0.0213       0.0213     8.38e-06        0.086        0.113       0.0735        0.111       0.0922       0.0954        0.142        0.119        0.176      0.00184\n",
            "     15     2       0.0207       0.0207     5.02e-06       0.0846        0.111       0.0717         0.11        0.091       0.0945        0.139        0.117        0.155      0.00162\n",
            "     15     3       0.0179       0.0179     5.75e-06       0.0799        0.104        0.068        0.104       0.0858       0.0876         0.13        0.109        0.147      0.00153\n",
            "     15     4        0.019        0.019     6.69e-06       0.0814        0.107         0.07        0.104       0.0871       0.0916        0.132        0.112        0.151      0.00157\n",
            "     15     5       0.0188       0.0188     3.46e-06       0.0815        0.106        0.069        0.107       0.0878       0.0889        0.134        0.111         0.11      0.00114\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              15   57.960    0.005       0.0201     4.68e-05       0.0202       0.0844         0.11       0.0733        0.107       0.0899       0.0948        0.135        0.115        0.404      0.00421\n",
            "! Validation         15   57.960    0.005       0.0195     5.86e-06       0.0195       0.0827        0.108       0.0704        0.107       0.0888       0.0916        0.135        0.113        0.148      0.00154\n",
            "Wall time: 57.96145870400005\n",
            "! Best model       15    0.020\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     16     1       0.0185       0.0185     4.64e-06        0.083        0.105       0.0741        0.101       0.0875        0.094        0.125        0.109        0.156      0.00163\n",
            "     16     2       0.0212       0.0212     5.74e-05       0.0863        0.113       0.0728        0.113       0.0931        0.094        0.143        0.118        0.549      0.00572\n",
            "     16     3       0.0211       0.0211     3.27e-06       0.0849        0.113       0.0702        0.114       0.0923       0.0934        0.143        0.118        0.108      0.00113\n",
            "     16     4       0.0186       0.0186     3.25e-05       0.0813        0.106       0.0685        0.107       0.0877       0.0872        0.135        0.111        0.418      0.00435\n",
            "     16     5       0.0181        0.018     3.56e-05       0.0796        0.104       0.0694          0.1       0.0848       0.0891        0.128        0.109        0.425      0.00442\n",
            "     16     6       0.0177       0.0177     2.31e-05       0.0789        0.103       0.0677        0.101       0.0845       0.0871        0.129        0.108        0.323      0.00337\n",
            "     16     7       0.0183       0.0183     2.57e-05       0.0811        0.105       0.0719       0.0995       0.0857       0.0931        0.124        0.109         0.37      0.00385\n",
            "     16     8       0.0207       0.0206     0.000102       0.0858        0.111        0.074        0.109       0.0917       0.0942        0.139        0.116        0.745      0.00776\n",
            "     16     9       0.0208       0.0208     1.93e-05       0.0837        0.112        0.071        0.109         0.09       0.0956        0.138        0.117        0.272      0.00284\n",
            "     16    10       0.0175       0.0174     9.47e-06       0.0774        0.102        0.065        0.102       0.0836       0.0853         0.13        0.107        0.212      0.00221\n",
            "     16    11        0.024        0.024     8.98e-05       0.0916         0.12       0.0778        0.119       0.0985          0.1        0.152        0.126        0.662       0.0069\n",
            "     16    12       0.0174       0.0174     5.49e-05       0.0782        0.102        0.069       0.0964       0.0827       0.0895        0.123        0.106        0.534      0.00556\n",
            "     16    13        0.017        0.017     1.37e-06       0.0768        0.101       0.0649          0.1       0.0827        0.086        0.126        0.106       0.0707     0.000736\n",
            "     16    14       0.0182       0.0182     4.47e-06        0.079        0.104       0.0675        0.102       0.0848       0.0882        0.131        0.109        0.133      0.00139\n",
            "     16    15       0.0154       0.0154     2.02e-06       0.0738       0.0959       0.0622       0.0971       0.0796       0.0812         0.12        0.101       0.0971      0.00101\n",
            "     16    16       0.0166       0.0166     1.64e-06       0.0768       0.0995        0.066       0.0983       0.0822       0.0856        0.123        0.104       0.0732     0.000763\n",
            "     16    17       0.0176       0.0176     4.01e-06       0.0787        0.103       0.0654        0.105       0.0854       0.0851        0.131        0.108        0.134       0.0014\n",
            "     16    18       0.0157       0.0157      4.7e-06       0.0741       0.0969       0.0622       0.0978         0.08        0.081        0.123        0.102        0.124      0.00129\n",
            "     16    19       0.0139       0.0138     3.47e-05       0.0707        0.091       0.0611       0.0899       0.0755       0.0765        0.115       0.0956        0.429      0.00447\n",
            "     16    20       0.0165       0.0165      7.3e-06       0.0759       0.0994       0.0648       0.0981       0.0815       0.0838        0.125        0.104        0.182      0.00189\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     16     1       0.0202       0.0202     6.92e-06       0.0836         0.11        0.071        0.109       0.0898       0.0923        0.138        0.115        0.159      0.00166\n",
            "     16     2       0.0196       0.0196     4.04e-06       0.0821        0.108       0.0691        0.108       0.0886       0.0913        0.136        0.114        0.141      0.00147\n",
            "     16     3        0.017        0.017     4.78e-06       0.0776        0.101       0.0657        0.101       0.0835       0.0848        0.127        0.106        0.134       0.0014\n",
            "     16     4        0.018        0.018     5.23e-06       0.0793        0.104        0.068        0.102       0.0849       0.0888        0.129        0.109        0.137      0.00143\n",
            "     16     5       0.0178       0.0178     2.84e-06        0.079        0.103       0.0665        0.104       0.0853       0.0858        0.131        0.108        0.105      0.00109\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              16   61.462    0.005       0.0182     2.59e-05       0.0182       0.0799        0.104       0.0683        0.103       0.0857       0.0887         0.13        0.109        0.301      0.00313\n",
            "! Validation         16   61.462    0.005       0.0185     4.76e-06       0.0185       0.0803        0.105       0.0681        0.105       0.0864       0.0887        0.132         0.11        0.135      0.00141\n",
            "Wall time: 61.463110041999926\n",
            "! Best model       16    0.019\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     17     1       0.0154       0.0154     4.44e-06       0.0743       0.0961       0.0623       0.0984       0.0804       0.0796        0.123        0.101        0.124      0.00129\n",
            "     17     2       0.0151       0.0151     2.13e-05       0.0734       0.0949       0.0626       0.0949       0.0788       0.0797         0.12       0.0997        0.327      0.00341\n",
            "     17     3       0.0167       0.0167     6.06e-05       0.0775       0.0999       0.0663          0.1       0.0832       0.0844        0.125        0.105        0.566      0.00589\n",
            "     17     4       0.0188       0.0187     3.36e-05       0.0802        0.106       0.0685        0.103        0.086       0.0906        0.131        0.111        0.387      0.00403\n",
            "     17     5       0.0159       0.0159     9.71e-06       0.0742       0.0975       0.0642       0.0943       0.0792       0.0843         0.12        0.102         0.22      0.00229\n",
            "     17     6       0.0159       0.0158      5.6e-05        0.074       0.0973       0.0618       0.0984       0.0801       0.0795        0.125        0.102        0.526      0.00547\n",
            "     17     7       0.0164       0.0163     6.28e-05       0.0761       0.0988       0.0635        0.101       0.0825       0.0818        0.126        0.104        0.579      0.00603\n",
            "     17     8       0.0192       0.0192     5.58e-06       0.0808        0.107       0.0676        0.107       0.0874       0.0895        0.136        0.113        0.142      0.00148\n",
            "     17     9       0.0151        0.015      0.00013        0.072       0.0947       0.0601       0.0957       0.0779       0.0797        0.119       0.0994        0.843      0.00878\n",
            "     17    10       0.0157       0.0155     0.000233       0.0748       0.0963       0.0639       0.0967       0.0803       0.0816         0.12        0.101         1.13       0.0118\n",
            "     17    11       0.0183       0.0182     1.21e-05        0.078        0.104       0.0643        0.105       0.0848       0.0852        0.135         0.11        0.201       0.0021\n",
            "     17    12       0.0163       0.0161      0.00022       0.0763       0.0982       0.0645       0.0998       0.0822        0.082        0.124        0.103         1.09       0.0114\n",
            "     17    13       0.0164        0.016      0.00038        0.074        0.098       0.0635       0.0949       0.0792       0.0837        0.122        0.103         1.44        0.015\n",
            "     17    14       0.0153       0.0153     1.53e-06       0.0745       0.0958       0.0636       0.0961       0.0799       0.0798        0.122        0.101       0.0816      0.00085\n",
            "     17    15       0.0178       0.0173     0.000499       0.0774        0.102       0.0671       0.0981       0.0826       0.0878        0.125        0.106         1.66       0.0172\n",
            "     17    16       0.0154       0.0151     0.000378       0.0733        0.095       0.0599          0.1         0.08       0.0755        0.125          0.1         1.44        0.015\n",
            "     17    17       0.0163       0.0163     6.59e-06       0.0756       0.0988       0.0647       0.0974        0.081       0.0833        0.124        0.104        0.162      0.00169\n",
            "     17    18       0.0174        0.017     0.000414       0.0767        0.101       0.0652       0.0996       0.0824       0.0856        0.126        0.106         1.51       0.0157\n",
            "     17    19       0.0187       0.0184     0.000325       0.0799        0.105       0.0667        0.106       0.0865       0.0887        0.132         0.11         1.34        0.014\n",
            "     17    20       0.0154       0.0154     7.58e-06       0.0738       0.0961       0.0627        0.096       0.0793       0.0808        0.121        0.101         0.18      0.00188\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     17     1       0.0191       0.0191     7.28e-06       0.0813        0.107       0.0687        0.106       0.0875       0.0895        0.135        0.112        0.166      0.00173\n",
            "     17     2       0.0185       0.0185     4.05e-06       0.0797        0.105       0.0667        0.106       0.0862       0.0883        0.133        0.111        0.139      0.00145\n",
            "     17     3       0.0161       0.0161     4.95e-06       0.0754       0.0982       0.0635       0.0992       0.0814        0.082        0.124        0.103        0.138      0.00143\n",
            "     17     4       0.0171       0.0171     5.58e-06       0.0772        0.101        0.066       0.0996       0.0828       0.0861        0.126        0.106        0.141      0.00147\n",
            "     17     5       0.0169       0.0169     2.97e-06       0.0769          0.1       0.0643        0.102       0.0831       0.0831        0.128        0.106        0.101      0.00105\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              17   64.951    0.005       0.0164     0.000143       0.0166       0.0758       0.0992       0.0641       0.0992       0.0817       0.0832        0.125        0.104        0.697      0.00726\n",
            "! Validation         17   64.951    0.005       0.0176     4.97e-06       0.0176       0.0781        0.102       0.0658        0.103       0.0842       0.0858         0.13        0.108        0.137      0.00143\n",
            "Wall time: 64.95244406500001\n",
            "! Best model       17    0.018\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     18     1       0.0173        0.017      0.00027       0.0756        0.101       0.0662       0.0945       0.0804       0.0879        0.123        0.105         1.22       0.0127\n",
            "     18     2       0.0149       0.0148     0.000146       0.0734        0.094       0.0623       0.0958        0.079       0.0782         0.12       0.0989        0.895      0.00932\n",
            "     18     3       0.0187       0.0187     3.53e-06       0.0803        0.106       0.0656         0.11       0.0876       0.0865        0.136        0.111       0.0998      0.00104\n",
            "     18     4       0.0169       0.0168      0.00015       0.0765          0.1       0.0633        0.103       0.0831       0.0816         0.13        0.106        0.886      0.00923\n",
            "     18     5       0.0156       0.0155     0.000135       0.0723       0.0963       0.0595       0.0979       0.0787       0.0795        0.123        0.101        0.854       0.0089\n",
            "     18     6       0.0149       0.0149     6.99e-06       0.0714       0.0945       0.0602       0.0938        0.077       0.0792        0.119       0.0993        0.149      0.00155\n",
            "     18     7        0.016       0.0159     0.000112       0.0743       0.0976       0.0615          0.1       0.0808       0.0804        0.125        0.103        0.771      0.00804\n",
            "     18     8       0.0166       0.0165     2.75e-05       0.0772       0.0995       0.0666       0.0984       0.0825       0.0847        0.124        0.104        0.382      0.00398\n",
            "     18     9       0.0158       0.0158      2.1e-06       0.0755       0.0972       0.0654       0.0958       0.0806        0.083        0.121        0.102       0.0895     0.000932\n",
            "     18    10       0.0169       0.0169     2.66e-05       0.0773          0.1       0.0662       0.0994       0.0828       0.0864        0.124        0.105        0.365      0.00381\n",
            "     18    11        0.015        0.015     1.09e-06       0.0738       0.0947       0.0648       0.0917       0.0782       0.0822        0.116       0.0989       0.0662      0.00069\n",
            "     18    12       0.0147       0.0147     2.77e-06       0.0726       0.0938       0.0612       0.0953       0.0783        0.077        0.121       0.0988        0.108      0.00113\n",
            "     18    13        0.016        0.016     9.74e-06       0.0762       0.0978       0.0661       0.0965       0.0813       0.0856        0.119        0.102        0.219      0.00228\n",
            "     18    14        0.017        0.017     5.32e-06       0.0785        0.101       0.0681       0.0992       0.0837       0.0856        0.126        0.106        0.138      0.00144\n",
            "     18    15       0.0178       0.0178     6.32e-06       0.0794        0.103       0.0664        0.105       0.0858       0.0845        0.133        0.109        0.178      0.00185\n",
            "     18    16       0.0154       0.0153     3.88e-05       0.0747       0.0957       0.0655       0.0931       0.0793       0.0828        0.117          0.1        0.454      0.00472\n",
            "     18    17       0.0144       0.0143     6.04e-05       0.0704       0.0926       0.0591        0.093        0.076       0.0767        0.118       0.0974        0.562      0.00586\n",
            "     18    18       0.0153       0.0152     6.33e-06       0.0726       0.0955       0.0636       0.0906       0.0771        0.083        0.117       0.0998        0.154      0.00161\n",
            "     18    19        0.019        0.019     8.84e-06       0.0824        0.107       0.0691        0.109       0.0891        0.088        0.136        0.112        0.204      0.00212\n",
            "     18    20       0.0176       0.0176     3.12e-05        0.079        0.103        0.069        0.099        0.084       0.0898        0.124        0.107        0.412      0.00429\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     18     1       0.0181       0.0181     6.91e-06       0.0791        0.104       0.0665        0.104       0.0853       0.0867        0.132         0.11        0.162      0.00169\n",
            "     18     2       0.0176       0.0176     3.65e-06       0.0776        0.103       0.0647        0.103        0.084       0.0855         0.13        0.108        0.131      0.00137\n",
            "     18     3       0.0153       0.0153     4.78e-06       0.0734       0.0958       0.0614       0.0973       0.0794       0.0794        0.122        0.101        0.136      0.00142\n",
            "     18     4       0.0164       0.0164     5.42e-06       0.0754        0.099       0.0642       0.0977       0.0809       0.0838        0.124        0.104         0.14      0.00146\n",
            "     18     5        0.016        0.016     2.85e-06       0.0748        0.098       0.0622       0.0999       0.0811       0.0806        0.126        0.103       0.0974      0.00101\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              18   68.461    0.005       0.0162     5.25e-05       0.0163       0.0757       0.0986       0.0645       0.0981       0.0813       0.0832        0.124        0.103         0.41      0.00428\n",
            "! Validation         18   68.461    0.005       0.0167     4.72e-06       0.0167        0.076          0.1       0.0638        0.101       0.0822       0.0832        0.127        0.105        0.133      0.00139\n",
            "Wall time: 68.46158432900006\n",
            "! Best model       18    0.017\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     19     1       0.0156       0.0155     4.04e-05       0.0732       0.0964       0.0645       0.0905       0.0775       0.0855        0.115          0.1        0.465      0.00485\n",
            "     19     2       0.0133       0.0133     4.29e-05       0.0674       0.0891       0.0555       0.0912       0.0734       0.0726        0.115       0.0939        0.478      0.00498\n",
            "     19     3       0.0136       0.0136     9.17e-07       0.0696       0.0901       0.0613       0.0861       0.0737       0.0785         0.11        0.094       0.0516     0.000537\n",
            "     19     4       0.0143       0.0142     8.88e-05       0.0709       0.0923       0.0591       0.0947       0.0769       0.0755        0.119       0.0973        0.698      0.00727\n",
            "     19     5       0.0158       0.0158     7.78e-05       0.0742       0.0971        0.063       0.0965       0.0797       0.0829        0.121        0.102         0.65      0.00677\n",
            "     19     6       0.0162       0.0162     2.39e-05       0.0758       0.0985       0.0642        0.099       0.0816       0.0825        0.125        0.104        0.323      0.00336\n",
            "     19     7       0.0148       0.0148     1.55e-05       0.0719       0.0941       0.0583        0.099       0.0786       0.0757        0.123       0.0993        0.281      0.00293\n",
            "     19     8       0.0132        0.013       0.0002       0.0675       0.0882        0.058       0.0867       0.0723       0.0747         0.11       0.0926         1.04       0.0109\n",
            "     19     9       0.0142       0.0141     0.000141       0.0703       0.0918       0.0584        0.094       0.0762       0.0756        0.118       0.0967        0.879      0.00916\n",
            "     19    10       0.0188       0.0188     5.61e-06       0.0805        0.106       0.0669        0.108       0.0873       0.0875        0.136        0.112        0.153      0.00159\n",
            "     19    11       0.0242       0.0242     3.23e-05       0.0933         0.12       0.0818        0.116       0.0991        0.105        0.146        0.126        0.402      0.00419\n",
            "     19    12       0.0244       0.0241     0.000252       0.0959         0.12       0.0883        0.111       0.0997         0.11        0.139        0.124         1.17       0.0122\n",
            "     19    13       0.0173       0.0173     2.28e-06       0.0782        0.102       0.0692        0.096       0.0826         0.09        0.122        0.106       0.0904     0.000942\n",
            "     19    14       0.0173       0.0173     1.88e-05       0.0785        0.102       0.0677          0.1       0.0839       0.0878        0.125        0.106        0.316      0.00329\n",
            "     19    15       0.0243       0.0242     6.33e-05       0.0959         0.12       0.0914        0.105       0.0982        0.114        0.133        0.123         0.59      0.00615\n",
            "     19    16       0.0213       0.0213     3.16e-06       0.0887        0.113       0.0797        0.107       0.0932       0.0993        0.136        0.118        0.112      0.00117\n",
            "     19    17       0.0157       0.0157     1.57e-06        0.074       0.0969       0.0619       0.0983       0.0801       0.0813        0.122        0.102       0.0781     0.000814\n",
            "     19    18       0.0158       0.0158     3.78e-05       0.0747       0.0972       0.0637       0.0968       0.0802       0.0828        0.121        0.102        0.429      0.00447\n",
            "     19    19       0.0164       0.0163     0.000106       0.0749       0.0987       0.0646       0.0956       0.0801       0.0844        0.122        0.103        0.763      0.00795\n",
            "     19    20       0.0174       0.0174     6.16e-06       0.0767        0.102       0.0644        0.101       0.0829       0.0855        0.129        0.107        0.175      0.00182\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     19     1       0.0173       0.0173     5.49e-06       0.0772        0.102       0.0646        0.102       0.0834       0.0844         0.13        0.107        0.141      0.00147\n",
            "     19     2       0.0168       0.0168     2.89e-06       0.0757          0.1       0.0629        0.101       0.0821       0.0832        0.128        0.105        0.117      0.00122\n",
            "     19     3       0.0146       0.0146     3.65e-06       0.0716       0.0935       0.0597       0.0953       0.0775       0.0773        0.119       0.0984        0.122      0.00127\n",
            "     19     4       0.0157       0.0157     3.76e-06       0.0737       0.0969       0.0626        0.096       0.0793       0.0816        0.122        0.102         0.12      0.00125\n",
            "     19     5       0.0154       0.0154     2.22e-06        0.073       0.0959       0.0606       0.0979       0.0792       0.0785        0.123        0.101       0.0944     0.000984\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              19   71.916    0.005       0.0171      5.8e-05       0.0172       0.0776        0.101       0.0671       0.0986       0.0829       0.0873        0.125        0.106        0.458      0.00477\n",
            "! Validation         19   71.916    0.005        0.016      3.6e-06        0.016       0.0742       0.0977       0.0621       0.0985       0.0803        0.081        0.125        0.103        0.119      0.00124\n",
            "Wall time: 71.91696148200003\n",
            "! Best model       19    0.016\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     20     1       0.0157       0.0157     6.58e-06       0.0746       0.0968       0.0635       0.0968       0.0801       0.0823        0.121        0.102        0.138      0.00143\n",
            "     20     2       0.0134       0.0134     6.41e-06       0.0688       0.0896       0.0579       0.0905       0.0742       0.0741        0.114       0.0943        0.173      0.00181\n",
            "     20     3       0.0169       0.0168     2.43e-05        0.076          0.1       0.0652       0.0976       0.0814       0.0857        0.125        0.105        0.338      0.00352\n",
            "     20     4       0.0174       0.0174      4.4e-06       0.0775        0.102       0.0649        0.103       0.0838       0.0853        0.129        0.107        0.136      0.00141\n",
            "     20     5       0.0143       0.0143     1.75e-05       0.0703       0.0925       0.0583       0.0942       0.0763       0.0775        0.117       0.0972        0.298       0.0031\n",
            "     20     6       0.0139       0.0139     2.44e-05       0.0705       0.0911       0.0594       0.0927        0.076       0.0762        0.115       0.0958        0.359      0.00374\n",
            "     20     7       0.0141       0.0141     9.73e-06       0.0696       0.0918       0.0578       0.0933       0.0755        0.075        0.118       0.0967        0.211       0.0022\n",
            "     20     8       0.0125       0.0125     3.98e-06       0.0664       0.0866       0.0543       0.0908       0.0725       0.0693        0.114       0.0914        0.113      0.00118\n",
            "     20     9       0.0132       0.0132     5.65e-06       0.0681       0.0889       0.0582       0.0879        0.073       0.0755        0.111       0.0932        0.162      0.00169\n",
            "     20    10       0.0179       0.0178     9.25e-05       0.0804        0.103       0.0676        0.106       0.0868       0.0851        0.132        0.109        0.698      0.00727\n",
            "     20    11       0.0169       0.0169     1.31e-05       0.0768          0.1       0.0662        0.098       0.0821       0.0869        0.123        0.105        0.243      0.00253\n",
            "     20    12       0.0136       0.0136     3.86e-06       0.0693       0.0903       0.0581       0.0915       0.0748       0.0752        0.115       0.0949         0.12      0.00125\n",
            "     20    13       0.0131       0.0131     2.69e-05        0.067       0.0885        0.055       0.0912       0.0731       0.0704        0.117       0.0935        0.381      0.00397\n",
            "     20    14       0.0133       0.0133     3.07e-05       0.0687       0.0891       0.0581       0.0901       0.0741       0.0736        0.114       0.0937        0.387      0.00403\n",
            "     20    15       0.0137       0.0137     1.08e-05       0.0695       0.0904       0.0578       0.0928       0.0753        0.075        0.115       0.0951        0.228      0.00237\n",
            "     20    16       0.0136       0.0136     7.41e-06       0.0678       0.0903       0.0555       0.0925        0.074       0.0731        0.118       0.0953        0.195      0.00203\n",
            "     20    17       0.0163       0.0163     1.06e-05       0.0753       0.0987        0.064       0.0979        0.081       0.0825        0.125        0.104        0.241      0.00251\n",
            "     20    18       0.0177       0.0177     6.36e-06       0.0809        0.103       0.0723        0.098       0.0852       0.0919        0.122        0.107        0.122      0.00128\n",
            "     20    19       0.0199       0.0199     1.05e-05       0.0827        0.109       0.0701        0.108        0.089       0.0936        0.135        0.114        0.216      0.00225\n",
            "     20    20       0.0132       0.0132      3.3e-05       0.0682       0.0888       0.0577       0.0892       0.0734       0.0748        0.112       0.0933        0.416      0.00434\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     20     1       0.0166       0.0166     5.08e-06       0.0755       0.0997       0.0631          0.1       0.0817       0.0824        0.127        0.105        0.134       0.0014\n",
            "     20     2       0.0161       0.0161     2.69e-06        0.074       0.0981       0.0613       0.0993       0.0803        0.081        0.125        0.103        0.112      0.00117\n",
            "     20     3        0.014        0.014     3.51e-06         0.07       0.0916       0.0582       0.0937       0.0759       0.0754        0.118       0.0965        0.121      0.00126\n",
            "     20     4       0.0151       0.0151     3.38e-06       0.0722        0.095       0.0612       0.0944       0.0778       0.0797         0.12       0.0998        0.115       0.0012\n",
            "     20     5       0.0147       0.0147     2.11e-06       0.0713       0.0938        0.059        0.096       0.0775       0.0766        0.121       0.0989       0.0921     0.000959\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              20   75.379    0.005        0.015     1.74e-05        0.015       0.0724       0.0948       0.0611       0.0951       0.0781       0.0794         0.12       0.0996        0.259       0.0027\n",
            "! Validation         20   75.379    0.005       0.0153     3.36e-06       0.0153       0.0726       0.0957       0.0606       0.0967       0.0786       0.0791        0.122        0.101        0.115       0.0012\n",
            "Wall time: 75.38023539599999\n",
            "! Best model       20    0.015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     21     1       0.0124       0.0123     7.36e-05       0.0663       0.0859        0.056       0.0869       0.0715       0.0709         0.11       0.0904        0.631      0.00657\n",
            "     21     2       0.0172       0.0172     1.71e-06       0.0781        0.101       0.0657        0.103       0.0843       0.0862        0.126        0.106       0.0775     0.000808\n",
            "     21     3       0.0169       0.0169     4.67e-05       0.0767        0.101       0.0654       0.0992       0.0823       0.0847        0.126        0.106        0.485      0.00505\n",
            "     21     4       0.0174       0.0173      0.00015       0.0793        0.102       0.0695       0.0989       0.0842       0.0885        0.124        0.106        0.906      0.00944\n",
            "     21     5        0.015        0.015     6.33e-06       0.0734       0.0947       0.0614       0.0972       0.0793       0.0785        0.121       0.0996        0.169      0.00176\n",
            "     21     6       0.0147       0.0147     4.79e-06       0.0718       0.0937       0.0607       0.0939       0.0773       0.0787        0.118       0.0984         0.15      0.00156\n",
            "     21     7        0.016       0.0159     7.51e-05        0.073       0.0975       0.0609        0.097        0.079       0.0788        0.127        0.103        0.631      0.00658\n",
            "     21     8       0.0188       0.0187     0.000147       0.0831        0.106       0.0739        0.101       0.0877       0.0929        0.128         0.11        0.897      0.00934\n",
            "     21     9       0.0121       0.0121     2.28e-06       0.0661        0.085       0.0564       0.0854       0.0709       0.0708        0.108       0.0894       0.0893      0.00093\n",
            "     21    10       0.0124       0.0123     5.75e-05        0.067        0.086       0.0569        0.087        0.072       0.0718        0.109       0.0903        0.553      0.00576\n",
            "     21    11       0.0156       0.0155     8.57e-05       0.0752       0.0964       0.0675       0.0906        0.079       0.0848        0.116          0.1        0.677      0.00705\n",
            "     21    12       0.0151       0.0151      7.4e-05       0.0721       0.0949       0.0596       0.0971       0.0783       0.0788        0.121       0.0998        0.625      0.00651\n",
            "     21    13       0.0138       0.0138     3.02e-06       0.0685        0.091       0.0576       0.0903       0.0739       0.0755        0.116       0.0957        0.126      0.00131\n",
            "     21    14       0.0161       0.0161     4.51e-05       0.0751       0.0981       0.0608        0.104       0.0823       0.0796        0.127        0.103        0.496      0.00516\n",
            "     21    15       0.0147       0.0147     2.61e-05       0.0717       0.0938       0.0625       0.0901       0.0763         0.08        0.117       0.0983        0.366      0.00381\n",
            "     21    16        0.015        0.015      8.1e-06       0.0728       0.0947       0.0606        0.097       0.0788       0.0786        0.121       0.0996        0.185      0.00192\n",
            "     21    17       0.0129       0.0129     3.33e-06       0.0667       0.0878       0.0538       0.0923       0.0731       0.0693        0.116       0.0927        0.118      0.00123\n",
            "     21    18       0.0148       0.0148     1.62e-06       0.0727       0.0942       0.0629       0.0922       0.0776       0.0799        0.118       0.0988       0.0916     0.000954\n",
            "     21    19       0.0163       0.0163     2.82e-05       0.0753       0.0987       0.0652       0.0956       0.0804       0.0836        0.123        0.104        0.386      0.00402\n",
            "     21    20        0.015        0.015     2.43e-05       0.0727       0.0947       0.0632       0.0916       0.0774       0.0821        0.116        0.099         0.36      0.00375\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     21     1        0.016        0.016     5.19e-06        0.074       0.0977       0.0616       0.0986       0.0801       0.0806        0.125        0.103        0.139      0.00145\n",
            "     21     2       0.0154       0.0154     2.54e-06       0.0724       0.0961       0.0598       0.0976       0.0787       0.0791        0.123        0.101        0.108      0.00113\n",
            "     21     3       0.0135       0.0135     3.53e-06       0.0686       0.0898       0.0569       0.0921       0.0745       0.0736        0.116       0.0946         0.12      0.00125\n",
            "     21     4       0.0146       0.0146     3.75e-06       0.0709       0.0933       0.0598       0.0929       0.0764       0.0779        0.118       0.0981        0.118      0.00123\n",
            "     21     5       0.0142       0.0142     2.18e-06         0.07       0.0921       0.0578       0.0943       0.0761        0.075        0.119       0.0971       0.0902      0.00094\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              21   78.864    0.005       0.0151     4.32e-05       0.0151       0.0729        0.095        0.062       0.0945       0.0783       0.0799         0.12       0.0997        0.401      0.00418\n",
            "! Validation         21   78.864    0.005       0.0147     3.44e-06       0.0147       0.0712       0.0939       0.0592       0.0951       0.0771       0.0773         0.12       0.0988        0.115       0.0012\n",
            "Wall time: 78.86449024500007\n",
            "! Best model       21    0.015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     22     1       0.0139       0.0138     8.64e-05       0.0687       0.0909       0.0574       0.0911       0.0743       0.0751        0.116       0.0957        0.681       0.0071\n",
            "     22     2       0.0168       0.0168     6.35e-05       0.0764          0.1       0.0655       0.0981       0.0818       0.0849        0.125        0.105         0.58      0.00604\n",
            "     22     3       0.0183       0.0183     4.96e-06       0.0796        0.105       0.0703       0.0982       0.0842       0.0937        0.123        0.109        0.112      0.00116\n",
            "     22     4       0.0135       0.0134     8.89e-05       0.0695       0.0895       0.0598        0.089       0.0744       0.0752        0.113       0.0939        0.697      0.00727\n",
            "     22     5       0.0129       0.0128     9.57e-05       0.0679       0.0875       0.0581       0.0875       0.0728       0.0739         0.11       0.0918        0.717      0.00747\n",
            "     22     6       0.0148       0.0148     4.11e-06       0.0718       0.0941       0.0596       0.0964        0.078       0.0759        0.123       0.0993        0.121      0.00126\n",
            "     22     7       0.0157       0.0157     8.18e-07       0.0737       0.0971       0.0606          0.1       0.0803       0.0781        0.127        0.102       0.0619     0.000645\n",
            "     22     8       0.0154       0.0153     5.85e-05       0.0736       0.0958       0.0621       0.0967       0.0794       0.0787        0.123        0.101        0.567       0.0059\n",
            "     22     9        0.013        0.013     3.17e-05       0.0682       0.0882       0.0564       0.0917       0.0741       0.0706        0.116       0.0931        0.409      0.00426\n",
            "     22    10       0.0153       0.0153     5.61e-06        0.073       0.0958       0.0613       0.0964       0.0788       0.0802        0.121        0.101        0.136      0.00141\n",
            "     22    11       0.0127       0.0127     2.44e-06       0.0657       0.0871       0.0548       0.0876       0.0712        0.072        0.111       0.0916       0.0949     0.000989\n",
            "     22    12       0.0137       0.0137     5.25e-06        0.069       0.0906       0.0575        0.092       0.0748        0.075        0.116       0.0954        0.165      0.00172\n",
            "     22    13       0.0121       0.0121     2.99e-05       0.0638        0.085       0.0524       0.0866       0.0695       0.0695         0.11       0.0895          0.4      0.00417\n",
            "     22    14        0.012        0.012     2.85e-06        0.066       0.0848       0.0551       0.0876       0.0714       0.0703        0.108       0.0893        0.099      0.00103\n",
            "     22    15       0.0135       0.0135     1.24e-05       0.0693       0.0899       0.0565       0.0949       0.0757       0.0735        0.116       0.0947        0.253      0.00263\n",
            "     22    16       0.0117       0.0117     5.39e-06        0.064       0.0836       0.0539       0.0842        0.069       0.0693        0.107       0.0879        0.161      0.00168\n",
            "     22    17       0.0127       0.0126     3.31e-05       0.0657       0.0869       0.0544       0.0883       0.0713       0.0718        0.111       0.0915        0.412      0.00429\n",
            "     22    18       0.0118       0.0118     1.68e-05       0.0634       0.0841       0.0526       0.0849       0.0688       0.0688        0.108       0.0887         0.29      0.00302\n",
            "     22    19        0.012        0.012     1.03e-05       0.0651       0.0847       0.0561       0.0832       0.0696        0.071        0.107        0.089        0.227      0.00236\n",
            "     22    20        0.013        0.013     4.43e-05       0.0669       0.0881       0.0537       0.0933       0.0735       0.0685        0.118       0.0932        0.489      0.00509\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     22     1       0.0153       0.0153      4.7e-06       0.0725       0.0958       0.0602        0.097       0.0786       0.0788        0.123        0.101         0.13      0.00136\n",
            "     22     2       0.0148       0.0148      2.2e-06        0.071       0.0942       0.0584        0.096       0.0772       0.0772        0.121       0.0992       0.0997      0.00104\n",
            "     22     3        0.013        0.013     3.21e-06       0.0673       0.0882       0.0557       0.0907       0.0732        0.072        0.114        0.093        0.115       0.0012\n",
            "     22     4       0.0141       0.0141     3.13e-06       0.0695       0.0917       0.0586       0.0915        0.075       0.0762        0.117       0.0965         0.11      0.00114\n",
            "     22     5       0.0137       0.0137     2.07e-06       0.0686       0.0904       0.0566       0.0927       0.0746       0.0735        0.117       0.0953        0.089     0.000927\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              22   82.374    0.005       0.0137     3.01e-05       0.0137       0.0691       0.0906       0.0579       0.0914       0.0746        0.075        0.116       0.0953        0.334      0.00348\n",
            "! Validation         22   82.374    0.005       0.0142     3.06e-06       0.0142       0.0698       0.0921       0.0579       0.0936       0.0757       0.0756        0.118        0.097        0.109      0.00113\n",
            "Wall time: 82.37608869199994\n",
            "! Best model       22    0.014\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     23     1       0.0115       0.0115     3.74e-07        0.064        0.083       0.0533       0.0855       0.0694       0.0693        0.105       0.0872       0.0445     0.000464\n",
            "     23     2       0.0126       0.0126     2.58e-05       0.0682       0.0869       0.0582       0.0882       0.0732       0.0734        0.109       0.0912         0.37      0.00385\n",
            "     23     3       0.0134       0.0134     5.11e-05       0.0682       0.0894       0.0562       0.0922       0.0742       0.0734        0.115       0.0941        0.528       0.0055\n",
            "     23     4       0.0141       0.0141     1.57e-05       0.0693       0.0918       0.0567       0.0946       0.0756       0.0747        0.119       0.0968        0.282      0.00293\n",
            "     23     5       0.0128       0.0127      9.1e-05       0.0658       0.0872       0.0547       0.0881       0.0714       0.0713        0.113       0.0919          0.7      0.00729\n",
            "     23     6       0.0142       0.0142     1.79e-05       0.0702        0.092       0.0584       0.0939       0.0762       0.0759        0.118       0.0969         0.31      0.00323\n",
            "     23     7       0.0127       0.0127     6.26e-05       0.0664        0.087       0.0554       0.0885       0.0719       0.0726         0.11       0.0915        0.572      0.00596\n",
            "     23     8       0.0141       0.0141     7.47e-05       0.0697       0.0917        0.057       0.0951       0.0761       0.0747        0.119       0.0967        0.621      0.00647\n",
            "     23     9       0.0132       0.0132      6.4e-06        0.067        0.089       0.0561       0.0887       0.0724       0.0728        0.115       0.0938        0.161      0.00168\n",
            "     23    10       0.0125       0.0124     8.05e-05       0.0656       0.0862        0.055       0.0869       0.0709        0.072        0.109       0.0906        0.648      0.00675\n",
            "     23    11       0.0135       0.0135     1.14e-05        0.069       0.0899       0.0587       0.0898       0.0742       0.0746        0.115       0.0946        0.232      0.00242\n",
            "     23    12       0.0131       0.0131     5.88e-06       0.0686       0.0887       0.0583       0.0892       0.0737        0.074        0.112       0.0932        0.156      0.00163\n",
            "     23    13       0.0116       0.0115     0.000104       0.0635        0.083       0.0527       0.0849       0.0688       0.0688        0.106       0.0873        0.752      0.00784\n",
            "     23    14       0.0127       0.0127     3.51e-06       0.0671       0.0871       0.0569       0.0876       0.0722       0.0729         0.11       0.0915        0.122      0.00127\n",
            "     23    15       0.0166       0.0165     7.52e-05       0.0777       0.0993       0.0716       0.0899       0.0808       0.0897        0.116        0.103        0.635      0.00662\n",
            "     23    16        0.016        0.016     4.78e-06       0.0772       0.0978       0.0701       0.0915       0.0808       0.0883        0.114        0.101        0.136      0.00142\n",
            "     23    17       0.0144       0.0144     4.31e-06       0.0715       0.0927       0.0596       0.0953       0.0774       0.0749        0.121       0.0978        0.152      0.00159\n",
            "     23    18       0.0146       0.0146     4.27e-05       0.0701       0.0934       0.0582       0.0939        0.076       0.0777        0.119       0.0982        0.477      0.00497\n",
            "     23    19       0.0141       0.0141     1.14e-06       0.0706       0.0919        0.057       0.0978       0.0774       0.0722        0.122       0.0971       0.0627     0.000653\n",
            "     23    20       0.0187       0.0186     9.52e-05       0.0827        0.106       0.0732        0.102       0.0874       0.0914        0.129         0.11        0.718      0.00748\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     23     1       0.0148       0.0148     4.49e-06       0.0712       0.0941        0.059       0.0955       0.0772       0.0772        0.121       0.0991         0.13      0.00135\n",
            "     23     2       0.0143       0.0143     1.95e-06       0.0695       0.0924       0.0571       0.0945       0.0758       0.0755        0.119       0.0974       0.0927     0.000965\n",
            "     23     3       0.0126       0.0126     2.97e-06       0.0661       0.0867       0.0545       0.0892       0.0719       0.0705        0.112       0.0914        0.111      0.00116\n",
            "     23     4       0.0136       0.0136     2.95e-06       0.0683       0.0902       0.0574       0.0901       0.0738       0.0747        0.115       0.0949        0.107      0.00112\n",
            "     23     5       0.0132       0.0132     2.06e-06       0.0673       0.0888       0.0554       0.0911       0.0733       0.0721        0.115       0.0937       0.0873     0.000909\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              23   85.850    0.005       0.0138     3.87e-05       0.0138       0.0696       0.0908       0.0589       0.0912        0.075        0.076        0.115       0.0955        0.384        0.004\n",
            "! Validation         23   85.850    0.005       0.0137     2.88e-06       0.0137       0.0685       0.0905       0.0567       0.0921       0.0744        0.074        0.117       0.0953        0.106       0.0011\n",
            "Wall time: 85.85057738499995\n",
            "! Best model       23    0.014\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     24     1       0.0198       0.0198     1.29e-06       0.0859        0.109       0.0783        0.101       0.0897       0.0981        0.128        0.113       0.0713     0.000743\n",
            "     24     2       0.0144       0.0144     1.59e-05       0.0713       0.0929       0.0615       0.0909       0.0762       0.0784        0.117       0.0975        0.289      0.00301\n",
            "     24     3       0.0117       0.0117     4.51e-05        0.063       0.0835       0.0518       0.0853       0.0686       0.0675        0.109       0.0881        0.482      0.00503\n",
            "     24     4       0.0148       0.0148     4.75e-06       0.0736       0.0943       0.0632       0.0943       0.0788       0.0793        0.119        0.099        0.157      0.00163\n",
            "     24     5       0.0137       0.0136     9.48e-05       0.0689       0.0902       0.0584       0.0897       0.0741        0.075        0.115       0.0948        0.722      0.00752\n",
            "     24     6       0.0122       0.0122     2.68e-05       0.0642       0.0855       0.0526       0.0874         0.07       0.0701         0.11       0.0901         0.37      0.00386\n",
            "     24     7       0.0126       0.0126     2.17e-05       0.0671       0.0868       0.0568       0.0877       0.0722       0.0727         0.11       0.0912        0.334      0.00348\n",
            "     24     8       0.0138       0.0138     2.62e-05       0.0699       0.0907       0.0593       0.0911       0.0752       0.0761        0.115       0.0953        0.366      0.00381\n",
            "     24     9       0.0137       0.0136     1.95e-05        0.068       0.0904       0.0555       0.0931       0.0743       0.0742        0.116       0.0952        0.313      0.00326\n",
            "     24    10       0.0132       0.0132     8.02e-06       0.0688       0.0889       0.0579       0.0907       0.0743       0.0744        0.112       0.0934        0.202       0.0021\n",
            "     24    11       0.0145       0.0144     7.74e-05       0.0715       0.0928        0.058       0.0985       0.0782        0.073        0.123       0.0981        0.647      0.00674\n",
            "     24    12       0.0126       0.0126     4.57e-06       0.0655       0.0869       0.0559       0.0846       0.0703       0.0732        0.109       0.0912        0.155      0.00161\n",
            "     24    13       0.0126       0.0126     1.62e-05        0.068       0.0869       0.0591       0.0856       0.0724       0.0749        0.107       0.0909        0.279      0.00291\n",
            "     24    14       0.0128       0.0128     1.42e-05        0.067       0.0874       0.0555       0.0901       0.0728       0.0711        0.113       0.0921        0.259       0.0027\n",
            "     24    15       0.0163       0.0163     7.31e-06       0.0756       0.0988       0.0646       0.0978       0.0812       0.0833        0.124        0.104        0.196      0.00204\n",
            "     24    16       0.0134       0.0133     6.97e-05       0.0686       0.0892       0.0557       0.0945       0.0751       0.0701        0.119       0.0943        0.611      0.00637\n",
            "     24    17       0.0141        0.014     4.36e-06       0.0708       0.0917       0.0576       0.0973       0.0774       0.0734         0.12       0.0968        0.122      0.00127\n",
            "     24    18       0.0125       0.0125     3.83e-05       0.0672       0.0864       0.0574       0.0868       0.0721       0.0735        0.108       0.0905        0.458      0.00477\n",
            "     24    19       0.0121       0.0121     2.82e-05       0.0641       0.0852       0.0532       0.0859       0.0696       0.0696         0.11       0.0897        0.388      0.00404\n",
            "     24    20       0.0111       0.0111     5.33e-07       0.0625       0.0815       0.0529       0.0816       0.0672       0.0682        0.103       0.0857        0.042     0.000437\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     24     1       0.0143       0.0143     3.98e-06         0.07       0.0925       0.0579       0.0941        0.076       0.0758        0.119       0.0974        0.122      0.00127\n",
            "     24     2       0.0138       0.0138      1.8e-06       0.0684        0.091        0.056       0.0932       0.0746       0.0741        0.118       0.0959       0.0884     0.000921\n",
            "     24     3       0.0122       0.0122     2.69e-06        0.065       0.0853       0.0535       0.0879       0.0707       0.0692        0.111         0.09        0.107      0.00112\n",
            "     24     4       0.0132       0.0132     2.48e-06       0.0672       0.0889       0.0564       0.0889       0.0726       0.0733        0.114       0.0936        0.101      0.00106\n",
            "     24     5       0.0128       0.0128     1.86e-06       0.0662       0.0875       0.0545       0.0897       0.0721       0.0709        0.114       0.0922       0.0846     0.000881\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              24   89.340    0.005       0.0136     2.62e-05       0.0136       0.0691       0.0901       0.0583       0.0907       0.0745       0.0751        0.114       0.0947        0.323      0.00337\n",
            "! Validation         24   89.340    0.005       0.0133     2.56e-06       0.0133       0.0674       0.0891       0.0557       0.0907       0.0732       0.0727        0.115       0.0939        0.101      0.00105\n",
            "Wall time: 89.3414332079999\n",
            "! Best model       24    0.013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     25     1        0.015       0.0149     4.64e-05       0.0716       0.0945       0.0599       0.0951       0.0775       0.0791        0.119       0.0992        0.498      0.00518\n",
            "     25     2        0.014        0.014     6.44e-06       0.0701       0.0917       0.0578       0.0947       0.0762       0.0729        0.121       0.0968        0.132      0.00138\n",
            "     25     3       0.0134       0.0134     2.32e-06       0.0695       0.0896       0.0604       0.0879       0.0741       0.0755        0.113       0.0941       0.0797      0.00083\n",
            "     25     4        0.012        0.012      5.3e-05       0.0661       0.0846       0.0566       0.0852       0.0709       0.0718        0.106       0.0888        0.512      0.00533\n",
            "     25     5       0.0109       0.0109     1.13e-06       0.0606       0.0807       0.0502       0.0813       0.0658       0.0659        0.104        0.085       0.0684     0.000712\n",
            "     25     6       0.0121       0.0119     0.000198       0.0652       0.0843       0.0561       0.0835       0.0698       0.0717        0.105       0.0884         1.04       0.0109\n",
            "     25     7       0.0136       0.0135     6.55e-05       0.0658         0.09       0.0524       0.0925       0.0725       0.0723        0.118        0.095          0.6      0.00625\n",
            "     25     8       0.0117       0.0116     3.93e-05       0.0639       0.0835       0.0537       0.0844        0.069       0.0684        0.108        0.088        0.458      0.00477\n",
            "     25     9       0.0111        0.011     0.000104       0.0626       0.0811        0.052       0.0838       0.0679        0.066        0.105       0.0855        0.755      0.00786\n",
            "     25    10       0.0121       0.0121      7.8e-05       0.0654       0.0849       0.0545       0.0872       0.0708       0.0692         0.11       0.0895         0.65      0.00677\n",
            "     25    11       0.0128       0.0127     2.11e-05       0.0676       0.0873       0.0588       0.0853       0.0721       0.0744        0.109       0.0915        0.338      0.00352\n",
            "     25    12        0.012        0.012     6.88e-05       0.0649       0.0847       0.0536       0.0875       0.0705       0.0683         0.11       0.0893        0.612      0.00638\n",
            "     25    13       0.0134       0.0134     7.47e-07       0.0676       0.0894       0.0565       0.0898       0.0731       0.0746        0.113        0.094       0.0531     0.000553\n",
            "     25    14       0.0113       0.0113     2.27e-05       0.0631       0.0823       0.0535       0.0823       0.0679       0.0681        0.105       0.0866        0.338      0.00353\n",
            "     25    15        0.012       0.0119     1.49e-05       0.0645       0.0845       0.0539       0.0857       0.0698       0.0701        0.108       0.0889        0.264      0.00275\n",
            "     25    16       0.0117       0.0116     2.08e-05        0.062       0.0835       0.0492       0.0877       0.0684       0.0653        0.111       0.0883        0.323      0.00337\n",
            "     25    17       0.0125       0.0124     4.61e-05       0.0654       0.0863       0.0529       0.0904       0.0717       0.0675        0.115       0.0912        0.493      0.00514\n",
            "     25    18        0.012        0.012     2.08e-06       0.0644       0.0847       0.0526       0.0879       0.0703       0.0688         0.11       0.0893       0.0902      0.00094\n",
            "     25    19       0.0125       0.0125      2.9e-05       0.0661       0.0865        0.054       0.0904       0.0722       0.0711        0.111       0.0911         0.39      0.00406\n",
            "     25    20       0.0106       0.0106     9.03e-06       0.0616       0.0796       0.0502       0.0843       0.0673       0.0628        0.105       0.0841        0.222      0.00231\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     25     1       0.0138       0.0138     3.73e-06       0.0689        0.091       0.0569       0.0928       0.0749       0.0745        0.117       0.0959        0.117      0.00122\n",
            "     25     2       0.0134       0.0134     1.64e-06       0.0672       0.0895       0.0549       0.0919       0.0734       0.0726        0.116       0.0944       0.0846     0.000881\n",
            "     25     3       0.0118       0.0118     2.48e-06        0.064       0.0841       0.0526       0.0867       0.0697        0.068        0.109       0.0887        0.104      0.00108\n",
            "     25     4       0.0128       0.0128     1.97e-06       0.0662       0.0876       0.0553       0.0878       0.0716       0.0719        0.113       0.0923       0.0911     0.000949\n",
            "     25     5       0.0124       0.0124     1.85e-06       0.0652       0.0861       0.0536       0.0883       0.0709       0.0698        0.112       0.0908       0.0843     0.000878\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              25   92.830    0.005       0.0123     4.15e-05       0.0123       0.0654       0.0858       0.0544       0.0873       0.0709       0.0703         0.11       0.0903        0.396      0.00413\n",
            "! Validation         25   92.830    0.005       0.0129     2.33e-06       0.0129       0.0663       0.0877       0.0547       0.0895       0.0721       0.0714        0.114       0.0925       0.0962        0.001\n",
            "Wall time: 92.83048846299994\n",
            "! Best model       25    0.013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     26     1       0.0136       0.0136     1.26e-05       0.0682       0.0901       0.0563       0.0919       0.0741       0.0741        0.116       0.0948         0.19      0.00198\n",
            "     26     2       0.0128       0.0128     4.96e-06       0.0677       0.0875       0.0597       0.0837       0.0717       0.0761        0.107       0.0914        0.146      0.00152\n",
            "     26     3       0.0121       0.0121     7.77e-06       0.0661       0.0852        0.056       0.0862       0.0711       0.0705        0.109       0.0897        0.201      0.00209\n",
            "     26     4       0.0124       0.0124     1.66e-05       0.0654       0.0863       0.0536        0.089       0.0713       0.0703        0.112       0.0909        0.292      0.00304\n",
            "     26     5         0.01         0.01     2.14e-05       0.0587       0.0774       0.0469       0.0824       0.0647       0.0606        0.103       0.0818         0.34      0.00354\n",
            "     26     6       0.0104       0.0104     2.25e-06       0.0602       0.0788       0.0483       0.0838       0.0661       0.0621        0.104       0.0833       0.0938     0.000977\n",
            "     26     7        0.012       0.0119     1.68e-05       0.0632       0.0846       0.0518       0.0861       0.0689       0.0685         0.11       0.0892        0.302      0.00314\n",
            "     26     8       0.0127       0.0126     4.02e-05       0.0667       0.0869       0.0568       0.0865       0.0717       0.0718        0.111       0.0915        0.466      0.00485\n",
            "     26     9       0.0137       0.0137     2.47e-06       0.0695       0.0904       0.0597       0.0892       0.0744       0.0762        0.114       0.0949        0.099      0.00103\n",
            "     26    10       0.0139       0.0139     1.73e-05        0.069       0.0913       0.0569       0.0931        0.075       0.0749        0.117       0.0961        0.297      0.00309\n",
            "     26    11       0.0109       0.0109     1.84e-05       0.0612       0.0806       0.0501       0.0835       0.0668       0.0642        0.106       0.0851        0.317       0.0033\n",
            "     26    12       0.0106       0.0106     9.74e-06       0.0618       0.0795       0.0519       0.0817       0.0668       0.0655        0.102       0.0837          0.2      0.00208\n",
            "     26    13       0.0136       0.0136     4.82e-06       0.0682       0.0901       0.0567       0.0912       0.0739       0.0724        0.118       0.0951        0.151      0.00157\n",
            "     26    14       0.0169       0.0169     1.98e-05       0.0789        0.101       0.0712       0.0941       0.0827       0.0903        0.119        0.104         0.32      0.00334\n",
            "     26    15       0.0159       0.0159     2.31e-06       0.0769       0.0976       0.0711       0.0885       0.0798       0.0893        0.112        0.101       0.0881     0.000918\n",
            "     26    16       0.0115       0.0115     1.39e-05       0.0639       0.0829       0.0537       0.0843        0.069       0.0683        0.106       0.0873        0.271      0.00283\n",
            "     26    17       0.0119       0.0118     4.34e-05       0.0646       0.0842        0.055       0.0839       0.0694       0.0711        0.106       0.0883        0.478      0.00498\n",
            "     26    18        0.017        0.017     1.34e-05       0.0781        0.101       0.0685       0.0973       0.0829        0.089        0.121        0.105        0.265      0.00276\n",
            "     26    19       0.0191       0.0189      0.00019       0.0845        0.106       0.0772       0.0992       0.0882       0.0957        0.125         0.11         1.02       0.0106\n",
            "     26    20       0.0186       0.0185     7.98e-05       0.0827        0.105       0.0709        0.106       0.0887       0.0884        0.133        0.111        0.659      0.00686\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     26     1       0.0134       0.0134     3.35e-06       0.0678       0.0896        0.056       0.0915       0.0737       0.0732        0.116       0.0944         0.11      0.00114\n",
            "     26     2        0.013        0.013     1.62e-06       0.0662       0.0881       0.0539       0.0907       0.0723       0.0713        0.115        0.093       0.0845      0.00088\n",
            "     26     3       0.0115       0.0115     2.28e-06        0.063       0.0829       0.0517       0.0855       0.0686       0.0668        0.108       0.0875       0.0988      0.00103\n",
            "     26     4       0.0125       0.0125     1.62e-06       0.0652       0.0864       0.0544       0.0868       0.0706       0.0707        0.111       0.0911       0.0832     0.000867\n",
            "     26     5        0.012        0.012     1.85e-06       0.0641       0.0849       0.0527       0.0869       0.0698       0.0688         0.11       0.0895       0.0841     0.000876\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              26   96.365    0.005       0.0134     2.69e-05       0.0135       0.0688       0.0897       0.0586       0.0891       0.0739       0.0756        0.113       0.0942         0.31      0.00323\n",
            "! Validation         26   96.365    0.005       0.0125     2.14e-06       0.0125       0.0653       0.0864       0.0538       0.0883        0.071       0.0702        0.112       0.0911       0.0921     0.000959\n",
            "Wall time: 96.36602801499998\n",
            "! Best model       26    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     27     1       0.0137       0.0137      5.6e-06       0.0697       0.0907        0.059       0.0912       0.0751       0.0743        0.117       0.0955        0.171      0.00178\n",
            "     27     2       0.0111       0.0109     0.000205       0.0602       0.0808       0.0501       0.0804       0.0652       0.0647        0.106       0.0854         1.06       0.0111\n",
            "     27     3       0.0144       0.0142     0.000222       0.0711       0.0921       0.0593       0.0948       0.0771       0.0756        0.118        0.097          1.1       0.0115\n",
            "     27     4       0.0226       0.0226      6.3e-05       0.0928        0.116       0.0868        0.105       0.0958        0.109         0.13        0.119        0.585       0.0061\n",
            "     27     5       0.0162       0.0162     2.19e-05       0.0764       0.0985       0.0683       0.0926       0.0804       0.0867        0.119        0.103        0.338      0.00352\n",
            "     27     6       0.0142       0.0141     2.82e-05       0.0679        0.092       0.0543       0.0952       0.0747       0.0748        0.119        0.097        0.393      0.00409\n",
            "     27     7       0.0145       0.0145     2.15e-06       0.0728       0.0931       0.0649       0.0886       0.0768       0.0823        0.112       0.0969       0.0955     0.000995\n",
            "     27     8       0.0194       0.0193     5.41e-05       0.0852        0.108       0.0756        0.104         0.09        0.093        0.132        0.112        0.536      0.00558\n",
            "     27     9       0.0168       0.0168      1.5e-05       0.0772          0.1       0.0666       0.0985       0.0825       0.0846        0.126        0.105        0.245      0.00255\n",
            "     27    10       0.0111       0.0111     5.43e-06        0.062       0.0816       0.0511       0.0836       0.0674       0.0664        0.106        0.086        0.139      0.00145\n",
            "     27    11       0.0176       0.0176     4.31e-05        0.081        0.103        0.072        0.099       0.0855       0.0903        0.123        0.107        0.475      0.00494\n",
            "     27    12       0.0148       0.0148     1.49e-06        0.073       0.0943       0.0654       0.0882       0.0768       0.0841        0.112        0.098       0.0768       0.0008\n",
            "     27    13       0.0116       0.0116     2.42e-05       0.0644       0.0833        0.052       0.0891       0.0706       0.0661         0.11        0.088        0.348      0.00363\n",
            "     27    14       0.0178       0.0178     3.88e-05       0.0813        0.103       0.0756       0.0927       0.0842        0.095        0.118        0.107        0.455      0.00474\n",
            "     27    15       0.0175       0.0174     6.85e-05       0.0802        0.102       0.0726       0.0955       0.0841       0.0902        0.122        0.106        0.611      0.00637\n",
            "     27    16       0.0111        0.011     5.22e-05       0.0614       0.0811       0.0513       0.0815       0.0664       0.0665        0.104       0.0855        0.531      0.00553\n",
            "     27    17        0.012        0.012     7.29e-06       0.0663       0.0847       0.0575       0.0839       0.0707        0.073        0.104       0.0886        0.192        0.002\n",
            "     27    18       0.0155       0.0155     5.49e-06       0.0766       0.0963       0.0651       0.0997       0.0824       0.0804        0.122        0.101        0.165      0.00172\n",
            "     27    19       0.0141       0.0141     2.34e-05       0.0698       0.0918       0.0593       0.0907        0.075       0.0784        0.114       0.0961        0.359      0.00374\n",
            "     27    20        0.014       0.0139     4.53e-05       0.0694       0.0912        0.057       0.0943       0.0756       0.0744        0.118       0.0962        0.484      0.00504\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     27     1       0.0131       0.0131     3.41e-06        0.067       0.0886       0.0552       0.0906       0.0729       0.0721        0.115       0.0933        0.112      0.00117\n",
            "     27     2       0.0127       0.0127     1.44e-06       0.0653       0.0871       0.0531       0.0896       0.0714       0.0703        0.113       0.0919       0.0793     0.000826\n",
            "     27     3       0.0112       0.0112     2.24e-06       0.0622       0.0819       0.0511       0.0845       0.0678       0.0659        0.107       0.0864       0.0982      0.00102\n",
            "     27     4       0.0122       0.0122     1.79e-06       0.0645       0.0855       0.0536       0.0862       0.0699       0.0697        0.111       0.0902       0.0874      0.00091\n",
            "     27     5       0.0118       0.0118     1.79e-06       0.0633       0.0839        0.052       0.0857       0.0689        0.068        0.109       0.0884       0.0811     0.000844\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              27   99.882    0.005        0.015     4.66e-05        0.015       0.0729       0.0946       0.0632       0.0924       0.0778       0.0813        0.117        0.099        0.418      0.00436\n",
            "! Validation         27   99.882    0.005       0.0122     2.13e-06       0.0122       0.0644       0.0854        0.053       0.0873       0.0702       0.0692        0.111       0.0901       0.0917     0.000955\n",
            "Wall time: 99.88308818699988\n",
            "! Best model       27    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     28     1       0.0165       0.0165     1.77e-06       0.0754       0.0993       0.0635        0.099       0.0813        0.084        0.124        0.104       0.0889     0.000926\n",
            "     28     2       0.0134       0.0133     3.82e-05       0.0682       0.0893       0.0574       0.0898       0.0736       0.0751        0.113       0.0938        0.437      0.00456\n",
            "     28     3       0.0127       0.0127     1.03e-06       0.0657       0.0871       0.0558       0.0855       0.0706        0.073         0.11       0.0916       0.0609     0.000635\n",
            "     28     4       0.0152       0.0151      4.8e-05       0.0744       0.0951       0.0642       0.0947       0.0794       0.0815        0.118       0.0996          0.5      0.00521\n",
            "     28     5       0.0156       0.0156     2.08e-05       0.0752       0.0967       0.0646       0.0966       0.0806       0.0817        0.121        0.101         0.33      0.00343\n",
            "     28     6       0.0128       0.0128      7.9e-06       0.0664       0.0874       0.0555       0.0884       0.0719       0.0717        0.112       0.0921        0.188      0.00196\n",
            "     28     7       0.0118       0.0118     3.58e-05       0.0654       0.0839       0.0544       0.0876        0.071        0.069        0.108       0.0883        0.435      0.00453\n",
            "     28     8       0.0134       0.0133     8.62e-05       0.0685       0.0891       0.0574       0.0909       0.0741       0.0728        0.115       0.0939        0.682       0.0071\n",
            "     28     9       0.0121       0.0121     1.58e-06        0.065       0.0849       0.0519       0.0912       0.0716       0.0656        0.114       0.0899       0.0783     0.000816\n",
            "     28    10       0.0103       0.0102     2.81e-05       0.0587       0.0782       0.0488       0.0785       0.0636       0.0645          0.1       0.0823        0.382      0.00398\n",
            "     28    11       0.0108       0.0108     1.39e-05       0.0611       0.0805       0.0519       0.0794       0.0657       0.0677        0.101       0.0845        0.259       0.0027\n",
            "     28    12       0.0119       0.0119        2e-06       0.0651       0.0845       0.0541       0.0872       0.0706       0.0678        0.111       0.0892       0.0883      0.00092\n",
            "     28    13       0.0101         0.01     2.46e-05       0.0587       0.0775       0.0489       0.0785       0.0637       0.0636       0.0996       0.0816        0.359      0.00374\n",
            "     28    14      0.00913      0.00912     5.94e-06       0.0564       0.0739       0.0461       0.0769       0.0615       0.0585       0.0976       0.0781        0.171      0.00178\n",
            "     28    15       0.0114       0.0113     7.44e-05       0.0632       0.0822       0.0519        0.086       0.0689       0.0669        0.106       0.0866        0.635      0.00661\n",
            "     28    16       0.0114       0.0113     6.88e-06       0.0628       0.0824       0.0529       0.0827       0.0678       0.0682        0.105       0.0867        0.178      0.00185\n",
            "     28    17      0.00977      0.00971     5.89e-05       0.0588       0.0762        0.049       0.0782       0.0636       0.0629       0.0976       0.0802        0.564      0.00588\n",
            "     28    18       0.0127       0.0126     8.05e-05       0.0667       0.0869       0.0557       0.0887       0.0722       0.0716        0.111       0.0915        0.666      0.00693\n",
            "     28    19       0.0136       0.0136     1.98e-06       0.0678       0.0901       0.0566       0.0903       0.0735       0.0746        0.115       0.0948       0.0855     0.000891\n",
            "     28    20        0.011       0.0109     0.000155       0.0618       0.0807       0.0512        0.083       0.0671       0.0659        0.104        0.085         0.92      0.00958\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     28     1       0.0128       0.0128     3.07e-06       0.0662       0.0875       0.0545       0.0897       0.0721       0.0712        0.113       0.0922        0.102      0.00107\n",
            "     28     2       0.0124       0.0124     1.66e-06       0.0646       0.0861       0.0525       0.0888       0.0707       0.0693        0.113       0.0909       0.0824     0.000859\n",
            "     28     3        0.011        0.011     2.09e-06       0.0616       0.0811       0.0505       0.0837       0.0671       0.0652        0.106       0.0856        0.092     0.000958\n",
            "     28     4        0.012        0.012     1.35e-06       0.0638       0.0847       0.0529       0.0854       0.0692       0.0688         0.11       0.0893       0.0778     0.000811\n",
            "     28     5       0.0115       0.0115     1.97e-06       0.0625       0.0829       0.0514       0.0847       0.0681       0.0672        0.108       0.0874       0.0852     0.000887\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              28  103.381    0.005       0.0122     3.46e-05       0.0123       0.0653       0.0856       0.0546       0.0867       0.0706       0.0706        0.109       0.0901        0.355       0.0037\n",
            "! Validation         28  103.381    0.005       0.0119     2.03e-06       0.0119       0.0637       0.0845       0.0524       0.0865       0.0694       0.0684         0.11       0.0891        0.088     0.000916\n",
            "Wall time: 103.38237758500009\n",
            "! Best model       28    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     29     1       0.0108       0.0107     7.08e-05        0.061       0.0801       0.0506        0.082       0.0663       0.0654        0.104       0.0844        0.623      0.00649\n",
            "     29     2       0.0138       0.0138     2.72e-06       0.0704       0.0908       0.0601       0.0908       0.0755       0.0767        0.114       0.0953        0.098      0.00102\n",
            "     29     3       0.0116       0.0116     3.52e-05        0.063       0.0832       0.0526       0.0837       0.0682       0.0683        0.107       0.0876        0.426      0.00444\n",
            "     29     4      0.00981      0.00969     0.000115       0.0582       0.0762       0.0475       0.0798       0.0636       0.0612       0.0996       0.0804         0.79      0.00823\n",
            "     29     5       0.0123       0.0122     2.85e-05       0.0659       0.0856       0.0549        0.088       0.0715       0.0683        0.112       0.0904        0.371      0.00387\n",
            "     29     6       0.0118       0.0117      6.1e-05       0.0633       0.0837       0.0527       0.0843       0.0685       0.0681        0.108       0.0882        0.564      0.00587\n",
            "     29     7      0.00941      0.00933     7.41e-05        0.057       0.0747       0.0463       0.0783       0.0623        0.059        0.099        0.079        0.637      0.00663\n",
            "     29     8       0.0103       0.0103     1.04e-06       0.0599       0.0783       0.0489        0.082       0.0655       0.0622        0.103       0.0828       0.0529     0.000551\n",
            "     29     9       0.0145       0.0145     1.93e-05       0.0714       0.0931        0.061       0.0921       0.0765        0.078        0.118       0.0978        0.304      0.00316\n",
            "     29    10       0.0131       0.0131     1.64e-05       0.0665       0.0886       0.0554       0.0886        0.072       0.0728        0.114       0.0933        0.297      0.00309\n",
            "     29    11       0.0128       0.0128      6.3e-06       0.0656       0.0874        0.054       0.0888       0.0714       0.0733         0.11       0.0918        0.177      0.00184\n",
            "     29    12       0.0149       0.0148     4.84e-05       0.0728       0.0942       0.0665       0.0855        0.076       0.0861        0.109       0.0974        0.515      0.00536\n",
            "     29    13       0.0118       0.0118     2.76e-06       0.0663       0.0839       0.0583       0.0821       0.0702       0.0722        0.103       0.0878       0.0828     0.000863\n",
            "     29    14      0.00954      0.00953     1.06e-05       0.0573       0.0755       0.0474       0.0771       0.0623       0.0603       0.0992       0.0798        0.221       0.0023\n",
            "     29    15       0.0165       0.0165     3.75e-05       0.0773       0.0992       0.0666       0.0987       0.0827       0.0846        0.123        0.104        0.449      0.00468\n",
            "     29    16       0.0182       0.0182     7.02e-05       0.0812        0.104       0.0718          0.1       0.0859       0.0911        0.127        0.109        0.619      0.00645\n",
            "     29    17       0.0125       0.0124      5.4e-05       0.0658       0.0862       0.0555       0.0864       0.0709       0.0716         0.11       0.0906         0.54      0.00563\n",
            "     29    18       0.0108       0.0107     0.000112       0.0605         0.08        0.049       0.0837       0.0663       0.0633        0.106       0.0845        0.776      0.00808\n",
            "     29    19       0.0129       0.0129     9.81e-07       0.0694       0.0878       0.0634       0.0816       0.0725       0.0789        0.103       0.0911       0.0703     0.000732\n",
            "     29    20       0.0139       0.0139     2.26e-05         0.07       0.0913       0.0594       0.0912       0.0753       0.0751        0.117       0.0961         0.35      0.00365\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     29     1       0.0125       0.0125     3.12e-06       0.0655       0.0865       0.0538       0.0888       0.0713       0.0704        0.112       0.0912        0.106      0.00111\n",
            "     29     2       0.0121       0.0121     1.37e-06       0.0639       0.0852       0.0518        0.088       0.0699       0.0683        0.111       0.0899       0.0763     0.000794\n",
            "     29     3       0.0108       0.0108     2.07e-06       0.0609       0.0803       0.0499       0.0829       0.0664       0.0643        0.105       0.0847       0.0932      0.00097\n",
            "     29     4       0.0117       0.0117     1.47e-06       0.0631       0.0838       0.0523       0.0847       0.0685       0.0679        0.109       0.0884       0.0794     0.000827\n",
            "     29     5       0.0112       0.0112     1.78e-06       0.0618        0.082       0.0508       0.0838       0.0673       0.0664        0.107       0.0865       0.0794     0.000827\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              29  106.912    0.005       0.0125     3.94e-05       0.0126       0.0661       0.0866       0.0561       0.0862       0.0712       0.0723         0.11        0.091        0.398      0.00415\n",
            "! Validation         29  106.912    0.005       0.0117     1.96e-06       0.0117        0.063       0.0836       0.0517       0.0856       0.0687       0.0675        0.109       0.0882       0.0869     0.000906\n",
            "Wall time: 106.91250408899987\n",
            "! Best model       29    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     30     1        0.013        0.013     6.99e-05       0.0675       0.0881       0.0553       0.0919       0.0736       0.0712        0.115       0.0929        0.618      0.00644\n",
            "     30     2        0.011        0.011     6.21e-06       0.0624       0.0811       0.0545       0.0783       0.0664        0.069        0.101        0.085        0.168      0.00175\n",
            "     30     3       0.0138       0.0137     4.29e-05         0.07       0.0906       0.0595        0.091       0.0753       0.0742        0.117       0.0955        0.482      0.00502\n",
            "     30     4       0.0131        0.013     1.14e-05        0.066       0.0884        0.053       0.0919       0.0725       0.0706        0.116       0.0933        0.235      0.00245\n",
            "     30     5       0.0099       0.0099     1.78e-07       0.0583        0.077       0.0474       0.0802       0.0638        0.061        0.102       0.0813       0.0277     0.000289\n",
            "     30     6       0.0108       0.0108     1.22e-05       0.0617       0.0802       0.0516       0.0818       0.0667       0.0666        0.102       0.0844        0.249      0.00259\n",
            "     30     7      0.00947      0.00946     2.74e-06       0.0581       0.0753       0.0491       0.0763       0.0627       0.0616        0.097       0.0793         0.09     0.000938\n",
            "     30     8       0.0109       0.0109     2.63e-06       0.0619       0.0809       0.0529       0.0799       0.0664       0.0681        0.102       0.0849       0.0973      0.00101\n",
            "     30     9       0.0115       0.0115     1.46e-06       0.0632       0.0831       0.0517       0.0863        0.069       0.0669        0.108       0.0877       0.0721     0.000751\n",
            "     30    10       0.0119       0.0119     5.49e-05       0.0627       0.0843        0.051       0.0861       0.0685       0.0672        0.111        0.089        0.549      0.00572\n",
            "     30    11       0.0124       0.0124     3.66e-05       0.0657        0.086       0.0541       0.0889       0.0715         0.07        0.111       0.0906        0.446      0.00464\n",
            "     30    12       0.0113       0.0113     4.07e-06        0.062       0.0822       0.0517       0.0828       0.0672        0.068        0.105       0.0864        0.129      0.00135\n",
            "     30    13       0.0106       0.0106     1.72e-05       0.0607       0.0795        0.049       0.0841       0.0666        0.063        0.105        0.084        0.275      0.00286\n",
            "     30    14       0.0103       0.0102     6.85e-05       0.0602       0.0783       0.0518       0.0771       0.0645       0.0668       0.0972        0.082        0.613      0.00639\n",
            "     30    15       0.0111       0.0111     5.55e-06       0.0624       0.0816       0.0522       0.0829       0.0676       0.0674        0.105       0.0859        0.149      0.00155\n",
            "     30    16        0.012        0.012     7.65e-07       0.0645       0.0848       0.0557        0.082       0.0689       0.0716        0.106        0.089       0.0602     0.000627\n",
            "     30    17       0.0164       0.0164     1.35e-06       0.0787       0.0989       0.0693       0.0977       0.0835       0.0853        0.122        0.104       0.0727     0.000757\n",
            "     30    18        0.015       0.0149     5.99e-05       0.0727       0.0945       0.0607       0.0967       0.0787       0.0762        0.123       0.0996        0.573      0.00597\n",
            "     30    19       0.0105       0.0105     9.92e-06       0.0605       0.0794       0.0494       0.0828       0.0661       0.0638        0.104       0.0838        0.219      0.00228\n",
            "     30    20       0.0137       0.0137     6.74e-06       0.0705       0.0907       0.0591       0.0933       0.0762       0.0744        0.117       0.0955        0.148      0.00155\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     30     1       0.0122       0.0122     2.99e-06       0.0648       0.0856       0.0531        0.088       0.0706       0.0695        0.111       0.0902        0.104      0.00109\n",
            "     30     2       0.0119       0.0119     1.31e-06       0.0633       0.0843       0.0513       0.0872       0.0693       0.0675        0.111        0.089       0.0737     0.000768\n",
            "     30     3       0.0106       0.0106     1.99e-06       0.0603       0.0796       0.0493       0.0822       0.0658       0.0636        0.104        0.084       0.0918     0.000956\n",
            "     30     4       0.0115       0.0115     1.36e-06       0.0624       0.0831       0.0516       0.0841       0.0679       0.0671        0.108       0.0876       0.0763     0.000794\n",
            "     30     5        0.011        0.011     1.85e-06       0.0611       0.0812       0.0503       0.0828       0.0666       0.0658        0.105       0.0856       0.0805     0.000838\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              30  110.424    0.005       0.0119     2.08e-05       0.0119       0.0645       0.0844        0.054       0.0856       0.0698       0.0694        0.108       0.0889        0.264      0.00275\n",
            "! Validation         30  110.424    0.005       0.0114      1.9e-06       0.0114       0.0624       0.0828       0.0511       0.0849        0.068       0.0667        0.108       0.0873       0.0854     0.000889\n",
            "Wall time: 110.4244237769999\n",
            "! Best model       30    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     31     1       0.0145       0.0145     4.62e-06        0.072       0.0931       0.0635       0.0892       0.0763       0.0806        0.114       0.0973        0.155      0.00161\n",
            "     31     2       0.0124       0.0123     2.35e-05       0.0667        0.086       0.0578       0.0846       0.0712       0.0744        0.105       0.0899         0.35      0.00365\n",
            "     31     3       0.0101       0.0101     3.29e-05       0.0586       0.0776       0.0483       0.0792       0.0638       0.0621        0.102        0.082        0.423      0.00441\n",
            "     31     4       0.0113       0.0113     1.06e-05       0.0633       0.0821       0.0552       0.0794       0.0673       0.0706        0.101       0.0859        0.237      0.00247\n",
            "     31     5       0.0107       0.0107     9.65e-06       0.0614         0.08       0.0501       0.0842       0.0671       0.0641        0.105       0.0845        0.192        0.002\n",
            "     31     6      0.00966      0.00966     3.47e-06       0.0591        0.076       0.0501       0.0772       0.0637       0.0629        0.097         0.08        0.109      0.00113\n",
            "     31     7       0.0121       0.0121     5.59e-05       0.0648       0.0851        0.054       0.0864       0.0702       0.0702        0.109       0.0895        0.553      0.00576\n",
            "     31     8       0.0106       0.0106     4.06e-05       0.0608       0.0795       0.0486       0.0851       0.0669       0.0623        0.106       0.0841        0.458      0.00477\n",
            "     31     9       0.0106       0.0106     3.85e-06       0.0605       0.0796       0.0496       0.0823        0.066       0.0638        0.104        0.084        0.141      0.00146\n",
            "     31    10       0.0121       0.0121     2.48e-06       0.0656       0.0852       0.0559        0.085       0.0704       0.0703        0.109       0.0897        0.102      0.00106\n",
            "     31    11         0.01         0.01     3.94e-05       0.0591       0.0774       0.0492        0.079       0.0641       0.0626        0.101       0.0816         0.46      0.00479\n",
            "     31    12       0.0119       0.0119     3.54e-05       0.0637       0.0843       0.0527       0.0858       0.0692       0.0694        0.108       0.0888        0.438      0.00456\n",
            "     31    13       0.0108       0.0108     9.75e-07       0.0609       0.0802       0.0508       0.0813        0.066       0.0657        0.103       0.0845       0.0668     0.000696\n",
            "     31    14       0.0101       0.0101     9.72e-06       0.0575       0.0778       0.0448        0.083       0.0639       0.0601        0.105       0.0823        0.206      0.00215\n",
            "     31    15       0.0099      0.00988     1.77e-05       0.0581       0.0769       0.0465       0.0814        0.064       0.0606        0.102       0.0813        0.305      0.00318\n",
            "     31    16      0.00975      0.00974     2.51e-06       0.0575       0.0764       0.0475       0.0774       0.0624       0.0614       0.0998       0.0806       0.0941     0.000981\n",
            "     31    17       0.0107       0.0107     2.14e-05       0.0603         0.08        0.049       0.0829        0.066       0.0644        0.105       0.0845        0.335      0.00349\n",
            "     31    18      0.00931      0.00931     4.46e-07       0.0573       0.0747       0.0475       0.0767       0.0621       0.0605        0.097       0.0787       0.0344     0.000358\n",
            "     31    19        0.011       0.0109      2.4e-05       0.0612       0.0809       0.0499       0.0839       0.0669       0.0646        0.106       0.0854        0.346       0.0036\n",
            "     31    20         0.01         0.01     1.38e-06       0.0589       0.0775       0.0479        0.081       0.0645       0.0614        0.102       0.0818       0.0623     0.000649\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     31     1        0.012        0.012     2.91e-06       0.0641       0.0847       0.0525       0.0873       0.0699       0.0686         0.11       0.0893          0.1      0.00105\n",
            "     31     2       0.0116       0.0116     1.32e-06       0.0626       0.0835       0.0506       0.0865       0.0686       0.0666         0.11       0.0881       0.0722     0.000752\n",
            "     31     3       0.0104       0.0104     1.88e-06       0.0597       0.0789       0.0488       0.0816       0.0652       0.0629        0.104       0.0833       0.0882     0.000919\n",
            "     31     4       0.0113       0.0113      1.2e-06       0.0618       0.0823        0.051       0.0835       0.0672       0.0662        0.107       0.0868       0.0743     0.000774\n",
            "     31     5       0.0108       0.0108     1.94e-06       0.0605       0.0804       0.0497        0.082       0.0659       0.0651        0.104       0.0847       0.0807      0.00084\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              31  113.946    0.005       0.0109      1.7e-05       0.0109       0.0614       0.0806       0.0509       0.0822       0.0666       0.0658        0.104        0.085        0.253      0.00264\n",
            "! Validation         31  113.946    0.005       0.0112     1.85e-06       0.0112       0.0617       0.0819       0.0505       0.0842       0.0674       0.0659        0.107       0.0865       0.0831     0.000866\n",
            "Wall time: 113.94668835099992\n",
            "! Best model       31    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     32     1       0.0108       0.0108     1.25e-05       0.0609       0.0803       0.0504       0.0818       0.0661        0.067        0.102       0.0844        0.252      0.00262\n",
            "     32     2      0.00861      0.00859     1.97e-05       0.0556       0.0717       0.0443       0.0782       0.0612       0.0558       0.0959       0.0759        0.324      0.00337\n",
            "     32     3        0.013        0.013     1.39e-06       0.0677       0.0881        0.058        0.087       0.0725       0.0744         0.11       0.0924       0.0736     0.000767\n",
            "     32     4       0.0112       0.0111      5.7e-05       0.0641       0.0815       0.0567        0.079       0.0678       0.0709       0.0994       0.0852        0.549      0.00572\n",
            "     32     5      0.00981      0.00979     2.35e-05       0.0581       0.0765       0.0464       0.0814       0.0639       0.0592        0.103        0.081        0.358      0.00373\n",
            "     32     6       0.0105       0.0105     5.33e-06        0.061       0.0792       0.0512       0.0805       0.0659        0.065        0.102       0.0834        0.148      0.00154\n",
            "     32     7       0.0128       0.0127     8.36e-05       0.0653       0.0872       0.0534       0.0892       0.0713       0.0707        0.113       0.0919        0.675      0.00703\n",
            "     32     8        0.011       0.0109     0.000126       0.0619       0.0807       0.0513       0.0831       0.0672       0.0648        0.106       0.0852        0.831      0.00866\n",
            "     32     9       0.0112       0.0111     2.64e-05       0.0625       0.0816       0.0526       0.0824       0.0675       0.0675        0.104       0.0859        0.381      0.00397\n",
            "     32    10       0.0118       0.0117     0.000176        0.064       0.0836       0.0528       0.0864       0.0696       0.0673        0.109       0.0882        0.982       0.0102\n",
            "     32    11       0.0122       0.0122     4.59e-05       0.0655       0.0853        0.056       0.0847       0.0703       0.0734        0.105       0.0893        0.493      0.00514\n",
            "     32    12       0.0098       0.0098     3.08e-06       0.0586       0.0766       0.0475       0.0809       0.0642       0.0608        0.101       0.0809        0.114      0.00119\n",
            "     32    13      0.00983      0.00975     7.79e-05       0.0583       0.0764       0.0506       0.0737       0.0622       0.0655       0.0945         0.08        0.649      0.00676\n",
            "     32    14       0.0103       0.0102     3.89e-05       0.0604       0.0783       0.0509       0.0796       0.0652       0.0639        0.101       0.0825         0.45      0.00469\n",
            "     32    15       0.0107       0.0107     1.56e-05         0.06         0.08       0.0493       0.0813       0.0653       0.0648        0.104       0.0843        0.291      0.00303\n",
            "     32    16      0.00999      0.00994     5.16e-05       0.0586       0.0771       0.0489       0.0781       0.0635       0.0618        0.101       0.0814        0.522      0.00544\n",
            "     32    17       0.0114       0.0114      8.3e-06       0.0618       0.0825       0.0498       0.0858       0.0678       0.0641         0.11       0.0873        0.208      0.00217\n",
            "     32    18        0.013        0.013     2.89e-05        0.068       0.0881       0.0599       0.0842       0.0721       0.0764        0.108       0.0921        0.396      0.00412\n",
            "     32    19       0.0106       0.0105     3.89e-05       0.0614       0.0794       0.0527       0.0788       0.0658       0.0679       0.0983       0.0831        0.449      0.00468\n",
            "     32    20       0.0118       0.0118     1.03e-05       0.0627       0.0839       0.0508       0.0865       0.0686       0.0672         0.11       0.0885        0.228      0.00237\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     32     1       0.0117       0.0117     2.86e-06       0.0634       0.0838       0.0518       0.0865       0.0692       0.0678        0.109       0.0884        0.102      0.00107\n",
            "     32     2       0.0114       0.0114     1.19e-06        0.062       0.0826       0.0501       0.0858       0.0679       0.0658        0.109       0.0873       0.0691      0.00072\n",
            "     32     3       0.0102       0.0102     1.88e-06       0.0592       0.0782       0.0483        0.081       0.0646       0.0622        0.103       0.0826       0.0882     0.000919\n",
            "     32     4       0.0111       0.0111     1.31e-06       0.0612       0.0815       0.0504       0.0829       0.0666       0.0655        0.107       0.0861       0.0764     0.000795\n",
            "     32     5       0.0106       0.0106     1.83e-06       0.0599       0.0796       0.0492       0.0812       0.0652       0.0645        0.103       0.0839       0.0775     0.000808\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              32  117.435    0.005        0.011     4.25e-05        0.011       0.0618        0.081       0.0517       0.0821       0.0669       0.0666        0.104       0.0853        0.419      0.00436\n",
            "! Validation         32  117.435    0.005        0.011     1.81e-06        0.011       0.0611       0.0812         0.05       0.0835       0.0667       0.0652        0.106       0.0857       0.0827     0.000862\n",
            "Wall time: 117.43626498100002\n",
            "! Best model       32    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     33     1       0.0105       0.0103     0.000177       0.0613       0.0785       0.0531       0.0777       0.0654       0.0671       0.0974       0.0823        0.987       0.0103\n",
            "     33     2       0.0132       0.0132     2.48e-06       0.0667        0.089       0.0519       0.0964       0.0741        0.066        0.123       0.0943        0.106      0.00111\n",
            "     33     3       0.0109       0.0109     3.31e-07       0.0624       0.0806       0.0518       0.0835       0.0676       0.0641        0.106       0.0852       0.0393     0.000409\n",
            "     33     4      0.00956      0.00949     6.43e-05       0.0577       0.0754       0.0477       0.0776       0.0626       0.0604       0.0988       0.0796        0.592      0.00616\n",
            "     33     5       0.0134       0.0134     1.75e-05       0.0672       0.0896       0.0534        0.095       0.0742       0.0695         0.12       0.0948        0.305      0.00318\n",
            "     33     6        0.017       0.0169     5.64e-05       0.0795        0.101       0.0726       0.0932       0.0829       0.0907        0.118        0.104        0.556      0.00579\n",
            "     33     7       0.0148       0.0147     9.09e-06        0.074       0.0939       0.0675       0.0868       0.0772       0.0845         0.11       0.0975        0.216      0.00225\n",
            "     33     8         0.01         0.01     4.93e-06       0.0591       0.0774       0.0485       0.0804       0.0644       0.0617        0.102       0.0818        0.151      0.00157\n",
            "     33     9       0.0127       0.0127     3.64e-06       0.0675       0.0873       0.0596       0.0835       0.0715       0.0755        0.107       0.0913         0.14      0.00145\n",
            "     33    10       0.0144       0.0144     1.07e-05       0.0709       0.0928       0.0592       0.0943       0.0768       0.0761        0.119       0.0977        0.233      0.00243\n",
            "     33    11       0.0113       0.0112     5.82e-05       0.0613       0.0819       0.0498       0.0842        0.067       0.0643        0.109       0.0866        0.565      0.00589\n",
            "     33    12        0.011        0.011     2.41e-05       0.0617       0.0811       0.0486       0.0878       0.0682       0.0628        0.109       0.0858        0.358      0.00373\n",
            "     33    13       0.0132       0.0131     5.69e-05       0.0682       0.0885       0.0574       0.0899       0.0737       0.0739        0.112       0.0931        0.558      0.00581\n",
            "     33    14       0.0123       0.0123     1.25e-05       0.0668       0.0857       0.0591       0.0822       0.0706       0.0742        0.105       0.0896        0.238      0.00248\n",
            "     33    15      0.00947      0.00947     2.98e-06       0.0575       0.0753       0.0478       0.0767       0.0623       0.0614       0.0973       0.0793         0.11      0.00115\n",
            "     33    16      0.00965      0.00964     9.61e-06       0.0588        0.076       0.0498       0.0767       0.0633       0.0638       0.0958       0.0798        0.222      0.00232\n",
            "     33    17       0.0111        0.011     2.39e-05       0.0621       0.0813       0.0502       0.0858        0.068       0.0663        0.105       0.0857        0.337      0.00351\n",
            "     33    18       0.0113       0.0113     4.37e-06        0.062       0.0821       0.0507       0.0848       0.0677       0.0668        0.106       0.0866        0.134       0.0014\n",
            "     33    19       0.0113       0.0113     5.85e-06       0.0631       0.0823       0.0525       0.0843       0.0684        0.068        0.105       0.0866        0.156      0.00163\n",
            "     33    20      0.00992      0.00992     5.41e-07       0.0575        0.077       0.0477       0.0771       0.0624       0.0633        0.099       0.0811       0.0412     0.000429\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     33     1       0.0115       0.0115     2.82e-06       0.0628        0.083       0.0512       0.0859       0.0685        0.067        0.108       0.0875        0.101      0.00105\n",
            "     33     2       0.0112       0.0112     1.21e-06       0.0614       0.0819       0.0496       0.0851       0.0673       0.0652        0.108       0.0865       0.0682      0.00071\n",
            "     33     3         0.01         0.01     1.82e-06       0.0587       0.0775       0.0478       0.0804       0.0641       0.0616        0.102       0.0819       0.0856     0.000892\n",
            "     33     4       0.0109       0.0109      1.2e-06       0.0607       0.0809       0.0498       0.0824       0.0661       0.0647        0.106       0.0854        0.074     0.000771\n",
            "     33     5       0.0104       0.0104      1.8e-06       0.0593       0.0789       0.0488       0.0805       0.0646       0.0639        0.103       0.0832       0.0764     0.000795\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              33  120.944    0.005       0.0118     2.73e-05       0.0118       0.0643       0.0841       0.0539       0.0849       0.0694       0.0694        0.108       0.0885        0.302      0.00315\n",
            "! Validation         33  120.944    0.005       0.0108     1.77e-06       0.0108       0.0606       0.0805       0.0494       0.0828       0.0661       0.0645        0.105       0.0849       0.0811     0.000845\n",
            "Wall time: 120.94528414199999\n",
            "! Best model       33    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     34     1       0.0102       0.0102     1.54e-06       0.0596       0.0782       0.0497       0.0794       0.0646        0.063        0.102       0.0825       0.0689     0.000718\n",
            "     34     2       0.0124       0.0124     4.28e-06       0.0659        0.086       0.0546       0.0883       0.0715       0.0696        0.112       0.0907        0.145      0.00151\n",
            "     34     3      0.00927      0.00927     1.15e-06       0.0583       0.0745       0.0498       0.0752       0.0625       0.0622       0.0944       0.0783       0.0547      0.00057\n",
            "     34     4       0.0113       0.0113     6.48e-06       0.0618       0.0823       0.0502        0.085       0.0676       0.0669        0.107       0.0868        0.168      0.00175\n",
            "     34     5      0.00994      0.00994     1.96e-06       0.0578       0.0771       0.0484       0.0767       0.0625       0.0644       0.0977       0.0811       0.0881     0.000918\n",
            "     34     6       0.0102       0.0102     3.31e-05       0.0601       0.0781       0.0511       0.0781       0.0646       0.0654       0.0989       0.0821        0.426      0.00444\n",
            "     34     7      0.00904      0.00904     4.61e-06       0.0555       0.0735       0.0443       0.0778        0.061       0.0575       0.0981       0.0778        0.145      0.00151\n",
            "     34     8      0.00976      0.00975      6.5e-06       0.0583       0.0764       0.0478       0.0793       0.0636       0.0619       0.0993       0.0806        0.176      0.00183\n",
            "     34     9      0.00991      0.00991     1.41e-06       0.0587        0.077       0.0475       0.0811       0.0643       0.0607        0.102       0.0814        0.085     0.000885\n",
            "     34    10      0.00897      0.00896     8.37e-06        0.056       0.0732       0.0463       0.0755       0.0609       0.0593       0.0951       0.0772        0.193      0.00201\n",
            "     34    11       0.0105       0.0105     6.14e-06       0.0606       0.0792       0.0511       0.0795       0.0653       0.0644        0.103       0.0835        0.165      0.00172\n",
            "     34    12       0.0102       0.0102     3.08e-06       0.0592        0.078       0.0471       0.0833       0.0652       0.0615        0.103       0.0824       0.0961        0.001\n",
            "     34    13       0.0106       0.0106     4.98e-07       0.0598       0.0795       0.0484       0.0825       0.0654       0.0635        0.104        0.084       0.0441      0.00046\n",
            "     34    14       0.0106       0.0106     1.01e-06        0.059       0.0798       0.0469       0.0832        0.065       0.0616        0.107       0.0844       0.0666     0.000694\n",
            "     34    15      0.00892      0.00892     1.39e-06       0.0566       0.0731       0.0466       0.0765       0.0615       0.0593       0.0948        0.077       0.0654     0.000682\n",
            "     34    16      0.00966      0.00965     4.08e-06       0.0571        0.076       0.0451       0.0812       0.0631        0.058        0.103       0.0805        0.137      0.00143\n",
            "     34    17       0.0104       0.0104     2.42e-05       0.0591       0.0789       0.0486         0.08       0.0643       0.0618        0.105       0.0835        0.363      0.00378\n",
            "     34    18       0.0121       0.0121     1.21e-06       0.0653        0.085       0.0561       0.0836       0.0699       0.0715        0.107       0.0893       0.0498     0.000519\n",
            "     34    19        0.012        0.012     3.48e-06       0.0646       0.0847       0.0546       0.0845       0.0696       0.0702        0.108       0.0891        0.117      0.00122\n",
            "     34    20      0.00991      0.00987     4.67e-05       0.0587       0.0768        0.048       0.0802       0.0641       0.0612        0.101       0.0812        0.506      0.00528\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     34     1       0.0113       0.0113     2.74e-06       0.0622       0.0822       0.0507       0.0852       0.0679       0.0662        0.107       0.0867       0.0992      0.00103\n",
            "     34     2        0.011        0.011     1.26e-06       0.0608       0.0812       0.0491       0.0844       0.0667       0.0644        0.107       0.0857       0.0681     0.000709\n",
            "     34     3      0.00989      0.00989     1.73e-06       0.0582       0.0769       0.0473         0.08       0.0636        0.061        0.102       0.0813       0.0812     0.000845\n",
            "     34     4       0.0107       0.0107     1.14e-06       0.0601       0.0802       0.0493       0.0818       0.0656        0.064        0.105       0.0847       0.0721     0.000751\n",
            "     34     5       0.0102       0.0102     1.87e-06       0.0588       0.0782       0.0483       0.0797        0.064       0.0633        0.102       0.0825       0.0759      0.00079\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              34  124.453    0.005       0.0103     8.06e-06       0.0103       0.0596       0.0785       0.0491       0.0805       0.0648       0.0633        0.102       0.0828        0.158      0.00165\n",
            "! Validation         34  124.453    0.005       0.0106     1.75e-06       0.0106         0.06       0.0798       0.0489       0.0822       0.0656       0.0638        0.105       0.0842       0.0793     0.000826\n",
            "Wall time: 124.45398797600001\n",
            "! Best model       34    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     35     1      0.00981      0.00974      6.6e-05       0.0575       0.0764       0.0462       0.0803       0.0632       0.0604        0.101       0.0807        0.598      0.00623\n",
            "     35     2      0.00996      0.00996     2.03e-06       0.0591       0.0772       0.0509       0.0756       0.0632       0.0649       0.0972       0.0811       0.0824     0.000859\n",
            "     35     3       0.0103       0.0103     2.52e-05        0.059       0.0786       0.0477       0.0816       0.0646       0.0632        0.103       0.0829        0.369      0.00385\n",
            "     35     4       0.0098      0.00975      5.3e-05       0.0577       0.0764       0.0476        0.078       0.0628       0.0619       0.0992       0.0806        0.535      0.00558\n",
            "     35     5       0.0115       0.0115      8.5e-06       0.0628       0.0829       0.0512        0.086       0.0686       0.0664        0.109       0.0875        0.193      0.00201\n",
            "     35     6        0.011       0.0109     8.05e-05       0.0613       0.0807       0.0504        0.083       0.0667       0.0648        0.106       0.0852        0.664      0.00692\n",
            "     35     7        0.011        0.011     1.33e-06       0.0609        0.081       0.0502       0.0823       0.0663       0.0651        0.106       0.0854       0.0746     0.000777\n",
            "     35     8       0.0101       0.0101     1.56e-05       0.0583       0.0776       0.0471       0.0807       0.0639       0.0617        0.102        0.082        0.285      0.00297\n",
            "     35     9       0.0103       0.0103     1.32e-06       0.0608       0.0785       0.0512       0.0801       0.0657       0.0653       0.0999       0.0826       0.0826     0.000861\n",
            "     35    10       0.0132       0.0132     2.19e-06       0.0683       0.0889       0.0577       0.0894       0.0736       0.0744        0.112       0.0935        0.101      0.00105\n",
            "     35    11       0.0132       0.0132     2.62e-05       0.0684       0.0888       0.0552       0.0949       0.0751       0.0696        0.118       0.0939        0.364       0.0038\n",
            "     35    12        0.013       0.0129     1.38e-05       0.0674        0.088       0.0564       0.0895       0.0729       0.0717        0.114       0.0928        0.256      0.00266\n",
            "     35    13       0.0108       0.0108     1.08e-05       0.0607       0.0803       0.0497       0.0828       0.0663       0.0645        0.105       0.0848        0.237      0.00247\n",
            "     35    14       0.0082       0.0082     6.56e-06       0.0543         0.07       0.0444       0.0741       0.0593       0.0564       0.0914       0.0739        0.186      0.00193\n",
            "     35    15      0.00997      0.00997     1.78e-07        0.058       0.0773       0.0476       0.0787       0.0632       0.0613        0.102       0.0816       0.0281     0.000293\n",
            "     35    16       0.0107       0.0106        7e-05        0.062       0.0798       0.0523       0.0813       0.0668       0.0659        0.102       0.0839        0.615       0.0064\n",
            "     35    17       0.0125       0.0125     1.76e-06       0.0665       0.0863       0.0586       0.0822       0.0704       0.0748        0.106       0.0903       0.0828     0.000863\n",
            "     35    18      0.00933      0.00931     2.17e-05       0.0577       0.0747       0.0485       0.0761       0.0623       0.0615       0.0957       0.0786        0.342      0.00356\n",
            "     35    19      0.00814      0.00808     5.77e-05       0.0535       0.0696       0.0449       0.0707       0.0578       0.0569       0.0897       0.0733        0.556      0.00579\n",
            "     35    20       0.0108       0.0108     1.09e-05       0.0615       0.0803       0.0521       0.0802       0.0662       0.0667        0.102       0.0844        0.223      0.00232\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     35     1       0.0111       0.0111     2.81e-06       0.0616       0.0814       0.0502       0.0846       0.0674       0.0655        0.106       0.0859        0.101      0.00105\n",
            "     35     2       0.0108       0.0108     1.08e-06       0.0603       0.0805       0.0486       0.0837       0.0662       0.0637        0.106        0.085       0.0636     0.000662\n",
            "     35     3      0.00975      0.00975     1.75e-06       0.0577       0.0764       0.0469       0.0795       0.0632       0.0604        0.101       0.0807       0.0831     0.000866\n",
            "     35     4       0.0106       0.0106     1.16e-06       0.0596       0.0796       0.0487       0.0813        0.065       0.0633        0.105        0.084       0.0716     0.000746\n",
            "     35     5         0.01         0.01     1.76e-06       0.0583       0.0775       0.0479       0.0791       0.0635       0.0628        0.101       0.0818       0.0729     0.000759\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              35  127.945    0.005       0.0106     2.38e-05       0.0107       0.0608       0.0798       0.0505       0.0814       0.0659        0.065        0.103       0.0841        0.294      0.00306\n",
            "! Validation         35  127.945    0.005       0.0105     1.71e-06       0.0105       0.0595       0.0791       0.0485       0.0816       0.0651       0.0632        0.104       0.0835       0.0785     0.000817\n",
            "Wall time: 127.94564863300002\n",
            "! Best model       35    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     36     1       0.0112       0.0111     0.000108       0.0634       0.0816       0.0531       0.0841       0.0686       0.0689        0.102       0.0856        0.769      0.00801\n",
            "     36     2      0.00975      0.00969     6.22e-05       0.0589       0.0762       0.0502       0.0763       0.0633       0.0641       0.0958         0.08        0.583      0.00608\n",
            "     36     3      0.00913      0.00913     2.35e-06       0.0564       0.0739        0.047       0.0752       0.0611       0.0599        0.096        0.078        0.105       0.0011\n",
            "     36     4       0.0125       0.0124     2.19e-05       0.0662       0.0863       0.0585       0.0818       0.0701       0.0756        0.104         0.09        0.338      0.00352\n",
            "     36     5       0.0112       0.0112     2.16e-05       0.0628        0.082       0.0544       0.0795       0.0669         0.07        0.102       0.0859        0.335      0.00349\n",
            "     36     6      0.00947      0.00946      3.3e-06       0.0571       0.0753       0.0457       0.0799       0.0628       0.0591          0.1       0.0796       0.0936     0.000975\n",
            "     36     7       0.0109       0.0109     3.89e-06       0.0621       0.0806       0.0524       0.0815        0.067       0.0681        0.101       0.0846        0.118      0.00123\n",
            "     36     8       0.0106       0.0106     3.17e-05       0.0612       0.0797       0.0501       0.0835       0.0668       0.0641        0.104       0.0841        0.409      0.00426\n",
            "     36     9      0.00897      0.00897     3.74e-06       0.0554       0.0733       0.0426       0.0808       0.0617       0.0537        0.102       0.0777         0.13      0.00135\n",
            "     36    10      0.00885      0.00885     8.82e-07       0.0557       0.0728       0.0455       0.0761       0.0608       0.0572       0.0967       0.0769       0.0627     0.000653\n",
            "     36    11      0.00848      0.00848     7.86e-07        0.054       0.0712       0.0439       0.0742       0.0591       0.0556       0.0951       0.0753       0.0516     0.000537\n",
            "     36    12      0.00966      0.00965     7.76e-06       0.0575        0.076       0.0476       0.0774       0.0625       0.0623       0.0979       0.0801          0.2      0.00209\n",
            "     36    13      0.00954      0.00954     1.12e-06       0.0576       0.0756       0.0468       0.0792        0.063       0.0588        0.101       0.0799       0.0568     0.000592\n",
            "     36    14      0.00942      0.00942     3.46e-06       0.0562       0.0751       0.0448       0.0791       0.0619       0.0575        0.102       0.0795        0.109      0.00113\n",
            "     36    15      0.00891      0.00889      1.3e-05       0.0558        0.073       0.0459       0.0755       0.0607       0.0573        0.097       0.0771        0.254      0.00265\n",
            "     36    16       0.0117       0.0117     2.55e-06       0.0627       0.0837        0.051       0.0859       0.0685       0.0678        0.109       0.0883        0.104      0.00108\n",
            "     36    17       0.0118       0.0118     3.38e-05       0.0638        0.084       0.0533       0.0847        0.069       0.0692        0.108       0.0884        0.429      0.00447\n",
            "     36    18       0.0103       0.0103     2.08e-05       0.0596       0.0785       0.0502       0.0784       0.0643       0.0643        0.101       0.0827         0.32      0.00334\n",
            "     36    19      0.00891      0.00889     1.44e-05       0.0556        0.073       0.0459       0.0749       0.0604       0.0586       0.0953        0.077        0.275      0.00286\n",
            "     36    20       0.0105       0.0105     2.81e-06       0.0607       0.0792       0.0522       0.0778        0.065       0.0671       0.0992       0.0831        0.103      0.00107\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     36     1       0.0109       0.0109     2.81e-06       0.0611       0.0807       0.0497        0.084       0.0668       0.0649        0.106       0.0852        0.103      0.00108\n",
            "     36     2       0.0106       0.0106     9.92e-07       0.0598       0.0798       0.0481       0.0832       0.0656       0.0631        0.106       0.0844       0.0603     0.000628\n",
            "     36     3      0.00961      0.00961      1.7e-06       0.0573       0.0758       0.0464        0.079       0.0627       0.0598          0.1       0.0801       0.0821     0.000856\n",
            "     36     4       0.0104       0.0104     1.22e-06       0.0591       0.0789       0.0483       0.0808       0.0645       0.0627        0.104       0.0834       0.0731     0.000762\n",
            "     36     5      0.00988      0.00988     1.77e-06       0.0579       0.0769       0.0475       0.0786        0.063       0.0622       0.0999       0.0811       0.0719     0.000749\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              36  131.431    0.005       0.0101      1.8e-05       0.0101       0.0591       0.0777       0.0491       0.0793       0.0642       0.0632          0.1       0.0819        0.242      0.00252\n",
            "! Validation         36  131.431    0.005       0.0103      1.7e-06       0.0103        0.059       0.0785        0.048       0.0811       0.0645       0.0626        0.103       0.0829       0.0781     0.000814\n",
            "Wall time: 131.43139588600002\n",
            "! Best model       36    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     37     1       0.0112       0.0112     3.89e-06       0.0632        0.082       0.0525       0.0847       0.0686       0.0665        0.106       0.0864         0.14      0.00146\n",
            "     37     2       0.0107       0.0107     2.09e-05       0.0609       0.0801       0.0479       0.0871       0.0675       0.0618        0.108       0.0847        0.319      0.00332\n",
            "     37     3       0.0102       0.0102     9.21e-06       0.0583       0.0783       0.0467       0.0816       0.0641       0.0612        0.104       0.0828        0.221       0.0023\n",
            "     37     4       0.0105       0.0105     5.38e-06       0.0589       0.0794       0.0453        0.086       0.0656       0.0604        0.108       0.0841        0.166      0.00173\n",
            "     37     5       0.0097      0.00965     4.93e-05       0.0571        0.076       0.0464       0.0785       0.0625       0.0606       0.0999       0.0802        0.516      0.00538\n",
            "     37     6       0.0097      0.00968     2.29e-05       0.0574       0.0761       0.0471       0.0781       0.0626       0.0594        0.102       0.0805        0.352      0.00367\n",
            "     37     7       0.0101       0.0101     4.82e-05       0.0585       0.0776       0.0474       0.0806        0.064       0.0604        0.104       0.0821        0.511      0.00532\n",
            "     37     8       0.0096       0.0095     0.000104       0.0567       0.0754       0.0463       0.0774       0.0619       0.0611        0.098       0.0795        0.748      0.00779\n",
            "     37     9       0.0101         0.01     2.84e-05       0.0591       0.0775       0.0501        0.077       0.0636       0.0636       0.0996       0.0816        0.395      0.00411\n",
            "     37    10         0.01         0.01     2.43e-05        0.059       0.0774       0.0482       0.0807       0.0645        0.062        0.101       0.0817        0.361      0.00376\n",
            "     37    11       0.0091      0.00895      0.00015       0.0561       0.0732       0.0457       0.0768       0.0613       0.0587       0.0958       0.0772        0.908      0.00946\n",
            "     37    12       0.0101       0.0101     8.15e-06       0.0597       0.0776       0.0483       0.0825       0.0654       0.0626        0.101       0.0819        0.203      0.00212\n",
            "     37    13      0.00877      0.00876     7.77e-06       0.0564       0.0724       0.0477       0.0738       0.0608       0.0592       0.0934       0.0763        0.172      0.00179\n",
            "     37    14      0.00871      0.00869      1.8e-05        0.055       0.0721       0.0438       0.0773       0.0605       0.0558       0.0969       0.0763        0.301      0.00313\n",
            "     37    15       0.0086      0.00859     1.02e-05       0.0548       0.0717       0.0447        0.075       0.0599       0.0575       0.0939       0.0757        0.222      0.00231\n",
            "     37    16       0.0107       0.0107     8.99e-06       0.0605         0.08       0.0483        0.085       0.0666        0.063        0.106       0.0846        0.202       0.0021\n",
            "     37    17      0.00855      0.00855     6.18e-06       0.0542       0.0715       0.0455       0.0717       0.0586       0.0591       0.0915       0.0753         0.16      0.00167\n",
            "     37    18      0.00855      0.00854     1.37e-05       0.0546       0.0715       0.0442       0.0753       0.0598       0.0563       0.0949       0.0756        0.262      0.00272\n",
            "     37    19      0.00932      0.00932     2.84e-06       0.0569       0.0747       0.0463       0.0781       0.0622       0.0594       0.0983       0.0789        0.103      0.00108\n",
            "     37    20      0.00936      0.00933     3.72e-05       0.0567       0.0747       0.0458       0.0787       0.0622       0.0588       0.0992        0.079         0.45      0.00468\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     37     1       0.0107       0.0107     2.82e-06       0.0606       0.0801       0.0493       0.0834       0.0663       0.0642        0.105       0.0845        0.105      0.00109\n",
            "     37     2       0.0105       0.0105     9.52e-07       0.0592       0.0792       0.0476       0.0825       0.0651       0.0624        0.105       0.0837       0.0589     0.000613\n",
            "     37     3      0.00948      0.00948     1.73e-06       0.0569       0.0753        0.046       0.0785       0.0623       0.0593       0.0999       0.0796       0.0816      0.00085\n",
            "     37     4       0.0103       0.0103     1.26e-06       0.0587       0.0784       0.0478       0.0804       0.0641        0.062        0.104       0.0828       0.0738     0.000769\n",
            "     37     5      0.00972      0.00972     1.71e-06       0.0574       0.0763       0.0471        0.078       0.0626       0.0617       0.0992       0.0804       0.0697     0.000726\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              37  134.941    0.005      0.00966     2.89e-05      0.00968       0.0577        0.076       0.0469       0.0793       0.0631       0.0604          0.1       0.0803        0.335      0.00349\n",
            "! Validation         37  134.941    0.005       0.0101      1.7e-06       0.0101       0.0586       0.0779       0.0476       0.0806       0.0641        0.062        0.103       0.0822       0.0777      0.00081\n",
            "Wall time: 134.94199209399994\n",
            "! Best model       37    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     38     1       0.0086      0.00856     3.49e-05       0.0548       0.0716       0.0444       0.0757         0.06       0.0565       0.0948       0.0757        0.435      0.00453\n",
            "     38     2       0.0102       0.0102     5.35e-06       0.0586       0.0782       0.0459       0.0842        0.065       0.0597        0.106       0.0828        0.147      0.00154\n",
            "     38     3      0.00959      0.00955     3.44e-05       0.0579       0.0756       0.0475       0.0786       0.0631       0.0602       0.0995       0.0799        0.431      0.00449\n",
            "     38     4       0.0101         0.01     2.86e-05       0.0586       0.0775       0.0487       0.0784       0.0636       0.0638       0.0994       0.0816         0.39      0.00406\n",
            "     38     5      0.00873      0.00873     1.45e-06       0.0556       0.0723        0.046       0.0748       0.0604       0.0589       0.0934       0.0762       0.0846     0.000881\n",
            "     38     6       0.0103       0.0103     1.22e-06       0.0605       0.0786       0.0519       0.0776       0.0648       0.0665       0.0985       0.0825       0.0754     0.000785\n",
            "     38     7      0.00859      0.00859     3.53e-06       0.0541       0.0717       0.0447       0.0728       0.0588       0.0572       0.0942       0.0757        0.137      0.00143\n",
            "     38     8      0.00908      0.00907     1.46e-05       0.0562       0.0737       0.0457       0.0771       0.0614       0.0587       0.0969       0.0778        0.259       0.0027\n",
            "     38     9       0.0103       0.0103     4.26e-05       0.0596       0.0785       0.0493         0.08       0.0647       0.0634        0.102       0.0829        0.469      0.00488\n",
            "     38    10       0.0108       0.0108     1.52e-05       0.0602       0.0803       0.0483       0.0841       0.0662       0.0633        0.106       0.0849        0.281      0.00292\n",
            "     38    11       0.0105       0.0104     5.65e-05       0.0614        0.079       0.0516       0.0812       0.0664       0.0663       0.0996        0.083        0.554      0.00578\n",
            "     38    12      0.00995      0.00993     2.15e-05       0.0597       0.0771       0.0515       0.0763       0.0639       0.0645       0.0976        0.081        0.339      0.00353\n",
            "     38    13       0.0105       0.0105     7.45e-07       0.0607       0.0793       0.0492       0.0838       0.0665       0.0638        0.104       0.0837       0.0475     0.000494\n",
            "     38    14      0.00942      0.00938     3.71e-05       0.0572       0.0749       0.0465       0.0788       0.0626       0.0594       0.0989       0.0792        0.449      0.00468\n",
            "     38    15       0.0111        0.011     7.39e-06       0.0614       0.0813        0.049       0.0861       0.0676       0.0635        0.109        0.086        0.187      0.00194\n",
            "     38    16      0.00887      0.00876     0.000104       0.0549       0.0724       0.0438       0.0769       0.0604       0.0552       0.0982       0.0767        0.755      0.00786\n",
            "     38    17      0.00866      0.00863     3.09e-05       0.0543       0.0719       0.0441       0.0748       0.0595       0.0563       0.0957        0.076        0.408      0.00425\n",
            "     38    18      0.00954      0.00952     2.04e-05       0.0555       0.0755       0.0453        0.076       0.0606       0.0597       0.0998       0.0798        0.331      0.00344\n",
            "     38    19       0.0082      0.00808     0.000119       0.0528       0.0695       0.0435       0.0713       0.0574       0.0552       0.0917       0.0734        0.808      0.00841\n",
            "     38    20      0.00886      0.00883     2.95e-05       0.0558       0.0727       0.0461       0.0752       0.0607       0.0592        0.094       0.0766        0.387      0.00403\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     38     1       0.0105       0.0105     2.75e-06       0.0602       0.0794       0.0488       0.0829       0.0658       0.0636        0.104       0.0839        0.103      0.00107\n",
            "     38     2       0.0103       0.0103     9.51e-07       0.0587       0.0785       0.0472       0.0819       0.0645       0.0618        0.104        0.083       0.0592     0.000616\n",
            "     38     3      0.00936      0.00935     1.68e-06       0.0565       0.0748       0.0456       0.0781       0.0619       0.0588       0.0994       0.0791       0.0806     0.000839\n",
            "     38     4       0.0101       0.0101     1.24e-06       0.0582       0.0778       0.0474         0.08       0.0637       0.0614        0.103       0.0822        0.073     0.000761\n",
            "     38     5      0.00956      0.00956     1.75e-06        0.057       0.0756       0.0467       0.0774       0.0621       0.0612       0.0984       0.0798       0.0706     0.000735\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              38  138.442    0.005      0.00956     3.04e-05      0.00959       0.0575       0.0757       0.0472       0.0782       0.0627       0.0607       0.0991       0.0799        0.349      0.00363\n",
            "! Validation         38  138.442    0.005      0.00997     1.67e-06      0.00998       0.0581       0.0773       0.0471       0.0801       0.0636       0.0614        0.102       0.0816       0.0773     0.000805\n",
            "Wall time: 138.44286397099995\n",
            "! Best model       38    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     39     1       0.0106       0.0106     8.23e-05       0.0607       0.0795       0.0502       0.0817        0.066        0.065        0.103       0.0838        0.668      0.00696\n",
            "     39     2       0.0109       0.0109     5.79e-05       0.0612       0.0807       0.0501       0.0834       0.0668       0.0644        0.106       0.0852        0.562      0.00585\n",
            "     39     3       0.0108       0.0108     2.05e-06       0.0608       0.0805       0.0493       0.0838       0.0665       0.0639        0.106        0.085       0.0848     0.000883\n",
            "     39     4       0.0105       0.0104     0.000105       0.0606       0.0791       0.0491       0.0835       0.0663       0.0619        0.105       0.0836        0.759      0.00791\n",
            "     39     5        0.009      0.00899     3.65e-06       0.0556       0.0734       0.0445       0.0779       0.0612       0.0574       0.0978       0.0776        0.116      0.00121\n",
            "     39     6       0.0102       0.0102     3.54e-05       0.0592        0.078       0.0502       0.0773       0.0637       0.0657       0.0981       0.0819        0.436      0.00454\n",
            "     39     7      0.00877      0.00876     3.17e-06       0.0554       0.0724       0.0479       0.0705       0.0592       0.0608       0.0913       0.0761        0.122      0.00127\n",
            "     39     8       0.0104       0.0104     1.39e-05       0.0602       0.0789       0.0496       0.0816       0.0656       0.0625        0.104       0.0834        0.269       0.0028\n",
            "     39     9       0.0114       0.0113     9.09e-05       0.0626       0.0822       0.0528       0.0821       0.0674       0.0679        0.105       0.0866        0.706      0.00736\n",
            "     39    10       0.0104       0.0103     5.44e-05       0.0614       0.0785       0.0537       0.0768       0.0653       0.0678       0.0964       0.0821        0.547       0.0057\n",
            "     39    11       0.0104       0.0104     2.72e-05       0.0606       0.0787       0.0514        0.079       0.0652       0.0664       0.0988       0.0826        0.377      0.00393\n",
            "     39    12      0.00735      0.00734        2e-06       0.0501       0.0663        0.041       0.0683       0.0546       0.0522       0.0879       0.0701       0.0949     0.000989\n",
            "     39    13       0.0102       0.0101     3.74e-05       0.0588       0.0778       0.0495       0.0774       0.0635       0.0639          0.1        0.082        0.443      0.00461\n",
            "     39    14       0.0101         0.01     2.08e-05       0.0595       0.0775       0.0489       0.0805       0.0647       0.0627        0.101       0.0817        0.312      0.00325\n",
            "     39    15       0.0097      0.00961     9.47e-05       0.0572       0.0758       0.0453       0.0809       0.0631       0.0588        0.102       0.0802        0.722      0.00752\n",
            "     39    16      0.00783      0.00776     7.08e-05       0.0518       0.0681       0.0424       0.0707       0.0565       0.0544       0.0895        0.072        0.617      0.00643\n",
            "     39    17       0.0106       0.0106     4.52e-05       0.0612       0.0796       0.0517       0.0802       0.0659        0.066        0.102       0.0838        0.488      0.00509\n",
            "     39    18         0.01      0.00991     0.000123       0.0577        0.077       0.0465         0.08       0.0632       0.0602        0.103       0.0815        0.824      0.00858\n",
            "     39    19      0.00946      0.00943     2.48e-05        0.057       0.0751       0.0458       0.0794       0.0626       0.0596       0.0991       0.0794         0.35      0.00365\n",
            "     39    20      0.00987       0.0098     6.57e-05        0.058       0.0766       0.0472       0.0795       0.0634        0.061        0.101       0.0809        0.597      0.00622\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     39     1       0.0104       0.0104     2.77e-06       0.0597       0.0788       0.0484       0.0823       0.0653       0.0629        0.103       0.0832        0.104      0.00108\n",
            "     39     2       0.0102       0.0102     9.63e-07       0.0583        0.078       0.0468       0.0814       0.0641       0.0613        0.104       0.0825       0.0577     0.000601\n",
            "     39     3      0.00924      0.00924     1.67e-06       0.0561       0.0744       0.0453       0.0778       0.0615       0.0583       0.0989       0.0786       0.0802     0.000835\n",
            "     39     4      0.00997      0.00997     1.21e-06       0.0578       0.0772       0.0469       0.0796       0.0632       0.0608        0.102       0.0816       0.0721     0.000751\n",
            "     39     5      0.00942      0.00941     1.73e-06       0.0565       0.0751       0.0463        0.077       0.0616       0.0607       0.0977       0.0792       0.0699     0.000728\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              39  141.926    0.005      0.00988      4.8e-05      0.00993       0.0585       0.0769       0.0484       0.0787       0.0635       0.0623       0.0999       0.0811        0.455      0.00474\n",
            "! Validation         39  141.926    0.005      0.00983     1.67e-06      0.00983       0.0577       0.0767       0.0467       0.0796       0.0632       0.0608        0.101        0.081       0.0768       0.0008\n",
            "Wall time: 141.927110975\n",
            "! Best model       39    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     40     1      0.00887      0.00881     5.89e-05       0.0558       0.0726       0.0456       0.0761       0.0609       0.0577       0.0957       0.0767        0.564      0.00587\n",
            "     40     2      0.00923      0.00922     1.04e-05       0.0558       0.0743       0.0454       0.0765        0.061       0.0583       0.0987       0.0785        0.225      0.00235\n",
            "     40     3       0.0102       0.0101     2.54e-05       0.0582       0.0779       0.0464       0.0816        0.064       0.0605        0.104       0.0824        0.371      0.00387\n",
            "     40     4         0.01         0.01     8.39e-07       0.0606       0.0775       0.0527       0.0764       0.0646       0.0669       0.0953       0.0811       0.0676     0.000704\n",
            "     40     5       0.0148       0.0148     1.76e-06       0.0752       0.0941        0.069       0.0878       0.0784       0.0857        0.109       0.0973       0.0867     0.000903\n",
            "     40     6       0.0148       0.0148     1.63e-05       0.0725        0.094       0.0594       0.0987       0.0791       0.0768        0.121       0.0991        0.293      0.00306\n",
            "     40     7       0.0117       0.0116     9.51e-05       0.0635       0.0833       0.0521       0.0863       0.0692       0.0661         0.11        0.088        0.719      0.00749\n",
            "     40     8      0.00987      0.00987      4.5e-06       0.0584       0.0768       0.0487       0.0776       0.0632       0.0627       0.0992        0.081        0.132      0.00137\n",
            "     40     9       0.0104       0.0103     7.62e-05       0.0588       0.0786       0.0477       0.0809       0.0643       0.0623        0.104        0.083        0.647      0.00674\n",
            "     40    10      0.00979      0.00969     9.92e-05       0.0569       0.0761       0.0449       0.0809       0.0629       0.0591        0.102       0.0806        0.737      0.00768\n",
            "     40    11      0.00985      0.00984     6.16e-06       0.0574       0.0767       0.0456       0.0811       0.0633       0.0595        0.103       0.0812        0.179      0.00186\n",
            "     40    12      0.00998      0.00989     8.96e-05       0.0582       0.0769       0.0468       0.0811       0.0639       0.0606        0.102       0.0813        0.702      0.00731\n",
            "     40    13       0.0115       0.0115     4.29e-05       0.0655        0.083       0.0607        0.075       0.0679       0.0762        0.095       0.0856         0.48        0.005\n",
            "     40    14       0.0111       0.0111     1.23e-05       0.0625       0.0815       0.0543       0.0789       0.0666       0.0696        0.101       0.0854        0.246      0.00256\n",
            "     40    15       0.0105       0.0104     3.68e-06       0.0593       0.0791       0.0467       0.0846       0.0656       0.0592        0.108       0.0838        0.138      0.00143\n",
            "     40    16      0.00892       0.0089     2.53e-05       0.0556        0.073       0.0437       0.0792       0.0615       0.0551       0.0995       0.0773        0.342      0.00357\n",
            "     40    17      0.00914      0.00914      3.3e-06       0.0563        0.074        0.046        0.077       0.0615       0.0592        0.097       0.0781        0.092     0.000958\n",
            "     40    18      0.00933      0.00932     5.18e-06       0.0573       0.0747       0.0463       0.0793       0.0628       0.0594       0.0984       0.0789        0.148      0.00154\n",
            "     40    19       0.0095       0.0095     5.82e-06        0.058       0.0754       0.0501        0.074        0.062       0.0628       0.0958       0.0793        0.174      0.00181\n",
            "     40    20      0.00943      0.00943      2.5e-06       0.0581       0.0751       0.0492       0.0759       0.0626       0.0626       0.0953        0.079        0.115       0.0012\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     40     1       0.0102       0.0102     2.67e-06       0.0593       0.0782       0.0479        0.082       0.0649       0.0624        0.103       0.0826        0.102      0.00107\n",
            "     40     2         0.01         0.01     9.07e-07       0.0579       0.0774       0.0463       0.0809       0.0636       0.0607        0.103       0.0819       0.0559     0.000582\n",
            "     40     3      0.00913      0.00913     1.64e-06       0.0557       0.0739       0.0449       0.0774       0.0611       0.0579       0.0985       0.0782       0.0789     0.000822\n",
            "     40     4      0.00984      0.00984     1.21e-06       0.0574       0.0767       0.0465       0.0792       0.0628       0.0602        0.102       0.0811       0.0733     0.000764\n",
            "     40     5      0.00928      0.00928     1.72e-06       0.0562       0.0745        0.046       0.0765       0.0612       0.0602       0.0971       0.0786       0.0688     0.000716\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              40  145.435    0.005       0.0104     2.93e-05       0.0104       0.0602        0.079       0.0501       0.0804       0.0653       0.0644        0.102       0.0832        0.323      0.00336\n",
            "! Validation         40  145.435    0.005       0.0097     1.63e-06       0.0097       0.0573       0.0762       0.0463       0.0792       0.0628       0.0603        0.101       0.0805       0.0758      0.00079\n",
            "Wall time: 145.436284742\n",
            "! Best model       40    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     41     1      0.00964      0.00961     3.06e-05       0.0576       0.0758       0.0462       0.0805       0.0634       0.0592        0.101       0.0802        0.408      0.00425\n",
            "     41     2      0.00915      0.00913     1.77e-05       0.0561       0.0739       0.0455       0.0772       0.0613       0.0585       0.0977       0.0781        0.294      0.00306\n",
            "     41     3      0.00834       0.0083     3.76e-05       0.0536       0.0705       0.0437       0.0735       0.0586       0.0553       0.0938       0.0745        0.454      0.00472\n",
            "     41     4      0.00948      0.00947        5e-07       0.0589       0.0753       0.0512       0.0745       0.0628       0.0641       0.0937       0.0789       0.0473     0.000492\n",
            "     41     5       0.0106       0.0106     1.77e-06       0.0619       0.0798       0.0532       0.0794       0.0663       0.0666        0.101       0.0838       0.0871     0.000907\n",
            "     41     6      0.00887      0.00885     1.96e-05       0.0546       0.0728       0.0435       0.0768       0.0602       0.0556       0.0986       0.0771        0.326       0.0034\n",
            "     41     7      0.00806      0.00806     2.29e-06       0.0532       0.0695       0.0429       0.0738       0.0584        0.055       0.0918       0.0734        0.101      0.00106\n",
            "     41     8      0.00845      0.00844     4.71e-06        0.054       0.0711       0.0453       0.0712       0.0583       0.0584       0.0913       0.0749        0.144       0.0015\n",
            "     41     9      0.00854      0.00853     1.15e-06        0.054       0.0715       0.0446       0.0728       0.0587       0.0567       0.0943       0.0755       0.0711     0.000741\n",
            "     41    10      0.00923      0.00922     6.04e-06       0.0569       0.0743       0.0481       0.0747       0.0614       0.0617       0.0946       0.0781        0.162      0.00169\n",
            "     41    11      0.00945      0.00944     1.13e-05       0.0552       0.0752       0.0424       0.0809       0.0616       0.0557        0.104       0.0797         0.24       0.0025\n",
            "     41    12         0.01         0.01     3.68e-05       0.0586       0.0773        0.049       0.0779       0.0634       0.0632       0.0998       0.0815        0.448      0.00467\n",
            "     41    13      0.00905      0.00905     1.04e-06       0.0557       0.0736       0.0457       0.0757       0.0607       0.0601        0.095       0.0776       0.0537     0.000559\n",
            "     41    14       0.0087      0.00869      1.4e-05       0.0541       0.0721       0.0419       0.0784       0.0602       0.0531       0.0998       0.0764        0.263      0.00274\n",
            "     41    15      0.00866      0.00866     7.84e-07        0.055        0.072       0.0447       0.0756       0.0602       0.0566       0.0956       0.0761       0.0602     0.000627\n",
            "     41    16       0.0139       0.0139     1.72e-06       0.0692       0.0913       0.0581       0.0914       0.0747       0.0756        0.116        0.096       0.0863     0.000899\n",
            "     41    17       0.0145       0.0144     0.000121       0.0734       0.0929       0.0649       0.0903       0.0776       0.0817        0.112       0.0968        0.812      0.00846\n",
            "     41    18       0.0127       0.0127     2.24e-06       0.0693       0.0872       0.0616       0.0847       0.0731       0.0765        0.105       0.0909       0.0867     0.000903\n",
            "     41    19         0.01      0.00996     4.98e-05       0.0592       0.0772       0.0478       0.0821        0.065       0.0602        0.103       0.0817        0.521      0.00542\n",
            "     41    20      0.00952       0.0094     0.000117       0.0562        0.075       0.0461       0.0762       0.0612       0.0597       0.0988       0.0792        0.803      0.00836\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     41     1       0.0101       0.0101     2.77e-06       0.0589       0.0777       0.0475       0.0816       0.0646       0.0618        0.102       0.0821        0.103      0.00108\n",
            "     41     2       0.0099       0.0099     9.15e-07       0.0575        0.077        0.046       0.0805       0.0632       0.0603        0.103       0.0814       0.0562     0.000586\n",
            "     41     3      0.00904      0.00904     1.62e-06       0.0554       0.0736       0.0446       0.0771       0.0608       0.0575       0.0981       0.0778       0.0796     0.000829\n",
            "     41     4      0.00972      0.00972     1.23e-06        0.057       0.0763       0.0461       0.0789       0.0625       0.0597        0.102       0.0806       0.0729      0.00076\n",
            "     41     5      0.00916      0.00916     1.67e-06       0.0558        0.074       0.0457       0.0761       0.0609       0.0597       0.0965       0.0781       0.0686     0.000714\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              41  148.956    0.005      0.00982     2.39e-05      0.00985       0.0583       0.0767       0.0483       0.0784       0.0634       0.0621       0.0996       0.0809        0.273      0.00285\n",
            "! Validation         41  148.956    0.005      0.00958     1.64e-06      0.00958       0.0569       0.0757        0.046       0.0788       0.0624       0.0598          0.1         0.08       0.0761     0.000793\n",
            "Wall time: 148.95746880699994\n",
            "! Best model       41    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     42     1       0.0105       0.0105     2.56e-05       0.0591       0.0792       0.0485       0.0803       0.0644       0.0641        0.103       0.0835        0.363      0.00379\n",
            "     42     2       0.0101       0.0098     0.000282       0.0576       0.0766       0.0463       0.0801       0.0632       0.0599        0.102        0.081         1.25        0.013\n",
            "     42     3        0.012        0.012      1.3e-05       0.0664       0.0846       0.0593       0.0805       0.0699        0.075        0.101       0.0881        0.257      0.00267\n",
            "     42     4       0.0123       0.0122     7.64e-05       0.0675       0.0856       0.0615       0.0795       0.0705       0.0762        0.102        0.089        0.647      0.00674\n",
            "     42     5       0.0112        0.011     0.000188       0.0619       0.0812       0.0504       0.0849       0.0676       0.0646        0.107       0.0858         1.01       0.0106\n",
            "     42     6      0.00962       0.0096      1.8e-05       0.0573       0.0758       0.0463       0.0791       0.0627         0.06          0.1       0.0801        0.312      0.00325\n",
            "     42     7      0.00826      0.00815     0.000107       0.0532       0.0698       0.0428       0.0738       0.0583       0.0542       0.0936       0.0739        0.766      0.00798\n",
            "     42     8      0.00892      0.00892     7.08e-07       0.0545       0.0731       0.0429       0.0776       0.0603       0.0549       0.0999       0.0774       0.0545     0.000568\n",
            "     42     9      0.00886      0.00879     6.61e-05       0.0542       0.0725       0.0443        0.074       0.0591       0.0585       0.0946       0.0765        0.601      0.00626\n",
            "     42    10      0.00802      0.00799     3.08e-05       0.0525       0.0692       0.0419       0.0736       0.0578       0.0536       0.0928       0.0732        0.407      0.00424\n",
            "     42    11      0.00827      0.00824     2.97e-05       0.0537       0.0702       0.0444       0.0721       0.0583       0.0563        0.092       0.0742        0.402      0.00418\n",
            "     42    12      0.00902      0.00893     8.77e-05       0.0554       0.0731       0.0443       0.0776       0.0609       0.0569       0.0978       0.0773         0.68      0.00708\n",
            "     42    13      0.00855      0.00854      2.1e-06       0.0542       0.0715       0.0436       0.0753       0.0595       0.0561       0.0951       0.0756        0.101      0.00105\n",
            "     42    14      0.00853       0.0085     2.96e-05       0.0539       0.0713       0.0439       0.0739       0.0589       0.0562       0.0945       0.0754        0.396      0.00412\n",
            "     42    15      0.00885      0.00884     1.93e-05       0.0545       0.0727       0.0435       0.0764         0.06        0.057       0.0967       0.0769        0.321      0.00334\n",
            "     42    16      0.00855      0.00855     8.08e-07       0.0543       0.0715       0.0417       0.0794       0.0606       0.0527        0.099       0.0758       0.0598     0.000623\n",
            "     42    17       0.0102       0.0102     4.03e-05       0.0601        0.078       0.0505       0.0794       0.0649       0.0641          0.1       0.0821        0.468      0.00488\n",
            "     42    18       0.0133       0.0132     1.86e-05       0.0695        0.089        0.061       0.0867       0.0738       0.0768        0.109       0.0931        0.309      0.00322\n",
            "     42    19       0.0131       0.0131     1.85e-06       0.0699       0.0884       0.0617       0.0862        0.074       0.0759        0.109       0.0926       0.0959     0.000999\n",
            "     42    20      0.00972      0.00972     4.73e-07       0.0582       0.0763       0.0479       0.0788       0.0633       0.0606        0.101       0.0806        0.041     0.000427\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     42     1      0.00996      0.00996     2.77e-06       0.0585       0.0772       0.0472       0.0812       0.0642       0.0613        0.102       0.0816        0.104      0.00108\n",
            "     42     2      0.00978      0.00978     8.63e-07       0.0571       0.0765       0.0457         0.08       0.0628       0.0598        0.102       0.0809       0.0538     0.000561\n",
            "     42     3      0.00895      0.00895     1.62e-06       0.0551       0.0732       0.0443       0.0768       0.0605       0.0571       0.0977       0.0774       0.0801     0.000834\n",
            "     42     4       0.0096       0.0096     1.28e-06       0.0567       0.0758       0.0457       0.0786       0.0621       0.0592        0.101       0.0802       0.0737     0.000768\n",
            "     42     5      0.00904      0.00904     1.64e-06       0.0555       0.0736       0.0453       0.0758       0.0606       0.0593       0.0959       0.0776       0.0688     0.000717\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              42  152.455    0.005      0.00984     5.19e-05      0.00989       0.0584       0.0767       0.0483       0.0785       0.0634       0.0622       0.0997       0.0809        0.427      0.00445\n",
            "! Validation         42  152.455    0.005      0.00947     1.63e-06      0.00947       0.0566       0.0753       0.0456       0.0785        0.062       0.0593       0.0998       0.0796        0.076     0.000792\n",
            "Wall time: 152.45624416400005\n",
            "! Best model       42    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     43     1      0.00831       0.0083     1.97e-06       0.0524       0.0705       0.0417       0.0738       0.0578       0.0548       0.0944       0.0746       0.0992      0.00103\n",
            "     43     2       0.0123       0.0123     1.36e-05        0.065       0.0857       0.0565       0.0821       0.0693       0.0737        0.106       0.0897        0.261      0.00272\n",
            "     43     3        0.015       0.0149     5.49e-05       0.0747       0.0945       0.0661        0.092       0.0791       0.0818        0.116       0.0988        0.541      0.00564\n",
            "     43     4       0.0142       0.0142     3.63e-06       0.0723        0.092       0.0629       0.0912        0.077       0.0787        0.114       0.0964        0.136      0.00142\n",
            "     43     5      0.00991      0.00988     3.16e-05       0.0589       0.0769       0.0494       0.0777       0.0636       0.0631       0.0989        0.081        0.414      0.00431\n",
            "     43     6      0.00851       0.0085     5.94e-06       0.0532       0.0713       0.0417       0.0762        0.059       0.0538       0.0973       0.0756        0.159      0.00166\n",
            "     43     7      0.00711      0.00706     4.91e-05       0.0491        0.065       0.0393       0.0688        0.054       0.0494       0.0884       0.0689        0.518      0.00539\n",
            "     43     8      0.00859      0.00858     9.77e-06       0.0542       0.0717       0.0442        0.074       0.0591       0.0574       0.0939       0.0757         0.21      0.00218\n",
            "     43     9      0.00877      0.00877     2.81e-06       0.0549       0.0725       0.0447       0.0752       0.0599       0.0579        0.095       0.0765       0.0885     0.000922\n",
            "     43    10      0.00805      0.00803     1.16e-05       0.0524       0.0693       0.0422        0.073       0.0576       0.0541       0.0926       0.0733        0.246      0.00257\n",
            "     43    11      0.00869      0.00869     1.89e-06        0.055       0.0721       0.0449       0.0751         0.06       0.0574        0.095       0.0762       0.0824     0.000859\n",
            "     43    12      0.00831      0.00829     2.14e-05       0.0528       0.0705       0.0429       0.0726       0.0577       0.0551       0.0939       0.0745        0.341      0.00355\n",
            "     43    13      0.00949      0.00949     4.76e-06       0.0567       0.0753       0.0463       0.0776        0.062       0.0601       0.0991       0.0796        0.152      0.00158\n",
            "     43    14      0.00825      0.00817     8.25e-05       0.0532       0.0699       0.0438       0.0721       0.0579       0.0554       0.0923       0.0739        0.667      0.00695\n",
            "     43    15      0.00933      0.00927     6.29e-05       0.0562       0.0745       0.0456       0.0773       0.0615       0.0593        0.098       0.0787        0.584      0.00609\n",
            "     43    16      0.00807      0.00801     5.82e-05       0.0519       0.0692       0.0425       0.0708       0.0566       0.0545       0.0919       0.0732        0.565      0.00588\n",
            "     43    17      0.00878      0.00861      0.00017       0.0544       0.0718       0.0446       0.0741       0.0594       0.0568        0.095       0.0759        0.965       0.0101\n",
            "     43    18      0.00822      0.00821     5.43e-06       0.0529       0.0701       0.0419       0.0749       0.0584       0.0535        0.095       0.0742        0.165      0.00172\n",
            "     43    19      0.00892      0.00887     5.54e-05       0.0553       0.0729       0.0435       0.0789       0.0612        0.055       0.0993       0.0772        0.551      0.00574\n",
            "     43    20      0.00873      0.00867     5.59e-05       0.0548        0.072       0.0441       0.0763       0.0602       0.0563       0.0961       0.0762        0.553      0.00576\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     43     1      0.00984      0.00984     2.75e-06       0.0581       0.0767       0.0468       0.0808       0.0638       0.0608        0.101       0.0811        0.104      0.00108\n",
            "     43     2      0.00968      0.00967     9.22e-07       0.0568       0.0761       0.0453       0.0796       0.0625       0.0593        0.102       0.0805       0.0554     0.000577\n",
            "     43     3      0.00886      0.00886     1.57e-06       0.0548       0.0728        0.044       0.0765       0.0602       0.0567       0.0973        0.077        0.077     0.000802\n",
            "     43     4      0.00949      0.00949     1.21e-06       0.0563       0.0754       0.0453       0.0782       0.0618       0.0587        0.101       0.0797       0.0726     0.000756\n",
            "     43     5      0.00893      0.00893      1.7e-06       0.0552       0.0731        0.045       0.0754       0.0602       0.0589       0.0954       0.0771       0.0695     0.000724\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              43  155.997    0.005      0.00934     3.51e-05      0.00937       0.0565       0.0748       0.0464       0.0767       0.0616         0.06       0.0978       0.0789        0.365       0.0038\n",
            "! Validation         43  155.997    0.005      0.00936     1.63e-06      0.00936       0.0562       0.0748       0.0453       0.0781       0.0617       0.0589       0.0993       0.0791       0.0756     0.000787\n",
            "Wall time: 155.998247065\n",
            "! Best model       43    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     44     1      0.00792      0.00791     4.96e-06       0.0521       0.0688        0.042       0.0723       0.0572        0.054       0.0915       0.0728         0.16      0.00167\n",
            "     44     2       0.0093      0.00927     2.31e-05       0.0558       0.0745       0.0456        0.076       0.0608         0.06       0.0972       0.0786        0.349      0.00363\n",
            "     44     3      0.00922      0.00922     1.13e-07       0.0571       0.0743       0.0471       0.0769        0.062       0.0599       0.0968       0.0784       0.0201      0.00021\n",
            "     44     4      0.00877      0.00872     5.53e-05       0.0547       0.0722       0.0435        0.077       0.0602       0.0567       0.0961       0.0764        0.542      0.00565\n",
            "     44     5      0.00823      0.00822     3.32e-06       0.0531       0.0702       0.0425       0.0744       0.0585       0.0546       0.0939       0.0742        0.118      0.00122\n",
            "     44     6       0.0104       0.0103     8.02e-05       0.0598       0.0784       0.0501       0.0792       0.0647       0.0649          0.1       0.0825         0.66      0.00687\n",
            "     44     7      0.00921      0.00916     4.51e-05       0.0568       0.0741       0.0471       0.0762       0.0617         0.06       0.0962       0.0781        0.495      0.00516\n",
            "     44     8      0.00945      0.00945     2.68e-06       0.0583       0.0752         0.05       0.0747       0.0624       0.0628       0.0953       0.0791       0.0969      0.00101\n",
            "     44     9      0.00875      0.00864     0.000108        0.054       0.0719       0.0428       0.0765       0.0596       0.0545       0.0978       0.0762        0.765      0.00797\n",
            "     44    10      0.00892      0.00887     4.08e-05       0.0551       0.0729       0.0436        0.078       0.0608       0.0556       0.0987       0.0772        0.461       0.0048\n",
            "     44    11      0.00935      0.00935     2.04e-06       0.0556       0.0748        0.044       0.0788       0.0614       0.0573        0.101       0.0792       0.0998      0.00104\n",
            "     44    12       0.0114       0.0114     9.55e-06       0.0635       0.0827        0.051       0.0884       0.0697       0.0646         0.11       0.0875        0.224      0.00233\n",
            "     44    13       0.0117       0.0116     0.000115       0.0648       0.0834       0.0568       0.0806       0.0687       0.0713        0.103       0.0873        0.796      0.00829\n",
            "     44    14       0.0108       0.0108     1.29e-05       0.0622       0.0803       0.0543        0.078       0.0662         0.07       0.0978       0.0839        0.246      0.00256\n",
            "     44    15      0.00868      0.00863     4.25e-05       0.0543       0.0719       0.0436       0.0758       0.0597       0.0555       0.0966       0.0761        0.476      0.00496\n",
            "     44    16      0.00953      0.00953        7e-07       0.0575       0.0755       0.0488       0.0749       0.0619       0.0627       0.0962       0.0794       0.0559     0.000582\n",
            "     44    17       0.0103       0.0103     6.59e-07       0.0602       0.0786        0.052       0.0766       0.0643       0.0659       0.0992       0.0825       0.0531     0.000553\n",
            "     44    18       0.0089      0.00889     4.31e-06       0.0553        0.073       0.0463       0.0734       0.0598       0.0583       0.0957        0.077        0.148      0.00154\n",
            "     44    19      0.00855      0.00855     2.67e-06       0.0536       0.0715       0.0426       0.0756       0.0591       0.0543       0.0972       0.0757       0.0891     0.000928\n",
            "     44    20      0.00997      0.00996      6.1e-06       0.0578       0.0772       0.0468       0.0798       0.0633       0.0614        0.102       0.0816        0.158      0.00164\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     44     1      0.00972      0.00972      2.7e-06       0.0578       0.0763       0.0465       0.0804       0.0634       0.0604        0.101       0.0806        0.101      0.00106\n",
            "     44     2      0.00957      0.00957     9.22e-07       0.0564       0.0757        0.045       0.0793       0.0621       0.0589        0.101         0.08       0.0554     0.000577\n",
            "     44     3      0.00877      0.00877     1.55e-06       0.0545       0.0725       0.0437       0.0762       0.0599       0.0563        0.097       0.0767       0.0783     0.000816\n",
            "     44     4      0.00939      0.00939     1.24e-06        0.056        0.075        0.045       0.0779       0.0615       0.0582          0.1       0.0793       0.0736     0.000767\n",
            "     44     5      0.00883      0.00883     1.59e-06       0.0549       0.0727       0.0448       0.0751       0.0599       0.0585        0.095       0.0767       0.0661     0.000689\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              44  159.485    0.005      0.00944      2.8e-05      0.00947       0.0571       0.0752        0.047       0.0772       0.0621       0.0604       0.0982       0.0793        0.301      0.00313\n",
            "! Validation         44  159.485    0.005      0.00926      1.6e-06      0.00926       0.0559       0.0744        0.045       0.0778       0.0614       0.0585       0.0989       0.0787        0.075     0.000781\n",
            "Wall time: 159.4855668749999\n",
            "! Best model       44    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     45     1      0.00941       0.0094     1.18e-05       0.0575        0.075       0.0478       0.0769       0.0624       0.0603        0.098       0.0792        0.237      0.00247\n",
            "     45     2       0.0103       0.0103      2.3e-06       0.0606       0.0787       0.0516       0.0786       0.0651       0.0659       0.0995       0.0827       0.0881     0.000918\n",
            "     45     3      0.00888      0.00888     1.28e-06       0.0553       0.0729       0.0462       0.0737       0.0599       0.0585       0.0954       0.0769       0.0654     0.000682\n",
            "     45     4       0.0104       0.0104      4.8e-06         0.06        0.079       0.0508       0.0785       0.0647       0.0655        0.101       0.0831        0.109      0.00114\n",
            "     45     5       0.0146       0.0146     1.92e-05       0.0741       0.0934        0.067       0.0883       0.0776       0.0828        0.112       0.0972        0.314      0.00328\n",
            "     45     6        0.013       0.0129     1.27e-05       0.0679        0.088       0.0574       0.0888       0.0731       0.0722        0.113       0.0927        0.255      0.00266\n",
            "     45     7      0.00903      0.00903     7.22e-07       0.0564       0.0735        0.045       0.0791        0.062       0.0571       0.0985       0.0778       0.0602     0.000627\n",
            "     45     8      0.00731      0.00731     2.84e-06       0.0497       0.0661       0.0393       0.0704       0.0548       0.0499       0.0903       0.0701        0.116      0.00121\n",
            "     45     9      0.00948      0.00947     6.64e-06        0.057       0.0753       0.0464       0.0782       0.0623       0.0591          0.1       0.0796        0.186      0.00194\n",
            "     45    10       0.0112       0.0112     1.19e-06       0.0633       0.0817       0.0534        0.083       0.0682       0.0674        0.105        0.086       0.0668     0.000696\n",
            "     45    11       0.0118       0.0118     2.76e-05       0.0657       0.0841       0.0583       0.0804       0.0694       0.0742        0.101       0.0876        0.387      0.00403\n",
            "     45    12      0.00903      0.00902     4.17e-06       0.0558       0.0735       0.0436       0.0803       0.0619       0.0556          0.1       0.0778        0.122      0.00127\n",
            "     45    13      0.00875      0.00871     3.11e-05       0.0544       0.0722       0.0436       0.0761       0.0598       0.0575       0.0951       0.0763        0.401      0.00417\n",
            "     45    14      0.00959      0.00957     2.14e-05        0.057       0.0757       0.0464       0.0782       0.0623       0.0594        0.101         0.08        0.336       0.0035\n",
            "     45    15      0.00931      0.00931     5.37e-06       0.0564       0.0746       0.0455       0.0784       0.0619       0.0588        0.099       0.0789        0.163       0.0017\n",
            "     45    16       0.0105       0.0104     8.44e-05       0.0595        0.079       0.0485       0.0816        0.065       0.0635        0.103       0.0834        0.679      0.00708\n",
            "     45    17      0.00821      0.00815      6.3e-05       0.0529       0.0698       0.0423        0.074       0.0581       0.0531       0.0949        0.074        0.589      0.00613\n",
            "     45    18       0.0103       0.0103     2.65e-05       0.0602       0.0784       0.0537       0.0732       0.0635       0.0687       0.0948       0.0818        0.379      0.00395\n",
            "     45    19      0.00921      0.00913     8.48e-05       0.0564       0.0739       0.0466        0.076       0.0613       0.0596       0.0963        0.078        0.675      0.00703\n",
            "     45    20      0.00996      0.00996     6.58e-06        0.059       0.0772       0.0482       0.0806       0.0644        0.062        0.101       0.0815        0.165      0.00172\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     45     1      0.00961      0.00961     2.77e-06       0.0574       0.0758       0.0461       0.0801       0.0631       0.0599          0.1       0.0801        0.104      0.00108\n",
            "     45     2      0.00948      0.00947     8.91e-07       0.0561       0.0753       0.0447       0.0789       0.0618       0.0585        0.101       0.0797       0.0537     0.000559\n",
            "     45     3       0.0087       0.0087     1.54e-06       0.0543       0.0721       0.0434       0.0759       0.0597        0.056       0.0967       0.0763       0.0778     0.000811\n",
            "     45     4      0.00929      0.00929     1.24e-06       0.0557       0.0746       0.0447       0.0777       0.0612       0.0578          0.1       0.0789       0.0731     0.000762\n",
            "     45     5      0.00873      0.00872     1.55e-06       0.0546       0.0723       0.0444       0.0748       0.0596        0.058       0.0945       0.0763       0.0661     0.000689\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              45  162.974    0.005         0.01     2.09e-05         0.01        0.059       0.0773       0.0491       0.0787       0.0639        0.063          0.1       0.0815         0.27      0.00281\n",
            "! Validation         45  162.974    0.005      0.00916      1.6e-06      0.00916       0.0556        0.074       0.0447       0.0775       0.0611       0.0581       0.0985       0.0783       0.0749      0.00078\n",
            "Wall time: 162.97506003700005\n",
            "! Best model       45    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     46     1      0.00805      0.00794     0.000107       0.0519       0.0689       0.0414       0.0731       0.0572       0.0534       0.0925       0.0729        0.767      0.00799\n",
            "     46     2       0.0114       0.0113     8.89e-05       0.0623       0.0824       0.0508       0.0854       0.0681        0.065        0.109       0.0871        0.695      0.00724\n",
            "     46     3       0.0109       0.0108     9.55e-05       0.0614       0.0802       0.0517       0.0807       0.0662       0.0668        0.102       0.0844        0.718      0.00748\n",
            "     46     4      0.00976      0.00968     8.51e-05       0.0596       0.0761       0.0533       0.0723       0.0628       0.0665       0.0923       0.0794        0.681       0.0071\n",
            "     46     5      0.00929      0.00929     4.11e-07        0.057       0.0746       0.0456         0.08       0.0628        0.058       0.0998       0.0789         0.04     0.000417\n",
            "     46     6      0.00816      0.00804     0.000117       0.0537       0.0694       0.0431       0.0749        0.059       0.0543       0.0925       0.0734        0.802      0.00835\n",
            "     46     7      0.00886      0.00885     9.42e-06       0.0552       0.0728        0.045       0.0755       0.0603       0.0581       0.0957       0.0769        0.222      0.00231\n",
            "     46     8      0.00994      0.00983     0.000108       0.0579       0.0767       0.0478       0.0783        0.063       0.0618          0.1       0.0809         0.77      0.00802\n",
            "     46     9         0.01         0.01     7.65e-06       0.0607       0.0774       0.0523       0.0774       0.0649       0.0659       0.0964       0.0812        0.183      0.00191\n",
            "     46    10       0.0106       0.0106     2.54e-05       0.0607       0.0795       0.0512       0.0797       0.0654       0.0656        0.102       0.0837        0.371      0.00387\n",
            "     46    11      0.00785      0.00775     9.17e-05       0.0516       0.0681       0.0392       0.0765       0.0578       0.0499       0.0945       0.0722        0.705      0.00734\n",
            "     46    12      0.00723      0.00723     1.14e-06       0.0488       0.0658        0.039       0.0683       0.0537       0.0498       0.0896       0.0697       0.0701      0.00073\n",
            "     46    13         0.01      0.00992     7.19e-05       0.0577       0.0771       0.0458       0.0815       0.0636       0.0593        0.104       0.0816        0.624       0.0065\n",
            "     46    14      0.00728      0.00728     3.34e-06       0.0507        0.066       0.0411       0.0698       0.0555       0.0522       0.0873       0.0697        0.111      0.00115\n",
            "     46    15      0.00924      0.00923     6.63e-06       0.0566       0.0743       0.0474        0.075       0.0612       0.0617       0.0947       0.0782        0.161      0.00168\n",
            "     46    16      0.00874      0.00874     1.04e-06       0.0546       0.0723       0.0445       0.0747       0.0596       0.0563       0.0967       0.0765       0.0646     0.000673\n",
            "     46    17      0.00973      0.00973     5.18e-06       0.0584       0.0763       0.0485        0.078       0.0633        0.062       0.0989       0.0805        0.166      0.00173\n",
            "     46    18       0.0101       0.0101     7.81e-06       0.0593       0.0776       0.0485       0.0809       0.0647       0.0625        0.101       0.0819        0.174      0.00181\n",
            "     46    19       0.0096      0.00956     4.27e-05       0.0571       0.0757       0.0455       0.0803       0.0629       0.0585        0.102       0.0801        0.467      0.00487\n",
            "     46    20       0.0084      0.00839     9.85e-06       0.0536       0.0709        0.043       0.0749       0.0589       0.0557       0.0942       0.0749        0.219      0.00228\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     46     1      0.00951       0.0095     2.69e-06       0.0571       0.0754       0.0458       0.0797       0.0627       0.0595       0.0999       0.0797        0.103      0.00108\n",
            "     46     2      0.00939      0.00939     9.32e-07       0.0559        0.075       0.0445       0.0787       0.0616       0.0582          0.1       0.0793       0.0541     0.000564\n",
            "     46     3      0.00863      0.00863     1.55e-06        0.054       0.0719       0.0432       0.0757       0.0594       0.0557       0.0964        0.076       0.0771     0.000804\n",
            "     46     4       0.0092       0.0092      1.2e-06       0.0554       0.0742       0.0443       0.0774       0.0609       0.0574       0.0997       0.0785       0.0722     0.000752\n",
            "     46     5      0.00863      0.00863     1.58e-06       0.0542       0.0719       0.0441       0.0745       0.0593       0.0576       0.0941       0.0758       0.0668     0.000696\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              46  166.465    0.005      0.00921     4.43e-05      0.00926       0.0564       0.0743       0.0462       0.0769       0.0615       0.0594       0.0974       0.0784        0.401      0.00417\n",
            "! Validation         46  166.465    0.005      0.00907     1.59e-06      0.00907       0.0553       0.0737       0.0444       0.0772       0.0608       0.0577       0.0981       0.0779       0.0747     0.000778\n",
            "Wall time: 166.465851703\n",
            "! Best model       46    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     47     1       0.0102       0.0102     2.81e-06       0.0578       0.0781       0.0465       0.0804       0.0634       0.0611        0.104       0.0826         0.11      0.00114\n",
            "     47     2       0.0108       0.0108     1.09e-05       0.0621       0.0804       0.0534       0.0795       0.0664       0.0673        0.102       0.0844        0.214      0.00223\n",
            "     47     3      0.00864      0.00863     1.67e-05        0.055       0.0719        0.045        0.075         0.06       0.0576       0.0941       0.0758        0.302      0.00314\n",
            "     47     4       0.0087       0.0087     5.47e-06       0.0545       0.0722       0.0443        0.075       0.0596       0.0566       0.0959       0.0763        0.161      0.00167\n",
            "     47     5      0.00976      0.00976     1.72e-06       0.0582       0.0764       0.0455       0.0835       0.0645        0.057        0.105        0.081       0.0693     0.000722\n",
            "     47     6       0.0108       0.0108     1.23e-05       0.0607       0.0804       0.0488       0.0845       0.0667       0.0633        0.107        0.085        0.258      0.00268\n",
            "     47     7      0.00969      0.00965     3.65e-05       0.0595        0.076       0.0526       0.0734        0.063        0.066       0.0928       0.0794        0.442       0.0046\n",
            "     47     8      0.00748      0.00746     2.15e-05       0.0509       0.0668       0.0417       0.0693       0.0555       0.0532       0.0879       0.0706        0.338      0.00352\n",
            "     47     9      0.00912      0.00911     8.96e-06       0.0559       0.0739       0.0467       0.0742       0.0604       0.0588       0.0972        0.078         0.21      0.00218\n",
            "     47    10       0.0121       0.0121     1.62e-05       0.0659       0.0852       0.0563       0.0852       0.0707       0.0707        0.108       0.0896        0.295      0.00307\n",
            "     47    11       0.0107       0.0106     3.13e-05       0.0618       0.0797       0.0523        0.081       0.0666       0.0663        0.101       0.0838        0.398      0.00415\n",
            "     47    12      0.00879      0.00876     3.06e-05       0.0549       0.0724        0.043       0.0785       0.0608       0.0565       0.0966       0.0766        0.396      0.00412\n",
            "     47    13      0.00962      0.00955     6.89e-05       0.0589       0.0756       0.0507       0.0754        0.063       0.0646       0.0938       0.0792        0.611      0.00637\n",
            "     47    14       0.0118       0.0117     1.11e-05       0.0648       0.0838       0.0557       0.0832       0.0694       0.0705        0.106        0.088        0.245      0.00255\n",
            "     47    15       0.0113       0.0113     3.33e-05        0.064       0.0821       0.0544       0.0833       0.0689       0.0681        0.105       0.0864        0.422       0.0044\n",
            "     47    16      0.00919      0.00918     5.01e-06        0.056       0.0741       0.0438       0.0804       0.0621       0.0558        0.101       0.0785        0.164      0.00171\n",
            "     47    17      0.00848      0.00846     2.24e-05        0.054       0.0712       0.0426       0.0768       0.0597       0.0554       0.0951       0.0753        0.339      0.00353\n",
            "     47    18       0.0124       0.0124     2.61e-05       0.0665       0.0861       0.0579       0.0837       0.0708       0.0726        0.108       0.0904        0.378      0.00394\n",
            "     47    19       0.0138       0.0138     3.57e-05       0.0721       0.0907       0.0656       0.0853       0.0754       0.0802        0.109       0.0945        0.438      0.00456\n",
            "     47    20      0.00847      0.00845     1.69e-05       0.0553       0.0711       0.0474       0.0712       0.0593       0.0597       0.0896       0.0747        0.302      0.00315\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     47     1       0.0094       0.0094     2.67e-06       0.0568        0.075       0.0455       0.0794       0.0624        0.059       0.0995       0.0793        0.102      0.00107\n",
            "     47     2      0.00929      0.00929     9.79e-07       0.0556       0.0746       0.0442       0.0783       0.0612       0.0578          0.1       0.0789       0.0569     0.000593\n",
            "     47     3      0.00855      0.00854     1.46e-06       0.0537       0.0715       0.0429       0.0754       0.0592       0.0553       0.0961       0.0757       0.0739      0.00077\n",
            "     47     4      0.00911       0.0091     1.18e-06       0.0551       0.0738        0.044       0.0771       0.0606       0.0569       0.0993       0.0781       0.0724     0.000754\n",
            "     47     5      0.00855      0.00855     1.55e-06        0.054       0.0715       0.0439       0.0742       0.0591       0.0573       0.0937       0.0755       0.0646     0.000673\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              47  169.949    0.005       0.0101     2.07e-05       0.0101       0.0594       0.0776       0.0497       0.0789       0.0643       0.0634          0.1       0.0818        0.305      0.00317\n",
            "! Validation         47  169.949    0.005      0.00898     1.57e-06      0.00898        0.055       0.0733       0.0441       0.0769       0.0605       0.0573       0.0978       0.0775        0.074     0.000771\n",
            "Wall time: 169.94982056100002\n",
            "! Best model       47    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     48     1      0.00927      0.00927     2.29e-06        0.057       0.0745       0.0473       0.0764       0.0618       0.0599       0.0973       0.0786        0.107      0.00111\n",
            "     48     2       0.0151       0.0151     2.67e-06       0.0761        0.095       0.0692       0.0897       0.0795       0.0855        0.112       0.0986       0.0963        0.001\n",
            "     48     3       0.0135       0.0134     8.55e-06       0.0703       0.0897       0.0646       0.0818       0.0732       0.0806        0.106       0.0931        0.197      0.00205\n",
            "     48     4      0.00945      0.00944     1.83e-06       0.0551       0.0752       0.0434       0.0785       0.0609       0.0583        0.101       0.0795       0.0873     0.000909\n",
            "     48     5      0.00932       0.0093     2.28e-05       0.0567       0.0746       0.0469       0.0762       0.0616       0.0595        0.098       0.0788        0.335      0.00349\n",
            "     48     6       0.0116       0.0115     9.01e-05       0.0642       0.0831       0.0544       0.0837       0.0691       0.0689        0.106       0.0875          0.7      0.00729\n",
            "     48     7        0.009      0.00897     2.22e-05       0.0554       0.0733        0.046       0.0741       0.0601       0.0591       0.0956       0.0773        0.345      0.00359\n",
            "     48     8      0.00799      0.00786     0.000127       0.0519       0.0686       0.0411       0.0736       0.0573       0.0526       0.0926       0.0726        0.835       0.0087\n",
            "     48     9      0.00941      0.00939     2.32e-05       0.0576        0.075       0.0485       0.0759       0.0622       0.0616       0.0963       0.0789        0.354      0.00369\n",
            "     48    10      0.00993      0.00993     1.15e-06       0.0598       0.0771       0.0516       0.0763       0.0639       0.0653       0.0964       0.0808       0.0518     0.000539\n",
            "     48    11      0.00821      0.00813      8.1e-05       0.0525       0.0697       0.0424       0.0727       0.0576       0.0549       0.0925       0.0737        0.665      0.00693\n",
            "     48    12      0.00879      0.00877     2.49e-05       0.0558       0.0724       0.0457        0.076       0.0609       0.0572       0.0959       0.0766        0.368      0.00384\n",
            "     48    13      0.00858      0.00858     1.41e-06       0.0548       0.0717       0.0437        0.077       0.0603       0.0558       0.0958       0.0758       0.0754     0.000785\n",
            "     48    14      0.00958      0.00958     3.36e-06       0.0567       0.0757       0.0442       0.0816       0.0629        0.059        0.101       0.0801        0.129      0.00134\n",
            "     48    15      0.00891      0.00891     1.07e-06       0.0553        0.073       0.0452       0.0755       0.0604        0.058       0.0963       0.0771       0.0715     0.000745\n",
            "     48    16      0.00873      0.00873     1.33e-06       0.0552       0.0723       0.0438        0.078       0.0609        0.056        0.097       0.0765        0.067     0.000698\n",
            "     48    17      0.00804      0.00803      5.5e-06        0.053       0.0693        0.043       0.0728       0.0579        0.055       0.0915       0.0733         0.15      0.00156\n",
            "     48    18      0.00816      0.00813     2.99e-05       0.0533       0.0698       0.0425        0.075       0.0587       0.0543       0.0934       0.0738        0.386      0.00402\n",
            "     48    19      0.00787      0.00787     5.09e-06       0.0515       0.0686       0.0413       0.0719       0.0566       0.0528       0.0925       0.0726         0.15      0.00156\n",
            "     48    20      0.00807      0.00806     2.25e-06       0.0515       0.0695       0.0409       0.0727       0.0568       0.0534       0.0936       0.0735       0.0986      0.00103\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     48     1      0.00932      0.00932     2.75e-06       0.0565       0.0747       0.0452       0.0791       0.0621       0.0587       0.0992       0.0789        0.104      0.00108\n",
            "     48     2      0.00922      0.00922     8.78e-07       0.0553       0.0743       0.0439        0.078        0.061       0.0575       0.0997       0.0786       0.0518     0.000539\n",
            "     48     3      0.00848      0.00848     1.52e-06       0.0535       0.0712       0.0426       0.0752       0.0589        0.055       0.0958       0.0754       0.0761     0.000792\n",
            "     48     4      0.00902      0.00902      1.2e-06       0.0548       0.0735       0.0438       0.0769       0.0603       0.0566        0.099       0.0778       0.0721     0.000751\n",
            "     48     5      0.00845      0.00845     1.59e-06       0.0537       0.0711       0.0436       0.0739       0.0588       0.0569       0.0933       0.0751       0.0664     0.000692\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              48  173.462    0.005      0.00945     2.29e-05      0.00947       0.0572       0.0752       0.0473        0.077       0.0621        0.061       0.0976       0.0793        0.263      0.00274\n",
            "! Validation         48  173.462    0.005       0.0089     1.59e-06       0.0089       0.0548        0.073       0.0438       0.0766       0.0602       0.0569       0.0974       0.0772        0.074     0.000771\n",
            "Wall time: 173.46276624200004\n",
            "! Best model       48    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     49     1      0.00795      0.00791     4.42e-05       0.0516       0.0688       0.0425         0.07       0.0562       0.0542       0.0913       0.0727        0.482      0.00502\n",
            "     49     2       0.0082      0.00818     1.89e-05       0.0525         0.07       0.0424       0.0727       0.0575        0.055       0.0929        0.074        0.321      0.00334\n",
            "     49     3      0.00814      0.00814     2.32e-06       0.0526       0.0698       0.0425       0.0728       0.0577       0.0549       0.0927       0.0738        0.104      0.00108\n",
            "     49     4      0.00861      0.00861        5e-06       0.0541       0.0718       0.0436        0.075       0.0593       0.0569       0.0947       0.0758        0.146      0.00152\n",
            "     49     5         0.01      0.00999     5.91e-05       0.0597       0.0773       0.0503       0.0783       0.0643       0.0636       0.0992       0.0814        0.566       0.0059\n",
            "     49     6       0.0109       0.0109      9.3e-06       0.0622       0.0806        0.053       0.0805       0.0668        0.068        0.101       0.0846        0.215      0.00224\n",
            "     49     7      0.00808      0.00808     9.51e-07       0.0533       0.0695       0.0431       0.0736       0.0584       0.0548       0.0922       0.0735       0.0533     0.000555\n",
            "     49     8      0.00886      0.00886     3.15e-06       0.0555       0.0728       0.0453       0.0759       0.0606       0.0571       0.0969        0.077        0.121      0.00126\n",
            "     49     9      0.00926      0.00921      5.5e-05        0.059       0.0742       0.0531       0.0709        0.062       0.0651       0.0897       0.0774        0.546      0.00569\n",
            "     49    10      0.00792      0.00791     7.28e-06       0.0524       0.0688       0.0425       0.0724       0.0574       0.0542       0.0913       0.0727        0.183       0.0019\n",
            "     49    11        0.009      0.00899      2.3e-06        0.055       0.0734       0.0458       0.0735       0.0596       0.0593       0.0955       0.0774       0.0914     0.000952\n",
            "     49    12       0.0111       0.0111     1.27e-05       0.0636       0.0815       0.0546       0.0814        0.068       0.0682        0.103       0.0857        0.259       0.0027\n",
            "     49    13       0.0138       0.0137      6.1e-05       0.0699       0.0906       0.0573       0.0953       0.0763        0.072        0.119       0.0957        0.578      0.00602\n",
            "     49    14       0.0108       0.0108      9.9e-06       0.0617       0.0804       0.0502       0.0847       0.0675       0.0633        0.107        0.085         0.23      0.00239\n",
            "     49    15      0.00821       0.0082     6.04e-06       0.0543       0.0701        0.045       0.0728       0.0589       0.0568        0.091       0.0739        0.165      0.00172\n",
            "     49    16       0.0098      0.00979     1.49e-05       0.0573       0.0765       0.0479        0.076        0.062       0.0631       0.0981       0.0806        0.285      0.00297\n",
            "     49    17       0.0129       0.0129     1.38e-05       0.0695        0.088       0.0636       0.0813       0.0725       0.0797        0.103       0.0911         0.26      0.00271\n",
            "     49    18      0.00935      0.00934     1.61e-05       0.0575       0.0748       0.0476       0.0771       0.0624       0.0602       0.0976       0.0789         0.28      0.00291\n",
            "     49    19      0.00752       0.0075     2.26e-05       0.0504        0.067       0.0399       0.0714       0.0556       0.0497       0.0924        0.071        0.346      0.00361\n",
            "     49    20       0.0086      0.00859     9.15e-06       0.0544       0.0717       0.0434       0.0764       0.0599       0.0558        0.096       0.0759         0.22      0.00229\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     49     1      0.00923      0.00922     2.76e-06       0.0562       0.0743       0.0449       0.0788       0.0618       0.0583       0.0988       0.0786        0.104      0.00109\n",
            "     49     2      0.00913      0.00913     9.83e-07        0.055       0.0739       0.0437       0.0778       0.0607       0.0571       0.0994       0.0782       0.0562     0.000585\n",
            "     49     3      0.00842      0.00842     1.49e-06       0.0533        0.071       0.0424       0.0751       0.0587       0.0547       0.0956       0.0751       0.0739      0.00077\n",
            "     49     4      0.00894      0.00894     1.18e-06       0.0545       0.0732       0.0435       0.0766         0.06       0.0562       0.0987       0.0774       0.0716     0.000746\n",
            "     49     5      0.00837      0.00836     1.51e-06       0.0534       0.0708       0.0433       0.0736       0.0585       0.0565       0.0929       0.0747        0.064     0.000666\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              49  176.954    0.005      0.00943     1.87e-05      0.00945       0.0573       0.0751       0.0477       0.0766       0.0621        0.061       0.0975       0.0792        0.272      0.00284\n",
            "! Validation         49  176.954    0.005      0.00882     1.58e-06      0.00882       0.0545       0.0726       0.0436       0.0764         0.06       0.0566       0.0971       0.0768        0.074      0.00077\n",
            "Wall time: 176.95483435899996\n",
            "! Best model       49    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     50     1      0.00875      0.00869     5.92e-05       0.0541       0.0721       0.0433       0.0758       0.0596       0.0548        0.098       0.0764        0.568      0.00592\n",
            "     50     2      0.00815      0.00815     4.02e-06       0.0528       0.0698        0.043       0.0723       0.0577       0.0545       0.0933       0.0739        0.145      0.00151\n",
            "     50     3      0.00835      0.00834     1.07e-05        0.053       0.0707       0.0429       0.0732        0.058       0.0565       0.0927       0.0746        0.229      0.00239\n",
            "     50     4      0.00785      0.00783     1.47e-05       0.0521       0.0685       0.0412       0.0739       0.0576       0.0517       0.0934       0.0725         0.28      0.00291\n",
            "     50     5      0.00759      0.00758     1.18e-06       0.0513       0.0674       0.0419       0.0702        0.056       0.0529       0.0895       0.0712       0.0797      0.00083\n",
            "     50     6      0.00723      0.00723     1.96e-06       0.0503       0.0658       0.0401       0.0705       0.0553       0.0501       0.0892       0.0697       0.0914     0.000952\n",
            "     50     7      0.00874      0.00873     1.15e-05       0.0557       0.0723       0.0483       0.0705       0.0594       0.0611       0.0906       0.0759         0.24       0.0025\n",
            "     50     8      0.00945      0.00943     1.47e-05       0.0571       0.0751       0.0484       0.0746       0.0615       0.0628       0.0951       0.0789        0.261      0.00271\n",
            "     50     9      0.00779      0.00779     2.13e-06       0.0522       0.0683       0.0421       0.0724       0.0572       0.0536       0.0908       0.0722        0.099      0.00103\n",
            "     50    10      0.00999      0.00997     2.03e-05       0.0593       0.0772       0.0505        0.077       0.0637       0.0649       0.0973       0.0811        0.332      0.00346\n",
            "     50    11       0.0106       0.0106     7.01e-07       0.0611       0.0796       0.0506       0.0822       0.0664       0.0638        0.104        0.084       0.0523     0.000545\n",
            "     50    12        0.008      0.00799     8.19e-06       0.0517       0.0691       0.0408       0.0736       0.0572       0.0532       0.0932       0.0732          0.2      0.00208\n",
            "     50    13      0.00877      0.00876     1.19e-05       0.0546       0.0724       0.0431       0.0777       0.0604       0.0554       0.0979       0.0767        0.223      0.00233\n",
            "     50    14      0.00743      0.00741     2.34e-05        0.051       0.0666       0.0413       0.0704       0.0559       0.0526       0.0881       0.0704        0.355      0.00369\n",
            "     50    15      0.00942      0.00942     4.99e-07       0.0559       0.0751       0.0434       0.0808       0.0621       0.0577        0.101       0.0795       0.0361     0.000376\n",
            "     50    16      0.00838      0.00837     6.61e-06       0.0531       0.0708       0.0417        0.076       0.0588       0.0546       0.0953       0.0749        0.151      0.00157\n",
            "     50    17      0.00984      0.00984     6.89e-07       0.0581       0.0767       0.0465       0.0814        0.064       0.0584        0.104       0.0813       0.0531     0.000553\n",
            "     50    18      0.00933      0.00933      1.2e-06        0.057       0.0747       0.0447       0.0817       0.0632       0.0568        0.102       0.0791       0.0656     0.000684\n",
            "     50    19      0.00745      0.00745     1.88e-06       0.0502       0.0668       0.0404       0.0698       0.0551        0.052       0.0893       0.0706        0.099      0.00103\n",
            "     50    20      0.00967      0.00967     1.44e-06       0.0567       0.0761       0.0446        0.081       0.0628       0.0584        0.103       0.0805       0.0605     0.000631\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     50     1      0.00913      0.00913     2.67e-06       0.0559       0.0739       0.0446       0.0784       0.0615       0.0579       0.0984       0.0782        0.102      0.00107\n",
            "     50     2      0.00906      0.00905     1.03e-06       0.0548       0.0736       0.0435       0.0775       0.0605       0.0568       0.0991       0.0779        0.057     0.000594\n",
            "     50     3      0.00836      0.00836     1.42e-06       0.0531       0.0707       0.0422       0.0748       0.0585       0.0544       0.0953       0.0749        0.072      0.00075\n",
            "     50     4      0.00886      0.00886     1.23e-06       0.0542       0.0728       0.0432       0.0763       0.0598       0.0558       0.0983       0.0771       0.0735     0.000766\n",
            "     50     5      0.00829      0.00829     1.56e-06       0.0532       0.0704       0.0431       0.0734       0.0582       0.0562       0.0925       0.0744       0.0647     0.000674\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              50  180.421    0.005      0.00863     9.85e-06      0.00864       0.0544       0.0719       0.0439       0.0753       0.0596       0.0564       0.0955        0.076        0.181      0.00189\n",
            "! Validation         50  180.421    0.005      0.00874     1.58e-06      0.00874       0.0542       0.0723       0.0433       0.0761       0.0597       0.0562       0.0968       0.0765       0.0739      0.00077\n",
            "Wall time: 180.42218940299995\n",
            "! Best model       50    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     51     1       0.0113       0.0113     4.33e-05       0.0644       0.0822       0.0555       0.0821       0.0688       0.0713          0.1       0.0859        0.483      0.00504\n",
            "     51     2      0.00959      0.00958     6.82e-07       0.0587       0.0757       0.0493       0.0773       0.0633       0.0613       0.0984       0.0799       0.0586      0.00061\n",
            "     51     3      0.00813      0.00813     9.18e-07       0.0519       0.0698       0.0403       0.0751       0.0577       0.0512       0.0968        0.074       0.0627     0.000653\n",
            "     51     4      0.00978      0.00976     1.42e-05       0.0579       0.0764       0.0463        0.081       0.0637       0.0594        0.102       0.0809        0.268      0.00279\n",
            "     51     5       0.0137       0.0136     0.000104       0.0702       0.0901       0.0618       0.0871       0.0745       0.0769        0.112       0.0944         0.75      0.00782\n",
            "     51     6        0.012        0.012     4.08e-05       0.0664       0.0846       0.0596       0.0798       0.0697       0.0751        0.101       0.0881        0.474      0.00494\n",
            "     51     7      0.00845      0.00842     2.32e-05       0.0538        0.071        0.044       0.0736       0.0588       0.0563       0.0937        0.075        0.355       0.0037\n",
            "     51     8      0.00872      0.00871     5.47e-06       0.0552       0.0722       0.0451       0.0755       0.0603       0.0577       0.0948       0.0762        0.122      0.00128\n",
            "     51     9      0.00941      0.00941     5.02e-06       0.0586        0.075       0.0503       0.0754       0.0628       0.0635        0.094       0.0787        0.161      0.00168\n",
            "     51    10      0.00985      0.00985     6.99e-06       0.0589       0.0768       0.0482       0.0803       0.0642       0.0613        0.101       0.0811        0.184      0.00192\n",
            "     51    11      0.00955      0.00951     4.29e-05       0.0578       0.0754       0.0462       0.0812       0.0637       0.0586        0.101       0.0798        0.485      0.00505\n",
            "     51    12      0.00843      0.00842     8.26e-06       0.0537        0.071       0.0433       0.0746       0.0589       0.0549       0.0953       0.0751        0.204      0.00213\n",
            "     51    13      0.00824      0.00815     8.73e-05       0.0526       0.0699       0.0423       0.0732       0.0578       0.0545       0.0933       0.0739        0.693      0.00722\n",
            "     51    14      0.00941      0.00939     1.77e-05       0.0571        0.075       0.0478       0.0758       0.0618       0.0616       0.0964        0.079        0.298       0.0031\n",
            "     51    15       0.0089      0.00885      5.6e-05        0.055       0.0728       0.0442       0.0766       0.0604       0.0563       0.0977        0.077        0.553      0.00576\n",
            "     51    16       0.0085      0.00842     8.12e-05       0.0536        0.071       0.0431       0.0746       0.0588       0.0544       0.0959       0.0752        0.665      0.00692\n",
            "     51    17       0.0073       0.0073     2.03e-06        0.052       0.0661       0.0453       0.0654       0.0554       0.0568       0.0816       0.0692       0.0793     0.000826\n",
            "     51    18      0.00835      0.00827     8.18e-05       0.0531       0.0703       0.0422       0.0749       0.0586       0.0539        0.095       0.0745        0.671      0.00699\n",
            "     51    19       0.0116       0.0116     6.34e-06       0.0624       0.0833       0.0493       0.0886        0.069       0.0642        0.112       0.0882        0.183      0.00191\n",
            "     51    20      0.00922      0.00922      1.9e-06       0.0576       0.0743       0.0495       0.0738       0.0617       0.0634       0.0923       0.0778        0.082     0.000854\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     51     1      0.00905      0.00905     2.73e-06       0.0556       0.0736       0.0443       0.0782       0.0612       0.0576       0.0981       0.0778        0.103      0.00108\n",
            "     51     2      0.00898      0.00898     9.48e-07       0.0546       0.0733       0.0432       0.0773       0.0603       0.0564       0.0987       0.0776       0.0533     0.000555\n",
            "     51     3       0.0083       0.0083     1.48e-06       0.0529       0.0705        0.042       0.0746       0.0583       0.0542       0.0951       0.0746       0.0751     0.000782\n",
            "     51     4      0.00878      0.00878     1.26e-06        0.054       0.0725       0.0429       0.0761       0.0595       0.0555        0.098       0.0767       0.0745     0.000776\n",
            "     51     5      0.00821      0.00821     1.47e-06       0.0529       0.0701       0.0429       0.0731        0.058       0.0559       0.0922        0.074       0.0637     0.000663\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              51  183.920    0.005      0.00949     3.15e-05      0.00952       0.0575       0.0754       0.0477       0.0773       0.0625        0.061        0.098       0.0795        0.342      0.00356\n",
            "! Validation         51  183.920    0.005      0.00866     1.58e-06      0.00867        0.054        0.072       0.0431       0.0759       0.0595       0.0559       0.0965       0.0762        0.074     0.000771\n",
            "Wall time: 183.92081911500009\n",
            "! Best model       51    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     52     1      0.00832      0.00831     7.22e-06       0.0528       0.0705       0.0414       0.0756       0.0585       0.0527       0.0968       0.0747        0.178      0.00186\n",
            "     52     2       0.0104       0.0104     1.01e-05       0.0623        0.079        0.056       0.0748       0.0654       0.0695       0.0951       0.0823        0.222      0.00231\n",
            "     52     3      0.00892      0.00892     1.53e-06       0.0559       0.0731       0.0478       0.0721         0.06       0.0615        0.092       0.0767       0.0842     0.000877\n",
            "     52     4      0.00804      0.00804     9.92e-07       0.0526       0.0694       0.0411       0.0755       0.0583       0.0532       0.0937       0.0735       0.0607     0.000633\n",
            "     52     5      0.00778      0.00776     2.22e-05        0.052       0.0682       0.0418       0.0723       0.0571       0.0536       0.0905       0.0721         0.33      0.00344\n",
            "     52     6      0.00916      0.00916     2.48e-06       0.0564        0.074       0.0453       0.0785       0.0619       0.0584       0.0982       0.0783        0.108      0.00112\n",
            "     52     7       0.0085      0.00848     1.73e-05       0.0545       0.0712       0.0431       0.0773       0.0602       0.0542       0.0967       0.0755        0.306      0.00318\n",
            "     52     8      0.00954      0.00953     3.49e-06       0.0567       0.0755       0.0459       0.0782       0.0621       0.0598       0.0998       0.0798       0.0936     0.000975\n",
            "     52     9      0.00786      0.00783     3.43e-05       0.0516       0.0685        0.042       0.0709       0.0564       0.0535       0.0913       0.0724        0.433      0.00451\n",
            "     52    10       0.0091       0.0091     1.19e-06       0.0568       0.0738        0.048       0.0743       0.0612       0.0607       0.0946       0.0777       0.0748     0.000779\n",
            "     52    11      0.00902      0.00896     6.21e-05       0.0565       0.0732       0.0488        0.072       0.0604       0.0626       0.0909       0.0767         0.58      0.00604\n",
            "     52    12       0.0076      0.00757      3.1e-05       0.0514       0.0673       0.0426       0.0689       0.0558       0.0537       0.0885       0.0711        0.409      0.00426\n",
            "     52    13      0.00824      0.00822     1.94e-05       0.0525       0.0701       0.0427       0.0721       0.0574       0.0554       0.0928       0.0741        0.325      0.00339\n",
            "     52    14       0.0094      0.00933     6.73e-05       0.0577       0.0747       0.0476       0.0779       0.0627         0.06       0.0977       0.0789        0.608      0.00633\n",
            "     52    15       0.0102       0.0101     4.32e-06       0.0584       0.0779       0.0455       0.0843       0.0649       0.0583        0.107       0.0826         0.13      0.00136\n",
            "     52    16      0.00734      0.00733     7.94e-06        0.051       0.0663       0.0421       0.0688       0.0554       0.0525       0.0875         0.07        0.195      0.00203\n",
            "     52    17      0.00879      0.00879     1.29e-06       0.0535       0.0725       0.0415       0.0774       0.0595       0.0542       0.0995       0.0769       0.0701      0.00073\n",
            "     52    18      0.00875      0.00875     3.66e-06       0.0561       0.0724       0.0457       0.0768       0.0612       0.0578        0.095       0.0764        0.138      0.00144\n",
            "     52    19      0.00815      0.00815     7.12e-07       0.0521       0.0698        0.041       0.0743       0.0576       0.0531       0.0948        0.074       0.0613     0.000639\n",
            "     52    20       0.0094      0.00939     1.48e-05       0.0578        0.075        0.051       0.0714       0.0612       0.0659       0.0904       0.0781        0.265      0.00276\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     52     1      0.00896      0.00896     2.71e-06       0.0553       0.0732       0.0441       0.0778        0.061       0.0572       0.0977       0.0775        0.103      0.00107\n",
            "     52     2       0.0089       0.0089     1.06e-06       0.0543        0.073        0.043        0.077         0.06       0.0561       0.0984       0.0773       0.0567     0.000591\n",
            "     52     3      0.00824      0.00824     1.46e-06       0.0527       0.0702       0.0418       0.0744       0.0581       0.0539       0.0949       0.0744       0.0723     0.000753\n",
            "     52     4       0.0087       0.0087     1.23e-06       0.0537       0.0721       0.0427       0.0758       0.0593       0.0551       0.0977       0.0764       0.0734     0.000765\n",
            "     52     5      0.00813      0.00813     1.44e-06       0.0527       0.0697       0.0426       0.0728       0.0577       0.0555       0.0918       0.0737       0.0616     0.000642\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              52  187.411    0.005      0.00871     1.57e-05      0.00873       0.0549       0.0722       0.0451       0.0747       0.0599       0.0577       0.0947       0.0762        0.234      0.00243\n",
            "! Validation         52  187.411    0.005      0.00858     1.58e-06      0.00859       0.0538       0.0717       0.0428       0.0756       0.0592       0.0556       0.0961       0.0758       0.0734     0.000764\n",
            "Wall time: 187.4118598529999\n",
            "! Best model       52    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     53     1      0.00924      0.00921     2.59e-05       0.0575       0.0742       0.0474       0.0778       0.0626       0.0604       0.0962       0.0783        0.371      0.00386\n",
            "     53     2       0.0102       0.0102     1.16e-05       0.0604       0.0782       0.0477        0.086       0.0668       0.0599        0.106       0.0828         0.24       0.0025\n",
            "     53     3      0.00793      0.00791     1.85e-05       0.0517       0.0688        0.041       0.0732       0.0571       0.0518        0.094       0.0729         0.31      0.00323\n",
            "     53     4      0.00806      0.00804     1.82e-05       0.0536       0.0694       0.0442       0.0723       0.0582       0.0558       0.0907       0.0732        0.298       0.0031\n",
            "     53     5      0.00844      0.00844     8.27e-07       0.0544       0.0711       0.0454       0.0726        0.059       0.0567       0.0934        0.075       0.0633     0.000659\n",
            "     53     6      0.00978      0.00973     4.74e-05       0.0565       0.0763       0.0447       0.0801       0.0624       0.0584        0.103       0.0808        0.508      0.00529\n",
            "     53     7      0.00742       0.0074      2.2e-05       0.0512       0.0665       0.0414       0.0709       0.0561       0.0518       0.0889       0.0704        0.345      0.00359\n",
            "     53     8      0.00977      0.00973     4.55e-05       0.0588       0.0763       0.0499       0.0765       0.0632       0.0638       0.0966       0.0802        0.493      0.00514\n",
            "     53     9      0.00706      0.00702     3.58e-05       0.0491       0.0648       0.0382       0.0711       0.0546       0.0483       0.0891       0.0687         0.44      0.00458\n",
            "     53    10      0.00879      0.00879     8.22e-06        0.057       0.0725       0.0508       0.0692         0.06       0.0638       0.0874       0.0756        0.202      0.00211\n",
            "     53    11      0.00823      0.00821     1.88e-05       0.0546       0.0701       0.0471       0.0696       0.0584       0.0586       0.0888       0.0737        0.313      0.00327\n",
            "     53    12      0.00786      0.00786     2.66e-06       0.0513       0.0686        0.039       0.0759       0.0574       0.0503       0.0951       0.0727        0.107      0.00111\n",
            "     53    13      0.00811      0.00804     6.82e-05       0.0518       0.0694       0.0395       0.0765        0.058       0.0502        0.097       0.0736        0.612      0.00638\n",
            "     53    14      0.00853      0.00849     3.57e-05       0.0538       0.0713       0.0423       0.0768       0.0595        0.054       0.0971       0.0755        0.441      0.00459\n",
            "     53    15      0.00729      0.00721     7.92e-05       0.0487       0.0657       0.0375       0.0712       0.0543       0.0491       0.0901       0.0696        0.656      0.00684\n",
            "     53    16      0.00821      0.00813     7.44e-05       0.0525       0.0698       0.0433        0.071       0.0571       0.0558       0.0916       0.0737        0.637      0.00663\n",
            "     53    17      0.00784      0.00783     3.05e-06       0.0519       0.0685       0.0411       0.0734       0.0572       0.0525       0.0924       0.0725         0.11      0.00115\n",
            "     53    18      0.00789      0.00783     5.52e-05       0.0519       0.0685       0.0424        0.071       0.0567       0.0542       0.0904       0.0723        0.543      0.00566\n",
            "     53    19         0.01         0.01     1.49e-06       0.0584       0.0774       0.0465        0.082       0.0643       0.0599        0.104       0.0819       0.0744     0.000775\n",
            "     53    20      0.00804      0.00804     1.14e-06        0.052       0.0694       0.0419        0.072        0.057       0.0539       0.0928       0.0734       0.0656     0.000684\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     53     1      0.00888      0.00888     2.77e-06       0.0551       0.0729       0.0438       0.0775       0.0607       0.0569       0.0973       0.0771        0.104      0.00108\n",
            "     53     2      0.00883      0.00882     1.04e-06       0.0541       0.0727       0.0428       0.0768       0.0598       0.0558       0.0981       0.0769       0.0563     0.000587\n",
            "     53     3      0.00818      0.00818     1.52e-06       0.0525         0.07       0.0416       0.0742       0.0579       0.0536       0.0946       0.0741       0.0732     0.000763\n",
            "     53     4      0.00862      0.00862      1.3e-06       0.0535       0.0718       0.0424       0.0756        0.059       0.0548       0.0974       0.0761       0.0759      0.00079\n",
            "     53     5      0.00806      0.00806     1.46e-06       0.0525       0.0694       0.0424       0.0726       0.0575       0.0552       0.0915       0.0734       0.0642     0.000668\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              53  190.909    0.005      0.00841     2.87e-05      0.00844       0.0539       0.0709       0.0436       0.0744        0.059       0.0557       0.0944        0.075        0.341      0.00356\n",
            "! Validation         53  190.909    0.005      0.00851     1.62e-06      0.00851       0.0535       0.0714       0.0426       0.0753        0.059       0.0552       0.0958       0.0755       0.0747     0.000778\n",
            "Wall time: 190.90950501399993\n",
            "! Best model       53    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     54     1      0.00731      0.00731     1.05e-06       0.0502       0.0662       0.0409       0.0688       0.0549       0.0525       0.0872       0.0699       0.0697     0.000726\n",
            "     54     2      0.00891      0.00891     6.76e-06       0.0548        0.073       0.0426       0.0792       0.0609       0.0547          0.1       0.0774        0.189      0.00197\n",
            "     54     3      0.00782      0.00781     1.03e-05       0.0532       0.0684       0.0457        0.068       0.0569       0.0577       0.0859       0.0718        0.237      0.00247\n",
            "     54     4      0.00841      0.00838     2.73e-05       0.0539       0.0708       0.0433        0.075       0.0592       0.0546       0.0953        0.075        0.372      0.00388\n",
            "     54     5      0.00802      0.00795     6.73e-05       0.0525        0.069       0.0409       0.0759       0.0584       0.0518       0.0944       0.0731        0.607      0.00632\n",
            "     54     6      0.00706      0.00704     2.36e-05         0.05       0.0649       0.0407       0.0687       0.0547       0.0511       0.0861       0.0686        0.355       0.0037\n",
            "     54     7       0.0103       0.0103     1.44e-05       0.0603       0.0785       0.0476       0.0855       0.0666       0.0609        0.105       0.0831        0.267      0.00278\n",
            "     54     8       0.0118       0.0116     0.000185        0.064       0.0835       0.0528       0.0865       0.0696       0.0682        0.108        0.088         1.01       0.0105\n",
            "     54     9      0.00914      0.00914     3.11e-06       0.0574        0.074       0.0509       0.0703       0.0606       0.0644       0.0901       0.0773        0.099      0.00103\n",
            "     54    10      0.00837      0.00834     2.74e-05       0.0528       0.0707       0.0415       0.0754       0.0584       0.0536       0.0961       0.0748        0.386      0.00402\n",
            "     54    11      0.00874      0.00874     4.54e-06       0.0552       0.0723       0.0461       0.0735       0.0598       0.0589       0.0935       0.0762        0.157      0.00164\n",
            "     54    12      0.00959      0.00959     3.45e-06       0.0589       0.0758       0.0493       0.0781       0.0637       0.0617        0.098       0.0799        0.134       0.0014\n",
            "     54    13      0.00894      0.00894     1.36e-06       0.0555       0.0732       0.0439       0.0785       0.0612        0.056       0.0989       0.0775       0.0635     0.000661\n",
            "     54    14      0.00792      0.00785     7.27e-05       0.0513       0.0685       0.0404       0.0731       0.0568       0.0526       0.0926       0.0726        0.629      0.00655\n",
            "     54    15      0.00772      0.00772     7.37e-06       0.0515        0.068       0.0418       0.0708       0.0563       0.0527       0.0911       0.0719        0.201      0.00209\n",
            "     54    16       0.0084      0.00834     6.03e-05       0.0528       0.0707       0.0423       0.0738        0.058       0.0548       0.0947       0.0748         0.57      0.00594\n",
            "     54    17      0.00932      0.00925     7.31e-05        0.055       0.0744        0.042       0.0808       0.0614       0.0547        0.103       0.0789        0.631      0.00657\n",
            "     54    18       0.0079      0.00789     8.35e-06       0.0522       0.0687       0.0416       0.0735       0.0575       0.0533       0.0922       0.0727        0.203      0.00212\n",
            "     54    19      0.00915      0.00905     9.77e-05       0.0559       0.0736       0.0472       0.0733       0.0602       0.0604       0.0947       0.0775        0.734      0.00765\n",
            "     54    20       0.0084      0.00834      5.4e-05       0.0547       0.0707       0.0465       0.0711       0.0588       0.0582       0.0905       0.0744        0.542      0.00564\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     54     1      0.00879      0.00878     2.63e-06       0.0548       0.0725       0.0435       0.0772       0.0604       0.0564        0.097       0.0767        0.101      0.00105\n",
            "     54     2      0.00874      0.00874     1.13e-06       0.0539       0.0723       0.0425       0.0765       0.0595       0.0554       0.0977       0.0766       0.0583     0.000607\n",
            "     54     3      0.00812      0.00812     1.42e-06       0.0523       0.0697       0.0414        0.074       0.0577       0.0533       0.0943       0.0738       0.0696     0.000725\n",
            "     54     4      0.00854      0.00854      1.2e-06       0.0532       0.0715       0.0422       0.0753       0.0587       0.0544        0.097       0.0757       0.0728     0.000758\n",
            "     54     5      0.00799      0.00798     1.44e-06       0.0522       0.0691       0.0421       0.0724       0.0573       0.0549       0.0911        0.073       0.0615     0.000641\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              54  194.418    0.005      0.00863     3.75e-05      0.00866       0.0546       0.0719       0.0444        0.075       0.0597       0.0568        0.095       0.0759        0.373      0.00388\n",
            "! Validation         54  194.418    0.005      0.00843     1.56e-06      0.00843       0.0533        0.071       0.0424       0.0751       0.0587       0.0549       0.0955       0.0752       0.0726     0.000756\n",
            "Wall time: 194.41877740199993\n",
            "! Best model       54    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     55     1       0.0065       0.0065     1.36e-06       0.0476       0.0624       0.0381       0.0665       0.0523       0.0479       0.0841        0.066       0.0811     0.000844\n",
            "     55     2      0.00764      0.00759     4.65e-05       0.0508       0.0674       0.0404       0.0716        0.056       0.0511       0.0917       0.0714        0.504      0.00526\n",
            "     55     3      0.00911      0.00907     3.43e-05       0.0563       0.0737       0.0462       0.0764       0.0613       0.0587        0.097       0.0778        0.426      0.00444\n",
            "     55     4      0.00794      0.00792     1.92e-05       0.0517       0.0689       0.0412       0.0728        0.057       0.0527       0.0932       0.0729        0.324      0.00338\n",
            "     55     5       0.0077      0.00769     1.82e-06       0.0514       0.0679       0.0408       0.0725       0.0566       0.0513       0.0924       0.0719       0.0873     0.000909\n",
            "     55     6      0.00806      0.00805     5.08e-06       0.0514       0.0694       0.0401       0.0739        0.057       0.0533       0.0937       0.0735         0.16      0.00167\n",
            "     55     7      0.00879      0.00878     3.79e-06       0.0552       0.0725       0.0443        0.077       0.0606       0.0569       0.0964       0.0767        0.127      0.00132\n",
            "     55     8      0.00995      0.00994     7.21e-06       0.0593       0.0771       0.0497       0.0785       0.0641       0.0626          0.1       0.0813        0.194      0.00202\n",
            "     55     9       0.0132       0.0131     0.000114       0.0693       0.0886        0.061        0.086       0.0735       0.0767        0.109       0.0927        0.792      0.00825\n",
            "     55    10       0.0124       0.0123     1.31e-05       0.0679       0.0859       0.0606       0.0825       0.0715       0.0754        0.104       0.0896        0.248      0.00258\n",
            "     55    11       0.0081      0.00805     4.63e-05       0.0535       0.0694       0.0437       0.0732       0.0584       0.0552       0.0915       0.0733        0.502      0.00523\n",
            "     55    12      0.00753       0.0075     2.98e-05       0.0508        0.067       0.0414       0.0695       0.0554       0.0529       0.0888       0.0708          0.4      0.00417\n",
            "     55    13      0.00971      0.00971     5.68e-06        0.058       0.0762       0.0486       0.0767       0.0626       0.0626       0.0979       0.0803        0.156      0.00162\n",
            "     55    14       0.0113       0.0112     5.08e-05       0.0626        0.082       0.0519       0.0839       0.0679       0.0666        0.106       0.0865        0.526      0.00548\n",
            "     55    15      0.00855      0.00855     2.09e-06        0.053       0.0715       0.0421       0.0748       0.0585       0.0567       0.0944       0.0756       0.0959     0.000999\n",
            "     55    16      0.00785      0.00784     2.55e-06       0.0527       0.0685       0.0418       0.0743       0.0581       0.0534       0.0916       0.0725       0.0941     0.000981\n",
            "     55    17         0.01      0.00999     7.19e-06       0.0596       0.0773       0.0486       0.0815        0.065        0.061        0.102       0.0817        0.177      0.00184\n",
            "     55    18      0.00971       0.0097      6.3e-06       0.0595       0.0762       0.0526       0.0733       0.0629       0.0664       0.0928       0.0796        0.169      0.00176\n",
            "     55    19      0.00874      0.00874      9.6e-07       0.0547       0.0723       0.0448       0.0745       0.0597       0.0564       0.0965       0.0765       0.0475     0.000494\n",
            "     55    20      0.00785      0.00779     5.65e-05       0.0516       0.0683       0.0409        0.073        0.057       0.0526       0.0919       0.0723        0.548      0.00571\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     55     1      0.00871      0.00871     2.75e-06       0.0545       0.0722       0.0433        0.077       0.0601       0.0561       0.0967       0.0764        0.104      0.00108\n",
            "     55     2      0.00867      0.00867     1.02e-06       0.0536       0.0721       0.0423       0.0763       0.0593       0.0551       0.0974       0.0763       0.0549     0.000572\n",
            "     55     3      0.00806      0.00806     1.44e-06        0.052       0.0694       0.0412       0.0737       0.0575        0.053       0.0941       0.0735       0.0731     0.000762\n",
            "     55     4      0.00847      0.00847     1.33e-06        0.053       0.0712       0.0419       0.0751       0.0585       0.0541       0.0968       0.0754       0.0762     0.000793\n",
            "     55     5      0.00792      0.00791     1.41e-06        0.052       0.0688       0.0419       0.0721        0.057       0.0546       0.0908       0.0727       0.0619     0.000645\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              55  197.904    0.005      0.00901     2.27e-05      0.00903       0.0558       0.0734       0.0459       0.0756       0.0608        0.059        0.096       0.0775        0.283      0.00295\n",
            "! Validation         55  197.904    0.005      0.00836     1.59e-06      0.00837        0.053       0.0708       0.0421       0.0748       0.0585       0.0546       0.0952       0.0749        0.074     0.000771\n",
            "Wall time: 197.9045581779999\n",
            "! Best model       55    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     56     1      0.00832      0.00828     4.18e-05       0.0533       0.0704       0.0441       0.0716       0.0579       0.0571       0.0914       0.0742        0.473      0.00493\n",
            "     56     2      0.00896      0.00895     9.54e-06        0.055       0.0732       0.0438       0.0774       0.0606       0.0568        0.098       0.0774        0.223      0.00233\n",
            "     56     3      0.00804      0.00802     1.98e-05       0.0534       0.0693       0.0439       0.0723       0.0581       0.0553       0.0911       0.0732        0.324      0.00337\n",
            "     56     4      0.00808      0.00807     1.02e-05       0.0535       0.0695       0.0428       0.0749       0.0589       0.0537       0.0934       0.0736        0.227      0.00236\n",
            "     56     5      0.00739      0.00739     5.63e-07         0.05       0.0665       0.0394        0.071       0.0552       0.0504       0.0905       0.0704       0.0436     0.000454\n",
            "     56     6      0.00788      0.00788     2.98e-06       0.0524       0.0687        0.042       0.0731       0.0576       0.0531       0.0923       0.0727        0.124      0.00129\n",
            "     56     7      0.00913      0.00912     9.26e-06       0.0564       0.0739       0.0461       0.0771       0.0616       0.0591        0.097        0.078        0.223      0.00232\n",
            "     56     8      0.00854      0.00854     5.87e-06       0.0542       0.0715       0.0441       0.0744       0.0593       0.0556       0.0956       0.0756        0.178      0.00186\n",
            "     56     9      0.00772      0.00772     5.45e-07       0.0511        0.068       0.0406        0.072       0.0563       0.0519       0.0921        0.072       0.0428     0.000446\n",
            "     56    10       0.0125       0.0125     1.04e-06       0.0654       0.0864       0.0527       0.0907       0.0717        0.068        0.115       0.0914        0.067     0.000698\n",
            "     56    11       0.0113       0.0112      6.1e-05       0.0655        0.082       0.0588       0.0789       0.0689       0.0715       0.0997       0.0856        0.575      0.00599\n",
            "     56    12       0.0107       0.0107     4.04e-06       0.0623       0.0799       0.0535       0.0798       0.0666       0.0681       0.0995       0.0838        0.122      0.00127\n",
            "     56    13      0.00839      0.00835     4.22e-05       0.0529       0.0707        0.041       0.0765       0.0588       0.0532       0.0966       0.0749        0.477      0.00497\n",
            "     56    14      0.00802      0.00799     2.81e-05       0.0525       0.0692       0.0408       0.0757       0.0583       0.0521       0.0945       0.0733        0.388      0.00405\n",
            "     56    15      0.00744      0.00743     4.76e-06       0.0501       0.0667       0.0394       0.0716       0.0555       0.0502       0.0912       0.0707        0.157      0.00164\n",
            "     56    16      0.00696      0.00694     2.38e-05       0.0481       0.0644       0.0377        0.069       0.0533       0.0476        0.089       0.0683        0.354      0.00369\n",
            "     56    17      0.00904      0.00902     1.14e-05       0.0558       0.0735       0.0443       0.0786       0.0615       0.0578       0.0976       0.0777        0.243      0.00253\n",
            "     56    18      0.00747      0.00747     2.54e-06       0.0508       0.0669       0.0415       0.0693       0.0554       0.0523       0.0891       0.0707        0.104      0.00109\n",
            "     56    19      0.00707      0.00706     3.16e-06       0.0497        0.065       0.0414       0.0665       0.0539       0.0522        0.085       0.0686        0.106      0.00111\n",
            "     56    20      0.00758      0.00758     3.12e-06       0.0505       0.0674       0.0393        0.073       0.0561       0.0509       0.0918       0.0714        0.112      0.00116\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     56     1      0.00864      0.00863     2.72e-06       0.0543       0.0719        0.043       0.0767       0.0599       0.0558       0.0963       0.0761        0.102      0.00107\n",
            "     56     2      0.00861      0.00861     1.11e-06       0.0534       0.0718       0.0421        0.076       0.0591       0.0549       0.0972        0.076       0.0569     0.000593\n",
            "     56     3        0.008        0.008     1.44e-06       0.0518       0.0692        0.041       0.0736       0.0573       0.0527       0.0939       0.0733       0.0709     0.000739\n",
            "     56     4      0.00841       0.0084     1.24e-06       0.0528       0.0709       0.0417       0.0749       0.0583       0.0537       0.0965       0.0751       0.0736     0.000767\n",
            "     56     5      0.00785      0.00785     1.37e-06       0.0517       0.0685       0.0417       0.0719       0.0568       0.0543       0.0905       0.0724       0.0604     0.000629\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              56  201.413    0.005      0.00851     1.43e-05      0.00853       0.0541       0.0714       0.0439       0.0747       0.0593       0.0562       0.0947       0.0754        0.228      0.00238\n",
            "! Validation         56  201.413    0.005       0.0083     1.58e-06       0.0083       0.0528       0.0705       0.0419       0.0746       0.0583       0.0543       0.0949       0.0746       0.0728     0.000758\n",
            "Wall time: 201.4144354089999\n",
            "! Best model       56    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     57     1      0.00818      0.00817     2.02e-06       0.0536       0.0699       0.0436       0.0736       0.0586       0.0551       0.0927       0.0739       0.0844     0.000879\n",
            "     57     2      0.00766      0.00765     5.43e-06       0.0514       0.0677       0.0418       0.0705       0.0562       0.0533       0.0898       0.0715        0.138      0.00144\n",
            "     57     3      0.00665      0.00665     3.15e-06       0.0477       0.0631       0.0379       0.0672       0.0526       0.0481       0.0855       0.0668        0.124      0.00129\n",
            "     57     4      0.00739      0.00739     2.99e-06       0.0497       0.0665       0.0396         0.07       0.0548       0.0512       0.0897       0.0704        0.102      0.00106\n",
            "     57     5      0.00781       0.0078     5.03e-06       0.0521       0.0683       0.0414       0.0734       0.0574       0.0526       0.0921       0.0723        0.159      0.00166\n",
            "     57     6      0.00935      0.00926     8.68e-05       0.0574       0.0745       0.0451       0.0819       0.0635       0.0565        0.101       0.0789        0.689      0.00718\n",
            "     57     7      0.00876      0.00876     6.23e-06        0.054       0.0724       0.0441        0.074        0.059       0.0574       0.0955       0.0765        0.162      0.00169\n",
            "     57     8      0.00832      0.00828     3.74e-05       0.0529       0.0704       0.0423       0.0741       0.0582       0.0554       0.0935       0.0744        0.451       0.0047\n",
            "     57     9      0.00743      0.00742      6.1e-06       0.0501       0.0667         0.04       0.0703       0.0551        0.052        0.089       0.0705        0.156      0.00163\n",
            "     57    10      0.00756      0.00755     5.24e-06       0.0513       0.0672       0.0417       0.0706       0.0562       0.0531        0.089       0.0711        0.155      0.00162\n",
            "     57    11      0.00731       0.0073     1.06e-05       0.0501       0.0661       0.0399       0.0706       0.0552       0.0512       0.0887       0.0699        0.236      0.00246\n",
            "     57    12      0.00739      0.00738     5.76e-06       0.0492       0.0665       0.0374       0.0728       0.0551       0.0476       0.0933       0.0705        0.174      0.00181\n",
            "     57    13      0.00706      0.00706     6.38e-07       0.0494        0.065       0.0399       0.0684       0.0541       0.0514       0.0859       0.0687       0.0527     0.000549\n",
            "     57    14      0.00764      0.00763     1.51e-05       0.0516       0.0676       0.0408       0.0734       0.0571       0.0512        0.092       0.0716        0.287      0.00298\n",
            "     57    15      0.00801      0.00801     3.14e-06       0.0525       0.0692       0.0436       0.0704        0.057       0.0553       0.0909       0.0731        0.109      0.00114\n",
            "     57    16      0.00818      0.00818     6.37e-06       0.0531         0.07       0.0448       0.0697       0.0572       0.0567       0.0908       0.0738        0.186      0.00194\n",
            "     57    17      0.00948      0.00946     2.55e-05       0.0565       0.0752       0.0445       0.0804       0.0625       0.0579        0.101       0.0796        0.372      0.00388\n",
            "     57    18      0.00811      0.00811     1.57e-06       0.0527       0.0697       0.0415       0.0752       0.0583       0.0531       0.0944       0.0738       0.0748     0.000779\n",
            "     57    19      0.00989      0.00987     2.25e-05       0.0577       0.0769       0.0469       0.0795       0.0632       0.0609        0.102       0.0812        0.352      0.00366\n",
            "     57    20       0.0103       0.0102     3.19e-05       0.0596       0.0783       0.0491       0.0807       0.0649       0.0619        0.103       0.0827        0.418      0.00435\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     57     1      0.00856      0.00856     2.71e-06        0.054       0.0716       0.0428       0.0764       0.0596       0.0555        0.096       0.0757        0.102      0.00106\n",
            "     57     2      0.00854      0.00854     1.19e-06       0.0532       0.0715       0.0419       0.0759       0.0589       0.0545       0.0969       0.0757       0.0607     0.000633\n",
            "     57     3      0.00795      0.00794     1.41e-06       0.0517        0.069       0.0408       0.0733       0.0571       0.0524       0.0937        0.073       0.0687     0.000715\n",
            "     57     4      0.00833      0.00833     1.29e-06       0.0525       0.0706       0.0415       0.0747       0.0581       0.0534       0.0962       0.0748       0.0745     0.000776\n",
            "     57     5      0.00778      0.00778     1.39e-06       0.0515       0.0682       0.0414       0.0717       0.0565        0.054       0.0902       0.0721       0.0607     0.000633\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              57  204.917    0.005      0.00811     1.42e-05      0.00812       0.0526       0.0697       0.0423       0.0733       0.0578       0.0542       0.0932       0.0737        0.224      0.00233\n",
            "! Validation         57  204.917    0.005      0.00823      1.6e-06      0.00823       0.0526       0.0702       0.0417       0.0744        0.058        0.054       0.0946       0.0743       0.0733     0.000764\n",
            "Wall time: 204.91787231099988\n",
            "! Best model       57    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     58     1      0.00785      0.00785     3.55e-06       0.0526       0.0685       0.0414        0.075       0.0582       0.0521       0.0931       0.0726        0.134      0.00139\n",
            "     58     2      0.00659      0.00658     1.22e-05        0.047       0.0627       0.0371        0.067        0.052       0.0474       0.0856       0.0665        0.254      0.00264\n",
            "     58     3      0.00885      0.00881     4.06e-05       0.0544       0.0726       0.0437       0.0758       0.0598       0.0563       0.0973       0.0768        0.465      0.00484\n",
            "     58     4       0.0084      0.00838      1.8e-05        0.054       0.0708       0.0447       0.0727       0.0587       0.0568       0.0927       0.0747        0.307       0.0032\n",
            "     58     5      0.00868      0.00867        1e-05       0.0544       0.0721       0.0434       0.0765       0.0599       0.0551       0.0975       0.0763        0.214      0.00223\n",
            "     58     6      0.00771      0.00769     1.76e-05       0.0516       0.0679       0.0414        0.072       0.0567       0.0533       0.0902       0.0717        0.304      0.00316\n",
            "     58     7      0.00666      0.00663     2.58e-05       0.0486        0.063       0.0398       0.0663       0.0531       0.0493       0.0839       0.0666        0.373      0.00388\n",
            "     58     8       0.0076       0.0076     2.91e-06       0.0511       0.0674       0.0414       0.0705        0.056       0.0538       0.0887       0.0712        0.112      0.00116\n",
            "     58     9      0.00787      0.00787     7.57e-06        0.052       0.0686       0.0408       0.0742       0.0575       0.0521       0.0933       0.0727        0.199      0.00208\n",
            "     58    10       0.0101       0.0101     3.85e-05       0.0594       0.0776       0.0484       0.0812       0.0648       0.0619        0.102        0.082        0.437      0.00456\n",
            "     58    11      0.00868      0.00867     4.92e-06       0.0548        0.072       0.0475       0.0694       0.0584       0.0601       0.0914       0.0757        0.154      0.00161\n",
            "     58    12      0.00748      0.00747     7.18e-06       0.0513       0.0669       0.0408       0.0724       0.0566       0.0518       0.0897       0.0708        0.178      0.00186\n",
            "     58    13      0.00837      0.00836     5.57e-06       0.0531       0.0707       0.0409       0.0775       0.0592       0.0529       0.0971        0.075        0.166      0.00173\n",
            "     58    14      0.00716      0.00715     1.86e-05       0.0499       0.0654       0.0399       0.0698       0.0549       0.0508       0.0876       0.0692         0.31      0.00323\n",
            "     58    15      0.00868      0.00867     1.24e-05       0.0543        0.072       0.0444        0.074       0.0592       0.0578       0.0943        0.076        0.259       0.0027\n",
            "     58    16      0.00782      0.00782     3.08e-06       0.0514       0.0684       0.0408       0.0725       0.0566        0.052       0.0929       0.0725        0.116      0.00121\n",
            "     58    17       0.0078      0.00777      3.1e-05       0.0515       0.0682       0.0402        0.074       0.0571       0.0508       0.0937       0.0723         0.41      0.00427\n",
            "     58    18      0.00769      0.00769     5.15e-07        0.051       0.0679       0.0409       0.0713       0.0561       0.0523       0.0913       0.0718       0.0471      0.00049\n",
            "     58    19      0.00701        0.007     1.02e-05       0.0485       0.0647       0.0389       0.0677       0.0533       0.0495       0.0875       0.0685        0.224      0.00234\n",
            "     58    20      0.00753      0.00752     9.74e-07       0.0504       0.0671       0.0395       0.0722       0.0558       0.0512       0.0909       0.0711       0.0656     0.000684\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     58     1       0.0085      0.00849     2.66e-06       0.0538       0.0713       0.0426       0.0762       0.0594       0.0552       0.0957       0.0755        0.101      0.00105\n",
            "     58     2      0.00848      0.00847     1.12e-06        0.053       0.0712       0.0417       0.0757       0.0587       0.0542       0.0966       0.0754       0.0575     0.000599\n",
            "     58     3      0.00789      0.00789     1.41e-06       0.0515       0.0687       0.0406       0.0732       0.0569       0.0521       0.0935       0.0728       0.0702     0.000731\n",
            "     58     4      0.00827      0.00826     1.25e-06       0.0523       0.0703       0.0412       0.0744       0.0578       0.0531       0.0959       0.0745       0.0736     0.000767\n",
            "     58     5      0.00772      0.00771     1.36e-06       0.0513        0.068       0.0412       0.0714       0.0563       0.0537       0.0899       0.0718       0.0603     0.000628\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              58  208.402    0.005      0.00791     1.36e-05      0.00793       0.0521       0.0688       0.0418       0.0726       0.0572       0.0535       0.0921       0.0728        0.236      0.00246\n",
            "! Validation         58  208.402    0.005      0.00817     1.56e-06      0.00817       0.0524       0.0699       0.0415       0.0742       0.0578       0.0537       0.0944        0.074       0.0725     0.000756\n",
            "Wall time: 208.40295737299994\n",
            "! Best model       58    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     59     1      0.00664      0.00664     3.39e-06       0.0472        0.063       0.0378       0.0661       0.0519       0.0481       0.0854       0.0668        0.125       0.0013\n",
            "     59     2      0.00713      0.00713     9.08e-07       0.0491       0.0653        0.038       0.0713       0.0546       0.0481       0.0904       0.0692        0.058     0.000604\n",
            "     59     3      0.00893      0.00892     3.12e-06        0.056       0.0731       0.0459        0.076        0.061       0.0587       0.0956       0.0771        0.112      0.00117\n",
            "     59     4      0.00872       0.0087     1.84e-05       0.0553       0.0722       0.0467       0.0724       0.0596       0.0595       0.0924        0.076        0.312      0.00325\n",
            "     59     5      0.00832      0.00832      2.4e-06       0.0523       0.0706       0.0404       0.0759       0.0582        0.053       0.0966       0.0748       0.0992      0.00103\n",
            "     59     6      0.00755      0.00754     1.32e-05       0.0515       0.0672       0.0414       0.0716       0.0565       0.0525       0.0896       0.0711        0.266      0.00277\n",
            "     59     7      0.00741       0.0074     1.06e-05       0.0513       0.0665       0.0423       0.0693       0.0558       0.0535        0.087       0.0702        0.241      0.00251\n",
            "     59     8      0.00881       0.0088     1.24e-05       0.0542       0.0726       0.0412       0.0801       0.0607       0.0531        0.101       0.0769         0.26      0.00271\n",
            "     59     9      0.00736      0.00732     4.47e-05       0.0497       0.0662       0.0399       0.0694       0.0547       0.0515       0.0885         0.07        0.496      0.00517\n",
            "     59    10      0.00795      0.00789     5.83e-05       0.0524       0.0687       0.0426       0.0722       0.0574       0.0549       0.0902       0.0725        0.558      0.00581\n",
            "     59    11      0.00667      0.00667     3.66e-06       0.0481       0.0632       0.0385       0.0673       0.0529       0.0487        0.085       0.0669        0.135       0.0014\n",
            "     59    12      0.00867      0.00865     1.94e-05       0.0539       0.0719       0.0433       0.0751       0.0592       0.0555       0.0968       0.0761        0.316      0.00329\n",
            "     59    13      0.00779      0.00777     1.83e-05       0.0507       0.0682       0.0404       0.0714       0.0559       0.0514       0.0931       0.0723        0.301      0.00313\n",
            "     59    14      0.00767      0.00766     1.29e-06       0.0515       0.0677       0.0402        0.074       0.0571       0.0512       0.0923       0.0717        0.073     0.000761\n",
            "     59    15      0.00695      0.00695     1.72e-06       0.0484       0.0645       0.0386       0.0679       0.0533       0.0488       0.0879       0.0683       0.0795     0.000828\n",
            "     59    16      0.00771       0.0077     3.59e-06       0.0514       0.0679       0.0401       0.0738        0.057       0.0521       0.0917       0.0719        0.122      0.00127\n",
            "     59    17      0.00709      0.00708     5.31e-06         0.05       0.0651       0.0409       0.0682       0.0546       0.0513       0.0863       0.0688        0.163       0.0017\n",
            "     59    18      0.00825      0.00824     1.48e-05       0.0536       0.0702       0.0427       0.0755       0.0591       0.0541       0.0945       0.0743        0.267      0.00278\n",
            "     59    19        0.008      0.00798     1.71e-05       0.0521       0.0691       0.0417       0.0729       0.0573       0.0544       0.0917       0.0731        0.292      0.00305\n",
            "     59    20      0.00964      0.00962     1.76e-05       0.0589       0.0759       0.0501       0.0765       0.0633       0.0634        0.096       0.0797        0.309      0.00322\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     59     1      0.00842      0.00842     2.67e-06       0.0535        0.071       0.0423       0.0759       0.0591       0.0548       0.0954       0.0751        0.101      0.00105\n",
            "     59     2       0.0084       0.0084     1.26e-06       0.0528       0.0709       0.0414       0.0754       0.0584       0.0539       0.0963       0.0751       0.0602     0.000627\n",
            "     59     3      0.00784      0.00784      1.4e-06       0.0513       0.0685       0.0404        0.073       0.0567       0.0518       0.0933       0.0726       0.0686     0.000714\n",
            "     59     4      0.00819      0.00819     1.27e-06       0.0521         0.07        0.041       0.0742       0.0576       0.0528       0.0956       0.0742       0.0735     0.000766\n",
            "     59     5      0.00764      0.00764     1.38e-06       0.0511       0.0676        0.041       0.0712       0.0561       0.0533       0.0896       0.0715       0.0604     0.000629\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              59  211.874    0.005      0.00785     1.35e-05      0.00786       0.0519       0.0685       0.0417       0.0724        0.057       0.0533       0.0917       0.0725        0.229      0.00239\n",
            "! Validation         59  211.874    0.005       0.0081      1.6e-06       0.0081       0.0521       0.0696       0.0412       0.0739       0.0576       0.0533       0.0941       0.0737       0.0727     0.000757\n",
            "Wall time: 211.8753066899999\n",
            "! Best model       59    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     60     1       0.0109       0.0109     1.41e-05       0.0632       0.0808       0.0533        0.083       0.0682       0.0678        0.102       0.0848        0.271      0.00282\n",
            "     60     2       0.0122       0.0122     3.24e-05       0.0658       0.0854       0.0539       0.0894       0.0717        0.068        0.112       0.0902        0.421      0.00439\n",
            "     60     3      0.00982      0.00981     6.97e-06       0.0586       0.0766       0.0477       0.0806       0.0641       0.0602        0.102        0.081        0.187      0.00194\n",
            "     60     4      0.00815      0.00814     6.52e-06       0.0522       0.0698       0.0416       0.0736       0.0576       0.0541       0.0937       0.0739        0.184      0.00192\n",
            "     60     5      0.00722      0.00717     5.27e-05       0.0493       0.0655       0.0408       0.0663       0.0535       0.0521       0.0862       0.0692        0.537      0.00559\n",
            "     60     6      0.00859      0.00858     9.15e-06       0.0557       0.0717       0.0479       0.0714       0.0596       0.0607       0.0896       0.0752        0.206      0.00215\n",
            "     60     7      0.00801      0.00797     3.85e-05       0.0516       0.0691       0.0409       0.0731        0.057       0.0533       0.0929       0.0731        0.459      0.00479\n",
            "     60     8      0.00772      0.00766     5.77e-05       0.0514       0.0677       0.0415       0.0712       0.0563       0.0519       0.0915       0.0717        0.558      0.00581\n",
            "     60     9        0.009      0.00899     1.46e-05       0.0568       0.0733       0.0489       0.0727       0.0608       0.0617       0.0923        0.077        0.273      0.00284\n",
            "     60    10      0.00804      0.00799     4.46e-05       0.0533       0.0692       0.0434       0.0732       0.0583       0.0542       0.0921       0.0731        0.495      0.00515\n",
            "     60    11      0.00859      0.00857     2.23e-05       0.0546       0.0716        0.042       0.0797       0.0609       0.0528       0.0991       0.0759        0.343      0.00357\n",
            "     60    12      0.00701      0.00701     3.66e-06       0.0493       0.0648       0.0387       0.0705       0.0546        0.049       0.0883       0.0686        0.114      0.00119\n",
            "     60    13      0.00764      0.00764      4.5e-06       0.0511       0.0676       0.0402       0.0728       0.0565       0.0511       0.0921       0.0716        0.133      0.00138\n",
            "     60    14       0.0082      0.00816     4.58e-05       0.0536       0.0699       0.0434       0.0739       0.0587       0.0539        0.094       0.0739        0.497      0.00518\n",
            "     60    15       0.0112       0.0111     1.03e-05       0.0629       0.0817       0.0541       0.0806       0.0674       0.0683        0.103       0.0858        0.212      0.00221\n",
            "     60    16       0.0122       0.0122     3.31e-06       0.0653       0.0856       0.0536       0.0887       0.0712       0.0693        0.111       0.0902        0.122      0.00127\n",
            "     60    17      0.00999      0.00999     5.37e-06       0.0591       0.0773       0.0488       0.0796       0.0642       0.0613        0.102       0.0817        0.166      0.00173\n",
            "     60    18      0.00859      0.00859     1.56e-06       0.0538       0.0717       0.0445       0.0725       0.0585       0.0576       0.0937       0.0757        0.082     0.000854\n",
            "     60    19      0.00865      0.00864     2.06e-06       0.0537       0.0719       0.0418       0.0776       0.0597       0.0549       0.0975       0.0762       0.0982      0.00102\n",
            "     60    20        0.011       0.0109     5.18e-05       0.0626       0.0808        0.055       0.0776       0.0663       0.0705       0.0984       0.0844        0.529      0.00551\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     60     1      0.00836      0.00835     2.67e-06       0.0533       0.0707       0.0421       0.0757       0.0589       0.0545       0.0952       0.0748        0.101      0.00105\n",
            "     60     2      0.00834      0.00833     1.07e-06       0.0526       0.0706       0.0412       0.0752       0.0582       0.0536        0.096       0.0748       0.0557      0.00058\n",
            "     60     3      0.00779      0.00778     1.35e-06       0.0511       0.0683       0.0402       0.0728       0.0565       0.0516       0.0931       0.0723       0.0699     0.000728\n",
            "     60     4      0.00814      0.00813     1.27e-06       0.0519       0.0698       0.0408       0.0741       0.0574       0.0524       0.0954       0.0739       0.0751     0.000782\n",
            "     60     5      0.00758      0.00758     1.29e-06       0.0508       0.0674       0.0408        0.071       0.0559        0.053       0.0894       0.0712       0.0585     0.000609\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              60  215.396    0.005      0.00911     2.14e-05      0.00914       0.0562       0.0739       0.0461       0.0764       0.0612        0.059        0.097        0.078        0.294      0.00307\n",
            "! Validation         60  215.396    0.005      0.00804     1.53e-06      0.00804       0.0519       0.0694        0.041       0.0738       0.0574        0.053       0.0938       0.0734       0.0721     0.000751\n",
            "Wall time: 215.3970267709999\n",
            "! Best model       60    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     61     1       0.0133       0.0133     2.79e-06       0.0701       0.0891       0.0625       0.0853       0.0739       0.0775        0.109       0.0931        0.121      0.00126\n",
            "     61     2       0.0093      0.00929     5.22e-06       0.0568       0.0746       0.0465       0.0775        0.062       0.0596       0.0979       0.0787        0.151      0.00157\n",
            "     61     3      0.00683      0.00679     3.69e-05        0.048       0.0638       0.0375       0.0692       0.0533       0.0478       0.0874       0.0676        0.448      0.00467\n",
            "     61     4      0.00761      0.00761     1.37e-06       0.0514       0.0675       0.0407       0.0728       0.0567       0.0511       0.0919       0.0715       0.0609     0.000635\n",
            "     61     5      0.00796      0.00789     7.27e-05       0.0528       0.0687       0.0446       0.0694        0.057       0.0553       0.0897       0.0725         0.63      0.00656\n",
            "     61     6      0.00675      0.00675     2.54e-07       0.0486       0.0636       0.0402       0.0653       0.0528       0.0508       0.0834       0.0671       0.0346      0.00036\n",
            "     61     7      0.00726      0.00724     2.72e-05       0.0497       0.0658       0.0393       0.0705       0.0549       0.0506       0.0887       0.0697        0.379      0.00394\n",
            "     61     8       0.0077       0.0077      2.7e-06       0.0522       0.0679       0.0426       0.0713       0.0569       0.0541       0.0893       0.0717        0.109      0.00113\n",
            "     61     9      0.00906      0.00904      1.7e-05       0.0567       0.0736       0.0479       0.0742       0.0611       0.0602       0.0948       0.0775        0.283      0.00295\n",
            "     61    10      0.00775      0.00774     1.26e-05       0.0516       0.0681       0.0406       0.0735        0.057       0.0514       0.0928       0.0721        0.259       0.0027\n",
            "     61    11      0.00749      0.00747     1.17e-05       0.0502       0.0669       0.0409       0.0686       0.0548        0.053       0.0883       0.0707        0.248      0.00258\n",
            "     61    12      0.00978      0.00974     4.44e-05       0.0578       0.0764       0.0465       0.0802       0.0634       0.0599        0.102       0.0807        0.491      0.00511\n",
            "     61    13      0.00976      0.00975     1.29e-05       0.0595       0.0764       0.0507       0.0769       0.0638       0.0631       0.0976       0.0804        0.262      0.00273\n",
            "     61    14      0.00946      0.00944     1.96e-05       0.0569       0.0752        0.047       0.0767       0.0619       0.0599       0.0989       0.0794        0.324      0.00338\n",
            "     61    15      0.00794      0.00794     3.39e-06        0.052       0.0689       0.0398       0.0763       0.0581       0.0514       0.0947        0.073        0.104      0.00108\n",
            "     61    16      0.00886      0.00885     7.09e-06       0.0562       0.0728       0.0481       0.0724       0.0602        0.061        0.092       0.0765         0.19      0.00198\n",
            "     61    17       0.0104       0.0104     3.12e-05       0.0612       0.0788        0.053       0.0775       0.0652       0.0667       0.0987       0.0827        0.405      0.00422\n",
            "     61    18      0.00889      0.00888      9.4e-06       0.0552       0.0729        0.044       0.0777       0.0608       0.0559       0.0985       0.0772        0.217      0.00226\n",
            "     61    19      0.00858      0.00857     5.56e-06       0.0541       0.0716       0.0415       0.0793       0.0604        0.053       0.0988       0.0759        0.151      0.00157\n",
            "     61    20       0.0072      0.00719     1.54e-05       0.0495       0.0656       0.0403       0.0681       0.0542       0.0517       0.0869       0.0693        0.288        0.003\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     61     1      0.00829      0.00829     2.73e-06       0.0531       0.0704       0.0419       0.0755       0.0587       0.0542       0.0949       0.0746        0.102      0.00106\n",
            "     61     2      0.00827      0.00827     1.07e-06       0.0523       0.0704        0.041        0.075        0.058       0.0533       0.0957       0.0745       0.0555     0.000578\n",
            "     61     3      0.00773      0.00773     1.41e-06       0.0509        0.068         0.04       0.0726       0.0563       0.0513       0.0928       0.0721       0.0702     0.000731\n",
            "     61     4      0.00807      0.00807     1.34e-06       0.0516       0.0695       0.0405       0.0739       0.0572       0.0521       0.0952       0.0737       0.0767     0.000799\n",
            "     61     5      0.00752      0.00752     1.29e-06       0.0506       0.0671       0.0405       0.0708       0.0557       0.0528       0.0891       0.0709       0.0574     0.000598\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              61  218.903    0.005      0.00858      1.7e-05      0.00859       0.0545       0.0716       0.0447       0.0741       0.0594       0.0571       0.0942       0.0757        0.258      0.00269\n",
            "! Validation         61  218.903    0.005      0.00798     1.57e-06      0.00798       0.0517       0.0691       0.0408       0.0735       0.0572       0.0528       0.0936       0.0732       0.0723     0.000754\n",
            "Wall time: 218.903906108\n",
            "! Best model       61    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     62     1      0.00707      0.00707      1.8e-06       0.0499       0.0651       0.0393       0.0711       0.0552       0.0493       0.0885       0.0689       0.0773     0.000806\n",
            "     62     2      0.00768      0.00766     2.32e-05        0.051       0.0677        0.041       0.0709        0.056        0.053       0.0902       0.0716        0.354      0.00369\n",
            "     62     3      0.00726      0.00724     2.03e-05       0.0495       0.0658        0.039       0.0705       0.0548       0.0507       0.0886       0.0697        0.324      0.00338\n",
            "     62     4       0.0066       0.0066     3.13e-06       0.0468       0.0629       0.0353       0.0699       0.0526       0.0451       0.0883       0.0667        0.105      0.00109\n",
            "     62     5      0.00843      0.00842     1.17e-05       0.0536        0.071        0.043       0.0749       0.0589       0.0549       0.0953       0.0751        0.251      0.00261\n",
            "     62     6      0.00723      0.00722     3.05e-06       0.0496       0.0658       0.0388        0.071       0.0549       0.0492       0.0902       0.0697        0.107      0.00112\n",
            "     62     7      0.00705      0.00704     4.48e-06       0.0502       0.0649       0.0408       0.0691       0.0549       0.0508       0.0865       0.0687        0.147      0.00153\n",
            "     62     8      0.00726      0.00726     8.17e-07       0.0497       0.0659         0.04       0.0692       0.0546       0.0512       0.0883       0.0698       0.0508     0.000529\n",
            "     62     9      0.00806      0.00803     2.76e-05       0.0532       0.0693       0.0447       0.0702       0.0574       0.0558       0.0905       0.0731        0.385      0.00401\n",
            "     62    10      0.00982      0.00982     1.78e-06       0.0587       0.0767       0.0485       0.0792       0.0638       0.0621       0.0996       0.0808       0.0859     0.000895\n",
            "     62    11      0.00705        0.007     5.02e-05       0.0487       0.0647       0.0371       0.0718       0.0545       0.0465       0.0908       0.0687        0.524      0.00545\n",
            "     62    12      0.00825      0.00822     3.24e-05       0.0522       0.0701       0.0411       0.0746       0.0578       0.0534       0.0952       0.0743        0.417      0.00435\n",
            "     62    13      0.00812      0.00809     3.96e-05       0.0523       0.0696       0.0432       0.0703       0.0568        0.056       0.0908       0.0734        0.464      0.00483\n",
            "     62    14      0.00782      0.00774     8.29e-05       0.0502       0.0681       0.0391       0.0726       0.0558       0.0499       0.0944       0.0722        0.674      0.00702\n",
            "     62    15      0.00784      0.00784     2.98e-06       0.0511       0.0685       0.0401       0.0732       0.0566       0.0527       0.0924       0.0725        0.107      0.00111\n",
            "     62    16      0.00719      0.00717     2.03e-05       0.0493       0.0655       0.0385        0.071       0.0547       0.0491       0.0897       0.0694        0.331      0.00344\n",
            "     62    17      0.00795      0.00795     1.97e-06        0.052        0.069       0.0425       0.0712       0.0568       0.0546       0.0912       0.0729        0.093     0.000968\n",
            "     62    18      0.00892      0.00892     4.82e-06       0.0556       0.0731        0.047        0.073         0.06       0.0596       0.0944        0.077        0.144       0.0015\n",
            "     62    19      0.00685      0.00684      7.2e-06       0.0487        0.064       0.0391       0.0679       0.0535       0.0491       0.0864       0.0677        0.185      0.00193\n",
            "     62    20      0.00736      0.00736     6.51e-07       0.0512       0.0664       0.0421       0.0694       0.0558       0.0532       0.0869         0.07       0.0475     0.000494\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     62     1      0.00823      0.00822     2.67e-06       0.0529       0.0702       0.0417       0.0752       0.0584       0.0539       0.0946       0.0743        0.101      0.00105\n",
            "     62     2      0.00822      0.00822     1.18e-06       0.0521       0.0701       0.0408       0.0748       0.0578       0.0531       0.0955       0.0743       0.0599     0.000624\n",
            "     62     3      0.00769      0.00769      1.4e-06       0.0507       0.0678       0.0398       0.0725       0.0562       0.0511       0.0926       0.0719       0.0686     0.000714\n",
            "     62     4      0.00801      0.00801     1.31e-06       0.0514       0.0693       0.0403       0.0737        0.057       0.0519       0.0949       0.0734       0.0755     0.000786\n",
            "     62     5      0.00747      0.00747     1.29e-06       0.0504       0.0669       0.0404       0.0706       0.0555       0.0525       0.0889       0.0707       0.0583     0.000607\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              62  222.408    0.005      0.00767      1.7e-05      0.00769       0.0512       0.0678        0.041       0.0715       0.0563       0.0525        0.091       0.0717        0.244      0.00254\n",
            "! Validation         62  222.408    0.005      0.00792     1.57e-06      0.00792       0.0515       0.0689       0.0406       0.0733        0.057       0.0525       0.0933       0.0729       0.0726     0.000756\n",
            "Wall time: 222.40926216699995\n",
            "! Best model       62    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     63     1      0.00846      0.00845     1.14e-05       0.0532       0.0711       0.0427       0.0742       0.0584       0.0558       0.0945       0.0752        0.244      0.00254\n",
            "     63     2       0.0076      0.00757     2.88e-05       0.0513       0.0673       0.0399       0.0741        0.057       0.0498       0.0929       0.0714        0.396      0.00413\n",
            "     63     3      0.00935      0.00934     1.09e-05       0.0558       0.0748       0.0427        0.082       0.0623       0.0553        0.103       0.0793        0.237      0.00247\n",
            "     63     4      0.00825      0.00824     3.54e-06       0.0542       0.0702       0.0464       0.0698       0.0581       0.0592       0.0883       0.0737        0.118      0.00122\n",
            "     63     5      0.00979      0.00978     1.01e-05       0.0592       0.0765       0.0509       0.0756       0.0633       0.0644       0.0962       0.0803        0.218      0.00227\n",
            "     63     6      0.00781       0.0078      5.6e-06       0.0526       0.0683       0.0429       0.0718       0.0574       0.0545       0.0898       0.0722        0.164      0.00171\n",
            "     63     7      0.00766      0.00765     3.89e-06       0.0507       0.0677       0.0401       0.0717       0.0559       0.0525       0.0908       0.0716        0.114      0.00119\n",
            "     63     8       0.0098       0.0098     3.23e-06        0.059       0.0766       0.0498       0.0773       0.0636       0.0627       0.0987       0.0807        0.124      0.00129\n",
            "     63     9       0.0124       0.0124     1.57e-05       0.0681       0.0862       0.0614       0.0814       0.0714       0.0755        0.104       0.0899         0.28      0.00291\n",
            "     63    10      0.00843      0.00842      8.7e-06       0.0542        0.071       0.0445       0.0737       0.0591       0.0558       0.0943        0.075        0.217      0.00226\n",
            "     63    11      0.00706      0.00706     3.88e-06       0.0487        0.065       0.0384       0.0692       0.0538       0.0494       0.0883       0.0688         0.13      0.00136\n",
            "     63    12      0.00928      0.00928     4.73e-06       0.0572       0.0745       0.0464       0.0788       0.0626       0.0589       0.0986       0.0787        0.149      0.00155\n",
            "     63    13       0.0108       0.0108     2.34e-05       0.0635       0.0802       0.0551       0.0802       0.0676       0.0682          0.1       0.0841        0.356      0.00371\n",
            "     63    14       0.0106       0.0106     1.11e-06       0.0621       0.0797       0.0544       0.0776        0.066       0.0676       0.0995       0.0836       0.0658     0.000686\n",
            "     63    15      0.00765      0.00765     4.93e-06       0.0515       0.0677       0.0427       0.0693        0.056       0.0543       0.0885       0.0714        0.156      0.00162\n",
            "     63    16      0.00753      0.00752     1.33e-05       0.0506       0.0671       0.0397       0.0725       0.0561       0.0507       0.0914       0.0711        0.264      0.00275\n",
            "     63    17       0.0103       0.0102     6.41e-05       0.0604       0.0781       0.0521       0.0771       0.0646       0.0656       0.0984        0.082        0.592      0.00616\n",
            "     63    18       0.0127       0.0127     7.97e-06        0.068       0.0871       0.0575        0.089       0.0733       0.0722        0.111       0.0916        0.198      0.00206\n",
            "     63    19       0.0106       0.0106     1.88e-05       0.0605       0.0796       0.0517       0.0782        0.065       0.0655        0.102       0.0838         0.32      0.00333\n",
            "     63    20      0.00684      0.00682     1.83e-05       0.0484       0.0639        0.039        0.067        0.053       0.0486       0.0867       0.0677        0.312      0.00325\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     63     1      0.00817      0.00817     2.63e-06       0.0526       0.0699       0.0415        0.075       0.0582       0.0537       0.0944        0.074       0.0998      0.00104\n",
            "     63     2      0.00816      0.00816     1.23e-06       0.0519       0.0699       0.0406       0.0746       0.0576       0.0528       0.0953        0.074        0.059     0.000614\n",
            "     63     3      0.00764      0.00764     1.35e-06       0.0505       0.0676       0.0396       0.0723        0.056       0.0508       0.0925       0.0717       0.0669     0.000697\n",
            "     63     4      0.00796      0.00796     1.27e-06       0.0513        0.069       0.0401       0.0735       0.0568       0.0516       0.0947       0.0731       0.0736     0.000767\n",
            "     63     5      0.00741      0.00741      1.3e-06       0.0502       0.0666       0.0402       0.0704       0.0553       0.0522       0.0886       0.0704       0.0581     0.000605\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              63  225.909    0.005      0.00913     1.31e-05      0.00914       0.0565       0.0739       0.0469       0.0755       0.0612       0.0598       0.0961        0.078        0.233      0.00242\n",
            "! Validation         63  225.909    0.005      0.00787     1.56e-06      0.00787       0.0513       0.0686       0.0404       0.0732       0.0568       0.0522       0.0931       0.0727       0.0715     0.000745\n",
            "Wall time: 225.91056225500006\n",
            "! Best model       63    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     64     1      0.00853      0.00852     1.19e-06       0.0536       0.0714       0.0432       0.0743       0.0587       0.0562       0.0949       0.0755       0.0662      0.00069\n",
            "     64     2      0.00956       0.0095     6.58e-05       0.0591       0.0754       0.0502        0.077       0.0636       0.0618        0.097       0.0794        0.602      0.00627\n",
            "     64     3      0.00859      0.00859     2.06e-06       0.0542       0.0717       0.0444       0.0738       0.0591        0.057       0.0944       0.0757        0.105       0.0011\n",
            "     64     4      0.00728      0.00728     3.33e-06       0.0499        0.066       0.0395       0.0708       0.0551       0.0496       0.0903       0.0699        0.123      0.00128\n",
            "     64     5      0.00803      0.00802     8.38e-06       0.0519       0.0693       0.0421       0.0715       0.0568       0.0539       0.0927       0.0733        0.206      0.00214\n",
            "     64     6      0.00771      0.00771      7.6e-06       0.0504       0.0679       0.0395       0.0721       0.0558       0.0507       0.0932        0.072        0.192        0.002\n",
            "     64     7      0.00754      0.00754     1.04e-06       0.0503       0.0672       0.0395        0.072       0.0557       0.0518       0.0904       0.0711       0.0586      0.00061\n",
            "     64     8      0.00771       0.0077     1.17e-05        0.051       0.0679       0.0416       0.0699       0.0557       0.0538       0.0897       0.0717        0.216      0.00225\n",
            "     64     9      0.00713      0.00713     1.99e-06       0.0488       0.0653       0.0395       0.0673       0.0534        0.051       0.0872       0.0691       0.0879     0.000916\n",
            "     64    10      0.00809      0.00808     2.34e-06       0.0524       0.0696       0.0414       0.0745        0.058       0.0534       0.0938       0.0736        0.109      0.00114\n",
            "     64    11      0.00657      0.00656     2.81e-06       0.0474       0.0627       0.0382        0.066       0.0521       0.0482       0.0845       0.0663        0.116       0.0012\n",
            "     64    12      0.00808      0.00807     4.71e-06       0.0518       0.0695       0.0393       0.0768       0.0581       0.0498       0.0977       0.0737         0.15      0.00156\n",
            "     64    13      0.00837      0.00837     1.11e-06       0.0526       0.0708       0.0419       0.0738       0.0579       0.0532       0.0968        0.075       0.0758     0.000789\n",
            "     64    14      0.00766      0.00761     5.08e-05       0.0519       0.0675       0.0437       0.0684       0.0561       0.0551       0.0871       0.0711        0.526      0.00548\n",
            "     64    15      0.00728      0.00728      1.2e-06       0.0496        0.066       0.0409        0.067        0.054       0.0524       0.0871       0.0697       0.0742     0.000773\n",
            "     64    16      0.00788      0.00784     3.66e-05       0.0523       0.0685       0.0411       0.0745       0.0578       0.0529       0.0922       0.0725        0.447      0.00466\n",
            "     64    17      0.00791      0.00789     2.61e-05       0.0518       0.0687        0.041       0.0735       0.0572       0.0533       0.0921       0.0727        0.374      0.00389\n",
            "     64    18      0.00773      0.00773     3.39e-06       0.0497        0.068       0.0376        0.074       0.0558       0.0502        0.094       0.0721        0.122      0.00128\n",
            "     64    19      0.00843       0.0084     3.36e-05       0.0532       0.0709       0.0396       0.0805       0.0601       0.0507       0.0996       0.0752        0.414      0.00431\n",
            "     64    20      0.00715      0.00715     8.23e-06       0.0495       0.0654       0.0398       0.0688       0.0543       0.0509       0.0875       0.0692        0.208      0.00216\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     64     1      0.00811       0.0081     2.62e-06       0.0524       0.0696       0.0412       0.0748        0.058       0.0534       0.0941       0.0737       0.0996      0.00104\n",
            "     64     2      0.00811      0.00811     1.26e-06       0.0518       0.0697       0.0404       0.0744       0.0574       0.0526        0.095       0.0738       0.0604      0.00063\n",
            "     64     3       0.0076       0.0076     1.43e-06       0.0504       0.0674       0.0395       0.0722       0.0558       0.0506       0.0923       0.0715       0.0707     0.000736\n",
            "     64     4       0.0079       0.0079     1.31e-06        0.051       0.0688       0.0399       0.0733       0.0566       0.0513       0.0944       0.0729        0.075     0.000781\n",
            "     64     5      0.00737      0.00737     1.24e-06       0.0501       0.0664         0.04       0.0702       0.0551        0.052       0.0885       0.0702       0.0566      0.00059\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              64  229.418    0.005      0.00785     1.37e-05      0.00786       0.0516       0.0685       0.0412       0.0723       0.0568       0.0529       0.0922       0.0725        0.214      0.00223\n",
            "! Validation         64  229.418    0.005      0.00782     1.57e-06      0.00782       0.0511       0.0684       0.0402        0.073       0.0566        0.052       0.0929       0.0724       0.0725     0.000755\n",
            "Wall time: 229.41907175300003\n",
            "! Best model       64    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     65     1      0.00775      0.00775      3.7e-07       0.0508       0.0681       0.0396       0.0732       0.0564       0.0515       0.0928       0.0721       0.0291     0.000303\n",
            "     65     2      0.00784      0.00783     5.05e-06       0.0512       0.0685        0.041       0.0716       0.0563       0.0523       0.0927       0.0725        0.162      0.00169\n",
            "     65     3      0.00855      0.00854     1.56e-05       0.0551       0.0715        0.047       0.0712       0.0591       0.0594        0.091       0.0752        0.274      0.00285\n",
            "     65     4      0.00766      0.00766     1.89e-06       0.0511       0.0677       0.0413       0.0706        0.056       0.0518       0.0915       0.0717       0.0885     0.000922\n",
            "     65     5       0.0105       0.0105     7.12e-06       0.0626       0.0793       0.0578       0.0721        0.065       0.0725       0.0913       0.0819        0.195      0.00203\n",
            "     65     6      0.00963      0.00963     6.53e-07       0.0598       0.0759       0.0561       0.0673       0.0617       0.0696       0.0873       0.0784       0.0416     0.000433\n",
            "     65     7      0.00793      0.00793     3.08e-07       0.0518       0.0689       0.0397       0.0759       0.0578         0.05       0.0962       0.0731       0.0396     0.000413\n",
            "     65     8      0.00762      0.00761     1.94e-05       0.0523       0.0675       0.0445       0.0679       0.0562       0.0555       0.0866        0.071        0.325      0.00339\n",
            "     65     9      0.00998      0.00998     5.44e-07       0.0586       0.0773        0.047       0.0817       0.0644       0.0602        0.103       0.0817       0.0467     0.000486\n",
            "     65    10      0.00963      0.00951     0.000123       0.0562       0.0754        0.044       0.0805       0.0623       0.0565        0.103       0.0799        0.821      0.00855\n",
            "     65    11      0.00829      0.00829     3.36e-06        0.055       0.0704       0.0462       0.0726       0.0594       0.0581       0.0902       0.0741        0.113      0.00117\n",
            "     65    12      0.00812      0.00799      0.00013       0.0527       0.0692       0.0427       0.0727       0.0577       0.0542       0.0921       0.0731        0.844       0.0088\n",
            "     65    13      0.00832      0.00831     1.21e-05       0.0546       0.0705       0.0476       0.0685       0.0581       0.0594       0.0887        0.074        0.249      0.00259\n",
            "     65    14      0.00814      0.00809      5.4e-05       0.0532       0.0696       0.0428        0.074       0.0584       0.0552       0.0917       0.0735        0.541      0.00563\n",
            "     65    15      0.00812      0.00807     5.58e-05       0.0529       0.0695       0.0438       0.0709       0.0574       0.0565         0.09       0.0732        0.544      0.00567\n",
            "     65    16      0.00849      0.00847     1.24e-05       0.0559       0.0712       0.0489       0.0699       0.0594       0.0611       0.0881       0.0746        0.247      0.00257\n",
            "     65    17      0.00799       0.0079     8.85e-05       0.0525       0.0688       0.0426       0.0723       0.0574       0.0535        0.092       0.0728        0.696      0.00725\n",
            "     65    18      0.00743      0.00742     5.25e-06       0.0502       0.0667       0.0403       0.0698       0.0551       0.0519       0.0891       0.0705        0.141      0.00147\n",
            "     65    19      0.00819      0.00816     2.74e-05       0.0524       0.0699       0.0405       0.0763       0.0584       0.0514       0.0967       0.0741        0.369      0.00385\n",
            "     65    20      0.00826      0.00824     1.32e-05       0.0534       0.0702       0.0426       0.0748       0.0587       0.0535       0.0953       0.0744        0.254      0.00264\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     65     1      0.00807      0.00806      2.7e-06       0.0522       0.0695       0.0411       0.0746       0.0578       0.0531        0.094       0.0736        0.101      0.00106\n",
            "     65     2      0.00806      0.00806     1.07e-06       0.0516       0.0694       0.0403       0.0743       0.0573       0.0523       0.0948       0.0736       0.0547      0.00057\n",
            "     65     3      0.00756      0.00756     1.39e-06       0.0502       0.0672       0.0393        0.072       0.0557       0.0504       0.0921       0.0713       0.0701      0.00073\n",
            "     65     4      0.00786      0.00785     1.38e-06       0.0509       0.0686       0.0397       0.0732       0.0565       0.0511       0.0943       0.0727       0.0777      0.00081\n",
            "     65     5      0.00731      0.00731     1.21e-06       0.0499       0.0662       0.0398         0.07       0.0549       0.0517       0.0882         0.07       0.0564     0.000588\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              65  232.943    0.005      0.00839     2.88e-05      0.00842       0.0541       0.0709       0.0448       0.0727       0.0587        0.057       0.0926       0.0748        0.301      0.00313\n",
            "! Validation         65  232.943    0.005      0.00777     1.55e-06      0.00777        0.051       0.0682         0.04       0.0728       0.0564       0.0517       0.0927       0.0722       0.0721     0.000751\n",
            "Wall time: 232.94400299900008\n",
            "! Best model       65    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     66     1      0.00826      0.00826     4.08e-06       0.0552       0.0703       0.0472       0.0712       0.0592       0.0595        0.088       0.0738        0.145      0.00151\n",
            "     66     2      0.00693      0.00692     1.21e-06       0.0485       0.0644       0.0388       0.0679       0.0534       0.0488       0.0876       0.0682       0.0717     0.000747\n",
            "     66     3      0.00834      0.00831     3.05e-05       0.0545       0.0705       0.0447       0.0741       0.0594       0.0564       0.0925       0.0744        0.408      0.00425\n",
            "     66     4      0.00674      0.00673     1.67e-06       0.0479       0.0635        0.038       0.0678       0.0529       0.0493       0.0851       0.0672       0.0824     0.000859\n",
            "     66     5      0.00758      0.00755     2.92e-05       0.0509       0.0672         0.04       0.0727       0.0564        0.051       0.0913       0.0712        0.391      0.00407\n",
            "     66     6      0.00767      0.00766     3.51e-06       0.0519       0.0677       0.0405       0.0745       0.0575       0.0519       0.0915       0.0717        0.107      0.00111\n",
            "     66     7      0.00796      0.00794     1.41e-05       0.0516       0.0689       0.0403       0.0744       0.0573       0.0513       0.0948       0.0731        0.276      0.00287\n",
            "     66     8      0.00762      0.00761     1.28e-05       0.0517       0.0675       0.0432       0.0688        0.056       0.0556       0.0865       0.0711        0.253      0.00264\n",
            "     66     9      0.00801        0.008     1.19e-05       0.0533       0.0692       0.0454        0.069       0.0572       0.0576        0.088       0.0728        0.254      0.00265\n",
            "     66    10       0.0068       0.0068     4.68e-06       0.0484       0.0638        0.038        0.069       0.0535       0.0481       0.0871       0.0676        0.129      0.00134\n",
            "     66    11      0.00823      0.00815     7.77e-05        0.053       0.0699       0.0414       0.0761       0.0588       0.0522       0.0959        0.074        0.645      0.00672\n",
            "     66    12      0.00744      0.00744     1.35e-06       0.0509       0.0667       0.0409        0.071        0.056       0.0524       0.0887       0.0705       0.0768       0.0008\n",
            "     66    13      0.00889      0.00883     6.05e-05       0.0564       0.0727       0.0471       0.0752       0.0611       0.0598       0.0933       0.0766        0.573      0.00597\n",
            "     66    14      0.00687      0.00685     1.39e-05       0.0482       0.0641       0.0381       0.0685       0.0533       0.0488       0.0869       0.0678        0.276      0.00287\n",
            "     66    15      0.00921      0.00918      3.1e-05       0.0558       0.0741       0.0458       0.0758       0.0608        0.059       0.0977       0.0783        0.413       0.0043\n",
            "     66    16      0.00685      0.00685     1.99e-06       0.0482        0.064       0.0381       0.0684       0.0532       0.0485       0.0871       0.0678       0.0947     0.000987\n",
            "     66    17      0.00781      0.00779     1.29e-05       0.0516       0.0683        0.041        0.073        0.057       0.0526        0.092       0.0723        0.262      0.00273\n",
            "     66    18      0.00723      0.00722     6.02e-06       0.0475       0.0657       0.0366       0.0691       0.0529       0.0473       0.0921       0.0697        0.175      0.00182\n",
            "     66    19        0.008      0.00796     3.63e-05        0.051        0.069       0.0401       0.0729       0.0565       0.0509       0.0955       0.0732        0.444      0.00463\n",
            "     66    20      0.00774      0.00773     1.27e-05       0.0514        0.068       0.0426       0.0689       0.0557       0.0545       0.0891       0.0718        0.257      0.00268\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     66     1      0.00801      0.00801     2.72e-06        0.052       0.0692       0.0409       0.0744       0.0576       0.0529       0.0937       0.0733        0.101      0.00106\n",
            "     66     2      0.00801      0.00801     1.15e-06       0.0514       0.0692       0.0401       0.0741       0.0571       0.0521       0.0946       0.0733       0.0567     0.000591\n",
            "     66     3      0.00752      0.00752     1.35e-06       0.0501       0.0671       0.0392       0.0719       0.0555       0.0502        0.092       0.0711       0.0678     0.000706\n",
            "     66     4       0.0078       0.0078     1.39e-06       0.0507       0.0683       0.0396        0.073       0.0563       0.0508        0.094       0.0724       0.0779     0.000812\n",
            "     66     5      0.00726      0.00726     1.21e-06       0.0497       0.0659       0.0396       0.0698       0.0547       0.0514        0.088       0.0697       0.0553     0.000576\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              66  236.460    0.005      0.00769     1.84e-05      0.00771       0.0514       0.0678       0.0414       0.0714       0.0564       0.0529       0.0906       0.0718        0.267      0.00278\n",
            "! Validation         66  236.460    0.005      0.00772     1.56e-06      0.00772       0.0508        0.068       0.0398       0.0726       0.0562       0.0515       0.0925        0.072       0.0718     0.000748\n",
            "Wall time: 236.4614143429999\n",
            "! Best model       66    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     67     1      0.00779      0.00773      5.8e-05       0.0519        0.068       0.0427       0.0703       0.0565       0.0544       0.0892       0.0718        0.559      0.00582\n",
            "     67     2      0.00729      0.00728     1.64e-06       0.0487        0.066       0.0375       0.0711       0.0543       0.0485       0.0916         0.07       0.0807      0.00084\n",
            "     67     3      0.00713      0.00712     1.33e-05       0.0501       0.0653       0.0412        0.068       0.0546        0.051        0.087        0.069        0.263      0.00274\n",
            "     67     4      0.00781      0.00779      2.2e-05       0.0513       0.0683       0.0395        0.075       0.0573       0.0507       0.0941       0.0724        0.343      0.00357\n",
            "     67     5       0.0093      0.00925     5.07e-05       0.0564       0.0744       0.0451        0.079       0.0621        0.057        0.101       0.0788        0.527      0.00549\n",
            "     67     6      0.00734      0.00734     7.82e-06       0.0496       0.0663       0.0383       0.0722       0.0553       0.0496       0.0908       0.0702        0.204      0.00212\n",
            "     67     7       0.0092      0.00914     6.31e-05       0.0551        0.074       0.0429       0.0796       0.0612       0.0552        0.102       0.0784        0.579      0.00603\n",
            "     67     8       0.0114       0.0114     2.87e-06       0.0639       0.0825       0.0571       0.0773       0.0672       0.0722       0.0999       0.0861        0.116      0.00121\n",
            "     67     9       0.0133       0.0132     4.69e-05       0.0708        0.089       0.0654       0.0814       0.0734       0.0808        0.103       0.0921          0.5      0.00521\n",
            "     67    10        0.011       0.0109     6.43e-05        0.063       0.0809       0.0528       0.0833       0.0681       0.0658        0.105       0.0853        0.594      0.00619\n",
            "     67    11      0.00619      0.00611     8.46e-05       0.0456       0.0604       0.0354        0.066       0.0507       0.0443       0.0839       0.0641        0.681      0.00709\n",
            "     67    12      0.00908      0.00898     9.32e-05       0.0557       0.0733       0.0441        0.079       0.0615       0.0559       0.0994       0.0777        0.715      0.00745\n",
            "     67    13       0.0117       0.0117     9.68e-05       0.0646       0.0835       0.0531       0.0874       0.0703       0.0666         0.11       0.0882        0.728      0.00758\n",
            "     67    14      0.00853      0.00853     6.56e-06       0.0563       0.0714       0.0517       0.0655       0.0586        0.065       0.0829       0.0739        0.166      0.00173\n",
            "     67    15      0.00763      0.00761     1.97e-05       0.0512       0.0675       0.0396       0.0742       0.0569       0.0507       0.0924       0.0715        0.315      0.00329\n",
            "     67    16         0.01         0.01      9.1e-07       0.0617       0.0775       0.0572       0.0706       0.0639       0.0708       0.0894       0.0801       0.0648     0.000675\n",
            "     67    17       0.0111       0.0111     2.36e-06       0.0653       0.0816       0.0596       0.0768       0.0682       0.0736       0.0955       0.0846        0.106      0.00111\n",
            "     67    18      0.00899      0.00897     2.03e-05       0.0549       0.0733       0.0423       0.0802       0.0612       0.0546        0.101       0.0776        0.332      0.00346\n",
            "     67    19      0.00783      0.00782     3.97e-06        0.052       0.0684       0.0431       0.0698       0.0565       0.0553       0.0891       0.0722        0.134      0.00139\n",
            "     67    20       0.0105       0.0105     1.78e-05       0.0595       0.0792       0.0454       0.0878       0.0666       0.0573        0.111        0.084        0.307       0.0032\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     67     1      0.00796      0.00796      2.6e-06       0.0519        0.069       0.0407       0.0742       0.0574       0.0526       0.0935       0.0731       0.0985      0.00103\n",
            "     67     2      0.00796      0.00796     1.19e-06       0.0513        0.069       0.0399        0.074       0.0569       0.0519       0.0944       0.0731       0.0574     0.000598\n",
            "     67     3      0.00747      0.00747     1.37e-06       0.0499       0.0669        0.039       0.0717       0.0553         0.05       0.0918       0.0709       0.0686     0.000714\n",
            "     67     4      0.00775      0.00775     1.41e-06       0.0505       0.0681       0.0393       0.0728       0.0561       0.0505       0.0939       0.0722        0.078     0.000813\n",
            "     67     5      0.00722      0.00721     1.22e-06       0.0495       0.0657       0.0394       0.0697       0.0545       0.0512       0.0878       0.0695       0.0565     0.000589\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              67  239.963    0.005      0.00912     3.38e-05      0.00916       0.0564       0.0739       0.0467       0.0757       0.0612       0.0597       0.0961       0.0779        0.366      0.00381\n",
            "! Validation         67  239.963    0.005      0.00767     1.56e-06      0.00767       0.0506       0.0678       0.0397       0.0725       0.0561       0.0512       0.0923       0.0718       0.0718     0.000748\n",
            "Wall time: 239.96377076299996\n",
            "! Best model       67    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     68     1       0.0106       0.0105     1.83e-05       0.0607       0.0794       0.0495        0.083       0.0663       0.0628        0.105       0.0839        0.311      0.00324\n",
            "     68     2      0.00868      0.00867        8e-06       0.0554        0.072       0.0457        0.075       0.0603       0.0582       0.0938        0.076         0.19      0.00198\n",
            "     68     3      0.00794      0.00792     2.38e-05       0.0509       0.0688       0.0394        0.074       0.0567       0.0507       0.0953        0.073        0.344      0.00359\n",
            "     68     4       0.0135       0.0135     3.42e-05       0.0717       0.0899       0.0681       0.0788       0.0734       0.0838        0.101       0.0924         0.43      0.00448\n",
            "     68     5       0.0154       0.0153     2.34e-06       0.0766       0.0958       0.0673        0.095       0.0812       0.0822        0.119          0.1          0.1      0.00104\n",
            "     68     6       0.0101         0.01     3.22e-05       0.0595       0.0775        0.048       0.0824       0.0652       0.0597        0.104       0.0821        0.417      0.00434\n",
            "     68     7      0.00616      0.00615     1.64e-05       0.0461       0.0607       0.0364       0.0655       0.0509        0.046       0.0825       0.0642        0.294      0.00306\n",
            "     68     8      0.00953      0.00944     8.86e-05       0.0563       0.0752       0.0453       0.0781       0.0617       0.0571        0.102       0.0796        0.696      0.00726\n",
            "     68     9      0.00942       0.0094     1.67e-05       0.0574        0.075       0.0492       0.0737       0.0615       0.0624       0.0953       0.0789          0.3      0.00312\n",
            "     68    10      0.00729      0.00728      1.7e-05       0.0496        0.066       0.0404        0.068       0.0542       0.0521       0.0873       0.0697        0.297      0.00309\n",
            "     68    11      0.00773      0.00773     1.45e-06       0.0524        0.068       0.0428       0.0715       0.0572       0.0542       0.0896       0.0719       0.0834     0.000869\n",
            "     68    12      0.00839      0.00839     1.26e-06       0.0538       0.0709       0.0442       0.0731       0.0586       0.0566        0.093       0.0748       0.0721     0.000751\n",
            "     68    13      0.00781      0.00781     2.46e-06       0.0522       0.0684        0.041       0.0745       0.0577       0.0519       0.0928       0.0724        0.103      0.00108\n",
            "     68    14      0.00794      0.00793     6.07e-06       0.0531       0.0689       0.0436       0.0722       0.0579       0.0556       0.0898       0.0727        0.153      0.00159\n",
            "     68    15       0.0099      0.00988     1.81e-05       0.0583       0.0769       0.0466       0.0817       0.0641       0.0594        0.103       0.0814        0.315      0.00328\n",
            "     68    16       0.0079       0.0079     2.89e-06       0.0519       0.0688       0.0411       0.0735       0.0573       0.0518       0.0939       0.0728        0.118      0.00123\n",
            "     68    17      0.00732       0.0073     1.83e-05       0.0495       0.0661        0.038       0.0725       0.0552       0.0488       0.0913       0.0701        0.297       0.0031\n",
            "     68    18      0.00769      0.00769     5.78e-07       0.0507       0.0678       0.0403       0.0714       0.0559       0.0517       0.0919       0.0718       0.0455     0.000474\n",
            "     68    19      0.00747      0.00746     8.27e-06       0.0507       0.0668       0.0416       0.0687       0.0552       0.0536       0.0875       0.0705         0.17      0.00177\n",
            "     68    20      0.00642       0.0064     1.77e-05       0.0469       0.0619       0.0371       0.0666       0.0519       0.0471       0.0841       0.0656        0.307       0.0032\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     68     1      0.00791      0.00791     2.69e-06       0.0517       0.0688       0.0405       0.0741       0.0573       0.0524       0.0934       0.0729        0.101      0.00105\n",
            "     68     2      0.00792      0.00792     1.16e-06       0.0511       0.0689       0.0398       0.0738       0.0568       0.0517       0.0942        0.073       0.0561     0.000584\n",
            "     68     3      0.00744      0.00744     1.37e-06       0.0498       0.0667       0.0388       0.0716       0.0552       0.0498       0.0917       0.0707       0.0702     0.000731\n",
            "     68     4      0.00771      0.00771     1.39e-06       0.0503       0.0679       0.0392       0.0727       0.0559       0.0503       0.0937        0.072       0.0775     0.000808\n",
            "     68     5      0.00718      0.00718     1.16e-06       0.0493       0.0655       0.0392       0.0696       0.0544        0.051       0.0877       0.0693       0.0551     0.000574\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              68  243.472    0.005      0.00884     1.67e-05      0.00885       0.0552       0.0727       0.0453        0.075       0.0601       0.0581       0.0955       0.0768        0.252      0.00263\n",
            "! Validation         68  243.472    0.005      0.00763     1.56e-06      0.00763       0.0504       0.0676       0.0395       0.0724       0.0559        0.051       0.0922       0.0716        0.072      0.00075\n",
            "Wall time: 243.4732064299999\n",
            "! Best model       68    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     69     1      0.00689      0.00688     6.42e-06       0.0488       0.0642       0.0396       0.0671       0.0534       0.0508       0.0848       0.0678         0.18      0.00188\n",
            "     69     2      0.00686      0.00685     5.45e-06       0.0478       0.0641       0.0388       0.0657       0.0523       0.0497       0.0858       0.0678        0.167      0.00174\n",
            "     69     3       0.0069      0.00689     5.17e-06        0.048       0.0642       0.0374       0.0693       0.0534       0.0476       0.0886       0.0681        0.162      0.00169\n",
            "     69     4      0.00803      0.00803     8.34e-06       0.0519       0.0693       0.0397       0.0762        0.058       0.0506       0.0964       0.0735        0.209      0.00218\n",
            "     69     5      0.00708      0.00707     3.99e-06       0.0491       0.0651       0.0388       0.0696       0.0542       0.0503       0.0874       0.0689        0.128      0.00133\n",
            "     69     6      0.00808      0.00806     1.65e-05       0.0533       0.0695       0.0436       0.0727       0.0581       0.0563       0.0902       0.0732        0.293      0.00305\n",
            "     69     7      0.00799      0.00799     3.25e-06       0.0524       0.0692       0.0415       0.0742       0.0578       0.0533       0.0931       0.0732         0.12      0.00125\n",
            "     69     8      0.00609      0.00607     2.03e-05       0.0457       0.0603        0.036       0.0652       0.0506        0.045       0.0827       0.0639        0.332      0.00346\n",
            "     69     9      0.00748      0.00748     4.06e-07       0.0511       0.0669       0.0405       0.0724       0.0565       0.0508       0.0909       0.0709       0.0381     0.000397\n",
            "     69    10      0.00758      0.00756      1.7e-05        0.051       0.0673       0.0399       0.0732       0.0565       0.0499       0.0927       0.0713        0.304      0.00316\n",
            "     69    11      0.00759      0.00759     5.44e-06       0.0508       0.0674       0.0402        0.072       0.0561        0.051       0.0918       0.0714        0.157      0.00163\n",
            "     69    12      0.00642      0.00641     1.11e-05       0.0469       0.0619       0.0375       0.0656       0.0515       0.0471       0.0841       0.0656        0.244      0.00254\n",
            "     69    13      0.00836      0.00835     4.64e-06       0.0548       0.0707       0.0463       0.0719       0.0591       0.0579        0.091       0.0745        0.149      0.00155\n",
            "     69    14       0.0096      0.00959     7.42e-06       0.0587       0.0758       0.0512       0.0736       0.0624       0.0646       0.0942       0.0794        0.193      0.00201\n",
            "     69    15       0.0073      0.00728     1.87e-05       0.0506        0.066       0.0423       0.0671       0.0547       0.0534       0.0858       0.0696        0.318      0.00331\n",
            "     69    16      0.00725      0.00722     3.13e-05       0.0502       0.0657        0.041       0.0686       0.0548       0.0519        0.087       0.0695         0.41      0.00427\n",
            "     69    17       0.0112       0.0111     4.37e-05       0.0632       0.0817       0.0523       0.0851       0.0687       0.0659        0.106       0.0862        0.485      0.00505\n",
            "     69    18       0.0139       0.0139     2.05e-05       0.0714       0.0911       0.0601       0.0939        0.077       0.0756        0.116       0.0959        0.324      0.00337\n",
            "     69    19      0.00969      0.00969     1.31e-06       0.0583       0.0761       0.0489       0.0772        0.063        0.063       0.0972       0.0801       0.0707     0.000736\n",
            "     69    20      0.00743      0.00743      1.2e-06       0.0501       0.0667       0.0395       0.0712       0.0554       0.0501       0.0912       0.0706       0.0723     0.000753\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     69     1      0.00788      0.00787     2.63e-06       0.0515       0.0686       0.0403       0.0739       0.0571       0.0522       0.0932       0.0727       0.0988      0.00103\n",
            "     69     2      0.00788      0.00788     1.17e-06        0.051       0.0687       0.0396       0.0737       0.0567       0.0515       0.0941       0.0728       0.0558     0.000581\n",
            "     69     3       0.0074       0.0074     1.42e-06       0.0496       0.0666       0.0387       0.0715       0.0551       0.0496       0.0915       0.0705       0.0704     0.000733\n",
            "     69     4      0.00767      0.00767     1.41e-06       0.0502       0.0677        0.039       0.0725       0.0558       0.0501       0.0935       0.0718       0.0776     0.000809\n",
            "     69     5      0.00714      0.00713     1.18e-06       0.0492       0.0653       0.0391       0.0694       0.0542       0.0507       0.0875       0.0691       0.0555     0.000578\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              69  246.986    0.005      0.00807     1.16e-05      0.00809       0.0527       0.0695       0.0428       0.0726       0.0577       0.0548       0.0922       0.0735        0.218      0.00227\n",
            "! Validation         69  246.986    0.005      0.00759     1.56e-06      0.00759       0.0503       0.0674       0.0393       0.0722       0.0558       0.0508        0.092       0.0714       0.0716     0.000746\n",
            "Wall time: 246.98748010500003\n",
            "! Best model       69    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     70     1       0.0103       0.0103     3.88e-05        0.062       0.0785       0.0567       0.0726       0.0647       0.0704       0.0925       0.0814        0.459      0.00478\n",
            "     70     2      0.00952      0.00951     2.71e-06       0.0585       0.0755       0.0507       0.0741       0.0624       0.0641       0.0941       0.0791         0.11      0.00115\n",
            "     70     3      0.00706      0.00704     1.94e-05       0.0489       0.0649       0.0407       0.0652        0.053       0.0527       0.0842       0.0684        0.317       0.0033\n",
            "     70     4      0.00875      0.00875     2.11e-06       0.0562       0.0724       0.0489       0.0708       0.0598       0.0616       0.0901       0.0758       0.0916     0.000954\n",
            "     70     5       0.0077       0.0077     1.53e-06       0.0504       0.0679       0.0391       0.0731       0.0561       0.0504       0.0935        0.072       0.0705     0.000734\n",
            "     70     6      0.00722      0.00718     3.42e-05       0.0504       0.0656       0.0415       0.0682       0.0548       0.0521       0.0865       0.0693        0.423       0.0044\n",
            "     70     7      0.00968      0.00968     1.73e-06       0.0575       0.0761       0.0457        0.081       0.0634        0.058        0.103       0.0806       0.0939     0.000979\n",
            "     70     8      0.00877      0.00869     8.33e-05       0.0539       0.0721       0.0426       0.0764       0.0595       0.0543       0.0985       0.0764        0.676      0.00704\n",
            "     70     9       0.0067       0.0067     8.23e-07       0.0479       0.0633       0.0376       0.0685       0.0531       0.0478       0.0864       0.0671       0.0576       0.0006\n",
            "     70    10      0.00927      0.00917     9.53e-05       0.0554       0.0741       0.0441       0.0781       0.0611       0.0565          0.1       0.0785        0.724      0.00754\n",
            "     70    11       0.0078      0.00779     7.18e-06       0.0523       0.0683       0.0431       0.0705       0.0568       0.0546       0.0895       0.0721        0.185      0.00193\n",
            "     70    12      0.00631      0.00628     3.22e-05       0.0472       0.0613        0.038       0.0657       0.0518        0.048       0.0816       0.0648        0.414      0.00432\n",
            "     70    13      0.00787      0.00785     2.67e-05       0.0521       0.0685       0.0421       0.0722       0.0571       0.0537       0.0913       0.0725        0.378      0.00394\n",
            "     70    14      0.00806      0.00806     8.81e-07       0.0534       0.0694       0.0444       0.0714       0.0579       0.0563       0.0901       0.0732       0.0551     0.000574\n",
            "     70    15      0.00687      0.00685     1.34e-05       0.0487       0.0641       0.0391       0.0681       0.0536       0.0497       0.0858       0.0678        0.258      0.00268\n",
            "     70    16      0.00888      0.00888     6.34e-06       0.0568       0.0729       0.0504       0.0695         0.06       0.0636       0.0885       0.0761        0.185      0.00193\n",
            "     70    17      0.00856      0.00856     8.84e-07       0.0541       0.0716       0.0439       0.0747       0.0593       0.0562       0.0951       0.0757       0.0592     0.000616\n",
            "     70    18      0.00794      0.00794     2.82e-06       0.0524       0.0689       0.0413       0.0746        0.058       0.0519       0.0942        0.073       0.0846     0.000881\n",
            "     70    19       0.0069      0.00689      8.7e-06       0.0488       0.0642        0.039       0.0684       0.0537       0.0492       0.0868        0.068        0.203      0.00212\n",
            "     70    20      0.00771       0.0077     7.48e-06       0.0514       0.0679       0.0398       0.0746       0.0572         0.05        0.094        0.072        0.193      0.00201\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     70     1      0.00783      0.00783     2.66e-06       0.0514       0.0685       0.0402       0.0737        0.057        0.052        0.093       0.0725       0.0996      0.00104\n",
            "     70     2      0.00784      0.00784     1.24e-06       0.0509       0.0685       0.0395       0.0736       0.0565       0.0513       0.0939       0.0726       0.0577     0.000601\n",
            "     70     3      0.00737      0.00737      1.4e-06       0.0495       0.0664       0.0385       0.0714        0.055       0.0494       0.0914       0.0704       0.0698     0.000727\n",
            "     70     4      0.00763      0.00762     1.38e-06         0.05       0.0676       0.0389       0.0724       0.0556       0.0499       0.0933       0.0716       0.0759      0.00079\n",
            "     70     5      0.00709      0.00709     1.16e-06        0.049       0.0651       0.0389       0.0693       0.0541       0.0505       0.0873       0.0689       0.0543     0.000566\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              70  250.507    0.005      0.00807     1.93e-05      0.00809       0.0529       0.0695       0.0434       0.0719       0.0577       0.0554       0.0915       0.0734        0.252      0.00262\n",
            "! Validation         70  250.507    0.005      0.00755     1.57e-06      0.00755       0.0502       0.0672       0.0392       0.0721       0.0556       0.0506       0.0918       0.0712       0.0715     0.000744\n",
            "Wall time: 250.5082195340001\n",
            "! Best model       70    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     71     1       0.0081      0.00809     9.67e-06        0.052       0.0696       0.0405       0.0751       0.0578       0.0527       0.0947       0.0737        0.215      0.00224\n",
            "     71     2      0.00859      0.00859     6.02e-07       0.0553       0.0717       0.0468       0.0724       0.0596       0.0589       0.0921       0.0755       0.0465     0.000484\n",
            "     71     3      0.00752      0.00752     5.36e-07        0.051       0.0671       0.0404       0.0723       0.0564        0.051       0.0911        0.071       0.0486     0.000507\n",
            "     71     4       0.0086      0.00859     7.87e-06        0.055       0.0717       0.0433       0.0782       0.0608        0.054       0.0979        0.076        0.196      0.00205\n",
            "     71     5      0.00712       0.0071     1.71e-05       0.0485       0.0652       0.0386       0.0681       0.0534         0.05        0.088        0.069        0.304      0.00316\n",
            "     71     6      0.00918      0.00914     3.58e-05       0.0558        0.074       0.0446       0.0781       0.0614       0.0567       0.0999       0.0783        0.439      0.00457\n",
            "     71     7      0.00637      0.00634     2.79e-05       0.0459       0.0616       0.0361       0.0655       0.0508       0.0458       0.0848       0.0653        0.392      0.00408\n",
            "     71     8      0.00741      0.00734      6.5e-05       0.0508       0.0663       0.0407        0.071       0.0558       0.0518       0.0885       0.0701        0.596       0.0062\n",
            "     71     9      0.00742       0.0074     2.41e-05       0.0488       0.0666        0.037       0.0723       0.0546       0.0486       0.0925       0.0706        0.355       0.0037\n",
            "     71    10      0.00672      0.00664     7.72e-05        0.049        0.063       0.0405       0.0661       0.0533       0.0505       0.0826       0.0665        0.647      0.00674\n",
            "     71    11      0.00687      0.00687     5.97e-07       0.0482       0.0641       0.0375       0.0696       0.0536       0.0477       0.0883        0.068       0.0521     0.000543\n",
            "     71    12      0.00787      0.00776     0.000105       0.0518       0.0682       0.0416       0.0722       0.0569       0.0533       0.0908       0.0721        0.759      0.00791\n",
            "     71    13      0.00771      0.00767     3.62e-05       0.0502       0.0678       0.0393       0.0721       0.0557       0.0503       0.0933       0.0718        0.442      0.00461\n",
            "     71    14      0.00712      0.00702     9.53e-05       0.0489       0.0648        0.039       0.0687       0.0538       0.0502        0.087       0.0686        0.717      0.00747\n",
            "     71    15      0.00724       0.0072     4.63e-05       0.0495       0.0656       0.0382       0.0722       0.0552        0.048       0.0912       0.0696        0.502      0.00523\n",
            "     71    16      0.00726      0.00724      1.6e-05       0.0488       0.0658       0.0385       0.0696        0.054       0.0488       0.0908       0.0698        0.292      0.00305\n",
            "     71    17       0.0081      0.00807     2.39e-05       0.0538       0.0695       0.0457       0.0702       0.0579       0.0576       0.0887       0.0731        0.355       0.0037\n",
            "     71    18      0.00723      0.00723     3.61e-06       0.0505       0.0658       0.0419       0.0678       0.0548       0.0532       0.0856       0.0694        0.122      0.00127\n",
            "     71    19      0.00703      0.00701     1.21e-05       0.0482       0.0648       0.0378       0.0689       0.0534       0.0483        0.089       0.0687        0.246      0.00256\n",
            "     71    20      0.00659      0.00658     1.36e-05        0.048       0.0627       0.0393       0.0656       0.0524       0.0502       0.0823       0.0662        0.248      0.00259\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     71     1      0.00779      0.00779     2.68e-06       0.0512       0.0683       0.0401       0.0735       0.0568       0.0518       0.0928       0.0723       0.0992      0.00103\n",
            "     71     2       0.0078      0.00779     1.24e-06       0.0507       0.0683       0.0393       0.0735       0.0564       0.0511       0.0937       0.0724       0.0565     0.000589\n",
            "     71     3      0.00734      0.00734      1.4e-06       0.0494       0.0663       0.0384       0.0714       0.0549       0.0492       0.0913       0.0702       0.0692     0.000721\n",
            "     71     4      0.00758      0.00758     1.43e-06       0.0499       0.0674       0.0387       0.0722       0.0555       0.0497       0.0931       0.0714       0.0776     0.000809\n",
            "     71     5      0.00705      0.00705     1.19e-06       0.0489       0.0649       0.0387       0.0691       0.0539       0.0503       0.0871       0.0687       0.0548     0.000571\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              71  254.047    0.005      0.00747     3.09e-05       0.0075       0.0505       0.0669       0.0404       0.0708       0.0556       0.0515       0.0901       0.0708        0.349      0.00363\n",
            "! Validation         71  254.047    0.005      0.00751     1.59e-06      0.00751         0.05        0.067       0.0391       0.0719       0.0555       0.0504       0.0916        0.071       0.0715     0.000745\n",
            "Wall time: 254.04757887300002\n",
            "! Best model       71    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     72     1      0.00672      0.00671     8.44e-06       0.0474       0.0634       0.0374       0.0674       0.0524       0.0485       0.0857       0.0671        0.197      0.00205\n",
            "     72     2      0.00656      0.00656     1.77e-06       0.0467       0.0627       0.0369       0.0662       0.0516       0.0475       0.0853       0.0664       0.0826     0.000861\n",
            "     72     3       0.0067      0.00669     6.48e-06        0.048       0.0633       0.0368       0.0705       0.0536       0.0459       0.0883       0.0671        0.164      0.00171\n",
            "     72     4      0.00734      0.00734     1.67e-06       0.0497       0.0663       0.0394       0.0702       0.0548       0.0513       0.0889       0.0701       0.0887     0.000924\n",
            "     72     5      0.00672      0.00672     3.51e-06       0.0481       0.0634       0.0376       0.0689       0.0533       0.0476       0.0868       0.0672        0.119      0.00124\n",
            "     72     6      0.00834      0.00834        9e-07       0.0534       0.0707       0.0418       0.0766       0.0592       0.0534       0.0963       0.0748       0.0641     0.000667\n",
            "     72     7      0.00712      0.00711     1.21e-05       0.0498       0.0652       0.0399       0.0698       0.0548       0.0503       0.0878        0.069        0.244      0.00254\n",
            "     72     8      0.00686      0.00685     1.16e-05        0.049        0.064       0.0401       0.0667       0.0534       0.0501       0.0853       0.0677        0.221      0.00231\n",
            "     72     9      0.00734      0.00734     4.75e-06       0.0483       0.0663       0.0363       0.0725       0.0544       0.0472       0.0934       0.0703         0.15      0.00156\n",
            "     72    10      0.00758      0.00758     8.67e-06       0.0509       0.0673       0.0402       0.0724       0.0563       0.0517       0.0909       0.0713        0.211      0.00219\n",
            "     72    11      0.00724      0.00724     6.11e-06       0.0491       0.0658       0.0384       0.0706       0.0545        0.049       0.0905       0.0697        0.179      0.00187\n",
            "     72    12      0.00701      0.00698     2.61e-05       0.0477       0.0647        0.037       0.0692       0.0531       0.0469       0.0902       0.0686        0.377      0.00393\n",
            "     72    13      0.00621      0.00621     3.63e-07       0.0465        0.061       0.0368       0.0659       0.0513       0.0462        0.083       0.0646       0.0404     0.000421\n",
            "     72    14      0.00681       0.0068     1.58e-05       0.0479       0.0638       0.0371       0.0693       0.0532       0.0474       0.0878       0.0676        0.292      0.00304\n",
            "     72    15      0.00814      0.00813     8.97e-06       0.0525       0.0697       0.0409       0.0758       0.0584       0.0521       0.0958       0.0739        0.217      0.00226\n",
            "     72    16      0.00793       0.0079     3.64e-05        0.052       0.0688        0.042       0.0719        0.057       0.0538       0.0916       0.0727        0.444      0.00463\n",
            "     72    17      0.00722      0.00722     2.36e-06       0.0501       0.0657       0.0396        0.071       0.0553       0.0512       0.0879       0.0695       0.0979      0.00102\n",
            "     72    18      0.00638      0.00637     5.12e-06       0.0465       0.0618       0.0363       0.0669       0.0516       0.0456       0.0853       0.0655        0.162      0.00169\n",
            "     72    19      0.00665      0.00664     2.01e-06       0.0478       0.0631       0.0384       0.0666       0.0525       0.0484       0.0851       0.0668       0.0809     0.000842\n",
            "     72    20       0.0073       0.0073     7.63e-07       0.0495       0.0661       0.0386       0.0711       0.0549       0.0499       0.0902         0.07       0.0594     0.000618\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     72     1      0.00774      0.00774     2.59e-06        0.051       0.0681       0.0399       0.0733       0.0566       0.0516       0.0925       0.0721       0.0976      0.00102\n",
            "     72     2      0.00774      0.00774     1.27e-06       0.0506       0.0681       0.0392       0.0733       0.0563       0.0508       0.0935       0.0721       0.0593     0.000617\n",
            "     72     3       0.0073       0.0073     1.45e-06       0.0492       0.0661       0.0382       0.0712       0.0547        0.049       0.0911       0.0701       0.0695     0.000724\n",
            "     72     4      0.00754      0.00754     1.44e-06       0.0498       0.0672       0.0386        0.072       0.0553       0.0495       0.0929       0.0712       0.0763     0.000794\n",
            "     72     5      0.00701      0.00701     1.18e-06       0.0487       0.0648       0.0386        0.069       0.0538       0.0501       0.0869       0.0685        0.055     0.000573\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              72  257.541    0.005       0.0071      8.2e-06      0.00711        0.049       0.0652       0.0386         0.07       0.0543       0.0493       0.0889       0.0691        0.175      0.00182\n",
            "! Validation         72  257.541    0.005      0.00746     1.59e-06      0.00747       0.0499       0.0668       0.0389       0.0718       0.0553       0.0502       0.0914       0.0708       0.0715     0.000745\n",
            "Wall time: 257.541555774\n",
            "! Best model       72    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     73     1      0.00704      0.00704     3.59e-06       0.0485       0.0649       0.0381       0.0692       0.0537       0.0481       0.0895       0.0688        0.126      0.00131\n",
            "     73     2      0.00678      0.00678     1.34e-06       0.0482       0.0637       0.0389       0.0669       0.0529       0.0498       0.0849       0.0674       0.0783     0.000816\n",
            "     73     3      0.00835      0.00834     1.47e-05        0.053       0.0706       0.0417       0.0758       0.0587        0.053       0.0968       0.0749        0.274      0.00285\n",
            "     73     4      0.00878      0.00877     8.45e-06       0.0549       0.0724       0.0438        0.077       0.0604       0.0549       0.0985       0.0767        0.194      0.00202\n",
            "     73     5      0.00765      0.00765     2.11e-06       0.0507       0.0677       0.0413       0.0696       0.0555       0.0538       0.0891       0.0715       0.0939     0.000979\n",
            "     73     6      0.00787      0.00786     7.81e-06       0.0518       0.0686       0.0414       0.0727       0.0571       0.0528       0.0925       0.0726        0.188      0.00196\n",
            "     73     7      0.00924      0.00924     1.15e-06       0.0577       0.0744       0.0473       0.0785       0.0629       0.0593       0.0978       0.0785       0.0637     0.000663\n",
            "     73     8      0.00993      0.00993     2.99e-06       0.0596       0.0771       0.0506       0.0775       0.0641       0.0638       0.0984       0.0811         0.12      0.00125\n",
            "     73     9      0.00861      0.00857     3.54e-05       0.0536       0.0716       0.0429        0.075       0.0589       0.0553       0.0963       0.0758        0.439      0.00458\n",
            "     73    10      0.00662      0.00662     5.62e-07       0.0472        0.063       0.0369       0.0678       0.0523       0.0474        0.086       0.0667       0.0439     0.000458\n",
            "     73    11      0.00911      0.00908     3.11e-05       0.0569       0.0737       0.0456       0.0795       0.0626        0.056          0.1       0.0781        0.413       0.0043\n",
            "     73    12      0.00886      0.00886     1.03e-06        0.057       0.0728       0.0469       0.0772       0.0621       0.0588       0.0948       0.0768       0.0707     0.000736\n",
            "     73    13      0.00697      0.00695     1.68e-05       0.0487       0.0645       0.0403       0.0654       0.0528       0.0513        0.085       0.0681        0.284      0.00296\n",
            "     73    14      0.00741      0.00741     7.89e-07       0.0503       0.0666        0.039       0.0727       0.0559       0.0495       0.0917       0.0706       0.0596     0.000621\n",
            "     73    15      0.00913      0.00906     7.15e-05       0.0568       0.0736       0.0477       0.0749       0.0613       0.0605       0.0946       0.0775        0.628      0.00654\n",
            "     73    16      0.00956      0.00956     1.53e-06       0.0585       0.0756       0.0482       0.0793       0.0637       0.0603       0.0995       0.0799       0.0734     0.000765\n",
            "     73    17      0.00706      0.00706     7.96e-07       0.0486        0.065       0.0387       0.0685       0.0536       0.0494       0.0884       0.0689       0.0514     0.000535\n",
            "     73    18      0.00765      0.00763     2.04e-05       0.0514       0.0676       0.0389       0.0762       0.0576       0.0497       0.0936       0.0717        0.326      0.00339\n",
            "     73    19      0.00976      0.00974     2.07e-05       0.0589       0.0764       0.0488        0.079       0.0639       0.0613       0.0999       0.0806        0.332      0.00346\n",
            "     73    20       0.0109       0.0109     3.83e-05       0.0629       0.0807       0.0543       0.0801       0.0672       0.0676        0.102       0.0848        0.455      0.00474\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     73     1       0.0077      0.00769     2.58e-06       0.0509       0.0679       0.0398       0.0732       0.0565       0.0514       0.0924       0.0719       0.0973      0.00101\n",
            "     73     2       0.0077       0.0077     1.21e-06       0.0504       0.0679        0.039       0.0732       0.0561       0.0506       0.0933       0.0719       0.0566      0.00059\n",
            "     73     3      0.00726      0.00726     1.41e-06       0.0491       0.0659       0.0381       0.0711       0.0546       0.0488        0.091       0.0699       0.0692     0.000721\n",
            "     73     4       0.0075      0.00749     1.41e-06       0.0496        0.067       0.0384       0.0719       0.0552       0.0493       0.0927        0.071       0.0761     0.000792\n",
            "     73     5      0.00696      0.00696     1.16e-06       0.0486       0.0646       0.0384       0.0688       0.0536       0.0499       0.0868       0.0683       0.0548     0.000571\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              73  261.006    0.005      0.00835     1.41e-05      0.00837       0.0538       0.0707       0.0436       0.0741       0.0589       0.0554       0.0941       0.0748        0.216      0.00225\n",
            "! Validation         73  261.006    0.005      0.00742     1.55e-06      0.00742       0.0497       0.0666       0.0387       0.0716       0.0552         0.05       0.0913       0.0706       0.0708     0.000738\n",
            "Wall time: 261.00719508399993\n",
            "! Best model       73    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     74     1      0.00742      0.00741     9.11e-06       0.0499       0.0666       0.0393        0.071       0.0552       0.0509       0.0901       0.0705        0.202      0.00211\n",
            "     74     2      0.00726      0.00725     6.85e-06       0.0501       0.0659       0.0387       0.0728       0.0558        0.049       0.0906       0.0698        0.182       0.0019\n",
            "     74     3      0.00816      0.00816     9.03e-07       0.0528       0.0699       0.0433       0.0717       0.0575       0.0552       0.0925       0.0739       0.0582     0.000606\n",
            "     74     4      0.00744      0.00743     1.04e-05         0.05       0.0667       0.0381        0.074        0.056       0.0483       0.0931       0.0707        0.234      0.00244\n",
            "     74     5      0.00749      0.00742     6.19e-05       0.0497       0.0667       0.0383       0.0726       0.0554       0.0495       0.0918       0.0707         0.58      0.00604\n",
            "     74     6       0.0075      0.00749     3.25e-06       0.0499        0.067       0.0389       0.0718       0.0553       0.0501       0.0919        0.071        0.109      0.00114\n",
            "     74     7       0.0085      0.00835     0.000153       0.0542       0.0707        0.044       0.0746       0.0593       0.0552       0.0943       0.0748        0.915      0.00953\n",
            "     74     8        0.008      0.00796     3.06e-05       0.0517        0.069       0.0422       0.0705       0.0564       0.0552       0.0906       0.0729        0.407      0.00424\n",
            "     74     9      0.00723      0.00722     7.89e-06       0.0499       0.0658       0.0396       0.0706       0.0551       0.0502        0.089       0.0696        0.195      0.00203\n",
            "     74    10      0.00804        0.008     4.57e-05       0.0525       0.0692       0.0427       0.0719       0.0573       0.0554       0.0907        0.073        0.496      0.00517\n",
            "     74    11      0.00777      0.00776     5.28e-06       0.0521       0.0682       0.0434       0.0696       0.0565       0.0547       0.0893        0.072        0.148      0.00154\n",
            "     74    12       0.0073      0.00728     2.86e-05       0.0498        0.066       0.0394       0.0708       0.0551       0.0499       0.0899       0.0699        0.394       0.0041\n",
            "     74    13      0.00712      0.00711     1.94e-06       0.0496       0.0653       0.0399       0.0689       0.0544       0.0507       0.0873        0.069       0.0906     0.000944\n",
            "     74    14      0.00797       0.0079      6.8e-05       0.0515       0.0688       0.0387       0.0772        0.058       0.0498       0.0961       0.0729         0.61      0.00636\n",
            "     74    15        0.009      0.00899     1.14e-05       0.0563       0.0734       0.0459       0.0771       0.0615       0.0574       0.0977       0.0776        0.244      0.00254\n",
            "     74    16      0.00701      0.00695     5.74e-05       0.0495       0.0645       0.0411       0.0664       0.0537       0.0519       0.0842       0.0681        0.561      0.00584\n",
            "     74    17      0.00646      0.00644     1.79e-05       0.0461       0.0621       0.0362       0.0659       0.0511        0.047       0.0845       0.0658        0.308       0.0032\n",
            "     74    18      0.00693      0.00693     1.91e-06       0.0475       0.0644       0.0368       0.0688       0.0528       0.0484       0.0881       0.0682        0.091     0.000948\n",
            "     74    19      0.00728      0.00726     2.35e-05       0.0497       0.0659       0.0393       0.0706       0.0549       0.0508       0.0887       0.0697        0.347      0.00361\n",
            "     74    20      0.00676      0.00671     4.68e-05       0.0488       0.0634        0.039       0.0684       0.0537       0.0488       0.0854       0.0671          0.5      0.00521\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     74     1      0.00765      0.00765     2.58e-06       0.0507       0.0677       0.0396        0.073       0.0563       0.0512       0.0922       0.0717       0.0958     0.000998\n",
            "     74     2      0.00766      0.00766     1.29e-06       0.0502       0.0677       0.0388        0.073       0.0559       0.0504       0.0931       0.0717       0.0588     0.000612\n",
            "     74     3      0.00722      0.00722      1.4e-06        0.049       0.0657       0.0379        0.071       0.0545       0.0486       0.0908       0.0697        0.068     0.000708\n",
            "     74     4      0.00745      0.00745     1.47e-06       0.0494       0.0668       0.0383       0.0718        0.055        0.049       0.0925       0.0708       0.0773     0.000806\n",
            "     74     5      0.00692      0.00692     1.14e-06       0.0484       0.0644       0.0383       0.0687       0.0535       0.0496       0.0866       0.0681       0.0539     0.000562\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              74  264.479    0.005       0.0075     2.96e-05      0.00753       0.0506        0.067       0.0402       0.0713       0.0558       0.0515       0.0904       0.0709        0.334      0.00348\n",
            "! Validation         74  264.479    0.005      0.00738     1.58e-06      0.00738       0.0496       0.0665       0.0386       0.0715        0.055       0.0498       0.0911       0.0704       0.0708     0.000737\n",
            "Wall time: 264.4796326389999\n",
            "! Best model       74    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     75     1      0.00702      0.00702     5.89e-07       0.0504       0.0648       0.0411        0.069       0.0551       0.0512       0.0857       0.0685         0.05     0.000521\n",
            "     75     2      0.00718      0.00714     3.52e-05       0.0488       0.0654       0.0394       0.0676       0.0535       0.0507       0.0876       0.0692        0.437      0.00456\n",
            "     75     3      0.00799      0.00799     2.98e-06       0.0509       0.0691       0.0398        0.073       0.0564       0.0525       0.0939       0.0732        0.119      0.00124\n",
            "     75     4      0.00864      0.00863     1.09e-05       0.0559       0.0719       0.0486       0.0705       0.0595        0.061       0.0898       0.0754        0.236      0.00246\n",
            "     75     5      0.00633      0.00633      3.6e-07       0.0472       0.0615       0.0391       0.0634       0.0513        0.049        0.081        0.065       0.0307     0.000319\n",
            "     75     6      0.00789      0.00787     2.24e-05       0.0518       0.0686       0.0427         0.07       0.0564        0.056       0.0886       0.0723        0.333      0.00347\n",
            "     75     7      0.00873      0.00872     1.14e-05       0.0569       0.0722       0.0489       0.0729       0.0609       0.0611       0.0905       0.0758        0.239      0.00249\n",
            "     75     8      0.00912       0.0091      1.8e-05       0.0557       0.0738       0.0454       0.0764       0.0609        0.057       0.0992       0.0781        0.313      0.00326\n",
            "     75     9      0.00733       0.0073     2.86e-05       0.0504       0.0661       0.0392       0.0727       0.0559       0.0493       0.0908       0.0701        0.391      0.00407\n",
            "     75    10      0.00717      0.00715     1.93e-05       0.0491       0.0654       0.0391       0.0691       0.0541       0.0493       0.0892       0.0693        0.321      0.00334\n",
            "     75    11      0.00923      0.00923     1.38e-06        0.056       0.0743       0.0455        0.077       0.0612        0.058       0.0992       0.0786       0.0662      0.00069\n",
            "     75    12      0.00911       0.0091     7.85e-06       0.0568       0.0738        0.048       0.0744       0.0612        0.061       0.0944       0.0777        0.191      0.00199\n",
            "     75    13      0.00713      0.00712     8.44e-06       0.0491       0.0653       0.0399       0.0675       0.0537       0.0513       0.0867        0.069        0.209      0.00218\n",
            "     75    14      0.00799      0.00797     1.54e-05       0.0524       0.0691       0.0411       0.0752       0.0581       0.0524        0.094       0.0732        0.282      0.00293\n",
            "     75    15      0.00914      0.00914     3.13e-07       0.0578        0.074       0.0494       0.0745       0.0619       0.0616        0.094       0.0778       0.0367     0.000382\n",
            "     75    16      0.00914      0.00914     2.68e-06       0.0577       0.0739       0.0493       0.0743       0.0618       0.0608       0.0949       0.0779        0.101      0.00105\n",
            "     75    17       0.0073      0.00728      1.5e-05         0.05        0.066       0.0393       0.0715       0.0554       0.0495       0.0904         0.07        0.286      0.00298\n",
            "     75    18      0.00695      0.00694     1.71e-05       0.0485       0.0644       0.0386       0.0685       0.0535       0.0497       0.0867       0.0682        0.274      0.00285\n",
            "     75    19      0.00963      0.00961      1.5e-05       0.0568       0.0758       0.0429       0.0846       0.0637       0.0552        0.106       0.0804        0.281      0.00293\n",
            "     75    20      0.00804      0.00804      3.8e-06       0.0526       0.0694       0.0426       0.0727       0.0577       0.0545       0.0922       0.0733        0.123      0.00128\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     75     1      0.00762      0.00762     2.63e-06       0.0506       0.0675       0.0395       0.0729       0.0562        0.051        0.092       0.0715        0.097      0.00101\n",
            "     75     2      0.00762      0.00762     1.22e-06       0.0501       0.0675       0.0387       0.0729       0.0558       0.0502       0.0929       0.0716       0.0572     0.000596\n",
            "     75     3      0.00719      0.00719      1.4e-06       0.0488       0.0656       0.0378       0.0709       0.0544       0.0484       0.0907       0.0695       0.0699     0.000728\n",
            "     75     4      0.00742      0.00741     1.48e-06       0.0493       0.0666       0.0381       0.0717       0.0549       0.0488       0.0924       0.0706       0.0779     0.000812\n",
            "     75     5      0.00688      0.00688     1.13e-06       0.0483       0.0642       0.0381       0.0686       0.0533       0.0494       0.0864       0.0679       0.0532     0.000554\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              75  268.024    0.005      0.00804     1.18e-05      0.00805       0.0527       0.0694        0.043       0.0722       0.0576       0.0548       0.0919       0.0733        0.216      0.00225\n",
            "! Validation         75  268.024    0.005      0.00734     1.57e-06      0.00735       0.0494       0.0663       0.0384       0.0714       0.0549       0.0496       0.0909       0.0703       0.0711      0.00074\n",
            "Wall time: 268.02526229599994\n",
            "! Best model       75    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     76     1      0.00663      0.00663     1.98e-06       0.0474        0.063       0.0376       0.0669       0.0523       0.0485       0.0848       0.0667       0.0877     0.000913\n",
            "     76     2      0.00704      0.00704     2.45e-06       0.0488       0.0649       0.0377        0.071       0.0544       0.0477       0.0899       0.0688        0.106       0.0011\n",
            "     76     3      0.00863      0.00863     7.91e-07       0.0552       0.0719       0.0462       0.0733       0.0598        0.057       0.0949       0.0759       0.0512     0.000533\n",
            "     76     4       0.0101       0.0101     2.11e-05       0.0598       0.0777       0.0481       0.0832       0.0657       0.0606        0.104       0.0822        0.328      0.00342\n",
            "     76     5      0.00828      0.00826     1.56e-05       0.0537       0.0703       0.0433       0.0744       0.0588       0.0551       0.0936       0.0744        0.286      0.00298\n",
            "     76     6      0.00652       0.0065     1.28e-05       0.0469       0.0624       0.0357       0.0695       0.0526       0.0453        0.087       0.0662         0.26      0.00271\n",
            "     76     7      0.00762      0.00761     3.97e-06       0.0509       0.0675         0.04       0.0726       0.0563       0.0508       0.0923       0.0715        0.142      0.00148\n",
            "     76     8      0.00682      0.00681     3.05e-06       0.0479       0.0639       0.0377       0.0682        0.053       0.0482       0.0871       0.0677        0.124      0.00129\n",
            "     76     9      0.00596      0.00595      4.5e-06       0.0449       0.0597       0.0329       0.0687       0.0508       0.0419       0.0847       0.0633        0.152      0.00158\n",
            "     76    10       0.0072       0.0072     5.33e-06       0.0493       0.0656       0.0382       0.0716       0.0549       0.0491         0.09       0.0695        0.157      0.00164\n",
            "     76    11      0.00718      0.00717     8.36e-06       0.0493       0.0655       0.0398       0.0683        0.054       0.0511       0.0875       0.0693        0.203      0.00211\n",
            "     76    12      0.00696      0.00696     2.28e-06       0.0494       0.0645       0.0388       0.0704       0.0546       0.0487        0.088       0.0684       0.0969      0.00101\n",
            "     76    13      0.00703      0.00702     9.85e-06       0.0493       0.0648       0.0391       0.0697       0.0544       0.0489       0.0884       0.0687        0.228      0.00237\n",
            "     76    14      0.00743      0.00743     2.22e-06       0.0504       0.0667       0.0407       0.0699       0.0553       0.0514       0.0897       0.0706       0.0949     0.000989\n",
            "     76    15      0.00625      0.00624     1.21e-05       0.0449       0.0611       0.0345       0.0656       0.0501       0.0451       0.0845       0.0648        0.247      0.00257\n",
            "     76    16      0.00787      0.00787     2.12e-06        0.051       0.0686       0.0397       0.0737       0.0567        0.051       0.0945       0.0728       0.0715     0.000745\n",
            "     76    17       0.0076      0.00756     4.56e-05       0.0506       0.0672       0.0406       0.0706       0.0556       0.0521       0.0902       0.0712        0.497      0.00518\n",
            "     76    18        0.007      0.00697     2.62e-05       0.0489       0.0646       0.0391       0.0685       0.0538       0.0495       0.0873       0.0684        0.374       0.0039\n",
            "     76    19      0.00723      0.00721     1.32e-05       0.0487       0.0657       0.0386       0.0688       0.0537       0.0505       0.0886       0.0696        0.256      0.00267\n",
            "     76    20      0.00692      0.00691     7.63e-06       0.0486       0.0643        0.039       0.0677       0.0533       0.0496       0.0866       0.0681        0.195      0.00203\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     76     1      0.00758      0.00758     2.59e-06       0.0505       0.0673       0.0393       0.0727        0.056       0.0508       0.0919       0.0713       0.0968      0.00101\n",
            "     76     2      0.00759      0.00759     1.23e-06         0.05       0.0674       0.0386       0.0728       0.0557       0.0501       0.0928       0.0714       0.0574     0.000598\n",
            "     76     3      0.00716      0.00716     1.43e-06       0.0487       0.0654       0.0376       0.0708       0.0542       0.0482       0.0906       0.0694       0.0703     0.000732\n",
            "     76     4      0.00738      0.00737     1.45e-06       0.0492       0.0664        0.038       0.0715       0.0548       0.0486       0.0922       0.0704       0.0778     0.000811\n",
            "     76     5      0.00684      0.00684     1.11e-06       0.0481        0.064        0.038       0.0684       0.0532       0.0492       0.0863       0.0677       0.0527     0.000549\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              76  271.558    0.005       0.0073     1.01e-05      0.00731       0.0498       0.0661       0.0394       0.0706        0.055       0.0503       0.0898         0.07        0.198      0.00206\n",
            "! Validation         76  271.558    0.005      0.00731     1.56e-06      0.00731       0.0493       0.0661       0.0383       0.0713       0.0548       0.0494       0.0908       0.0701        0.071      0.00074\n",
            "Wall time: 271.5589516519999\n",
            "! Best model       76    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     77     1       0.0063       0.0063     1.31e-06       0.0463       0.0614       0.0354        0.068       0.0517       0.0443        0.086       0.0651       0.0689     0.000718\n",
            "     77     2      0.00776      0.00776     3.39e-06        0.051       0.0681       0.0401       0.0729       0.0565       0.0511       0.0934       0.0722        0.125       0.0013\n",
            "     77     3      0.00644      0.00644     4.95e-06       0.0468       0.0621       0.0384       0.0637       0.0511       0.0486       0.0827       0.0656        0.146      0.00153\n",
            "     77     4      0.00615      0.00615     7.81e-07        0.045       0.0607       0.0349       0.0653       0.0501       0.0443       0.0844       0.0643       0.0629     0.000655\n",
            "     77     5      0.00755      0.00753     1.93e-05         0.05       0.0671       0.0403       0.0693       0.0548       0.0516       0.0905        0.071        0.317       0.0033\n",
            "     77     6      0.00751      0.00749     1.19e-05       0.0502        0.067        0.038       0.0744       0.0562       0.0488       0.0932        0.071        0.251      0.00261\n",
            "     77     7      0.00633      0.00632     1.03e-05       0.0461       0.0615       0.0358       0.0667       0.0513       0.0456       0.0847       0.0652        0.233      0.00243\n",
            "     77     8      0.00695      0.00694     3.11e-06       0.0489       0.0645        0.038       0.0705       0.0543       0.0485       0.0882       0.0683        0.129      0.00135\n",
            "     77     9      0.00783      0.00782      5.7e-06       0.0521       0.0684        0.041       0.0742       0.0576       0.0518       0.0932       0.0725        0.164      0.00171\n",
            "     77    10      0.00828      0.00827     5.99e-06       0.0527       0.0704       0.0404       0.0773       0.0589       0.0512        0.098       0.0746        0.174      0.00181\n",
            "     77    11      0.00766      0.00765     4.21e-06       0.0512       0.0677       0.0418         0.07       0.0559       0.0538       0.0891       0.0715        0.137      0.00143\n",
            "     77    12      0.00705      0.00705     4.57e-06       0.0496       0.0649       0.0394         0.07       0.0547       0.0498       0.0877       0.0688         0.14      0.00146\n",
            "     77    13      0.00631      0.00629      2.1e-05       0.0461       0.0613        0.036       0.0664       0.0512       0.0468       0.0831        0.065        0.323      0.00337\n",
            "     77    14      0.00626      0.00625     1.32e-05       0.0465       0.0611        0.037       0.0656       0.0513        0.047       0.0825       0.0647        0.244      0.00254\n",
            "     77    15      0.00747      0.00746     1.01e-05       0.0502       0.0668       0.0404       0.0697        0.055       0.0522       0.0892       0.0707        0.212      0.00221\n",
            "     77    16      0.00728      0.00728     1.96e-06       0.0507        0.066       0.0419       0.0684       0.0551       0.0527       0.0867       0.0697       0.0865     0.000901\n",
            "     77    17      0.00642      0.00642     6.57e-06       0.0479        0.062       0.0383        0.067       0.0527       0.0481       0.0831       0.0656        0.173       0.0018\n",
            "     77    18      0.00613      0.00613     1.63e-06       0.0461       0.0606       0.0375       0.0632       0.0504       0.0467       0.0815       0.0641       0.0896     0.000934\n",
            "     77    19      0.00806      0.00806     3.23e-06       0.0528       0.0695       0.0431       0.0722       0.0577       0.0553       0.0914       0.0734        0.117      0.00122\n",
            "     77    20       0.0072       0.0072     3.41e-06       0.0491       0.0656       0.0365       0.0745       0.0555       0.0462        0.093       0.0696        0.133      0.00139\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     77     1      0.00754      0.00753      2.6e-06       0.0503       0.0671       0.0392       0.0725       0.0559       0.0506       0.0916       0.0711        0.097      0.00101\n",
            "     77     2      0.00754      0.00754     1.23e-06       0.0499       0.0672       0.0384       0.0727       0.0556       0.0498       0.0926       0.0712       0.0569     0.000593\n",
            "     77     3      0.00713      0.00712     1.36e-06       0.0486       0.0653       0.0375       0.0707       0.0541        0.048       0.0905       0.0692       0.0682      0.00071\n",
            "     77     4      0.00734      0.00734     1.48e-06        0.049       0.0663       0.0379       0.0714       0.0546       0.0485       0.0921       0.0703       0.0783     0.000816\n",
            "     77     5      0.00681       0.0068     1.12e-06        0.048       0.0638       0.0379       0.0683       0.0531        0.049       0.0861       0.0676       0.0528      0.00055\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              77  275.126    0.005      0.00704     6.83e-06      0.00705        0.049       0.0649       0.0387       0.0695       0.0541       0.0493       0.0882       0.0688        0.166      0.00173\n",
            "! Validation         77  275.126    0.005      0.00727     1.56e-06      0.00727       0.0492       0.0659       0.0382       0.0711       0.0546       0.0492       0.0906       0.0699       0.0706     0.000736\n",
            "Wall time: 275.12681803400005\n",
            "! Best model       77    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     78     1      0.00656      0.00656     5.25e-06       0.0474       0.0627       0.0378       0.0667       0.0523       0.0475       0.0852       0.0664        0.124      0.00129\n",
            "     78     2       0.0083      0.00828     1.45e-05       0.0541       0.0704       0.0436       0.0753       0.0594       0.0541        0.095       0.0745        0.277      0.00289\n",
            "     78     3      0.00854      0.00853     1.32e-05       0.0546       0.0714        0.044       0.0759       0.0599       0.0557       0.0954       0.0756        0.269      0.00281\n",
            "     78     4      0.00744      0.00744     4.07e-07        0.051       0.0667       0.0424       0.0683       0.0554        0.054       0.0867       0.0704       0.0412     0.000429\n",
            "     78     5      0.00683      0.00682     9.09e-06       0.0479       0.0639       0.0373       0.0691       0.0532       0.0477       0.0878       0.0677        0.218      0.00227\n",
            "     78     6      0.00856      0.00856      5.3e-07       0.0552       0.0716       0.0475       0.0708       0.0591       0.0595       0.0911       0.0753       0.0363     0.000378\n",
            "     78     7      0.00764      0.00764     2.21e-06       0.0517       0.0676       0.0417       0.0716       0.0567       0.0532       0.0898       0.0715        0.104      0.00109\n",
            "     78     8      0.00803      0.00803     5.63e-07       0.0524       0.0693       0.0423       0.0724       0.0574       0.0543       0.0923       0.0733       0.0467     0.000486\n",
            "     78     9      0.00682      0.00682     1.45e-06       0.0487       0.0639       0.0403       0.0656       0.0529       0.0512       0.0838       0.0675       0.0545     0.000568\n",
            "     78    10      0.00668      0.00668     7.75e-07       0.0478       0.0632       0.0373       0.0686        0.053       0.0473       0.0866        0.067       0.0598     0.000623\n",
            "     78    11      0.00616      0.00616     3.44e-06       0.0448       0.0607       0.0341       0.0661       0.0501       0.0428       0.0859       0.0644        0.128      0.00133\n",
            "     78    12      0.00773      0.00772     2.68e-06       0.0507        0.068       0.0389       0.0742       0.0566         0.05       0.0942       0.0721       0.0896     0.000934\n",
            "     78    13      0.00804        0.008     3.88e-05       0.0522       0.0692       0.0421       0.0724       0.0572       0.0544       0.0919       0.0731        0.456      0.00475\n",
            "     78    14      0.00716      0.00715     1.03e-05       0.0494       0.0654       0.0397       0.0689       0.0543       0.0502       0.0884       0.0693        0.229      0.00239\n",
            "     78    15      0.00699      0.00691     7.39e-05       0.0482       0.0643       0.0369       0.0706       0.0538       0.0463       0.0901       0.0682        0.636      0.00663\n",
            "     78    16      0.00886      0.00885     1.17e-05       0.0552       0.0728       0.0483        0.069       0.0587       0.0621       0.0904       0.0763        0.227      0.00236\n",
            "     78    17      0.00751      0.00747     3.84e-05       0.0507       0.0669       0.0405        0.071       0.0558       0.0524        0.089       0.0707        0.454      0.00473\n",
            "     78    18      0.00663      0.00661     1.52e-05       0.0479       0.0629       0.0386       0.0666       0.0526       0.0484       0.0848       0.0666        0.279       0.0029\n",
            "     78    19      0.00718      0.00714     4.08e-05       0.0509       0.0654       0.0417       0.0693       0.0555       0.0526       0.0854        0.069        0.469      0.00489\n",
            "     78    20       0.0092      0.00915     4.71e-05       0.0556        0.074       0.0435       0.0798       0.0616       0.0551        0.102       0.0785        0.502      0.00523\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     78     1      0.00749      0.00749     2.59e-06       0.0502        0.067       0.0391       0.0724       0.0557       0.0504       0.0915       0.0709       0.0957     0.000997\n",
            "     78     2      0.00749      0.00749     1.18e-06       0.0497        0.067       0.0383       0.0726       0.0554       0.0496       0.0924        0.071       0.0554     0.000577\n",
            "     78     3      0.00709      0.00709     1.44e-06       0.0484       0.0651       0.0373       0.0706        0.054       0.0478       0.0903       0.0691       0.0707     0.000736\n",
            "     78     4       0.0073       0.0073     1.47e-06       0.0489       0.0661       0.0377       0.0712       0.0545       0.0483       0.0919       0.0701        0.077     0.000802\n",
            "     78     5      0.00677      0.00677     1.06e-06       0.0479       0.0637       0.0377       0.0682       0.0529       0.0488       0.0859       0.0674       0.0519      0.00054\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              78  278.668    0.005      0.00753     1.65e-05      0.00754       0.0508       0.0671       0.0409       0.0706       0.0558       0.0521       0.0899        0.071        0.235      0.00245\n",
            "! Validation         78  278.668    0.005      0.00723     1.55e-06      0.00723        0.049       0.0658        0.038        0.071       0.0545        0.049       0.0904       0.0697       0.0701      0.00073\n",
            "Wall time: 278.6689001079999\n",
            "! Best model       78    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     79     1      0.00878      0.00871     6.48e-05       0.0543       0.0722       0.0447       0.0737       0.0592       0.0565       0.0962       0.0764        0.596      0.00621\n",
            "     79     2      0.00735      0.00733     1.91e-05       0.0498       0.0662       0.0389       0.0717       0.0553       0.0492       0.0913       0.0702        0.322      0.00335\n",
            "     79     3      0.00768      0.00764      4.2e-05       0.0503       0.0676       0.0381       0.0748       0.0564       0.0489       0.0945       0.0717        0.474      0.00493\n",
            "     79     4      0.00821      0.00821     2.58e-06       0.0543       0.0701       0.0455       0.0719       0.0587       0.0572       0.0906       0.0739        0.098      0.00102\n",
            "     79     5      0.00746      0.00744     2.07e-05       0.0517       0.0667       0.0427       0.0696       0.0562       0.0539        0.087       0.0704        0.323      0.00337\n",
            "     79     6      0.00751       0.0075     1.02e-05       0.0504        0.067       0.0394       0.0725       0.0559       0.0495       0.0926        0.071        0.231      0.00241\n",
            "     79     7      0.00708      0.00705     3.58e-05       0.0494        0.065       0.0407        0.067       0.0538       0.0525       0.0845       0.0685        0.433      0.00451\n",
            "     79     8      0.00719      0.00715     3.31e-05       0.0484       0.0654       0.0373       0.0706        0.054       0.0477       0.0911       0.0694        0.421      0.00438\n",
            "     79     9      0.00734      0.00729     5.08e-05       0.0488       0.0661       0.0372       0.0718       0.0545       0.0487       0.0913         0.07        0.526      0.00548\n",
            "     79    10      0.00674      0.00667      6.3e-05       0.0476       0.0632       0.0371       0.0686       0.0529        0.047        0.087        0.067        0.586       0.0061\n",
            "     79    11      0.00766      0.00763     3.62e-05       0.0508       0.0676       0.0397       0.0731       0.0564       0.0511       0.0921       0.0716         0.44      0.00458\n",
            "     79    12      0.00854      0.00852      2.3e-05       0.0561       0.0714         0.05       0.0684       0.0592       0.0621       0.0872       0.0746        0.353      0.00367\n",
            "     79    13      0.00774      0.00774      1.3e-06       0.0531       0.0681       0.0467       0.0661       0.0564       0.0583       0.0843       0.0713        0.059     0.000614\n",
            "     79    14      0.00679      0.00677     1.94e-05        0.048       0.0637       0.0367       0.0707       0.0537       0.0467       0.0883       0.0675        0.318      0.00332\n",
            "     79    15      0.00852      0.00852     5.65e-07       0.0551       0.0714       0.0467        0.072       0.0593       0.0589       0.0915       0.0752       0.0484     0.000505\n",
            "     79    16      0.00801      0.00797     3.99e-05       0.0518       0.0691       0.0419       0.0716       0.0567       0.0528       0.0935       0.0731        0.466      0.00486\n",
            "     79    17      0.00683      0.00683     1.06e-06       0.0472       0.0639       0.0362       0.0692       0.0527        0.047       0.0885       0.0678       0.0742     0.000773\n",
            "     79    18       0.0084      0.00834     6.15e-05       0.0544       0.0706       0.0436        0.076       0.0598       0.0553       0.0941       0.0747        0.577      0.00601\n",
            "     79    19      0.00912      0.00912     2.41e-06       0.0569       0.0739       0.0475       0.0757       0.0616       0.0591       0.0969        0.078        0.103      0.00108\n",
            "     79    20      0.00719      0.00718     5.57e-06       0.0501       0.0656       0.0411       0.0681       0.0546       0.0519       0.0867       0.0693        0.161      0.00168\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     79     1      0.00745      0.00745     2.56e-06         0.05       0.0668       0.0389       0.0722       0.0556       0.0502       0.0913       0.0708       0.0952     0.000992\n",
            "     79     2      0.00745      0.00745     1.23e-06       0.0496       0.0668       0.0382       0.0724       0.0553       0.0494       0.0922       0.0708       0.0566      0.00059\n",
            "     79     3      0.00706      0.00706     1.39e-06       0.0483        0.065       0.0372       0.0705       0.0539       0.0476       0.0902       0.0689         0.07     0.000729\n",
            "     79     4      0.00727      0.00727     1.51e-06       0.0488       0.0659       0.0376       0.0712       0.0544       0.0481       0.0918       0.0699       0.0779     0.000812\n",
            "     79     5      0.00673      0.00673     1.03e-06       0.0477       0.0635       0.0376       0.0681       0.0528       0.0486       0.0858       0.0672       0.0499      0.00052\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              79  282.229    0.005      0.00768     2.66e-05      0.00771       0.0514       0.0678       0.0416       0.0712       0.0564       0.0529       0.0905       0.0717         0.33      0.00344\n",
            "! Validation         79  282.229    0.005      0.00719     1.54e-06      0.00719       0.0489       0.0656       0.0379       0.0709       0.0544       0.0488       0.0903       0.0695       0.0699     0.000729\n",
            "Wall time: 282.23021695400007\n",
            "! Best model       79    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     80     1      0.00652       0.0065     2.01e-05       0.0471       0.0624       0.0368       0.0675       0.0522       0.0463       0.0859       0.0661        0.328      0.00342\n",
            "     80     2      0.00776      0.00775     9.01e-06       0.0516       0.0681       0.0404        0.074       0.0572       0.0512       0.0931       0.0722        0.197      0.00206\n",
            "     80     3      0.00845      0.00843     1.85e-05       0.0543        0.071        0.045       0.0729        0.059       0.0568       0.0932        0.075        0.308      0.00321\n",
            "     80     4      0.00673      0.00673     2.63e-07       0.0486       0.0635       0.0393       0.0674       0.0533       0.0496       0.0846       0.0671       0.0307     0.000319\n",
            "     80     5      0.00669      0.00668     2.44e-06       0.0479       0.0633       0.0384       0.0669       0.0527       0.0483       0.0857        0.067        0.109      0.00113\n",
            "     80     6      0.00577      0.00577     2.28e-06       0.0438       0.0588       0.0336       0.0642       0.0489       0.0434       0.0812       0.0623        0.106      0.00111\n",
            "     80     7      0.00646      0.00646     2.26e-06       0.0473       0.0622       0.0371       0.0677       0.0524       0.0468        0.085       0.0659        0.103      0.00107\n",
            "     80     8      0.00664      0.00664        1e-06       0.0467        0.063       0.0361       0.0679        0.052       0.0473       0.0863       0.0668       0.0678     0.000706\n",
            "     80     9      0.00669      0.00669      1.5e-06       0.0472       0.0633       0.0355       0.0706        0.053        0.046       0.0882       0.0671       0.0584     0.000608\n",
            "     80    10      0.00662      0.00662     1.21e-06       0.0472       0.0629       0.0359       0.0699       0.0529       0.0455        0.088       0.0667       0.0801     0.000834\n",
            "     80    11      0.00619      0.00619     4.57e-06       0.0451       0.0609       0.0335       0.0681       0.0508        0.042       0.0871       0.0645        0.146      0.00152\n",
            "     80    12      0.00711      0.00711     3.37e-06       0.0485       0.0652       0.0375       0.0705        0.054       0.0486       0.0897       0.0691        0.112      0.00116\n",
            "     80    13      0.00643      0.00642     4.12e-06       0.0466        0.062       0.0367       0.0664       0.0516       0.0474       0.0839       0.0657        0.111      0.00116\n",
            "     80    14      0.00745      0.00745      3.7e-07       0.0481       0.0668       0.0361       0.0721       0.0541        0.048       0.0936       0.0708       0.0354     0.000368\n",
            "     80    15      0.00778      0.00778     2.23e-06       0.0508       0.0682        0.038       0.0763       0.0572       0.0495       0.0952       0.0724       0.0926     0.000964\n",
            "     80    16      0.00705      0.00704      6.2e-06       0.0496       0.0649       0.0388       0.0712        0.055       0.0486        0.089       0.0688        0.163      0.00169\n",
            "     80    17      0.00639      0.00638     1.07e-05       0.0472       0.0618       0.0391       0.0634       0.0512       0.0499       0.0805       0.0652        0.236      0.00246\n",
            "     80    18      0.00755      0.00754     7.09e-06       0.0517       0.0672       0.0437       0.0677       0.0557       0.0557       0.0857       0.0707        0.186      0.00193\n",
            "     80    19      0.00781      0.00781     4.43e-06       0.0498       0.0684       0.0365       0.0764       0.0565       0.0468       0.0982       0.0725        0.153      0.00159\n",
            "     80    20      0.00887      0.00883     3.93e-05        0.055       0.0727       0.0434       0.0784       0.0609       0.0551        0.099        0.077         0.46      0.00479\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     80     1      0.00741      0.00741     2.63e-06       0.0499       0.0666       0.0388        0.072       0.0554         0.05       0.0911       0.0706       0.0962        0.001\n",
            "     80     2      0.00742      0.00742     1.22e-06       0.0495       0.0666        0.038       0.0723       0.0552       0.0492        0.092       0.0706       0.0564     0.000588\n",
            "     80     3      0.00703      0.00703     1.39e-06       0.0482       0.0649       0.0371       0.0704       0.0538       0.0475       0.0901       0.0688       0.0695     0.000724\n",
            "     80     4      0.00723      0.00723     1.48e-06       0.0486       0.0658       0.0375        0.071       0.0542       0.0479       0.0916       0.0698        0.078     0.000813\n",
            "     80     5       0.0067       0.0067     1.07e-06       0.0476       0.0633       0.0374        0.068       0.0527       0.0484       0.0857       0.0671       0.0521     0.000543\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              80  285.773    0.005      0.00704     7.05e-06      0.00705       0.0487       0.0649       0.0381         0.07        0.054       0.0488       0.0888       0.0688        0.154      0.00161\n",
            "! Validation         80  285.773    0.005      0.00716     1.56e-06      0.00716       0.0488       0.0654       0.0378       0.0707       0.0543       0.0486       0.0901       0.0694       0.0705     0.000734\n",
            "Wall time: 285.7736752139999\n",
            "! Best model       80    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     81     1      0.00722      0.00722      6.3e-07       0.0503       0.0657       0.0407       0.0695       0.0551       0.0517       0.0873       0.0695       0.0576       0.0006\n",
            "     81     2      0.00644      0.00644     3.71e-06       0.0476       0.0621       0.0377       0.0674       0.0526       0.0473       0.0841       0.0657        0.128      0.00133\n",
            "     81     3      0.00631      0.00631     2.28e-06       0.0468       0.0614        0.038       0.0644       0.0512       0.0481       0.0818        0.065       0.0846     0.000881\n",
            "     81     4      0.00631      0.00631     3.75e-06       0.0474       0.0614       0.0393       0.0636       0.0515       0.0494       0.0802       0.0648         0.12      0.00125\n",
            "     81     5      0.00669      0.00668     5.15e-06       0.0474       0.0632       0.0356       0.0711       0.0533       0.0448       0.0894       0.0671        0.154      0.00161\n",
            "     81     6      0.00699      0.00699     4.89e-07       0.0497       0.0647       0.0405       0.0681       0.0543       0.0503       0.0866       0.0684       0.0516     0.000537\n",
            "     81     7      0.00729      0.00728     1.03e-05       0.0492        0.066       0.0389         0.07       0.0544       0.0503       0.0896       0.0699        0.234      0.00243\n",
            "     81     8      0.00634      0.00633     9.71e-06       0.0466       0.0615       0.0354       0.0691       0.0522       0.0452       0.0853       0.0653        0.226      0.00236\n",
            "     81     9      0.00588      0.00586     1.49e-05       0.0445       0.0592       0.0351       0.0633       0.0492       0.0444       0.0811       0.0628        0.283      0.00294\n",
            "     81    10      0.00803      0.00803     4.31e-06       0.0516       0.0693       0.0404        0.074       0.0572       0.0524       0.0945       0.0734        0.152      0.00158\n",
            "     81    11      0.00837      0.00837     1.71e-06       0.0535       0.0708       0.0435       0.0736       0.0586       0.0562       0.0933       0.0748        0.082     0.000854\n",
            "     81    12      0.00666      0.00666     1.91e-06       0.0473       0.0631       0.0377       0.0665       0.0521       0.0478       0.0859       0.0669       0.0781     0.000814\n",
            "     81    13      0.00668      0.00668     3.15e-06       0.0481       0.0632       0.0396       0.0651       0.0523       0.0498       0.0838       0.0668        0.117      0.00122\n",
            "     81    14      0.00733      0.00731     1.92e-05       0.0512       0.0661       0.0412       0.0711       0.0562       0.0512       0.0888         0.07         0.32      0.00333\n",
            "     81    15      0.00871      0.00871     1.16e-06       0.0549       0.0722       0.0428       0.0791        0.061       0.0544       0.0986       0.0765       0.0729     0.000759\n",
            "     81    16      0.00933      0.00925     7.72e-05       0.0571       0.0744       0.0461        0.079       0.0626       0.0575          0.1       0.0788        0.647      0.00674\n",
            "     81    17      0.00835      0.00834     5.55e-06       0.0543       0.0707       0.0442       0.0745       0.0593        0.056       0.0933       0.0747        0.163       0.0017\n",
            "     81    18      0.00715      0.00712     2.71e-05       0.0492       0.0653       0.0392       0.0693       0.0542       0.0499       0.0883       0.0691        0.381      0.00397\n",
            "     81    19      0.00811      0.00811     2.72e-06       0.0528       0.0697       0.0426       0.0732       0.0579       0.0544       0.0929       0.0737        0.108      0.00112\n",
            "     81    20      0.00875      0.00873      2.2e-05       0.0558       0.0723        0.046       0.0754       0.0607        0.058       0.0946       0.0763        0.334      0.00348\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     81     1      0.00738      0.00738     2.56e-06       0.0497       0.0664       0.0387       0.0719       0.0553       0.0498        0.091       0.0704       0.0951     0.000991\n",
            "     81     2      0.00737      0.00737     1.14e-06       0.0493       0.0664       0.0379       0.0722        0.055        0.049       0.0918       0.0704       0.0555     0.000578\n",
            "     81     3        0.007        0.007     1.35e-06       0.0481       0.0647        0.037       0.0703       0.0536       0.0473       0.0899       0.0686       0.0698     0.000727\n",
            "     81     4      0.00719      0.00719     1.52e-06       0.0485       0.0656       0.0373       0.0709       0.0541       0.0477       0.0915       0.0696       0.0791     0.000824\n",
            "     81     5      0.00667      0.00667     1.02e-06       0.0475       0.0632       0.0373       0.0678       0.0526       0.0482       0.0856       0.0669       0.0504     0.000525\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              81  289.268    0.005      0.00734     1.08e-05      0.00735       0.0503       0.0663       0.0402       0.0704       0.0553       0.0511       0.0891       0.0701         0.19      0.00198\n",
            "! Validation         81  289.268    0.005      0.00712     1.52e-06      0.00712       0.0486       0.0653       0.0376       0.0706       0.0541       0.0484         0.09       0.0692         0.07     0.000729\n",
            "Wall time: 289.268795316\n",
            "! Best model       81    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     82     1      0.00939      0.00937      2.1e-05       0.0578       0.0749       0.0483       0.0768       0.0626       0.0606       0.0973        0.079        0.336       0.0035\n",
            "     82     2      0.00878      0.00871     7.27e-05        0.053       0.0722       0.0413       0.0766       0.0589       0.0535       0.0996       0.0765        0.633      0.00659\n",
            "     82     3      0.00619      0.00619     3.14e-06        0.046       0.0608       0.0353       0.0675       0.0514       0.0447       0.0843       0.0645         0.11      0.00115\n",
            "     82     4      0.00697      0.00695     2.26e-05       0.0485       0.0645       0.0378       0.0699       0.0539       0.0482       0.0885       0.0684        0.346      0.00361\n",
            "     82     5      0.00713      0.00713     8.39e-07       0.0492       0.0653       0.0403       0.0669       0.0536       0.0523       0.0856       0.0689       0.0637     0.000663\n",
            "     82     6      0.00661      0.00661     1.64e-06       0.0477       0.0629       0.0373       0.0686       0.0529        0.047       0.0864       0.0667       0.0836     0.000871\n",
            "     82     7      0.00629      0.00629      8.9e-07       0.0455       0.0614       0.0355       0.0657       0.0506       0.0462       0.0838        0.065       0.0607     0.000633\n",
            "     82     8      0.00698      0.00698     2.81e-06       0.0484       0.0646       0.0387       0.0677       0.0532         0.05       0.0868       0.0684       0.0922      0.00096\n",
            "     82     9      0.00798      0.00798     3.94e-07       0.0508       0.0691       0.0397        0.073       0.0563       0.0507       0.0958       0.0733       0.0418     0.000435\n",
            "     82    10      0.00646      0.00644     2.33e-05       0.0465       0.0621       0.0351       0.0692       0.0522       0.0446       0.0871       0.0658        0.348      0.00363\n",
            "     82    11      0.00648      0.00648     1.97e-06       0.0464       0.0623       0.0343       0.0706       0.0524       0.0434       0.0887       0.0661       0.0938     0.000977\n",
            "     82    12      0.00788      0.00783     5.34e-05       0.0512       0.0684       0.0398       0.0742        0.057       0.0506       0.0945       0.0726        0.535      0.00557\n",
            "     82    13      0.00683      0.00683     2.88e-06       0.0495       0.0639       0.0412       0.0661       0.0537       0.0519       0.0829       0.0674        0.121      0.00126\n",
            "     82    14      0.00678      0.00676     2.24e-05       0.0487       0.0636       0.0379       0.0701        0.054       0.0471       0.0877       0.0674        0.345      0.00359\n",
            "     82    15      0.00608      0.00608     2.38e-07       0.0452       0.0603        0.035       0.0656       0.0503       0.0447       0.0832       0.0639       0.0281     0.000293\n",
            "     82    16      0.00678      0.00677     1.38e-05       0.0486       0.0636       0.0391       0.0678       0.0534       0.0497        0.085       0.0673        0.265      0.00276\n",
            "     82    17      0.00622      0.00621     7.58e-06       0.0463        0.061       0.0367       0.0654       0.0511       0.0471        0.082       0.0645        0.176      0.00184\n",
            "     82    18      0.00655      0.00654     1.33e-05       0.0479       0.0626       0.0388        0.066       0.0524       0.0486       0.0838       0.0662        0.268       0.0028\n",
            "     82    19      0.00802      0.00801     1.44e-05       0.0521       0.0692        0.042       0.0722       0.0571       0.0534       0.0932       0.0733        0.254      0.00265\n",
            "     82    20      0.00872       0.0087     1.39e-05       0.0543       0.0722        0.042       0.0791       0.0605       0.0528          0.1       0.0765        0.276      0.00287\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     82     1      0.00734      0.00734     2.66e-06       0.0496       0.0663       0.0385       0.0718       0.0552       0.0497       0.0908       0.0702       0.0973      0.00101\n",
            "     82     2      0.00734      0.00734     1.15e-06       0.0492       0.0663       0.0378       0.0721       0.0549       0.0489       0.0917       0.0703       0.0542     0.000565\n",
            "     82     3      0.00697      0.00697     1.36e-06        0.048       0.0646       0.0368       0.0702       0.0535       0.0471       0.0898       0.0685       0.0696     0.000725\n",
            "     82     4      0.00716      0.00716     1.54e-06       0.0484       0.0654       0.0372       0.0708        0.054       0.0475       0.0913       0.0694       0.0795     0.000828\n",
            "     82     5      0.00664      0.00664     1.01e-06       0.0473        0.063       0.0372       0.0677       0.0524        0.048       0.0854       0.0667         0.05     0.000521\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              82  292.730    0.005      0.00714     1.47e-05      0.00716       0.0492       0.0654       0.0388         0.07       0.0544       0.0495        0.089       0.0693        0.224      0.00233\n",
            "! Validation         82  292.730    0.005      0.00709     1.54e-06      0.00709       0.0485       0.0651       0.0375       0.0705        0.054       0.0482       0.0898        0.069       0.0701      0.00073\n",
            "Wall time: 292.73058872799993\n",
            "! Best model       82    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     83     1        0.007        0.007     3.63e-07       0.0479       0.0647       0.0376       0.0684        0.053       0.0479       0.0893       0.0686       0.0418     0.000435\n",
            "     83     2      0.00695      0.00695     8.99e-06       0.0491       0.0645        0.038       0.0714       0.0547       0.0473       0.0894       0.0684        0.216      0.00225\n",
            "     83     3      0.00829      0.00826     3.27e-05       0.0552       0.0703       0.0465       0.0726       0.0596       0.0582       0.0898        0.074        0.419      0.00437\n",
            "     83     4       0.0102       0.0102     4.57e-06       0.0626       0.0781       0.0585       0.0708       0.0647       0.0725       0.0882       0.0803        0.133      0.00139\n",
            "     83     5        0.008        0.008     1.22e-06       0.0534       0.0692       0.0462        0.068       0.0571       0.0583       0.0869       0.0726       0.0625     0.000651\n",
            "     83     6      0.00795      0.00789     6.02e-05       0.0518       0.0687       0.0405       0.0745       0.0575        0.052       0.0936       0.0728        0.575      0.00599\n",
            "     83     7      0.00699      0.00698     5.32e-06       0.0487       0.0646       0.0376        0.071       0.0543       0.0482       0.0888       0.0685        0.154       0.0016\n",
            "     83     8      0.00801        0.008     1.45e-05       0.0523       0.0692       0.0403       0.0764       0.0584       0.0503       0.0964       0.0734        0.279      0.00291\n",
            "     83     9      0.00788      0.00787     5.24e-06       0.0518       0.0686        0.042       0.0714       0.0567       0.0534       0.0918       0.0726        0.148      0.00154\n",
            "     83    10      0.00759      0.00758      5.8e-06       0.0511       0.0674       0.0413       0.0707        0.056       0.0519       0.0907       0.0713        0.173      0.00181\n",
            "     83    11      0.00706      0.00703      2.7e-05       0.0496       0.0649       0.0392       0.0704       0.0548       0.0486       0.0888       0.0687         0.38      0.00396\n",
            "     83    12      0.00976      0.00976     1.31e-06       0.0593       0.0764       0.0514       0.0751       0.0632       0.0639       0.0967       0.0803       0.0773     0.000806\n",
            "     83    13       0.0103       0.0103     1.49e-06       0.0608       0.0786       0.0515       0.0795       0.0655       0.0647        0.101       0.0828       0.0699     0.000728\n",
            "     83    14       0.0087      0.00867      3.4e-05       0.0538        0.072       0.0417        0.078       0.0598       0.0544       0.0983       0.0763        0.429      0.00447\n",
            "     83    15      0.00696      0.00695     9.66e-06       0.0481       0.0645       0.0379       0.0685       0.0532       0.0478       0.0889       0.0683         0.23      0.00239\n",
            "     83    16      0.00674      0.00674     1.53e-06       0.0485       0.0635       0.0388       0.0679       0.0533       0.0484       0.0861       0.0673       0.0809     0.000842\n",
            "     83    17      0.00804      0.00804     1.04e-06        0.053       0.0694       0.0429        0.073        0.058       0.0548       0.0918       0.0733       0.0582     0.000606\n",
            "     83    18       0.0071      0.00705      4.8e-05       0.0508        0.065       0.0441       0.0642       0.0542        0.055       0.0813       0.0682        0.509       0.0053\n",
            "     83    19      0.00651      0.00649     1.86e-05       0.0468       0.0623       0.0361        0.068       0.0521       0.0466       0.0855        0.066        0.313      0.00326\n",
            "     83    20      0.00788      0.00785     3.58e-05       0.0526       0.0685        0.042       0.0737       0.0578       0.0536       0.0913       0.0725        0.442       0.0046\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     83     1      0.00731      0.00731     2.58e-06       0.0495       0.0661       0.0384       0.0717       0.0551       0.0495       0.0907       0.0701       0.0949     0.000989\n",
            "     83     2      0.00731      0.00731      1.2e-06       0.0491       0.0661       0.0377       0.0719       0.0548       0.0487       0.0915       0.0701       0.0555     0.000578\n",
            "     83     3      0.00694      0.00694     1.39e-06       0.0478       0.0644       0.0367       0.0701       0.0534        0.047       0.0897       0.0683       0.0701      0.00073\n",
            "     83     4      0.00713      0.00713     1.55e-06       0.0483       0.0653        0.037       0.0707       0.0539       0.0473       0.0912       0.0693       0.0794     0.000827\n",
            "     83     5      0.00661       0.0066     1.02e-06       0.0472       0.0629        0.037       0.0676       0.0523       0.0478       0.0853       0.0666       0.0518     0.000539\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              83  296.176    0.005      0.00788     1.59e-05       0.0079       0.0524       0.0687       0.0427       0.0717       0.0572       0.0543       0.0908       0.0726         0.24       0.0025\n",
            "! Validation         83  296.176    0.005      0.00706     1.55e-06      0.00706       0.0484        0.065       0.0374       0.0704       0.0539       0.0481       0.0897       0.0689       0.0703     0.000733\n",
            "Wall time: 296.177307705\n",
            "! Best model       83    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     84     1      0.00744      0.00741     2.61e-05       0.0502       0.0666       0.0407       0.0693        0.055       0.0517       0.0892       0.0705        0.373      0.00388\n",
            "     84     2      0.00705      0.00705     1.58e-06       0.0482        0.065       0.0375       0.0696       0.0535       0.0482       0.0895       0.0688       0.0795     0.000828\n",
            "     84     3      0.00681      0.00679     2.46e-05       0.0486       0.0637       0.0397       0.0662        0.053       0.0503       0.0845       0.0674        0.363      0.00378\n",
            "     84     4      0.00717      0.00717     2.43e-07       0.0496       0.0655       0.0392       0.0702       0.0547       0.0498        0.089       0.0694       0.0299     0.000311\n",
            "     84     5      0.00854      0.00848     5.68e-05       0.0527       0.0713       0.0395       0.0791       0.0593       0.0512          0.1       0.0756        0.546      0.00569\n",
            "     84     6      0.00757      0.00755     1.88e-05       0.0522       0.0672        0.044       0.0685       0.0563       0.0555        0.086       0.0707        0.317       0.0033\n",
            "     84     7      0.00706      0.00702     3.84e-05       0.0486       0.0648       0.0378       0.0702        0.054       0.0482       0.0892       0.0687        0.453      0.00472\n",
            "     84     8       0.0061      0.00609     8.32e-06       0.0448       0.0604       0.0346       0.0651       0.0498       0.0444       0.0836        0.064        0.192        0.002\n",
            "     84     9      0.00693       0.0069     3.53e-05       0.0495       0.0643       0.0419       0.0646       0.0533       0.0521       0.0834       0.0678        0.439      0.00457\n",
            "     84    10      0.00739      0.00739     1.44e-06       0.0504       0.0665       0.0394       0.0724       0.0559       0.0499        0.091       0.0705        0.077     0.000802\n",
            "     84    11      0.00724      0.00718     6.04e-05       0.0496       0.0656       0.0388       0.0711        0.055       0.0492       0.0898       0.0695        0.576        0.006\n",
            "     84    12       0.0057      0.00569     4.04e-06       0.0445       0.0584       0.0354       0.0626        0.049       0.0447       0.0789       0.0618        0.126      0.00131\n",
            "     84    13      0.00812      0.00807      4.6e-05       0.0525       0.0695       0.0414       0.0749       0.0581       0.0519       0.0954       0.0737         0.49       0.0051\n",
            "     84    14      0.00854      0.00853     5.14e-06       0.0561       0.0715       0.0471        0.074       0.0605       0.0586        0.092       0.0753        0.154       0.0016\n",
            "     84    15      0.00737      0.00737     2.73e-06       0.0516       0.0664       0.0428       0.0693       0.0561       0.0539       0.0861         0.07        0.113      0.00118\n",
            "     84    16      0.00654      0.00651     3.04e-05       0.0467       0.0624       0.0361        0.068       0.0521       0.0467       0.0856       0.0661        0.401      0.00417\n",
            "     84    17      0.00592      0.00592     3.33e-06       0.0448       0.0595       0.0347       0.0651       0.0499       0.0435       0.0827       0.0631        0.126      0.00131\n",
            "     84    18       0.0074      0.00738     2.83e-05       0.0498       0.0664        0.039       0.0713       0.0552       0.0501       0.0907       0.0704        0.384        0.004\n",
            "     84    19      0.00739      0.00738     1.41e-05       0.0494       0.0665       0.0399       0.0686       0.0542        0.051       0.0897       0.0704        0.275      0.00287\n",
            "     84    20      0.00719      0.00718     3.39e-06        0.049       0.0656       0.0384       0.0703       0.0544       0.0492       0.0898       0.0695         0.13      0.00136\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     84     1      0.00728      0.00728     2.53e-06       0.0494        0.066       0.0383       0.0716       0.0549       0.0493       0.0905       0.0699       0.0946     0.000986\n",
            "     84     2      0.00727      0.00727     1.17e-06        0.049        0.066       0.0375       0.0718       0.0547       0.0485       0.0913       0.0699       0.0547      0.00057\n",
            "     84     3      0.00691      0.00691     1.37e-06       0.0477       0.0643       0.0366         0.07       0.0533       0.0468       0.0896       0.0682       0.0704     0.000733\n",
            "     84     4       0.0071       0.0071      1.5e-06       0.0482       0.0652       0.0369       0.0706       0.0538       0.0472       0.0911       0.0691       0.0781     0.000814\n",
            "     84     5      0.00657      0.00657     9.81e-07       0.0471       0.0627       0.0369       0.0675       0.0522       0.0477       0.0852       0.0664       0.0492     0.000513\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              84  299.640    0.005      0.00715     2.05e-05      0.00717       0.0494       0.0654       0.0394       0.0695       0.0545       0.0501       0.0884       0.0693        0.282      0.00294\n",
            "! Validation         84  299.640    0.005      0.00703     1.51e-06      0.00703       0.0483       0.0648       0.0372       0.0703       0.0538       0.0479       0.0896       0.0687       0.0694     0.000723\n",
            "Wall time: 299.64123902899996\n",
            "! Best model       84    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     85     1      0.00635      0.00635     1.01e-06       0.0462       0.0617       0.0356       0.0675       0.0515        0.046       0.0847       0.0654       0.0711     0.000741\n",
            "     85     2      0.00691      0.00689     1.62e-05       0.0481       0.0642       0.0381       0.0683       0.0532       0.0489       0.0872        0.068        0.281      0.00292\n",
            "     85     3      0.00841      0.00839     1.51e-05       0.0533       0.0709       0.0423       0.0754       0.0588       0.0543       0.0958        0.075        0.278       0.0029\n",
            "     85     4      0.00663      0.00656     6.92e-05       0.0473       0.0627       0.0369       0.0679       0.0524       0.0469       0.0859       0.0664        0.616      0.00641\n",
            "     85     5      0.00581      0.00581      3.6e-06       0.0438        0.059       0.0336       0.0641       0.0488       0.0424       0.0827       0.0625        0.134      0.00139\n",
            "     85     6      0.00663      0.00661     1.31e-05       0.0467       0.0629        0.036       0.0682       0.0521       0.0452       0.0882       0.0667        0.265      0.00276\n",
            "     85     7      0.00723      0.00722     5.26e-06       0.0494       0.0657       0.0376       0.0729       0.0553       0.0471       0.0924       0.0697        0.169      0.00176\n",
            "     85     8      0.00652      0.00652      2.1e-06       0.0469       0.0625       0.0364       0.0677       0.0521       0.0464        0.086       0.0662       0.0969      0.00101\n",
            "     85     9      0.00592      0.00591     1.35e-05       0.0445       0.0595       0.0351       0.0634       0.0492       0.0455       0.0805        0.063         0.27      0.00281\n",
            "     85    10      0.00634      0.00633     1.44e-05       0.0468       0.0615       0.0371       0.0663       0.0517       0.0473        0.083       0.0651        0.261      0.00271\n",
            "     85    11      0.00729      0.00728     1.42e-05       0.0499        0.066         0.04       0.0696       0.0548       0.0506       0.0891       0.0699        0.276      0.00288\n",
            "     85    12      0.00601        0.006     1.16e-05       0.0454       0.0599       0.0354       0.0653       0.0504       0.0444       0.0826       0.0635        0.239      0.00249\n",
            "     85    13      0.00677      0.00677     6.88e-07       0.0471       0.0636       0.0367        0.068       0.0523       0.0473       0.0876       0.0675       0.0455     0.000474\n",
            "     85    14      0.00675      0.00674     1.25e-05       0.0472       0.0635       0.0365       0.0687       0.0526       0.0478       0.0868       0.0673        0.255      0.00266\n",
            "     85    15      0.00669      0.00667     1.34e-05       0.0482       0.0632       0.0386       0.0675        0.053       0.0485       0.0853       0.0669        0.255      0.00266\n",
            "     85    16      0.00645      0.00643      2.1e-05       0.0469        0.062       0.0364       0.0678       0.0521        0.046       0.0855       0.0658        0.337      0.00351\n",
            "     85    17      0.00684      0.00683     1.48e-05       0.0481       0.0639       0.0379       0.0686       0.0533       0.0482       0.0872       0.0677        0.279      0.00291\n",
            "     85    18      0.00704      0.00704     6.16e-07       0.0487       0.0649       0.0373       0.0717       0.0545       0.0477         0.09       0.0688       0.0498     0.000519\n",
            "     85    19      0.00666      0.00663     2.69e-05       0.0476        0.063       0.0372       0.0684       0.0528        0.048       0.0855       0.0667        0.379      0.00394\n",
            "     85    20      0.00708      0.00706     2.81e-05       0.0491        0.065       0.0374       0.0725       0.0549       0.0477       0.0901       0.0689        0.382      0.00398\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     85     1      0.00724      0.00724     2.47e-06       0.0493       0.0658       0.0382       0.0715       0.0548       0.0492       0.0904       0.0698       0.0925     0.000963\n",
            "     85     2      0.00724      0.00724     1.17e-06       0.0488       0.0658       0.0374       0.0717       0.0546       0.0484       0.0912       0.0698       0.0539     0.000562\n",
            "     85     3      0.00688      0.00688     1.34e-06       0.0476       0.0642       0.0365         0.07       0.0532       0.0466       0.0894        0.068       0.0693     0.000722\n",
            "     85     4      0.00707      0.00706     1.56e-06        0.048        0.065       0.0368       0.0705       0.0536        0.047       0.0909        0.069       0.0796     0.000829\n",
            "     85     5      0.00655      0.00655     9.67e-07        0.047       0.0626       0.0368       0.0674       0.0521       0.0475       0.0851       0.0663       0.0495     0.000516\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              85  303.112    0.005       0.0067     1.49e-05      0.00672       0.0476       0.0633       0.0371       0.0685       0.0528       0.0474       0.0869       0.0671        0.247      0.00257\n",
            "! Validation         85  303.112    0.005      0.00699      1.5e-06        0.007       0.0481       0.0647       0.0371       0.0702       0.0537       0.0477       0.0894       0.0686        0.069     0.000718\n",
            "Wall time: 303.11298261499996\n",
            "! Best model       85    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     86     1      0.00612      0.00608     3.45e-05       0.0454       0.0603       0.0348       0.0668       0.0508       0.0436       0.0844        0.064        0.434      0.00452\n",
            "     86     2      0.00653      0.00646     6.85e-05       0.0461       0.0622       0.0346        0.069       0.0518        0.044       0.0879       0.0659        0.611      0.00637\n",
            "     86     3      0.00747      0.00746     1.67e-05         0.05       0.0668       0.0398       0.0706       0.0552       0.0513       0.0901       0.0707        0.301      0.00314\n",
            "     86     4      0.00699      0.00694     4.74e-05       0.0488       0.0645       0.0389       0.0685       0.0537       0.0493       0.0872       0.0683        0.506      0.00527\n",
            "     86     5       0.0071       0.0071     1.83e-06       0.0495       0.0652       0.0385       0.0715        0.055       0.0488       0.0894       0.0691       0.0762     0.000793\n",
            "     86     6      0.00738      0.00734      3.6e-05       0.0488       0.0663       0.0374       0.0716       0.0545       0.0482       0.0924       0.0703        0.442      0.00461\n",
            "     86     7      0.00677      0.00676     7.96e-06       0.0476       0.0636       0.0375        0.068       0.0527       0.0485       0.0862       0.0674        0.196      0.00204\n",
            "     86     8      0.00567      0.00564     3.05e-05       0.0439       0.0581        0.033       0.0656       0.0493       0.0413        0.082       0.0616        0.404       0.0042\n",
            "     86     9      0.00662      0.00659     2.89e-05       0.0478       0.0628       0.0379       0.0675       0.0527        0.048        0.085       0.0665         0.39      0.00406\n",
            "     86    10      0.00637      0.00631     5.75e-05       0.0462       0.0615        0.036       0.0667       0.0513       0.0462       0.0841       0.0651        0.561      0.00584\n",
            "     86    11        0.006      0.00599      4.7e-06       0.0458       0.0599       0.0359       0.0656       0.0507       0.0452       0.0817       0.0634        0.145      0.00151\n",
            "     86    12      0.00668      0.00665     2.74e-05       0.0479       0.0631       0.0385       0.0665       0.0525       0.0491       0.0844       0.0667        0.385      0.00401\n",
            "     86    13      0.00633      0.00629      3.3e-05       0.0463       0.0614       0.0364       0.0659       0.0512        0.046       0.0841        0.065        0.424      0.00442\n",
            "     86    14      0.00652      0.00652     1.46e-06       0.0475       0.0625       0.0385       0.0655        0.052        0.048       0.0842       0.0661        0.073     0.000761\n",
            "     86    15      0.00687      0.00685     2.43e-05        0.048        0.064       0.0365        0.071       0.0537       0.0469       0.0888       0.0679        0.362      0.00377\n",
            "     86    16      0.00769      0.00769     1.16e-06       0.0512       0.0679       0.0399        0.074       0.0569       0.0509       0.0929       0.0719       0.0717     0.000747\n",
            "     86    17      0.00823      0.00822     9.46e-06       0.0528       0.0701       0.0416       0.0752       0.0584       0.0538       0.0947       0.0742        0.217      0.00226\n",
            "     86    18      0.00841      0.00841     1.67e-06       0.0549        0.071       0.0489       0.0668       0.0579       0.0611       0.0874       0.0742       0.0881     0.000918\n",
            "     86    19      0.00777      0.00777      9.6e-07       0.0516       0.0682       0.0418       0.0711       0.0565       0.0541         0.09        0.072       0.0574     0.000598\n",
            "     86    20      0.00765      0.00762     2.71e-05       0.0508       0.0675       0.0402       0.0721       0.0562       0.0501        0.093       0.0716        0.371      0.00387\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     86     1      0.00721      0.00721     2.58e-06       0.0491       0.0657        0.038       0.0713       0.0547        0.049       0.0902       0.0696       0.0942     0.000982\n",
            "     86     2      0.00721      0.00721     1.11e-06       0.0487       0.0657       0.0373       0.0716       0.0544       0.0482       0.0911       0.0696       0.0536     0.000558\n",
            "     86     3      0.00685      0.00685     1.39e-06       0.0475        0.064       0.0363       0.0699       0.0531       0.0465       0.0894       0.0679       0.0705     0.000734\n",
            "     86     4      0.00703      0.00703     1.54e-06       0.0479       0.0649       0.0367       0.0704       0.0535       0.0468       0.0908       0.0688        0.079     0.000823\n",
            "     86     5      0.00652      0.00652     9.76e-07       0.0469       0.0625       0.0367       0.0673        0.052       0.0473        0.085       0.0662       0.0494     0.000515\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              86  306.572    0.005      0.00694     2.31e-05      0.00696       0.0485       0.0644       0.0383        0.069       0.0536       0.0489       0.0876       0.0682        0.306      0.00318\n",
            "! Validation         86  306.572    0.005      0.00696     1.52e-06      0.00697        0.048       0.0646        0.037       0.0701       0.0535       0.0476       0.0893       0.0684       0.0694     0.000722\n",
            "Wall time: 306.5732417930001\n",
            "! Best model       86    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     87     1      0.00604      0.00604     7.32e-07       0.0455       0.0601       0.0366       0.0634         0.05       0.0469       0.0803       0.0636        0.058     0.000604\n",
            "     87     2       0.0077      0.00769     5.81e-06         0.05       0.0678        0.038       0.0739       0.0559       0.0481       0.0958        0.072        0.175      0.00182\n",
            "     87     3      0.00743      0.00741     1.39e-05       0.0499       0.0666       0.0399       0.0698       0.0548       0.0513       0.0897       0.0705        0.276      0.00287\n",
            "     87     4      0.00908      0.00907      3.7e-06       0.0571       0.0737       0.0511       0.0693       0.0602        0.065       0.0885       0.0768        0.134      0.00139\n",
            "     87     5      0.00732      0.00727     5.05e-05        0.051       0.0659       0.0423       0.0684       0.0554       0.0525       0.0867       0.0696        0.525      0.00547\n",
            "     87     6      0.00672      0.00672     4.53e-06       0.0482       0.0634       0.0377       0.0692       0.0535       0.0469       0.0875       0.0672        0.144       0.0015\n",
            "     87     7      0.00676      0.00667     8.18e-05       0.0483       0.0632       0.0374       0.0701       0.0538       0.0466       0.0874        0.067        0.662       0.0069\n",
            "     87     8      0.00744      0.00742     1.76e-05       0.0513       0.0667       0.0402       0.0735       0.0568       0.0513       0.0898       0.0706        0.297      0.00309\n",
            "     87     9      0.00756      0.00752     3.99e-05       0.0503       0.0671       0.0385       0.0738       0.0562       0.0489       0.0934       0.0711        0.469      0.00489\n",
            "     87    10      0.00725      0.00724     1.07e-05       0.0498       0.0658       0.0393       0.0708        0.055       0.0501       0.0894       0.0697        0.231       0.0024\n",
            "     87    11      0.00793      0.00792     1.61e-06       0.0526       0.0689       0.0447       0.0685       0.0566       0.0567       0.0883       0.0725       0.0859     0.000895\n",
            "     87    12      0.00741       0.0074     1.49e-05       0.0503       0.0665       0.0387       0.0735       0.0561       0.0489       0.0921       0.0705        0.287      0.00298\n",
            "     87    13      0.00882      0.00881     9.47e-06       0.0542       0.0726         0.04       0.0826       0.0613       0.0507        0.103        0.077        0.223      0.00232\n",
            "     87    14      0.00703        0.007     2.74e-05       0.0482       0.0647       0.0385       0.0675        0.053       0.0495       0.0876       0.0685        0.379      0.00395\n",
            "     87    15      0.00739      0.00737     1.54e-05        0.051       0.0664       0.0408       0.0713       0.0561       0.0508       0.0898       0.0703        0.268      0.00279\n",
            "     87    16      0.00603        0.006      3.1e-05       0.0449       0.0599        0.034       0.0665       0.0503       0.0432       0.0839       0.0635        0.408      0.00425\n",
            "     87    17      0.00679      0.00677      1.4e-05       0.0482       0.0637       0.0393       0.0659       0.0526       0.0511       0.0833       0.0672        0.271      0.00282\n",
            "     87    18      0.00749      0.00749     3.01e-06         0.05       0.0669       0.0395       0.0711       0.0553       0.0518       0.0899       0.0708       0.0998      0.00104\n",
            "     87    19      0.00654       0.0065     4.47e-05       0.0463       0.0624       0.0361       0.0666       0.0513        0.047       0.0851       0.0661        0.487      0.00507\n",
            "     87    20      0.00623      0.00623     1.18e-06       0.0458       0.0611       0.0358       0.0658       0.0508       0.0459       0.0836       0.0647       0.0666     0.000694\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     87     1      0.00717      0.00717      2.5e-06        0.049       0.0655       0.0379       0.0712       0.0546       0.0488       0.0901       0.0694       0.0941     0.000981\n",
            "     87     2      0.00718      0.00717     1.19e-06       0.0486       0.0655       0.0372       0.0715       0.0543        0.048       0.0909       0.0695        0.055     0.000573\n",
            "     87     3      0.00683      0.00683     1.33e-06       0.0474       0.0639       0.0362       0.0698        0.053       0.0463       0.0893       0.0678       0.0685     0.000713\n",
            "     87     4        0.007        0.007     1.51e-06       0.0478       0.0647       0.0366       0.0702       0.0534       0.0466       0.0907       0.0686       0.0773     0.000806\n",
            "     87     5      0.00648      0.00648     9.95e-07       0.0467       0.0623       0.0365       0.0672       0.0518       0.0471       0.0848        0.066       0.0485     0.000506\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              87  310.050    0.005      0.00723     1.96e-05      0.00725       0.0496       0.0658       0.0394       0.0701       0.0547       0.0504       0.0889       0.0696        0.277      0.00289\n",
            "! Validation         87  310.050    0.005      0.00693      1.5e-06      0.00693       0.0479       0.0644       0.0369         0.07       0.0534       0.0474       0.0892       0.0683       0.0687     0.000716\n",
            "Wall time: 310.05136248099984\n",
            "! Best model       87    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     88     1      0.00728      0.00724     4.07e-05       0.0496       0.0658       0.0389        0.071        0.055       0.0499       0.0896       0.0697        0.473      0.00493\n",
            "     88     2      0.00762      0.00762     1.19e-06       0.0511       0.0675        0.041       0.0713       0.0561       0.0515       0.0916       0.0715       0.0686     0.000714\n",
            "     88     3       0.0064      0.00639     9.29e-06       0.0474       0.0618       0.0381       0.0659        0.052       0.0485       0.0823       0.0654        0.218      0.00227\n",
            "     88     4      0.00578      0.00577     1.31e-05        0.044       0.0588       0.0346       0.0628       0.0487       0.0441       0.0804       0.0623        0.265      0.00276\n",
            "     88     5      0.00811      0.00811     4.47e-06       0.0537       0.0697       0.0452       0.0706       0.0579       0.0575       0.0891       0.0733        0.135      0.00141\n",
            "     88     6      0.00815      0.00811     4.41e-05       0.0532       0.0697        0.044       0.0715       0.0577       0.0559       0.0912       0.0735         0.49      0.00511\n",
            "     88     7      0.00663      0.00662     2.67e-06       0.0469        0.063       0.0348       0.0712        0.053       0.0437       0.0899       0.0668        0.107      0.00111\n",
            "     88     8      0.00615      0.00613     2.82e-05       0.0456       0.0605       0.0367       0.0634         0.05       0.0464       0.0818       0.0641         0.39      0.00406\n",
            "     88     9      0.00813      0.00813     1.35e-06       0.0527       0.0697       0.0407       0.0768       0.0587       0.0517       0.0962       0.0739       0.0703     0.000732\n",
            "     88    10      0.00857      0.00857     1.21e-06       0.0554       0.0716       0.0439       0.0784       0.0612       0.0538        0.098       0.0759        0.076     0.000791\n",
            "     88    11      0.00849      0.00848     1.33e-05       0.0556       0.0712       0.0476       0.0717       0.0597       0.0595       0.0902       0.0749        0.247      0.00257\n",
            "     88    12      0.00655      0.00655     1.77e-06       0.0471       0.0626       0.0389       0.0637       0.0513       0.0501       0.0821       0.0661       0.0842     0.000877\n",
            "     88    13      0.00681       0.0068     8.56e-06       0.0476       0.0638       0.0358       0.0713       0.0535       0.0458       0.0895       0.0677        0.216      0.00225\n",
            "     88    14      0.00701      0.00701     1.59e-06       0.0491       0.0648       0.0382       0.0711       0.0546       0.0483        0.089       0.0686       0.0779     0.000812\n",
            "     88    15      0.00923      0.00923     7.55e-07       0.0569       0.0743       0.0467       0.0772       0.0619       0.0583       0.0989       0.0786        0.048       0.0005\n",
            "     88    16      0.00968      0.00965     2.76e-05       0.0571        0.076       0.0462       0.0788       0.0625       0.0596        0.101       0.0803         0.38      0.00396\n",
            "     88    17      0.00703      0.00698     4.75e-05       0.0499       0.0646       0.0423       0.0653       0.0538       0.0529       0.0832       0.0681        0.509       0.0053\n",
            "     88    18      0.00604      0.00604     5.91e-06       0.0454       0.0601       0.0351       0.0659       0.0505       0.0444       0.0831       0.0637        0.173       0.0018\n",
            "     88    19      0.00683      0.00681     2.58e-05        0.048       0.0638       0.0388       0.0665       0.0526       0.0503       0.0847       0.0675        0.375      0.00391\n",
            "     88    20      0.00863      0.00863     2.04e-06       0.0558       0.0719       0.0475       0.0725         0.06       0.0598       0.0914       0.0756       0.0896     0.000934\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     88     1      0.00714      0.00714      2.5e-06       0.0489       0.0654       0.0378       0.0711       0.0544       0.0486       0.0899       0.0693        0.093     0.000968\n",
            "     88     2      0.00714      0.00714     1.13e-06       0.0485       0.0654        0.037       0.0714       0.0542       0.0479       0.0908       0.0693        0.054     0.000563\n",
            "     88     3      0.00679      0.00679     1.34e-06       0.0473       0.0638        0.036       0.0697       0.0529       0.0461       0.0891       0.0676        0.069     0.000719\n",
            "     88     4      0.00697      0.00697     1.55e-06       0.0477       0.0646       0.0364       0.0702       0.0533       0.0464       0.0906       0.0685       0.0786     0.000819\n",
            "     88     5      0.00645      0.00645     9.56e-07       0.0466       0.0621       0.0364       0.0671       0.0517       0.0469       0.0847       0.0658       0.0483     0.000504\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              88  313.512    0.005      0.00744     1.41e-05      0.00746       0.0506       0.0667       0.0407       0.0703       0.0555       0.0519       0.0894       0.0706        0.225      0.00234\n",
            "! Validation         88  313.512    0.005       0.0069      1.5e-06       0.0069       0.0478       0.0643       0.0367       0.0699       0.0533       0.0472       0.0891       0.0681       0.0686     0.000715\n",
            "Wall time: 313.5127771760001\n",
            "! Best model       88    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     89     1      0.00879      0.00879     8.74e-07       0.0561       0.0725       0.0477       0.0729       0.0603       0.0593       0.0936       0.0764       0.0602     0.000627\n",
            "     89     2      0.00654      0.00654     3.95e-06       0.0469       0.0626        0.037       0.0667       0.0518       0.0466        0.086       0.0663        0.144       0.0015\n",
            "     89     3       0.0068       0.0068     7.13e-07       0.0479       0.0638       0.0377       0.0683        0.053       0.0483       0.0869       0.0676       0.0465     0.000484\n",
            "     89     4      0.00537      0.00536      2.7e-06        0.043       0.0567       0.0336       0.0617       0.0477       0.0421        0.078       0.0601        0.102      0.00106\n",
            "     89     5      0.00615      0.00614     5.29e-06       0.0461       0.0606       0.0359       0.0664       0.0511       0.0452       0.0833       0.0643        0.155      0.00161\n",
            "     89     6      0.00642      0.00642     3.44e-06       0.0461        0.062       0.0353       0.0676       0.0514       0.0452       0.0863       0.0657        0.129      0.00135\n",
            "     89     7      0.00692      0.00691     1.03e-05       0.0488       0.0643       0.0383         0.07       0.0541       0.0483       0.0879       0.0681        0.214      0.00223\n",
            "     89     8      0.00758      0.00758     9.42e-07       0.0499       0.0673       0.0402       0.0693       0.0547       0.0519       0.0907       0.0713       0.0635     0.000661\n",
            "     89     9      0.00656      0.00656     1.76e-06        0.048       0.0626       0.0392       0.0655       0.0524       0.0497       0.0827       0.0662       0.0891     0.000928\n",
            "     89    10      0.00733      0.00731      1.9e-05       0.0497       0.0661       0.0392       0.0706       0.0549       0.0498       0.0903       0.0701        0.323      0.00336\n",
            "     89    11      0.00686      0.00686     1.22e-06       0.0483       0.0641       0.0368       0.0713        0.054       0.0468       0.0891       0.0679       0.0748     0.000779\n",
            "     89    12       0.0082      0.00818     2.65e-05       0.0526         0.07       0.0425       0.0726       0.0576       0.0537       0.0944       0.0741        0.376      0.00392\n",
            "     89    13      0.00715      0.00715     6.13e-07       0.0505       0.0654        0.042       0.0673       0.0547       0.0526       0.0855        0.069       0.0486     0.000507\n",
            "     89    14        0.007        0.007     2.59e-06       0.0483       0.0647       0.0362       0.0725       0.0544       0.0466       0.0907       0.0686        0.101      0.00105\n",
            "     89    15      0.00674      0.00672     1.83e-05       0.0479       0.0634       0.0379       0.0678       0.0529        0.049       0.0853       0.0671        0.305      0.00318\n",
            "     89    16      0.00718      0.00717     3.12e-06       0.0494       0.0655       0.0398       0.0687       0.0542       0.0511       0.0876       0.0693         0.12      0.00125\n",
            "     89    17      0.00712       0.0071     2.37e-05        0.048       0.0652       0.0351        0.074       0.0545       0.0445       0.0937       0.0691        0.359      0.00374\n",
            "     89    18      0.00669      0.00669     6.52e-07       0.0476       0.0633       0.0365       0.0696       0.0531       0.0461       0.0881       0.0671       0.0506     0.000527\n",
            "     89    19      0.00597      0.00596     4.22e-06       0.0445       0.0597        0.034       0.0655       0.0497       0.0436       0.0831       0.0634        0.116       0.0012\n",
            "     89    20      0.00637      0.00637     5.01e-06       0.0463       0.0617       0.0358       0.0673       0.0516        0.046       0.0849       0.0654        0.148      0.00155\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     89     1      0.00711      0.00711     2.42e-06       0.0488       0.0652       0.0377        0.071       0.0543       0.0485       0.0898       0.0691       0.0906     0.000944\n",
            "     89     2      0.00711      0.00711     1.06e-06       0.0484       0.0652       0.0369       0.0713       0.0541       0.0477       0.0906       0.0692       0.0521     0.000542\n",
            "     89     3      0.00677      0.00677      1.4e-06       0.0472       0.0636       0.0359       0.0697       0.0528        0.046        0.089       0.0675       0.0708     0.000738\n",
            "     89     4      0.00694      0.00694     1.53e-06       0.0476       0.0644       0.0363       0.0701       0.0532       0.0463       0.0904       0.0684       0.0783     0.000816\n",
            "     89     5      0.00642      0.00642     9.14e-07       0.0465        0.062       0.0363        0.067       0.0516       0.0468       0.0846       0.0657       0.0488     0.000509\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              89  316.978    0.005      0.00688     6.75e-06      0.00689       0.0483       0.0642        0.038       0.0688       0.0534       0.0485       0.0875        0.068        0.151      0.00158\n",
            "! Validation         89  316.978    0.005      0.00687     1.47e-06      0.00687       0.0477       0.0641       0.0366       0.0698       0.0532       0.0471       0.0889        0.068       0.0681      0.00071\n",
            "Wall time: 316.97941358599996\n",
            "! Best model       89    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     90     1      0.00706      0.00705     6.85e-06       0.0477        0.065       0.0363       0.0705       0.0534       0.0475       0.0903       0.0689        0.188      0.00196\n",
            "     90     2      0.00669      0.00669     1.14e-06       0.0483       0.0633       0.0374       0.0702       0.0538       0.0472       0.0869        0.067       0.0686     0.000714\n",
            "     90     3       0.0072       0.0072     4.48e-06       0.0491       0.0656       0.0387       0.0699       0.0543        0.049       0.0901       0.0696        0.107      0.00111\n",
            "     90     4      0.00706      0.00706     2.04e-06       0.0487        0.065         0.04       0.0662       0.0531       0.0511       0.0863       0.0687        0.103      0.00108\n",
            "     90     5       0.0066      0.00659     1.28e-05       0.0459       0.0628       0.0355       0.0668       0.0511       0.0462       0.0869       0.0666        0.262      0.00273\n",
            "     90     6      0.00604      0.00604     1.47e-06       0.0457       0.0601       0.0346       0.0679       0.0513       0.0436       0.0839       0.0638       0.0648     0.000675\n",
            "     90     7      0.00628      0.00628     1.82e-06       0.0462       0.0613       0.0364       0.0659       0.0511       0.0463       0.0836        0.065       0.0965      0.00101\n",
            "     90     8      0.00655      0.00655     5.21e-07       0.0477       0.0626       0.0367       0.0696       0.0532       0.0461       0.0866       0.0664       0.0518     0.000539\n",
            "     90     9      0.00603      0.00603     1.95e-06       0.0453       0.0601       0.0348       0.0664       0.0506        0.044       0.0834       0.0637       0.0982      0.00102\n",
            "     90    10      0.00628      0.00628     4.38e-06       0.0458       0.0613       0.0352       0.0671       0.0512       0.0448       0.0852        0.065        0.144       0.0015\n",
            "     90    11      0.00754      0.00754     4.03e-06       0.0498       0.0672       0.0378       0.0738       0.0558       0.0484        0.094       0.0712        0.142      0.00148\n",
            "     90    12      0.00751      0.00747      3.8e-05        0.051       0.0669       0.0419       0.0693       0.0556       0.0522       0.0892       0.0707        0.453      0.00472\n",
            "     90    13      0.00748      0.00747     2.54e-06       0.0511       0.0669       0.0423       0.0689       0.0556       0.0535       0.0878       0.0706       0.0816      0.00085\n",
            "     90    14      0.00677      0.00676     6.45e-06       0.0478       0.0636       0.0366       0.0701       0.0533       0.0464       0.0885       0.0674         0.18      0.00187\n",
            "     90    15       0.0069      0.00688     1.91e-05       0.0475       0.0642       0.0371       0.0682       0.0526       0.0482       0.0879        0.068        0.323      0.00336\n",
            "     90    16      0.00574      0.00574     1.05e-06       0.0448       0.0586       0.0354       0.0634       0.0494        0.044       0.0802       0.0621       0.0623     0.000649\n",
            "     90    17      0.00711      0.00706     4.93e-05        0.049        0.065       0.0389       0.0691        0.054       0.0501       0.0875       0.0688        0.514      0.00536\n",
            "     90    18      0.00606      0.00606     1.12e-06       0.0445       0.0602       0.0338       0.0659       0.0498       0.0431       0.0847       0.0639       0.0674     0.000702\n",
            "     90    19      0.00695      0.00691     3.63e-05       0.0491       0.0643       0.0391       0.0692       0.0542        0.049       0.0872       0.0681        0.446      0.00465\n",
            "     90    20      0.00605      0.00605     4.86e-06       0.0447       0.0602       0.0336       0.0668       0.0502       0.0436        0.084       0.0638        0.156      0.00163\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     90     1      0.00708      0.00708     2.48e-06       0.0487       0.0651       0.0376       0.0709       0.0542       0.0483       0.0896        0.069        0.092     0.000958\n",
            "     90     2      0.00708      0.00707     1.09e-06       0.0483       0.0651       0.0368       0.0712        0.054       0.0475       0.0905        0.069       0.0526     0.000548\n",
            "     90     3      0.00674      0.00674     1.36e-06       0.0471       0.0635       0.0358       0.0696       0.0527       0.0458       0.0889       0.0674       0.0707     0.000736\n",
            "     90     4      0.00691      0.00691     1.53e-06       0.0475       0.0643       0.0362       0.0699       0.0531       0.0461       0.0903       0.0682       0.0785     0.000818\n",
            "     90     5       0.0064       0.0064      9.4e-07       0.0464       0.0619       0.0361       0.0669       0.0515       0.0466       0.0845       0.0656       0.0467     0.000486\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              90  320.450    0.005      0.00669        1e-05       0.0067       0.0475       0.0633       0.0371       0.0682       0.0527       0.0473       0.0868        0.067        0.181      0.00188\n",
            "! Validation         90  320.450    0.005      0.00684     1.48e-06      0.00684       0.0476        0.064       0.0365       0.0697       0.0531       0.0469       0.0888       0.0678       0.0681     0.000709\n",
            "Wall time: 320.4505756599999\n",
            "! Best model       90    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     91     1      0.00793       0.0079     3.25e-05       0.0525       0.0688        0.044       0.0693       0.0567       0.0564       0.0884       0.0724         0.41      0.00427\n",
            "     91     2      0.00643      0.00643     3.34e-06       0.0464        0.062       0.0356       0.0681       0.0518       0.0446        0.087       0.0658        0.123      0.00128\n",
            "     91     3      0.00932      0.00928     3.89e-05        0.057       0.0745       0.0444       0.0823       0.0634       0.0553        0.103        0.079        0.454      0.00472\n",
            "     91     4       0.0088      0.00879     1.22e-05        0.056       0.0725       0.0467       0.0747       0.0607       0.0592       0.0936       0.0764        0.249      0.00259\n",
            "     91     5      0.00752      0.00752     2.18e-06       0.0525       0.0671       0.0464       0.0648       0.0556       0.0574       0.0831       0.0703       0.0891     0.000928\n",
            "     91     6      0.00619      0.00619     1.02e-06       0.0457       0.0609       0.0353       0.0666       0.0509       0.0451       0.0839       0.0645       0.0727     0.000757\n",
            "     91     7      0.00626      0.00624     1.44e-05       0.0462       0.0611       0.0367       0.0654        0.051       0.0464       0.0831       0.0647        0.263      0.00274\n",
            "     91     8      0.00617      0.00617     6.62e-07       0.0455       0.0608       0.0335       0.0695       0.0515       0.0426       0.0863       0.0645       0.0402     0.000419\n",
            "     91     9      0.00652      0.00651     9.68e-06       0.0468       0.0624       0.0371       0.0663       0.0517       0.0469       0.0854       0.0661        0.226      0.00235\n",
            "     91    10      0.00646      0.00645     5.31e-06       0.0465       0.0621       0.0357       0.0681       0.0519       0.0449       0.0869       0.0659         0.17      0.00177\n",
            "     91    11      0.00759      0.00759     6.97e-06       0.0498       0.0674       0.0371       0.0752       0.0561       0.0479       0.0951       0.0715        0.193      0.00201\n",
            "     91    12      0.00641      0.00641     3.03e-06       0.0465       0.0619       0.0368       0.0659       0.0513        0.046       0.0853       0.0656        0.113      0.00118\n",
            "     91    13      0.00765      0.00765     2.57e-06        0.052       0.0677       0.0444       0.0673       0.0558        0.056       0.0864       0.0712        0.112      0.00117\n",
            "     91    14      0.00623      0.00622     7.01e-06       0.0464        0.061        0.037       0.0652       0.0511       0.0473       0.0818       0.0646        0.187      0.00194\n",
            "     91    15      0.00606      0.00606     1.09e-06        0.045       0.0602       0.0341       0.0667       0.0504       0.0432       0.0845       0.0639       0.0672       0.0007\n",
            "     91    16       0.0068      0.00678     2.42e-05       0.0478       0.0637        0.038       0.0673       0.0527        0.048       0.0869       0.0675        0.361      0.00376\n",
            "     91    17      0.00779      0.00777     1.56e-05       0.0504       0.0682       0.0387       0.0737       0.0562        0.051       0.0936       0.0723         0.29      0.00302\n",
            "     91    18      0.00638      0.00637     1.52e-05       0.0444       0.0617       0.0335       0.0662       0.0498       0.0435       0.0875       0.0655        0.287      0.00299\n",
            "     91    19      0.00674      0.00673      9.9e-06       0.0476       0.0635       0.0357       0.0714       0.0535       0.0449       0.0897       0.0673        0.218      0.00227\n",
            "     91    20       0.0086      0.00854     5.77e-05       0.0552       0.0715       0.0457       0.0743         0.06       0.0576       0.0933       0.0754        0.561      0.00584\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     91     1      0.00705      0.00705     2.51e-06       0.0486        0.065       0.0374       0.0708       0.0541       0.0482       0.0895       0.0689        0.093     0.000968\n",
            "     91     2      0.00706      0.00706     1.09e-06       0.0482        0.065       0.0367       0.0711       0.0539       0.0474       0.0904       0.0689       0.0516     0.000537\n",
            "     91     3      0.00672      0.00672     1.36e-06        0.047       0.0634       0.0357       0.0695       0.0526       0.0457       0.0888       0.0672       0.0705     0.000734\n",
            "     91     4      0.00688      0.00688     1.53e-06       0.0473       0.0642       0.0361       0.0698        0.053        0.046       0.0901       0.0681       0.0799     0.000832\n",
            "     91     5      0.00637      0.00637     9.47e-07       0.0463       0.0617        0.036       0.0668       0.0514       0.0464       0.0844       0.0654       0.0474     0.000493\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              91  323.914    0.005      0.00708     1.32e-05      0.00709        0.049       0.0651       0.0388       0.0694       0.0541       0.0495       0.0884       0.0689        0.224      0.00234\n",
            "! Validation         91  323.914    0.005      0.00681     1.49e-06      0.00682       0.0475       0.0639       0.0364       0.0696        0.053       0.0468       0.0887       0.0677       0.0685     0.000713\n",
            "Wall time: 323.9152034709998\n",
            "! Best model       91    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     92     1      0.00935      0.00935     4.08e-06       0.0587       0.0748        0.053       0.0702       0.0616       0.0652        0.091       0.0781        0.136      0.00141\n",
            "     92     2      0.00671      0.00666     4.68e-05       0.0478       0.0631       0.0387        0.066       0.0523       0.0485       0.0852       0.0668        0.505      0.00526\n",
            "     92     3      0.00622      0.00621      1.1e-05       0.0466        0.061        0.038       0.0638       0.0509       0.0475       0.0815       0.0645        0.229      0.00239\n",
            "     92     4      0.00851      0.00847     4.32e-05       0.0549       0.0712       0.0454        0.074       0.0597       0.0573       0.0929       0.0751        0.486      0.00506\n",
            "     92     5      0.00964      0.00949     0.000144       0.0579       0.0754       0.0469       0.0799       0.0634       0.0588        0.101       0.0797        0.889      0.00926\n",
            "     92     6      0.00868      0.00862     6.05e-05       0.0544       0.0718       0.0438       0.0755       0.0596       0.0547       0.0975       0.0761        0.576        0.006\n",
            "     92     7      0.00644      0.00634       0.0001       0.0466       0.0616       0.0373       0.0652       0.0513       0.0475       0.0828       0.0652        0.741      0.00772\n",
            "     92     8      0.00691       0.0069     3.12e-06       0.0482       0.0643       0.0376       0.0694       0.0535       0.0471       0.0893       0.0682        0.128      0.00133\n",
            "     92     9      0.00649      0.00647     1.81e-05       0.0479       0.0622       0.0401       0.0636       0.0518       0.0506       0.0805       0.0656        0.312      0.00325\n",
            "     92    10      0.00747      0.00746     1.93e-05       0.0509       0.0668       0.0434        0.066       0.0547       0.0549       0.0858       0.0703        0.325      0.00338\n",
            "     92    11      0.00629      0.00628     1.11e-05       0.0458       0.0613       0.0365       0.0642       0.0504       0.0461       0.0838        0.065        0.238      0.00248\n",
            "     92    12      0.00657      0.00656      7.5e-06       0.0482       0.0627       0.0388        0.067       0.0529       0.0491       0.0834       0.0663         0.18      0.00187\n",
            "     92    13       0.0069       0.0069     6.22e-07       0.0477       0.0643       0.0371       0.0688        0.053       0.0483       0.0879       0.0681       0.0498     0.000519\n",
            "     92    14      0.00724      0.00723      3.7e-06       0.0496       0.0658       0.0389        0.071        0.055       0.0494       0.0901       0.0697         0.14      0.00145\n",
            "     92    15      0.00587      0.00587     1.34e-06       0.0446       0.0593       0.0337       0.0664       0.0501       0.0428        0.083       0.0629       0.0713     0.000743\n",
            "     92    16      0.00742      0.00742     1.43e-06       0.0508       0.0667       0.0393       0.0737       0.0565       0.0492       0.0921       0.0707       0.0701      0.00073\n",
            "     92    17      0.00683      0.00682     1.54e-05       0.0478       0.0639       0.0355       0.0725        0.054       0.0453       0.0902       0.0677        0.287      0.00299\n",
            "     92    18      0.00782      0.00781     3.09e-06       0.0527       0.0684       0.0432       0.0716       0.0574       0.0548       0.0896       0.0722        0.111      0.00115\n",
            "     92    19      0.00669      0.00667     1.46e-05       0.0475       0.0632       0.0371       0.0683       0.0527       0.0467       0.0873        0.067        0.274      0.00285\n",
            "     92    20      0.00709      0.00708     4.92e-06       0.0498       0.0651       0.0397       0.0702       0.0549       0.0497       0.0881       0.0689        0.156      0.00162\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     92     1      0.00703      0.00703     2.42e-06       0.0485       0.0649       0.0373       0.0707        0.054       0.0481       0.0894       0.0688       0.0908     0.000946\n",
            "     92     2      0.00703      0.00703     1.04e-06       0.0481       0.0648       0.0366        0.071       0.0538       0.0473       0.0903       0.0688       0.0508     0.000529\n",
            "     92     3      0.00669      0.00669     1.35e-06       0.0469       0.0633       0.0356       0.0695       0.0525       0.0455       0.0887       0.0671       0.0697     0.000726\n",
            "     92     4      0.00685      0.00685     1.57e-06       0.0473        0.064        0.036       0.0698       0.0529       0.0458         0.09       0.0679       0.0796     0.000829\n",
            "     92     5      0.00634      0.00634     8.98e-07       0.0462       0.0616       0.0359       0.0667       0.0513       0.0463       0.0843       0.0653       0.0453     0.000472\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              92  327.391    0.005      0.00723     2.57e-05      0.00726       0.0499       0.0658       0.0402       0.0694       0.0548       0.0509       0.0883       0.0696        0.295      0.00307\n",
            "! Validation         92  327.391    0.005      0.00679     1.46e-06      0.00679       0.0474       0.0637       0.0363       0.0695       0.0529       0.0466       0.0886       0.0676       0.0672       0.0007\n",
            "Wall time: 327.39173332899986\n",
            "! Best model       92    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     93     1      0.00661       0.0066     1.16e-05       0.0469       0.0628       0.0361       0.0684       0.0523       0.0464       0.0869       0.0666         0.24       0.0025\n",
            "     93     2      0.00794      0.00794     8.32e-07        0.055       0.0689       0.0498       0.0654       0.0576       0.0611       0.0825       0.0718       0.0533     0.000555\n",
            "     93     3      0.00818      0.00817     5.09e-06       0.0536       0.0699       0.0445       0.0718       0.0581        0.057       0.0904       0.0737        0.165      0.00172\n",
            "     93     4      0.00755      0.00754     3.37e-06       0.0504       0.0672       0.0389       0.0732       0.0561       0.0489       0.0936       0.0712        0.133      0.00139\n",
            "     93     5      0.00645      0.00642     2.78e-05       0.0459        0.062        0.034       0.0697       0.0519       0.0436       0.0879       0.0658        0.384        0.004\n",
            "     93     6      0.00696      0.00693     3.14e-05       0.0482       0.0644       0.0367        0.071       0.0539       0.0465       0.0901       0.0683        0.412      0.00429\n",
            "     93     7      0.00754      0.00744     9.52e-05       0.0509       0.0667       0.0421       0.0685       0.0553       0.0538        0.087       0.0704        0.722      0.00752\n",
            "     93     8      0.00715      0.00714     1.94e-05       0.0487       0.0654       0.0396       0.0669       0.0533       0.0504       0.0879       0.0692        0.317       0.0033\n",
            "     93     9      0.00676      0.00673     2.79e-05       0.0478       0.0635       0.0363        0.071       0.0536       0.0461       0.0886       0.0673        0.379      0.00395\n",
            "     93    10      0.00719      0.00714     4.73e-05       0.0491       0.0654       0.0407       0.0658       0.0533       0.0527       0.0852        0.069        0.506      0.00527\n",
            "     93    11      0.00746      0.00746      1.6e-06       0.0506       0.0668       0.0394       0.0729       0.0562       0.0499       0.0918       0.0708       0.0719     0.000749\n",
            "     93    12      0.00747      0.00733     0.000133       0.0499       0.0663       0.0386       0.0724       0.0555        0.048       0.0925       0.0703        0.853      0.00888\n",
            "     93    13      0.00574      0.00574     7.49e-06       0.0443       0.0586       0.0344       0.0641       0.0493       0.0438       0.0804       0.0621        0.194      0.00202\n",
            "     93    14      0.00761      0.00755     6.15e-05       0.0505       0.0672       0.0394       0.0728       0.0561       0.0504       0.0921       0.0712         0.58      0.00605\n",
            "     93    15      0.00726      0.00726     2.64e-06       0.0491       0.0659       0.0385       0.0705       0.0545       0.0493       0.0904       0.0698        0.113      0.00118\n",
            "     93    16      0.00685      0.00684     8.44e-06       0.0486        0.064         0.04       0.0657       0.0528       0.0506       0.0847       0.0676        0.208      0.00216\n",
            "     93    17      0.00544      0.00544     5.07e-06        0.043        0.057       0.0344       0.0602       0.0473       0.0443       0.0764       0.0603        0.124      0.00129\n",
            "     93    18      0.00741      0.00741      5.3e-06       0.0515       0.0666       0.0411       0.0723       0.0567       0.0512       0.0897       0.0705        0.158      0.00164\n",
            "     93    19       0.0104       0.0104     1.99e-05       0.0597       0.0787       0.0484       0.0823       0.0654       0.0615        0.105       0.0832        0.326       0.0034\n",
            "     93    20      0.00948      0.00936     0.000112        0.058       0.0749       0.0481       0.0779        0.063         0.06        0.098        0.079        0.784      0.00817\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     93     1        0.007        0.007     2.45e-06       0.0484       0.0647       0.0372       0.0706       0.0539       0.0479       0.0893       0.0686       0.0935     0.000974\n",
            "     93     2        0.007        0.007     9.82e-07        0.048       0.0647       0.0365        0.071       0.0537       0.0471       0.0902       0.0686       0.0476     0.000495\n",
            "     93     3      0.00667      0.00667     1.38e-06       0.0468       0.0632       0.0355       0.0694       0.0524       0.0454       0.0886        0.067       0.0729     0.000759\n",
            "     93     4      0.00683      0.00682     1.65e-06       0.0471       0.0639       0.0359       0.0697       0.0528       0.0457       0.0899       0.0678        0.083     0.000865\n",
            "     93     5      0.00633      0.00632     8.83e-07       0.0461       0.0615       0.0358       0.0666       0.0512       0.0461       0.0843       0.0652       0.0459     0.000478\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              93  330.924    0.005      0.00734     3.13e-05      0.00737       0.0501       0.0663       0.0401       0.0701       0.0551       0.0511       0.0893       0.0702        0.336       0.0035\n",
            "! Validation         93  330.924    0.005      0.00676     1.47e-06      0.00676       0.0473       0.0636       0.0362       0.0695       0.0528       0.0465       0.0885       0.0675       0.0686     0.000714\n",
            "Wall time: 330.9246967460001\n",
            "! Best model       93    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     94     1        0.007      0.00698     2.29e-05       0.0488       0.0646       0.0389       0.0685       0.0537       0.0496       0.0872       0.0684        0.348      0.00363\n",
            "     94     2      0.00692      0.00688     4.18e-05       0.0478       0.0642       0.0364       0.0706       0.0535       0.0471        0.089        0.068        0.478      0.00498\n",
            "     94     3      0.00743      0.00739     4.41e-05        0.051       0.0665        0.041       0.0709       0.0559       0.0512       0.0895       0.0704        0.491      0.00511\n",
            "     94     4      0.00824      0.00824     2.43e-06       0.0556       0.0702       0.0482       0.0705       0.0594       0.0591       0.0883       0.0737       0.0861     0.000897\n",
            "     94     5      0.00852      0.00847     5.95e-05       0.0548       0.0712       0.0449       0.0746       0.0597       0.0564        0.094       0.0752        0.567      0.00591\n",
            "     94     6      0.00649      0.00649     4.95e-07        0.046       0.0623       0.0359        0.066        0.051        0.046       0.0861       0.0661       0.0461      0.00048\n",
            "     94     7      0.00658      0.00657     1.32e-05       0.0471       0.0627       0.0365       0.0683       0.0524       0.0473       0.0855       0.0664        0.261      0.00271\n",
            "     94     8      0.00833      0.00832     5.45e-06       0.0539       0.0706       0.0426       0.0764       0.0595       0.0529       0.0967       0.0748        0.167      0.00174\n",
            "     94     9      0.00884      0.00883     1.26e-05       0.0558       0.0727       0.0462       0.0749       0.0606       0.0582       0.0953       0.0767        0.248      0.00258\n",
            "     94    10      0.00694      0.00693     7.26e-06       0.0485       0.0644       0.0393       0.0668       0.0531       0.0508       0.0854       0.0681        0.196      0.00204\n",
            "     94    11      0.00647      0.00647     1.15e-06        0.047       0.0622       0.0354         0.07       0.0527       0.0447       0.0873        0.066        0.052     0.000541\n",
            "     94    12      0.00759      0.00757     1.75e-05       0.0512       0.0673       0.0434       0.0667       0.0551       0.0554       0.0863       0.0709        0.286      0.00298\n",
            "     94    13      0.00766      0.00765     7.56e-06       0.0518       0.0677       0.0417       0.0721       0.0569       0.0527       0.0904       0.0716         0.17      0.00177\n",
            "     94    14       0.0057      0.00569     7.19e-06       0.0433       0.0584        0.033        0.064       0.0485       0.0419       0.0819       0.0619        0.183      0.00191\n",
            "     94    15      0.00669      0.00669     9.93e-07       0.0484       0.0633       0.0394       0.0665        0.053       0.0495       0.0843       0.0669       0.0689     0.000718\n",
            "     94    16      0.00796      0.00795     8.57e-07       0.0522        0.069       0.0424       0.0717       0.0571       0.0538       0.0922        0.073        0.058     0.000604\n",
            "     94    17      0.00753      0.00751     2.93e-05       0.0494        0.067       0.0374       0.0734       0.0554       0.0483       0.0938       0.0711        0.398      0.00415\n",
            "     94    18      0.00614      0.00614     1.22e-06       0.0456       0.0606       0.0348       0.0672        0.051       0.0438       0.0849       0.0643       0.0621     0.000647\n",
            "     94    19      0.00587      0.00586     9.69e-06       0.0451       0.0592       0.0361       0.0633       0.0497       0.0451       0.0803       0.0627        0.226      0.00236\n",
            "     94    20      0.00618      0.00618     1.42e-06       0.0456       0.0608       0.0355       0.0659       0.0507       0.0454       0.0834       0.0644       0.0805     0.000838\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     94     1      0.00698      0.00698     2.41e-06       0.0483       0.0646       0.0371       0.0705       0.0538       0.0478       0.0892       0.0685       0.0908     0.000946\n",
            "     94     2      0.00698      0.00698     1.07e-06       0.0479       0.0646       0.0364       0.0709       0.0536        0.047       0.0901       0.0685       0.0525     0.000547\n",
            "     94     3      0.00665      0.00665     1.37e-06       0.0467       0.0631       0.0354       0.0693       0.0524       0.0453       0.0885       0.0669       0.0703     0.000732\n",
            "     94     4       0.0068       0.0068     1.58e-06        0.047       0.0638       0.0358       0.0696       0.0527       0.0456       0.0898       0.0677       0.0792     0.000825\n",
            "     94     5       0.0063       0.0063     8.95e-07        0.046       0.0614       0.0357       0.0665       0.0511        0.046       0.0842       0.0651       0.0458     0.000477\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              94  334.446    0.005      0.00714     1.43e-05      0.00715       0.0494       0.0654       0.0394       0.0694       0.0544       0.0502       0.0882       0.0692        0.224      0.00233\n",
            "! Validation         94  334.446    0.005      0.00674     1.46e-06      0.00674       0.0472       0.0635       0.0361       0.0694       0.0527       0.0463       0.0884       0.0674       0.0677     0.000706\n",
            "Wall time: 334.4469329699998\n",
            "! Best model       94    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     95     1       0.0064       0.0064     8.77e-07       0.0464       0.0619       0.0357       0.0678       0.0517       0.0455       0.0857       0.0656       0.0561     0.000584\n",
            "     95     2       0.0067      0.00669     1.17e-05       0.0487       0.0633       0.0392       0.0676       0.0534       0.0491       0.0848       0.0669        0.218      0.00227\n",
            "     95     3      0.00605      0.00605     4.28e-07       0.0458       0.0602       0.0372       0.0628         0.05       0.0467       0.0806       0.0636       0.0451      0.00047\n",
            "     95     4      0.00691      0.00689     2.18e-05       0.0475       0.0642       0.0363       0.0699       0.0531       0.0473       0.0888       0.0681        0.346      0.00361\n",
            "     95     5      0.00608      0.00604     3.42e-05       0.0451       0.0601       0.0348       0.0657       0.0503       0.0439       0.0836       0.0638        0.432       0.0045\n",
            "     95     6      0.00739      0.00739     1.45e-06        0.049       0.0665       0.0363       0.0745       0.0554       0.0468       0.0943       0.0705       0.0758     0.000789\n",
            "     95     7      0.00721      0.00705     0.000165       0.0498        0.065       0.0408       0.0677       0.0543       0.0516       0.0857       0.0686        0.953      0.00993\n",
            "     95     8      0.00657      0.00656     6.31e-06       0.0477       0.0627       0.0373       0.0685       0.0529       0.0473       0.0855       0.0664        0.185      0.00192\n",
            "     95     9      0.00672      0.00662     9.33e-05       0.0467        0.063       0.0352       0.0696       0.0524       0.0445        0.089       0.0668        0.717      0.00747\n",
            "     95    10      0.00832       0.0083     2.13e-05       0.0546       0.0705       0.0481       0.0678       0.0579       0.0618       0.0853       0.0735        0.318      0.00332\n",
            "     95    11      0.00795      0.00794     8.91e-06       0.0528       0.0689        0.042       0.0744       0.0582       0.0529        0.093        0.073         0.22      0.00229\n",
            "     95    12      0.00583       0.0058     3.05e-05       0.0443       0.0589       0.0334       0.0662       0.0498       0.0421       0.0829       0.0625        0.399      0.00416\n",
            "     95    13      0.00613      0.00611     2.08e-05       0.0452       0.0605       0.0365       0.0628       0.0496       0.0468       0.0812        0.064        0.333      0.00347\n",
            "     95    14      0.00782      0.00776     5.84e-05       0.0505       0.0682       0.0378       0.0758       0.0568       0.0488       0.0958       0.0723        0.565      0.00588\n",
            "     95    15      0.00695       0.0069      5.5e-05       0.0487       0.0643        0.038       0.0699        0.054       0.0485       0.0877       0.0681        0.549      0.00572\n",
            "     95    16      0.00765      0.00756     8.81e-05       0.0507       0.0673         0.04       0.0721       0.0561       0.0513       0.0912       0.0713        0.695      0.00724\n",
            "     95    17      0.00608      0.00607     7.17e-06       0.0447       0.0603       0.0345       0.0653       0.0499       0.0445       0.0832       0.0639        0.194      0.00202\n",
            "     95    18      0.00704      0.00695     8.71e-05        0.049       0.0645       0.0397       0.0678       0.0537       0.0505        0.086       0.0682        0.689      0.00717\n",
            "     95    19      0.00706      0.00704     2.62e-05       0.0473       0.0649       0.0361       0.0696       0.0528       0.0465       0.0911       0.0688        0.376      0.00392\n",
            "     95    20      0.00626       0.0062     6.03e-05       0.0457       0.0609       0.0353       0.0666       0.0509       0.0452        0.084       0.0646        0.573      0.00597\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     95     1      0.00696      0.00695     2.49e-06       0.0482       0.0645       0.0371       0.0704       0.0537       0.0477       0.0891       0.0684       0.0931     0.000969\n",
            "     95     2      0.00696      0.00695        1e-06       0.0478       0.0645       0.0363       0.0708       0.0536       0.0469         0.09       0.0684       0.0483     0.000504\n",
            "     95     3      0.00662      0.00662     1.35e-06       0.0466        0.063       0.0353       0.0693       0.0523       0.0451       0.0884       0.0668       0.0714     0.000744\n",
            "     95     4      0.00678      0.00678     1.58e-06       0.0469       0.0637       0.0357       0.0695       0.0526       0.0454       0.0897       0.0676         0.08     0.000833\n",
            "     95     5      0.00628      0.00628     8.66e-07       0.0459       0.0613       0.0356       0.0665        0.051       0.0458       0.0841        0.065       0.0447     0.000466\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              95  337.967    0.005      0.00682        4e-05      0.00686        0.048       0.0639       0.0377       0.0686       0.0532       0.0483       0.0871       0.0677        0.397      0.00413\n",
            "! Validation         95  337.967    0.005      0.00672     1.46e-06      0.00672       0.0471       0.0634        0.036       0.0693       0.0526       0.0462       0.0883       0.0672       0.0675     0.000703\n",
            "Wall time: 337.968431861\n",
            "! Best model       95    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     96     1      0.00695      0.00688     7.26e-05       0.0489       0.0642       0.0386       0.0696       0.0541        0.048        0.088        0.068         0.63      0.00656\n",
            "     96     2       0.0087      0.00865     4.51e-05       0.0536        0.072       0.0432       0.0744       0.0588       0.0547       0.0977       0.0762        0.496      0.00517\n",
            "     96     3      0.00839      0.00839     3.96e-06       0.0552       0.0708       0.0453       0.0748       0.0601       0.0567       0.0928       0.0748        0.146      0.00152\n",
            "     96     4      0.00668      0.00668     5.68e-07       0.0473       0.0632       0.0373       0.0673       0.0523       0.0474       0.0866        0.067       0.0482     0.000503\n",
            "     96     5      0.00642      0.00642     1.94e-06       0.0457        0.062       0.0354       0.0661       0.0508        0.046       0.0854       0.0657       0.0914     0.000952\n",
            "     96     6      0.00747      0.00746     7.15e-06       0.0512       0.0668        0.042       0.0695       0.0558       0.0542       0.0867       0.0704        0.189      0.00197\n",
            "     96     7       0.0082      0.00818     2.25e-05       0.0525         0.07        0.042       0.0734       0.0577       0.0538       0.0944       0.0741         0.35      0.00365\n",
            "     96     8      0.00761      0.00761     1.51e-06       0.0507       0.0675       0.0415       0.0692       0.0553       0.0525       0.0903       0.0714       0.0842     0.000877\n",
            "     96     9      0.00662      0.00661     2.14e-06       0.0474       0.0629       0.0386        0.065       0.0518       0.0494       0.0837       0.0665       0.0975      0.00102\n",
            "     96    10      0.00629      0.00628     3.71e-06        0.045       0.0613       0.0333       0.0682       0.0508       0.0429       0.0872        0.065        0.132      0.00138\n",
            "     96    11      0.00683      0.00683     6.22e-06       0.0483       0.0639       0.0382       0.0684       0.0533       0.0489       0.0864       0.0677        0.171      0.00178\n",
            "     96    12      0.00544      0.00543     5.86e-06        0.043        0.057       0.0334       0.0622       0.0478       0.0422       0.0787       0.0605        0.157      0.00164\n",
            "     96    13      0.00599      0.00598     1.03e-05       0.0451       0.0598       0.0353       0.0648       0.0501       0.0441       0.0828       0.0634        0.215      0.00224\n",
            "     96    14      0.00587      0.00586     1.46e-05       0.0435       0.0592       0.0321       0.0663       0.0492       0.0407       0.0849       0.0628        0.282      0.00294\n",
            "     96    15      0.00606      0.00605     1.03e-05       0.0452       0.0602       0.0342       0.0672       0.0507       0.0433       0.0844       0.0638        0.224      0.00233\n",
            "     96    16      0.00526      0.00521     4.87e-05       0.0418       0.0559        0.033       0.0594       0.0462        0.042       0.0764       0.0592        0.516      0.00537\n",
            "     96    17      0.00639      0.00639     1.52e-06       0.0466       0.0618       0.0371       0.0657       0.0514       0.0477       0.0832       0.0654       0.0834     0.000869\n",
            "     96    18      0.00581      0.00575     6.22e-05       0.0444       0.0587       0.0339       0.0654       0.0496       0.0427       0.0817       0.0622        0.584      0.00609\n",
            "     96    19      0.00573      0.00572      1.2e-05       0.0439       0.0585       0.0337       0.0644        0.049       0.0437       0.0803        0.062        0.252      0.00262\n",
            "     96    20       0.0064      0.00636     3.93e-05       0.0463       0.0617       0.0346       0.0697       0.0522       0.0436       0.0873       0.0655        0.457      0.00476\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     96     1      0.00693      0.00693     2.43e-06       0.0481       0.0644        0.037       0.0703       0.0536       0.0476        0.089       0.0683       0.0921     0.000959\n",
            "     96     2      0.00693      0.00693     9.89e-07       0.0477       0.0644       0.0362       0.0707       0.0535       0.0468       0.0898       0.0683        0.048       0.0005\n",
            "     96     3       0.0066       0.0066     1.37e-06       0.0465       0.0629       0.0352       0.0692       0.0522        0.045       0.0883       0.0667       0.0714     0.000744\n",
            "     96     4      0.00676      0.00675     1.57e-06       0.0469       0.0636       0.0356       0.0694       0.0525       0.0453       0.0896       0.0674       0.0809     0.000842\n",
            "     96     5      0.00626      0.00626     9.17e-07       0.0458       0.0612       0.0355       0.0664        0.051       0.0457        0.084       0.0649       0.0471      0.00049\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              96  341.446    0.005      0.00664     1.86e-05      0.00666       0.0473        0.063       0.0371       0.0676       0.0523       0.0475       0.0861       0.0668         0.26      0.00271\n",
            "! Validation         96  341.446    0.005      0.00669     1.46e-06       0.0067        0.047       0.0633       0.0359       0.0692       0.0526       0.0461       0.0882       0.0671       0.0679     0.000707\n",
            "Wall time: 341.44684109700006\n",
            "! Best model       96    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     97     1      0.00651       0.0065     1.66e-05       0.0463       0.0624       0.0354       0.0682       0.0518       0.0454       0.0869       0.0661        0.298       0.0031\n",
            "     97     2      0.00749      0.00747      2.3e-05       0.0509       0.0669       0.0415       0.0696       0.0556       0.0527       0.0887       0.0707        0.345      0.00359\n",
            "     97     3      0.00627      0.00627     5.48e-06       0.0464       0.0613       0.0372       0.0649       0.0511       0.0471       0.0826       0.0648        0.169      0.00176\n",
            "     97     4      0.00636      0.00635     1.19e-05       0.0449       0.0617       0.0339       0.0668       0.0504       0.0434       0.0874       0.0654        0.253      0.00263\n",
            "     97     5       0.0065      0.00647     3.15e-05       0.0466       0.0622       0.0365        0.067       0.0517       0.0473       0.0846       0.0659        0.411      0.00428\n",
            "     97     6      0.00587      0.00585     1.89e-05        0.045       0.0592       0.0351        0.065         0.05       0.0438       0.0817       0.0627        0.312      0.00325\n",
            "     97     7      0.00649      0.00639     9.49e-05        0.045       0.0619       0.0342       0.0666       0.0504       0.0443       0.0869       0.0656        0.719      0.00748\n",
            "     97     8      0.00609      0.00608     1.17e-05       0.0453       0.0603       0.0354       0.0652       0.0503       0.0453       0.0825       0.0639        0.251      0.00262\n",
            "     97     9      0.00645      0.00633     0.000124       0.0461       0.0616       0.0366       0.0653       0.0509       0.0466       0.0838       0.0652        0.823      0.00857\n",
            "     97    10      0.00686      0.00686     2.28e-06       0.0479       0.0641       0.0357       0.0723        0.054        0.046         0.09        0.068       0.0908     0.000946\n",
            "     97    11      0.00775      0.00771      3.7e-05       0.0522       0.0679       0.0431       0.0705       0.0568       0.0549       0.0885       0.0717        0.443      0.00462\n",
            "     97    12      0.00757      0.00749     7.85e-05        0.052       0.0669       0.0437       0.0686       0.0562        0.055       0.0861       0.0705        0.655      0.00682\n",
            "     97    13      0.00708      0.00707     1.55e-05       0.0483        0.065       0.0388       0.0673        0.053       0.0506        0.087       0.0688        0.286      0.00298\n",
            "     97    14      0.00552      0.00551     1.32e-05       0.0426       0.0574        0.033       0.0617       0.0474       0.0419       0.0799       0.0609        0.266      0.00277\n",
            "     97    15      0.00745      0.00745     9.47e-07       0.0484       0.0668       0.0362       0.0727       0.0544       0.0475       0.0941       0.0708       0.0559     0.000582\n",
            "     97    16       0.0078       0.0078     7.64e-06       0.0525       0.0683       0.0416       0.0742       0.0579       0.0516       0.0931       0.0724        0.199      0.00208\n",
            "     97    17      0.00846      0.00846     7.91e-06       0.0547       0.0711       0.0464       0.0714       0.0589       0.0584       0.0914       0.0749        0.189      0.00197\n",
            "     97    18      0.00746      0.00745     1.48e-05       0.0499       0.0668       0.0387       0.0722       0.0554       0.0501       0.0914       0.0708        0.284      0.00296\n",
            "     97    19      0.00597      0.00597     1.37e-06       0.0456       0.0598       0.0365       0.0638       0.0501       0.0463       0.0802       0.0632       0.0824     0.000859\n",
            "     97    20      0.00631      0.00631     2.47e-06       0.0461       0.0614       0.0367       0.0649       0.0508       0.0466       0.0835       0.0651        0.106       0.0011\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     97     1      0.00691      0.00691     2.45e-06        0.048       0.0643       0.0369       0.0702       0.0535       0.0474       0.0889       0.0682       0.0924     0.000962\n",
            "     97     2      0.00691       0.0069     1.01e-06       0.0476       0.0643       0.0361       0.0707       0.0534       0.0466       0.0897       0.0682       0.0491     0.000512\n",
            "     97     3      0.00658      0.00658     1.33e-06       0.0465       0.0628       0.0351       0.0692       0.0521       0.0449       0.0882       0.0666       0.0711     0.000741\n",
            "     97     4      0.00674      0.00673     1.58e-06       0.0468       0.0635       0.0355       0.0694       0.0524       0.0452       0.0895       0.0673       0.0813     0.000847\n",
            "     97     5      0.00624      0.00624     8.56e-07       0.0457       0.0611       0.0354       0.0663       0.0509       0.0456       0.0839       0.0648       0.0449     0.000468\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              97  344.911    0.005      0.00679      2.6e-05      0.00681       0.0478       0.0637       0.0378       0.0679       0.0529       0.0484       0.0866       0.0675        0.312      0.00325\n",
            "! Validation         97  344.911    0.005      0.00667     1.45e-06      0.00667       0.0469       0.0632       0.0358       0.0692       0.0525        0.046       0.0881        0.067       0.0678     0.000706\n",
            "Wall time: 344.9117906590002\n",
            "! Best model       97    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     98     1        0.007      0.00697     2.84e-05        0.049       0.0646       0.0384       0.0701       0.0542       0.0484       0.0885       0.0684        0.388      0.00404\n",
            "     98     2        0.007      0.00699     3.86e-06       0.0492       0.0647       0.0379       0.0719       0.0549        0.048       0.0891       0.0686        0.111      0.00116\n",
            "     98     3      0.00582      0.00581     1.42e-05       0.0432        0.059       0.0319       0.0657       0.0488       0.0402       0.0848       0.0625        0.276      0.00287\n",
            "     98     4      0.00726      0.00726     4.39e-07       0.0483       0.0659       0.0356       0.0736       0.0546       0.0461       0.0937       0.0699       0.0395     0.000411\n",
            "     98     5      0.00698      0.00696     2.29e-05       0.0475       0.0645        0.036       0.0705       0.0532       0.0461       0.0908       0.0684        0.354      0.00369\n",
            "     98     6      0.00578      0.00578     3.33e-06       0.0442       0.0588       0.0327       0.0671       0.0499       0.0415       0.0833       0.0624        0.119      0.00123\n",
            "     98     7      0.00624      0.00622     1.62e-05       0.0455        0.061       0.0348       0.0668       0.0508       0.0447       0.0847       0.0647        0.296      0.00308\n",
            "     98     8      0.00577      0.00577     2.88e-06       0.0443       0.0588       0.0349        0.063        0.049       0.0445         0.08       0.0623        0.112      0.00117\n",
            "     98     9      0.00686      0.00686     2.62e-06       0.0484       0.0641       0.0386       0.0681       0.0534       0.0488       0.0869       0.0678        0.108      0.00113\n",
            "     98    10      0.00717      0.00713     3.72e-05       0.0491       0.0653       0.0387       0.0699       0.0543       0.0499       0.0885       0.0692         0.45      0.00468\n",
            "     98    11      0.00718      0.00717     6.48e-06       0.0491       0.0655       0.0375       0.0724        0.055       0.0481       0.0909       0.0695        0.183       0.0019\n",
            "     98    12      0.00634      0.00634     6.23e-06       0.0467       0.0616       0.0376       0.0649       0.0512       0.0473       0.0831       0.0652         0.18      0.00188\n",
            "     98    13      0.00581      0.00581     4.24e-06       0.0445        0.059       0.0338       0.0658       0.0498       0.0428       0.0823       0.0625        0.138      0.00144\n",
            "     98    14       0.0062       0.0062     5.19e-06       0.0452       0.0609       0.0353        0.065       0.0501       0.0445       0.0847       0.0646         0.16      0.00167\n",
            "     98    15      0.00583      0.00583      9.8e-07        0.044       0.0591        0.034       0.0639        0.049       0.0429       0.0823       0.0626       0.0707     0.000736\n",
            "     98    16      0.00496      0.00496     1.53e-06       0.0416       0.0545       0.0332       0.0584       0.0458       0.0418       0.0736       0.0577       0.0752     0.000783\n",
            "     98    17      0.00721      0.00721      3.1e-07       0.0484       0.0657       0.0369       0.0713       0.0541       0.0477       0.0916       0.0697       0.0367     0.000382\n",
            "     98    18      0.00642       0.0064     1.91e-05       0.0471       0.0619       0.0365       0.0682       0.0523       0.0455       0.0858       0.0656        0.323      0.00336\n",
            "     98    19       0.0063       0.0063     1.14e-06       0.0463       0.0614       0.0367       0.0654        0.051       0.0474       0.0826        0.065       0.0672       0.0007\n",
            "     98    20      0.00573      0.00572     9.96e-06       0.0434       0.0585       0.0334       0.0633       0.0484       0.0438       0.0802        0.062        0.232      0.00241\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     98     1      0.00688      0.00688     2.39e-06       0.0479       0.0642       0.0368       0.0701       0.0535       0.0473       0.0888        0.068       0.0901     0.000939\n",
            "     98     2      0.00688      0.00688     1.07e-06       0.0476       0.0642        0.036       0.0706       0.0533       0.0465       0.0896        0.068       0.0503     0.000524\n",
            "     98     3      0.00656      0.00656     1.39e-06       0.0464       0.0627        0.035       0.0691        0.052       0.0448       0.0881       0.0664       0.0709     0.000739\n",
            "     98     4      0.00671      0.00671     1.57e-06       0.0467       0.0634       0.0354       0.0693       0.0523        0.045       0.0894       0.0672        0.079     0.000823\n",
            "     98     5      0.00622      0.00622     8.39e-07       0.0456        0.061       0.0353       0.0663       0.0508       0.0455       0.0839       0.0647       0.0444     0.000463\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              98  348.401    0.005      0.00638     9.35e-06      0.00639       0.0462       0.0618       0.0357       0.0673       0.0515       0.0456       0.0855       0.0655        0.186      0.00194\n",
            "! Validation         98  348.401    0.005      0.00665     1.45e-06      0.00665       0.0468       0.0631       0.0357       0.0691       0.0524       0.0458        0.088       0.0669        0.067     0.000697\n",
            "Wall time: 348.40235154900006\n",
            "! Best model       98    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     99     1      0.00634      0.00633     1.29e-05        0.046       0.0616       0.0356       0.0669       0.0512       0.0453       0.0852       0.0653        0.256      0.00267\n",
            "     99     2      0.00599      0.00598     1.07e-05       0.0443       0.0598       0.0349       0.0629       0.0489       0.0454       0.0813       0.0633        0.216      0.00225\n",
            "     99     3      0.00641       0.0064     5.19e-06        0.046       0.0619       0.0342       0.0698        0.052       0.0442        0.087       0.0656        0.162      0.00169\n",
            "     99     4      0.00864      0.00859     4.82e-05       0.0546       0.0717       0.0449       0.0741       0.0595       0.0568       0.0948       0.0758        0.514      0.00535\n",
            "     99     5      0.00912      0.00911     2.66e-06       0.0566       0.0739       0.0459       0.0779       0.0619       0.0575       0.0988       0.0781        0.105      0.00109\n",
            "     99     6      0.00862      0.00862     6.71e-07       0.0564       0.0718       0.0462       0.0769       0.0615       0.0564       0.0955       0.0759        0.049     0.000511\n",
            "     99     7      0.00806      0.00803     3.15e-05       0.0535       0.0693       0.0444       0.0718       0.0581       0.0569       0.0891        0.073        0.413      0.00431\n",
            "     99     8      0.00605      0.00605     2.71e-06       0.0445       0.0602       0.0349       0.0637       0.0493       0.0442       0.0834       0.0638        0.113      0.00118\n",
            "     99     9       0.0066       0.0066     2.52e-06       0.0468       0.0629       0.0354       0.0696       0.0525       0.0452       0.0881       0.0667       0.0996      0.00104\n",
            "     99    10        0.007        0.007     6.29e-06       0.0484       0.0647       0.0392       0.0669        0.053       0.0508        0.086       0.0684        0.183       0.0019\n",
            "     99    11      0.00712       0.0071     1.75e-05       0.0498       0.0652       0.0421       0.0651       0.0536       0.0537       0.0836       0.0686        0.302      0.00314\n",
            "     99    12      0.00568      0.00568     4.44e-06       0.0435       0.0583       0.0338       0.0628       0.0483       0.0429       0.0807       0.0618        0.148      0.00154\n",
            "     99    13      0.00585      0.00585     9.44e-07        0.044       0.0592       0.0324       0.0672       0.0498       0.0413       0.0842       0.0628       0.0576       0.0006\n",
            "     99    14      0.00694      0.00694     1.02e-06       0.0467       0.0645       0.0354       0.0692       0.0523       0.0467         0.09       0.0684       0.0648     0.000675\n",
            "     99    15       0.0064      0.00638     1.98e-05       0.0462       0.0618       0.0355       0.0675       0.0515       0.0449       0.0862       0.0656         0.32      0.00333\n",
            "     99    16      0.00619      0.00618     6.53e-06       0.0452       0.0608       0.0332       0.0694       0.0513       0.0422       0.0868       0.0645        0.188      0.00196\n",
            "     99    17      0.00657      0.00654     2.54e-05       0.0477       0.0626       0.0384       0.0664       0.0524       0.0482       0.0842       0.0662        0.369      0.00385\n",
            "     99    18      0.00755      0.00755      8.3e-07       0.0526       0.0672       0.0443       0.0693       0.0568       0.0553       0.0862       0.0708       0.0508     0.000529\n",
            "     99    19      0.00822      0.00821     2.46e-06        0.053       0.0701       0.0427       0.0736       0.0581        0.055       0.0933       0.0741       0.0943     0.000983\n",
            "     99    20      0.00628      0.00628     2.32e-06       0.0456       0.0613       0.0348       0.0672        0.051       0.0445       0.0856        0.065        0.099      0.00103\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     99     1      0.00686      0.00685     2.46e-06       0.0478        0.064       0.0367         0.07       0.0534       0.0472       0.0886       0.0679       0.0919     0.000957\n",
            "     99     2      0.00686      0.00686     9.86e-07       0.0475       0.0641       0.0359       0.0706       0.0532       0.0464       0.0895       0.0679       0.0493     0.000514\n",
            "     99     3      0.00654      0.00654     1.35e-06       0.0463       0.0626       0.0349       0.0691        0.052       0.0446       0.0881       0.0664       0.0703     0.000732\n",
            "     99     4      0.00669      0.00669     1.57e-06       0.0466       0.0633       0.0353       0.0693       0.0523       0.0449       0.0893       0.0671       0.0798     0.000831\n",
            "     99     5      0.00619      0.00619     8.61e-07       0.0455       0.0609       0.0352       0.0662       0.0507       0.0453       0.0837       0.0645       0.0456     0.000475\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              99  352.039    0.005      0.00697     1.02e-05      0.00698       0.0486       0.0646       0.0384       0.0689       0.0536       0.0492       0.0876       0.0684         0.19      0.00198\n",
            "! Validation         99  352.039    0.005      0.00663     1.44e-06      0.00663       0.0467        0.063       0.0356        0.069       0.0523       0.0457       0.0879       0.0668       0.0674     0.000702\n",
            "Wall time: 352.03973030300017\n",
            "! Best model       99    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    100     1      0.00575      0.00575     2.97e-06       0.0442       0.0587       0.0339       0.0647       0.0493        0.042       0.0824       0.0622       0.0943     0.000983\n",
            "    100     2      0.00585      0.00584     5.57e-06       0.0445       0.0591       0.0345       0.0645       0.0495       0.0444       0.0809       0.0627        0.162      0.00168\n",
            "    100     3      0.00614      0.00613      6.5e-06        0.045       0.0606        0.034       0.0668       0.0504       0.0442       0.0843       0.0643        0.176      0.00184\n",
            "    100     4      0.00541      0.00539     1.35e-05       0.0431       0.0568       0.0341        0.061       0.0476       0.0427       0.0777       0.0602        0.265      0.00276\n",
            "    100     5      0.00612      0.00611     7.12e-06       0.0445       0.0605       0.0334       0.0666         0.05       0.0422        0.086       0.0641        0.191      0.00199\n",
            "    100     6      0.00636      0.00636     5.65e-07       0.0444       0.0617       0.0333       0.0666         0.05        0.043       0.0879       0.0654       0.0434     0.000452\n",
            "    100     7      0.00645      0.00645     1.16e-06       0.0466       0.0621       0.0369       0.0659       0.0514       0.0476        0.084       0.0658       0.0672       0.0007\n",
            "    100     8      0.00663      0.00662     1.18e-05       0.0483        0.063       0.0403       0.0643       0.0523       0.0512       0.0815       0.0664        0.244      0.00255\n",
            "    100     9      0.00644      0.00644     1.28e-06       0.0475       0.0621       0.0384       0.0657        0.052       0.0481       0.0832       0.0657       0.0764     0.000795\n",
            "    100    10      0.00577      0.00577     1.56e-06       0.0442       0.0588       0.0333       0.0661       0.0497       0.0423       0.0824       0.0623       0.0748     0.000779\n",
            "    100    11      0.00703      0.00703      2.8e-06       0.0488       0.0648       0.0394       0.0676       0.0535       0.0501       0.0872       0.0686        0.107      0.00111\n",
            "    100    12      0.00658      0.00658     1.05e-06        0.047       0.0627       0.0363       0.0685       0.0524       0.0455       0.0876       0.0665       0.0646     0.000673\n",
            "    100    13      0.00745      0.00743     2.54e-05       0.0494       0.0667       0.0372       0.0738       0.0555       0.0477       0.0938       0.0707        0.355       0.0037\n",
            "    100    14      0.00614      0.00614     7.87e-07       0.0455       0.0606       0.0343       0.0679       0.0511       0.0434       0.0852       0.0643        0.058     0.000604\n",
            "    100    15      0.00553      0.00552     1.51e-05       0.0437       0.0575       0.0341       0.0628       0.0484       0.0435       0.0783       0.0609        0.277      0.00288\n",
            "    100    16      0.00693      0.00693     1.68e-06       0.0483       0.0644       0.0383       0.0682       0.0533       0.0498       0.0865       0.0682       0.0902      0.00094\n",
            "    100    17      0.00652      0.00651      9.5e-06       0.0486       0.0624       0.0388        0.068       0.0534       0.0485       0.0836        0.066        0.221       0.0023\n",
            "    100    18      0.00654      0.00654     1.98e-06       0.0468       0.0625       0.0351       0.0703       0.0527        0.045       0.0877       0.0663       0.0703     0.000732\n",
            "    100    19      0.00617      0.00616     6.19e-06       0.0449       0.0607        0.035       0.0647       0.0499       0.0446       0.0841       0.0644        0.181      0.00188\n",
            "    100    20      0.00662      0.00661     3.78e-06       0.0464       0.0629       0.0338       0.0716       0.0527       0.0437       0.0898       0.0667        0.131      0.00137\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    100     1      0.00683      0.00683     2.41e-06       0.0477       0.0639       0.0366       0.0699       0.0533        0.047       0.0885       0.0678        0.091     0.000948\n",
            "    100     2      0.00683      0.00683     1.03e-06       0.0474       0.0639       0.0358       0.0705       0.0532       0.0462       0.0894       0.0678       0.0493     0.000514\n",
            "    100     3      0.00652      0.00652     1.36e-06       0.0462       0.0625       0.0348        0.069       0.0519       0.0445        0.088       0.0663       0.0702     0.000731\n",
            "    100     4      0.00666      0.00666     1.59e-06       0.0465       0.0631       0.0352       0.0692       0.0522       0.0448       0.0892        0.067       0.0802     0.000835\n",
            "    100     5      0.00617      0.00617     8.34e-07       0.0454       0.0608       0.0351       0.0661       0.0506       0.0452       0.0836       0.0644        0.045     0.000469\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             100  355.553    0.005      0.00632     6.02e-06      0.00632       0.0461       0.0615       0.0357       0.0668       0.0512       0.0456       0.0848       0.0652        0.147      0.00154\n",
            "! Validation        100  355.553    0.005       0.0066     1.44e-06       0.0066       0.0466       0.0629       0.0355       0.0689       0.0522       0.0456       0.0878       0.0667       0.0671     0.000699\n",
            "Wall time: 355.5537620470002\n",
            "! Best model      100    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    101     1      0.00603      0.00603     4.29e-07       0.0451       0.0601       0.0343       0.0666       0.0505       0.0431       0.0844       0.0637       0.0369     0.000385\n",
            "    101     2      0.00635      0.00634     7.63e-06       0.0456       0.0616       0.0332       0.0703       0.0517       0.0424       0.0882       0.0653        0.189      0.00197\n",
            "    101     3      0.00515      0.00514     2.84e-06       0.0422       0.0555       0.0319       0.0627       0.0473         0.04       0.0777       0.0588       0.0762     0.000793\n",
            "    101     4      0.00612      0.00611     1.01e-05       0.0457       0.0605       0.0345       0.0683       0.0514       0.0436       0.0846       0.0641        0.231       0.0024\n",
            "    101     5      0.00725      0.00725     1.02e-06       0.0505       0.0659       0.0427       0.0661       0.0544       0.0537       0.0851       0.0694       0.0682      0.00071\n",
            "    101     6      0.00793      0.00793     2.35e-06       0.0531       0.0689       0.0451       0.0689        0.057       0.0568       0.0882       0.0725        0.104      0.00108\n",
            "    101     7      0.00667      0.00665     2.53e-05       0.0476       0.0631       0.0367       0.0694       0.0531        0.046       0.0878       0.0669        0.369      0.00384\n",
            "    101     8       0.0059       0.0059     6.12e-07       0.0436       0.0594       0.0334       0.0641       0.0487        0.043        0.083        0.063       0.0562     0.000586\n",
            "    101     9      0.00676      0.00673     3.19e-05       0.0483       0.0635        0.038        0.069       0.0535        0.048       0.0865       0.0672        0.416      0.00434\n",
            "    101    10      0.00729      0.00728     5.06e-06       0.0501        0.066       0.0413       0.0677       0.0545       0.0529       0.0864       0.0697        0.162      0.00169\n",
            "    101    11      0.00568      0.00568     1.46e-06       0.0441       0.0583        0.034       0.0644       0.0492       0.0431       0.0805       0.0618       0.0812     0.000846\n",
            "    101    12      0.00625      0.00625     1.17e-06       0.0457       0.0612       0.0342       0.0686       0.0514        0.043       0.0868       0.0649       0.0686     0.000714\n",
            "    101    13      0.00579      0.00578        1e-05       0.0442       0.0588       0.0344       0.0638       0.0491       0.0443       0.0803       0.0623        0.232      0.00242\n",
            "    101    14      0.00732      0.00732     6.15e-07       0.0492       0.0662       0.0377       0.0722        0.055        0.049       0.0913       0.0701       0.0396     0.000413\n",
            "    101    15      0.00592      0.00591     1.33e-05       0.0453       0.0595       0.0342       0.0676       0.0509       0.0426       0.0835       0.0631         0.26       0.0027\n",
            "    101    16      0.00608      0.00608     9.89e-07       0.0452       0.0603        0.035       0.0654       0.0502       0.0452       0.0826       0.0639        0.067     0.000698\n",
            "    101    17      0.00592      0.00592     3.97e-06       0.0439       0.0595       0.0328       0.0663       0.0495       0.0419       0.0844       0.0631        0.132      0.00138\n",
            "    101    18      0.00645      0.00645     9.93e-07       0.0462       0.0621       0.0343       0.0702       0.0522       0.0441       0.0877       0.0659       0.0656     0.000684\n",
            "    101    19      0.00609      0.00609      3.2e-06       0.0448       0.0604       0.0356       0.0631       0.0494       0.0456       0.0823        0.064       0.0943     0.000983\n",
            "    101    20      0.00717      0.00715     1.72e-05        0.049       0.0654        0.039        0.069        0.054       0.0496        0.089       0.0693        0.291      0.00304\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    101     1       0.0068       0.0068     2.41e-06       0.0476       0.0638       0.0365       0.0698       0.0532       0.0469       0.0884       0.0677       0.0917     0.000955\n",
            "    101     2      0.00681      0.00681     9.95e-07       0.0473       0.0638       0.0358       0.0704       0.0531       0.0461       0.0893       0.0677       0.0484     0.000505\n",
            "    101     3       0.0065       0.0065     1.31e-06       0.0461       0.0624       0.0347       0.0689       0.0518       0.0444       0.0879       0.0662       0.0694     0.000723\n",
            "    101     4      0.00664      0.00664      1.6e-06       0.0464        0.063       0.0351       0.0691       0.0521       0.0446       0.0891       0.0669       0.0803     0.000836\n",
            "    101     5      0.00615      0.00615     8.54e-07       0.0454       0.0607        0.035        0.066       0.0505        0.045       0.0836       0.0643       0.0455     0.000474\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             101  359.104    0.005       0.0064     7.01e-06      0.00641       0.0465       0.0619       0.0361       0.0672       0.0516       0.0461       0.0851       0.0656        0.152      0.00158\n",
            "! Validation        101  359.104    0.005      0.00658     1.43e-06      0.00658       0.0466       0.0628       0.0354       0.0689       0.0521       0.0454       0.0877       0.0666       0.0671     0.000699\n",
            "Wall time: 359.10533098300016\n",
            "! Best model      101    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    102     1      0.00582      0.00581     2.46e-06       0.0443        0.059       0.0331       0.0669         0.05       0.0423       0.0828       0.0626        0.104      0.00108\n",
            "    102     2      0.00694      0.00693     9.73e-06        0.049       0.0644       0.0396       0.0678       0.0537       0.0508       0.0853       0.0681        0.231       0.0024\n",
            "    102     3      0.00761      0.00761     1.97e-06       0.0513       0.0675       0.0412       0.0714       0.0563       0.0521       0.0908       0.0714       0.0953     0.000993\n",
            "    102     4      0.00801        0.008     1.98e-06       0.0524       0.0692       0.0413       0.0748        0.058       0.0526        0.094       0.0733       0.0771     0.000804\n",
            "    102     5      0.00773      0.00772      8.1e-06       0.0521        0.068       0.0415       0.0735       0.0575       0.0518       0.0922        0.072          0.2      0.00209\n",
            "    102     6      0.00646      0.00645     1.02e-05       0.0472       0.0621       0.0393       0.0628       0.0511       0.0497       0.0815       0.0656         0.22      0.00229\n",
            "    102     7      0.00671      0.00671     4.71e-06       0.0483       0.0634       0.0386       0.0678       0.0532        0.049        0.085        0.067        0.154       0.0016\n",
            "    102     8      0.00587      0.00585     2.15e-05       0.0445       0.0591       0.0349       0.0639       0.0494       0.0438       0.0816       0.0627        0.338      0.00352\n",
            "    102     9      0.00834      0.00834     1.67e-06       0.0546       0.0706       0.0453       0.0732       0.0592       0.0568       0.0923       0.0745        0.073     0.000761\n",
            "    102    10      0.00848      0.00847     7.97e-06       0.0545       0.0712       0.0427       0.0781       0.0604       0.0538        0.097       0.0754        0.202      0.00211\n",
            "    102    11      0.00785      0.00782     2.48e-05       0.0523       0.0684        0.042       0.0731       0.0575       0.0535       0.0912       0.0724        0.361      0.00376\n",
            "    102    12      0.00637      0.00633     4.06e-05        0.047       0.0616       0.0387       0.0635       0.0511       0.0485       0.0817       0.0651        0.468      0.00487\n",
            "    102    13      0.00572      0.00569     2.45e-05       0.0443       0.0584       0.0354       0.0621       0.0487       0.0449       0.0787       0.0618        0.363      0.00378\n",
            "    102    14      0.00665      0.00659     5.76e-05       0.0475       0.0628       0.0364       0.0696        0.053       0.0466       0.0866       0.0666        0.556      0.00579\n",
            "    102    15      0.00742      0.00737     4.54e-05       0.0513       0.0664       0.0422       0.0694       0.0558       0.0524        0.088       0.0702        0.496      0.00516\n",
            "    102    16       0.0079       0.0079     9.07e-07       0.0518       0.0688       0.0406       0.0742       0.0574       0.0513       0.0945       0.0729       0.0578     0.000602\n",
            "    102    17      0.00653      0.00635     0.000181       0.0467       0.0617       0.0377       0.0645       0.0511       0.0476       0.0829       0.0652        0.999       0.0104\n",
            "    102    18      0.00594      0.00593     1.34e-05       0.0436       0.0596       0.0331       0.0647       0.0489       0.0422       0.0841       0.0632        0.262      0.00273\n",
            "    102    19      0.00757      0.00746     0.000107         0.05       0.0668       0.0383       0.0734       0.0558       0.0486       0.0931       0.0709        0.767      0.00799\n",
            "    102    20       0.0092       0.0092     2.09e-06       0.0575       0.0742       0.0498        0.073       0.0614       0.0633       0.0922       0.0777       0.0949     0.000989\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    102     1      0.00678      0.00678     2.32e-06       0.0475       0.0637       0.0364       0.0697       0.0531       0.0468       0.0883       0.0675       0.0888     0.000925\n",
            "    102     2      0.00678      0.00678     1.01e-06       0.0472       0.0637       0.0356       0.0703        0.053        0.046       0.0891       0.0675       0.0488     0.000509\n",
            "    102     3      0.00648      0.00648      1.3e-06        0.046       0.0623       0.0346       0.0688       0.0517       0.0442       0.0878        0.066       0.0697     0.000726\n",
            "    102     4      0.00662      0.00662     1.58e-06       0.0463       0.0629        0.035        0.069        0.052       0.0445        0.089       0.0668       0.0798     0.000831\n",
            "    102     5      0.00612      0.00612     8.77e-07       0.0452       0.0605       0.0349        0.066       0.0504       0.0449       0.0835       0.0642       0.0459     0.000478\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             102  362.657    0.005      0.00713     2.84e-05      0.00715       0.0495       0.0653       0.0396       0.0694       0.0545       0.0503       0.0879       0.0691        0.306      0.00319\n",
            "! Validation        102  362.657    0.005      0.00655     1.42e-06      0.00656       0.0465       0.0626       0.0353       0.0688        0.052       0.0453       0.0876       0.0664       0.0666     0.000694\n",
            "Wall time: 362.65806286600014\n",
            "! Best model      102    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    103     1      0.00816      0.00816     4.89e-06       0.0546       0.0699       0.0469       0.0699       0.0584       0.0583       0.0886       0.0735        0.139      0.00144\n",
            "    103     2      0.00655      0.00655     3.48e-06       0.0468       0.0626       0.0348       0.0708       0.0528       0.0441       0.0888       0.0664        0.129      0.00134\n",
            "    103     3      0.00691      0.00689     2.48e-05       0.0481       0.0642       0.0367       0.0707       0.0537       0.0468       0.0894       0.0681        0.363      0.00379\n",
            "    103     4      0.00642      0.00641     5.21e-06       0.0465       0.0619        0.036       0.0674       0.0517       0.0457       0.0856       0.0657        0.146      0.00152\n",
            "    103     5      0.00543      0.00536     6.99e-05       0.0435       0.0567       0.0347       0.0612       0.0479       0.0434       0.0766         0.06        0.613      0.00639\n",
            "    103     6      0.00593      0.00592     6.02e-06       0.0443       0.0595       0.0335       0.0659       0.0497       0.0427       0.0835       0.0631        0.169      0.00176\n",
            "    103     7      0.00753      0.00747     5.54e-05       0.0504       0.0669        0.038       0.0753       0.0566       0.0479        0.094       0.0709         0.55      0.00573\n",
            "    103     8      0.00775      0.00775     2.82e-07       0.0519       0.0681       0.0413        0.073       0.0571       0.0522        0.092       0.0721       0.0291     0.000303\n",
            "    103     9      0.00814      0.00812      1.9e-05       0.0551       0.0697       0.0506        0.064       0.0573       0.0622       0.0827       0.0724        0.323      0.00336\n",
            "    103    10      0.00692      0.00691     8.54e-06       0.0482       0.0643        0.039       0.0667       0.0528       0.0501       0.0859        0.068        0.215      0.00224\n",
            "    103    11       0.0068      0.00679     1.14e-05       0.0484       0.0637       0.0364       0.0723       0.0543       0.0456       0.0896       0.0676        0.242      0.00252\n",
            "    103    12      0.00605      0.00605     2.86e-06       0.0457       0.0602       0.0369       0.0635       0.0502       0.0475       0.0797       0.0636        0.109      0.00114\n",
            "    103    13      0.00811      0.00809      2.3e-05        0.052       0.0696        0.039       0.0782       0.0586       0.0489       0.0987       0.0738        0.349      0.00363\n",
            "    103    14       0.0067       0.0067     7.46e-06       0.0468       0.0633       0.0363       0.0679       0.0521       0.0469       0.0874       0.0671        0.186      0.00193\n",
            "    103    15      0.00626      0.00626     7.99e-06       0.0471       0.0612       0.0381       0.0651       0.0516       0.0471       0.0825       0.0648        0.197      0.00205\n",
            "    103    16      0.00687      0.00687     3.78e-06       0.0486       0.0641       0.0376       0.0706       0.0541       0.0468       0.0891        0.068        0.137      0.00143\n",
            "    103    17       0.0086       0.0086      4.1e-07       0.0574       0.0717        0.052       0.0682       0.0601       0.0636       0.0858       0.0747       0.0416     0.000433\n",
            "    103    18      0.00919      0.00918     5.17e-06       0.0587       0.0741        0.053       0.0701       0.0615       0.0654        0.089       0.0772        0.155      0.00161\n",
            "    103    19      0.00954      0.00953      1.2e-05       0.0573       0.0755       0.0442       0.0835       0.0639        0.056        0.104       0.0801        0.247      0.00257\n",
            "    103    20       0.0073      0.00729     1.01e-05       0.0501       0.0661       0.0389       0.0726       0.0558       0.0483       0.0919       0.0701        0.233      0.00243\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    103     1      0.00676      0.00675     2.33e-06       0.0474       0.0636       0.0363       0.0696        0.053       0.0466       0.0882       0.0674         0.09     0.000938\n",
            "    103     2      0.00676      0.00675     9.45e-07       0.0471       0.0636       0.0356       0.0702       0.0529       0.0459        0.089       0.0674       0.0481     0.000502\n",
            "    103     3      0.00646      0.00646     1.36e-06       0.0459       0.0622       0.0345       0.0687       0.0516       0.0441       0.0877       0.0659       0.0707     0.000736\n",
            "    103     4       0.0066       0.0066      1.6e-06       0.0462       0.0628       0.0349        0.069       0.0519       0.0444       0.0889       0.0666       0.0812     0.000845\n",
            "    103     5      0.00611      0.00611     8.15e-07       0.0452       0.0605       0.0348       0.0659       0.0504       0.0448       0.0834       0.0641       0.0438     0.000457\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             103  366.168    0.005      0.00724     1.41e-05      0.00726       0.0501       0.0658       0.0402       0.0698        0.055       0.0509       0.0885       0.0697        0.229      0.00238\n",
            "! Validation        103  366.168    0.005      0.00653     1.41e-06      0.00654       0.0464       0.0625       0.0352       0.0687        0.052       0.0452       0.0875       0.0663       0.0668     0.000696\n",
            "Wall time: 366.16880708099984\n",
            "! Best model      103    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    104     1      0.00525      0.00525     2.94e-06       0.0423        0.056       0.0315        0.064       0.0478       0.0398       0.0791       0.0594        0.116      0.00121\n",
            "    104     2      0.00726      0.00724     1.85e-05       0.0485       0.0658       0.0357       0.0742       0.0549       0.0459       0.0938       0.0698        0.315      0.00328\n",
            "    104     3      0.00773      0.00772     1.03e-05       0.0537        0.068       0.0475       0.0662       0.0569       0.0593       0.0826        0.071        0.222      0.00231\n",
            "    104     4      0.00919      0.00919     1.95e-06       0.0574       0.0742       0.0497       0.0727       0.0612       0.0632       0.0923       0.0777       0.0719     0.000749\n",
            "    104     5      0.00781       0.0078     1.62e-05       0.0516       0.0683       0.0417       0.0714       0.0566       0.0528       0.0918       0.0723        0.284      0.00296\n",
            "    104     6      0.00558      0.00556     1.74e-05       0.0429       0.0577       0.0323        0.064       0.0482       0.0408       0.0816       0.0612        0.305      0.00318\n",
            "    104     7      0.00563      0.00562     7.37e-06       0.0433        0.058       0.0336       0.0626       0.0481       0.0423       0.0807       0.0615          0.2      0.00208\n",
            "    104     8      0.00679      0.00677     1.84e-05       0.0479       0.0636       0.0384        0.067       0.0527       0.0481       0.0868       0.0674        0.317       0.0033\n",
            "    104     9      0.00732      0.00732     2.96e-07       0.0507       0.0662        0.042       0.0681        0.055       0.0529       0.0868       0.0699       0.0344     0.000358\n",
            "    104    10      0.00636      0.00636     4.26e-06       0.0475       0.0617       0.0377       0.0669       0.0523       0.0469       0.0838       0.0653        0.141      0.00146\n",
            "    104    11      0.00675      0.00675     6.77e-06       0.0486       0.0635       0.0381       0.0697       0.0539       0.0488       0.0857       0.0673        0.172      0.00179\n",
            "    104    12      0.00731      0.00731     3.43e-06       0.0513       0.0661       0.0442       0.0655       0.0549       0.0556       0.0833       0.0694        0.134       0.0014\n",
            "    104    13      0.00742      0.00742     1.69e-06       0.0501       0.0667       0.0393       0.0718       0.0555         0.05       0.0913       0.0706        0.075     0.000781\n",
            "    104    14      0.00666      0.00663      3.5e-05       0.0469        0.063       0.0352       0.0701       0.0527       0.0447       0.0889       0.0668        0.435      0.00453\n",
            "    104    15      0.00603      0.00603     3.82e-06       0.0441       0.0601       0.0347        0.063       0.0489       0.0451       0.0822       0.0637         0.11      0.00114\n",
            "    104    16      0.00607      0.00605     2.31e-05       0.0455       0.0602       0.0362       0.0641       0.0501        0.046       0.0815       0.0637        0.345      0.00359\n",
            "    104    17      0.00773      0.00773     1.22e-06       0.0526        0.068       0.0435       0.0708       0.0572       0.0548       0.0887       0.0718       0.0676     0.000704\n",
            "    104    18      0.00724      0.00724     3.13e-07       0.0513       0.0658       0.0429        0.068       0.0555       0.0534       0.0854       0.0694       0.0355      0.00037\n",
            "    104    19      0.00716      0.00716     1.27e-06        0.048       0.0654        0.035       0.0742       0.0546       0.0445       0.0943       0.0694       0.0676     0.000704\n",
            "    104    20      0.00778      0.00778     6.25e-07       0.0528       0.0682       0.0459       0.0667       0.0563       0.0574       0.0858       0.0716       0.0551     0.000574\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    104     1      0.00674      0.00673     2.32e-06       0.0473       0.0635       0.0362       0.0696       0.0529       0.0465       0.0881       0.0673       0.0908     0.000946\n",
            "    104     2      0.00674      0.00674     9.57e-07        0.047       0.0635       0.0355       0.0702       0.0528       0.0458       0.0889       0.0673       0.0465     0.000484\n",
            "    104     3      0.00644      0.00644     1.32e-06       0.0459       0.0621       0.0345       0.0687       0.0516       0.0441       0.0877       0.0659       0.0702     0.000731\n",
            "    104     4      0.00658      0.00658      1.6e-06       0.0462       0.0628       0.0348       0.0689       0.0518       0.0443       0.0888       0.0666       0.0812     0.000846\n",
            "    104     5      0.00609      0.00609     8.43e-07       0.0451       0.0604       0.0347       0.0659       0.0503       0.0446       0.0834        0.064       0.0444     0.000463\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             104  369.720    0.005      0.00695     8.75e-06      0.00695       0.0489       0.0645       0.0393       0.0681       0.0537         0.05       0.0864       0.0682        0.175      0.00182\n",
            "! Validation        104  369.720    0.005      0.00652     1.41e-06      0.00652       0.0463       0.0625       0.0351       0.0686       0.0519       0.0451       0.0874       0.0662       0.0666     0.000694\n",
            "Wall time: 369.72123333900004\n",
            "! Best model      104    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    105     1      0.00718      0.00718     1.21e-06         0.05       0.0655       0.0417       0.0667       0.0542       0.0533       0.0849       0.0691       0.0758     0.000789\n",
            "    105     2      0.00697      0.00696     1.37e-05       0.0475       0.0645       0.0344       0.0738       0.0541       0.0445       0.0924       0.0684        0.273      0.00285\n",
            "    105     3      0.00696      0.00696     3.63e-07       0.0485       0.0646       0.0386       0.0683       0.0534       0.0499       0.0868       0.0683       0.0318     0.000332\n",
            "    105     4       0.0063      0.00627     2.95e-05        0.047       0.0613       0.0379       0.0652       0.0516        0.047       0.0827       0.0649        0.401      0.00417\n",
            "    105     5      0.00617      0.00617      1.4e-06       0.0458       0.0608       0.0359       0.0654       0.0507       0.0464       0.0823       0.0644       0.0756     0.000787\n",
            "    105     6      0.00607      0.00606     1.58e-05       0.0449       0.0602       0.0344       0.0657       0.0501       0.0431       0.0846       0.0639        0.289      0.00302\n",
            "    105     7      0.00634      0.00634     6.44e-07       0.0461       0.0616        0.035       0.0685       0.0517       0.0444       0.0863       0.0654         0.05     0.000521\n",
            "    105     8        0.007        0.007     4.29e-06       0.0496       0.0647       0.0413       0.0661       0.0537       0.0521       0.0845       0.0683        0.135       0.0014\n",
            "    105     9      0.00608      0.00607      2.6e-06       0.0443       0.0603        0.032       0.0689       0.0505        0.041       0.0869       0.0639        0.115       0.0012\n",
            "    105    10      0.00681       0.0068     8.46e-06       0.0502       0.0638       0.0422       0.0663       0.0542       0.0526       0.0817       0.0672         0.21      0.00218\n",
            "    105    11      0.00832       0.0083     1.89e-05       0.0548       0.0705       0.0458       0.0726       0.0592       0.0577       0.0908       0.0743        0.313      0.00326\n",
            "    105    12      0.00849      0.00847     2.32e-05       0.0545       0.0712       0.0435       0.0765         0.06       0.0537       0.0971       0.0754        0.354      0.00369\n",
            "    105    13      0.00644      0.00639     5.09e-05       0.0458       0.0619       0.0357       0.0658       0.0508       0.0457       0.0855       0.0656        0.524      0.00546\n",
            "    105    14      0.00655      0.00649      5.9e-05       0.0461       0.0623       0.0343       0.0697        0.052       0.0435       0.0887       0.0661        0.565      0.00589\n",
            "    105    15      0.00771      0.00766      5.1e-05        0.051       0.0677       0.0419       0.0692       0.0556       0.0539       0.0891       0.0715        0.524      0.00546\n",
            "    105    16      0.00828      0.00828     3.68e-07       0.0553       0.0704       0.0499        0.066       0.0579        0.062       0.0848       0.0734       0.0301     0.000313\n",
            "    105    17      0.00778      0.00777     3.13e-06       0.0512       0.0682       0.0397        0.074       0.0569       0.0503       0.0944       0.0723        0.125       0.0013\n",
            "    105    18      0.00668      0.00667     1.08e-06       0.0476       0.0632       0.0375       0.0677       0.0526       0.0468       0.0872        0.067       0.0686     0.000714\n",
            "    105    19      0.00579      0.00576     2.82e-05       0.0438       0.0587       0.0341       0.0631       0.0486       0.0434        0.081       0.0622        0.387      0.00403\n",
            "    105    20      0.00799      0.00797      2.7e-05       0.0522        0.069       0.0401       0.0764       0.0582       0.0508       0.0957       0.0732        0.372      0.00388\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    105     1      0.00672      0.00672      2.3e-06       0.0473       0.0634       0.0362       0.0695       0.0528       0.0464        0.088       0.0672       0.0882     0.000919\n",
            "    105     2      0.00672      0.00672        1e-06        0.047       0.0634       0.0354       0.0701       0.0528       0.0457       0.0889       0.0673       0.0479     0.000498\n",
            "    105     3      0.00643      0.00643     1.35e-06       0.0458        0.062       0.0344       0.0687       0.0516        0.044       0.0876       0.0658       0.0701      0.00073\n",
            "    105     4      0.00657      0.00657      1.6e-06       0.0461       0.0627       0.0347       0.0689       0.0518       0.0442       0.0888       0.0665       0.0811     0.000844\n",
            "    105     5      0.00607      0.00607     8.34e-07        0.045       0.0603       0.0346       0.0658       0.0502       0.0445       0.0833       0.0639       0.0455     0.000474\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             105  373.281    0.005      0.00698      1.7e-05        0.007       0.0488       0.0646       0.0388       0.0688       0.0538       0.0494       0.0875       0.0684        0.246      0.00256\n",
            "! Validation        105  373.281    0.005       0.0065     1.42e-06       0.0065       0.0462       0.0624       0.0351       0.0686       0.0518        0.045       0.0873       0.0662       0.0665     0.000693\n",
            "Wall time: 373.2816839489999\n",
            "! Best model      105    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    106     1      0.00682      0.00668     0.000138       0.0479       0.0632        0.038       0.0677       0.0529       0.0481       0.0858        0.067         0.87      0.00906\n",
            "    106     2      0.00729      0.00724     5.06e-05       0.0513       0.0658       0.0441       0.0656       0.0549       0.0549       0.0835       0.0692        0.526      0.00548\n",
            "    106     3      0.00626      0.00624     2.79e-05       0.0462       0.0611       0.0353       0.0678       0.0516       0.0457       0.0838       0.0647        0.379      0.00395\n",
            "    106     4       0.0065      0.00635     0.000154       0.0462       0.0616       0.0373       0.0642       0.0507       0.0472       0.0834       0.0653        0.921      0.00959\n",
            "    106     5      0.00607      0.00607     4.75e-07       0.0445       0.0603       0.0337       0.0661       0.0499       0.0433       0.0845       0.0639       0.0402     0.000419\n",
            "    106     6      0.00716      0.00704     0.000124       0.0481       0.0649       0.0364       0.0716        0.054       0.0473       0.0904       0.0688        0.824      0.00858\n",
            "    106     7      0.00714      0.00714     3.19e-06       0.0494       0.0654       0.0398       0.0688       0.0543         0.05       0.0884       0.0692        0.111      0.00116\n",
            "    106     8      0.00639      0.00638     1.28e-05       0.0473       0.0618       0.0384       0.0651       0.0517       0.0482       0.0825       0.0654        0.257      0.00267\n",
            "    106     9      0.00552      0.00552     6.09e-07        0.043       0.0575       0.0325       0.0639       0.0482       0.0405       0.0814       0.0609       0.0506     0.000527\n",
            "    106    10      0.00648      0.00647     5.93e-06       0.0472       0.0622       0.0384       0.0647       0.0516       0.0483       0.0834       0.0659        0.159      0.00165\n",
            "    106    11      0.00502      0.00502     3.97e-07       0.0409       0.0548       0.0309        0.061        0.046       0.0394       0.0768       0.0581       0.0354     0.000368\n",
            "    106    12      0.00712      0.00708     3.23e-05       0.0499       0.0651       0.0403       0.0691       0.0547       0.0509       0.0868       0.0689        0.421      0.00438\n",
            "    106    13      0.00655      0.00655     2.26e-06       0.0474       0.0626       0.0396        0.063       0.0513       0.0499       0.0823       0.0661         0.11      0.00115\n",
            "    106    14      0.00771      0.00764     7.43e-05       0.0511       0.0676       0.0399       0.0735       0.0567         0.05       0.0933       0.0717        0.638      0.00665\n",
            "    106    15      0.00642      0.00642     3.88e-06       0.0461        0.062       0.0342       0.0699        0.052       0.0432       0.0882       0.0657        0.136      0.00142\n",
            "    106    16      0.00655      0.00653     2.75e-05       0.0469       0.0625       0.0368       0.0673        0.052       0.0469       0.0855       0.0662        0.386      0.00402\n",
            "    106    17      0.00648      0.00648      7.1e-06       0.0452       0.0623       0.0327       0.0702       0.0514       0.0418       0.0902        0.066        0.196      0.00204\n",
            "    106    18      0.00638      0.00638     9.66e-06       0.0459       0.0618       0.0356       0.0665        0.051       0.0463       0.0847       0.0655          0.2      0.00209\n",
            "    106    19      0.00584      0.00584     8.54e-07       0.0448       0.0591       0.0351       0.0642       0.0496       0.0443        0.081       0.0626       0.0621     0.000647\n",
            "    106    20      0.00665      0.00665     7.89e-07       0.0474       0.0631       0.0361         0.07        0.053       0.0456       0.0882       0.0669       0.0523     0.000545\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    106     1       0.0067       0.0067     2.29e-06       0.0472       0.0633       0.0361       0.0694       0.0528       0.0464       0.0879       0.0671       0.0895     0.000932\n",
            "    106     2       0.0067       0.0067     9.51e-07       0.0469       0.0633       0.0354       0.0701       0.0527       0.0456       0.0888       0.0672       0.0453     0.000472\n",
            "    106     3      0.00642      0.00642     1.34e-06       0.0458        0.062       0.0344       0.0687       0.0515       0.0439       0.0876       0.0657       0.0702     0.000731\n",
            "    106     4      0.00655      0.00655     1.55e-06       0.0461       0.0626       0.0347       0.0688       0.0518       0.0441       0.0887       0.0664       0.0791     0.000824\n",
            "    106     5      0.00606      0.00606     8.21e-07       0.0449       0.0602       0.0345       0.0657       0.0501       0.0444       0.0832       0.0638       0.0457     0.000476\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             106  376.834    0.005      0.00648     3.39e-05      0.00652       0.0468       0.0623       0.0367        0.067       0.0519       0.0467       0.0853        0.066        0.319      0.00332\n",
            "! Validation        106  376.834    0.005      0.00649     1.39e-06      0.00649       0.0462       0.0623        0.035       0.0686       0.0518       0.0449       0.0873       0.0661        0.066     0.000687\n",
            "Wall time: 376.834960811\n",
            "! Best model      106    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    107     1      0.00575      0.00574     4.81e-06       0.0441       0.0586       0.0346       0.0633       0.0489       0.0438       0.0805       0.0621        0.152      0.00158\n",
            "    107     2       0.0058       0.0058     4.16e-06        0.045       0.0589       0.0358       0.0634       0.0496       0.0452       0.0795       0.0624        0.137      0.00142\n",
            "    107     3      0.00648      0.00647     1.39e-05       0.0458       0.0622       0.0354       0.0668       0.0511       0.0461       0.0858       0.0659        0.269       0.0028\n",
            "    107     4      0.00623      0.00622     3.54e-06       0.0451        0.061       0.0346       0.0662       0.0504       0.0443       0.0852       0.0647        0.123      0.00129\n",
            "    107     5      0.00598      0.00597     9.72e-06       0.0433       0.0598       0.0323       0.0653       0.0488       0.0417       0.0851       0.0634        0.223      0.00232\n",
            "    107     6      0.00617      0.00615      1.7e-05       0.0454       0.0607       0.0347       0.0666       0.0507       0.0442       0.0844       0.0643          0.3      0.00312\n",
            "    107     7      0.00618      0.00617     3.29e-06        0.046       0.0608       0.0351       0.0677       0.0514       0.0441       0.0848       0.0645        0.128      0.00133\n",
            "    107     8      0.00695      0.00689     5.62e-05       0.0483       0.0642       0.0378       0.0692       0.0535       0.0486       0.0875        0.068        0.554      0.00577\n",
            "    107     9      0.00591      0.00591     3.54e-06       0.0445       0.0595       0.0333        0.067       0.0501       0.0425       0.0836       0.0631        0.106      0.00111\n",
            "    107    10      0.00608      0.00606     2.83e-05       0.0451       0.0602       0.0335       0.0683       0.0509       0.0426       0.0851       0.0639        0.387      0.00404\n",
            "    107    11      0.00608      0.00605     3.08e-05       0.0455       0.0602       0.0353       0.0659       0.0506       0.0454        0.082       0.0637        0.409      0.00426\n",
            "    107    12      0.00637      0.00636     2.59e-06       0.0459       0.0617       0.0347       0.0683       0.0515       0.0444       0.0865       0.0655        0.108      0.00113\n",
            "    107    13      0.00611      0.00607      4.1e-05       0.0447       0.0603       0.0336       0.0669       0.0503       0.0423       0.0856       0.0639        0.469      0.00489\n",
            "    107    14      0.00578      0.00574     4.27e-05       0.0442       0.0586       0.0334       0.0657       0.0495       0.0428       0.0815       0.0622        0.484      0.00504\n",
            "    107    15      0.00585      0.00583      1.3e-05       0.0446       0.0591       0.0355       0.0627       0.0491       0.0449       0.0803       0.0626        0.263      0.00274\n",
            "    107    16      0.00618      0.00617     5.97e-06       0.0451       0.0608       0.0342        0.067       0.0506       0.0443       0.0845       0.0644        0.173      0.00181\n",
            "    107    17      0.00589      0.00587     1.62e-05       0.0442       0.0593       0.0345       0.0635        0.049       0.0436       0.0821       0.0628        0.298       0.0031\n",
            "    107    18      0.00591      0.00591     2.93e-06       0.0438       0.0595       0.0332        0.065       0.0491       0.0426       0.0836       0.0631        0.104      0.00108\n",
            "    107    19      0.00693      0.00691     2.71e-05       0.0485       0.0643       0.0385       0.0686       0.0535        0.049       0.0872       0.0681        0.385      0.00401\n",
            "    107    20      0.00711      0.00711     6.16e-07       0.0506       0.0652       0.0411       0.0695       0.0553       0.0515       0.0864       0.0689       0.0459     0.000478\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    107     1      0.00668      0.00668     2.29e-06       0.0471       0.0632        0.036       0.0694       0.0527       0.0462       0.0878        0.067       0.0889     0.000926\n",
            "    107     2      0.00668      0.00668     9.72e-07       0.0468       0.0632       0.0353         0.07       0.0526       0.0454       0.0887       0.0671       0.0468     0.000487\n",
            "    107     3       0.0064       0.0064     1.33e-06       0.0457       0.0619       0.0343       0.0686       0.0514       0.0438       0.0875       0.0656       0.0699     0.000728\n",
            "    107     4      0.00653      0.00653     1.59e-06        0.046       0.0625       0.0346       0.0687       0.0517        0.044       0.0886       0.0663       0.0799     0.000832\n",
            "    107     5      0.00604      0.00604     7.94e-07       0.0449       0.0601       0.0344       0.0657       0.0501       0.0443       0.0832       0.0637        0.044     0.000459\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             107  380.419    0.005      0.00617     1.64e-05      0.00619       0.0455       0.0608        0.035       0.0663       0.0507       0.0448       0.0841       0.0644        0.256      0.00267\n",
            "! Validation        107  380.419    0.005      0.00647     1.39e-06      0.00647       0.0461       0.0622       0.0349       0.0685       0.0517       0.0448       0.0872        0.066       0.0659     0.000686\n",
            "Wall time: 380.4196925499998\n",
            "! Best model      107    0.006\n",
            "! Stop training: Early stopping: validation_loss has not reduced for 50 epochs\n",
            "Wall time: 380.4558169200002\n",
            "Cumulative wall time: 380.4558169200002\n"
          ]
        }
      ],
      "source": [
        "!rm -rf ./results\n",
        "!nequip-train /content/nequip/configs/my-full-example.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJitSZgLYNNF"
      },
      "source": [
        "### Deploy the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo_kIpYV00as"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/deploy.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VoeGtlA02KQ"
      },
      "source": [
        "We now convert the model to a potential file. This makes it independent of NequIP and we can use it any downstream application, such as LAMMPS. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLlagzVhrVGz",
        "outputId": "9a1fb555-67f7-4cb0-c905-bb09d0ee82d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4.0K\n",
            "-rw-r--r-- 1 root root  0 Jul  6 13:58 metrics_initialization.csv\n",
            "-rw-r--r-- 1 root root  0 Jul  6 13:58 metrics_epoch.csv\n",
            "-rw-r--r-- 1 root root  0 Jul  6 13:58 metrics_batch_val.csv\n",
            "-rw-r--r-- 1 root root  0 Jul  6 13:58 metrics_batch_train.csv\n",
            "-rw-r--r-- 1 root root 19 Jul  6 13:58 log\n"
          ]
        }
      ],
      "source": [
        "! ls -lrth results/water/example-run-water"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJmFAbBzez3P",
        "outputId": "e20e9ba4-f68d-44d2-ce79-a0c52152dfa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: nequip-deploy [-h] [--verbose VERBOSE] {info,build} ...\n",
            "\n",
            "Create and view information about deployed NequIP potentials.\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help         show this help message and exit\n",
            "  --verbose VERBOSE  log level\n",
            "\n",
            "commands:\n",
            "  {info,build}\n",
            "    info             Get information from a deployed model file\n",
            "    build            Build a deployment model\n"
          ]
        }
      ],
      "source": [
        "! nequip-deploy -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3NJJgtDIDNc",
        "outputId": "15f5ef44-8a2b-45a9-da80-c32fac8cc053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Loading best_model from training session...\n",
            "INFO:root:Compiled & optimized model.\n"
          ]
        }
      ],
      "source": [
        "!nequip-deploy build --train-dir /content/results/water/example-run-water water-deploy.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXpcE3oP0LyD"
      },
      "source": [
        "## Evaluate Test Error on all remaining frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wRKKCZ2PRl3"
      },
      "source": [
        "Before running inference, we'd like to know how well the model is doing on a hold-out test set. We run the nequip-evaluate command to compute the test error on all data that we didn't use for training or validation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB54WSrN0PaS",
        "outputId": "051ca387-e33c-44e8-8baf-a9aa007abbfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "WARNING: please note that models running on CUDA are usually nondeterministc and that this manifests in the final test errors; for a _more_ deterministic result, please use `--device cpu`\n",
            "Loading model... \n",
            "loaded model from training session\n",
            "Loading original dataset...\n",
            "Loaded dataset specified in config.yaml.\n",
            "Using origial training dataset (10000 frames) minus training (100 frames) and validation frames (50 frames), yielding a test set size of 9850 frames.\n",
            "Starting...\n",
            "  0% 0/9850 [00:00<?, ?it/s]\n",
            "\u001b[A\n",
            "  1% 50/9850 [00:00<01:25, 114.51it/s]\n",
            "  1% 100/9850 [00:01<02:29, 65.33it/s]\n",
            "  2% 150/9850 [00:03<03:47, 42.71it/s]\n",
            "  2% 200/9850 [00:04<04:22, 36.72it/s]\n",
            "  3% 250/9850 [00:04<03:04, 51.90it/s]\n",
            "  3% 300/9850 [00:05<02:18, 69.01it/s]\n",
            "  4% 350/9850 [00:05<01:48, 87.22it/s]\n",
            "  4% 400/9850 [00:05<01:29, 105.63it/s]\n",
            "  5% 450/9850 [00:06<01:16, 122.84it/s]\n",
            "  5% 500/9850 [00:06<01:07, 138.30it/s]\n",
            "  6% 550/9850 [00:06<01:02, 149.93it/s]\n",
            "  6% 600/9850 [00:06<00:57, 160.59it/s]\n",
            "  7% 650/9850 [00:07<00:54, 168.55it/s]\n",
            "  7% 700/9850 [00:07<00:52, 173.55it/s]\n",
            "  8% 750/9850 [00:07<00:50, 178.92it/s]\n",
            "  8% 800/9850 [00:07<00:49, 182.98it/s]\n",
            "  9% 850/9850 [00:08<00:48, 185.88it/s]\n",
            "  9% 900/9850 [00:08<00:47, 188.29it/s]\n",
            " 10% 950/9850 [00:08<00:46, 189.89it/s]\n",
            " 10% 1000/9850 [00:08<00:46, 191.21it/s]\n",
            " 11% 1050/9850 [00:09<00:45, 191.40it/s]\n",
            " 11% 1100/9850 [00:09<00:45, 192.55it/s]\n",
            " 12% 1150/9850 [00:09<00:45, 191.95it/s]\n",
            " 12% 1200/9850 [00:09<00:44, 192.45it/s]\n",
            " 13% 1250/9850 [00:10<00:44, 193.14it/s]\n",
            " 13% 1300/9850 [00:10<00:44, 193.20it/s]\n",
            " 14% 1350/9850 [00:10<00:43, 194.29it/s]\n",
            " 14% 1400/9850 [00:10<00:43, 193.47it/s]\n",
            " 15% 1450/9850 [00:11<00:43, 193.88it/s]\n",
            " 15% 1500/9850 [00:11<00:43, 192.76it/s]\n",
            " 16% 1550/9850 [00:11<00:42, 193.05it/s]\n",
            " 16% 1600/9850 [00:12<00:42, 194.20it/s]\n",
            " 17% 1650/9850 [00:12<00:42, 194.91it/s]\n",
            " 17% 1700/9850 [00:12<00:41, 194.84it/s]\n",
            " 18% 1750/9850 [00:12<00:41, 193.82it/s]\n",
            " 18% 1800/9850 [00:13<00:41, 194.27it/s]\n",
            " 19% 1850/9850 [00:13<00:41, 194.18it/s]\n",
            " 19% 1900/9850 [00:13<00:40, 195.17it/s]\n",
            " 20% 1950/9850 [00:13<00:40, 194.51it/s]\n",
            " 20% 2000/9850 [00:14<00:40, 194.89it/s]\n",
            " 21% 2050/9850 [00:14<00:40, 194.23it/s]\n",
            " 21% 2100/9850 [00:14<00:40, 193.63it/s]\n",
            " 22% 2150/9850 [00:14<00:39, 194.23it/s]\n",
            " 22% 2200/9850 [00:15<00:39, 194.48it/s]\n",
            " 23% 2250/9850 [00:15<00:39, 194.64it/s]\n",
            " 23% 2300/9850 [00:15<00:39, 192.99it/s]\n",
            " 24% 2350/9850 [00:15<00:38, 194.18it/s]\n",
            " 24% 2400/9850 [00:16<00:38, 194.62it/s]\n",
            " 25% 2450/9850 [00:16<00:37, 195.13it/s]\n",
            " 25% 2500/9850 [00:16<00:37, 194.73it/s]\n",
            " 26% 2550/9850 [00:16<00:37, 194.48it/s]\n",
            " 26% 2600/9850 [00:17<00:37, 194.51it/s]\n",
            " 27% 2650/9850 [00:17<00:37, 194.43it/s]\n",
            " 27% 2700/9850 [00:17<00:36, 194.80it/s]\n",
            " 28% 2750/9850 [00:17<00:36, 194.80it/s]\n",
            " 28% 2800/9850 [00:18<00:35, 195.90it/s]\n",
            " 29% 2850/9850 [00:18<00:35, 195.20it/s]\n",
            " 29% 2900/9850 [00:18<00:35, 194.62it/s]\n",
            " 30% 2950/9850 [00:18<00:35, 194.40it/s]\n",
            " 30% 3000/9850 [00:19<00:35, 194.86it/s]\n",
            " 31% 3050/9850 [00:19<00:34, 195.88it/s]\n",
            " 31% 3100/9850 [00:19<00:34, 196.68it/s]\n",
            " 32% 3150/9850 [00:19<00:33, 197.15it/s]\n",
            " 32% 3200/9850 [00:20<00:33, 195.91it/s]\n",
            " 33% 3250/9850 [00:20<00:33, 196.59it/s]\n",
            " 34% 3300/9850 [00:20<00:33, 195.88it/s]\n",
            " 34% 3350/9850 [00:20<00:33, 195.93it/s]\n",
            " 35% 3400/9850 [00:21<00:32, 195.63it/s]\n",
            " 35% 3450/9850 [00:21<00:32, 195.65it/s]\n",
            " 36% 3500/9850 [00:21<00:32, 195.34it/s]\n",
            " 36% 3550/9850 [00:22<00:32, 194.59it/s]\n",
            " 37% 3600/9850 [00:22<00:32, 194.32it/s]\n",
            " 37% 3650/9850 [00:22<00:32, 193.02it/s]\n",
            " 38% 3700/9850 [00:22<00:31, 192.57it/s]\n",
            " 38% 3750/9850 [00:23<00:31, 192.52it/s]\n",
            " 39% 3800/9850 [00:23<00:31, 192.53it/s]\n",
            " 39% 3850/9850 [00:23<00:31, 192.54it/s]\n",
            " 40% 3900/9850 [00:23<00:30, 192.72it/s]\n",
            " 40% 3950/9850 [00:24<00:30, 193.10it/s]\n",
            " 41% 4000/9850 [00:24<00:30, 192.78it/s]\n",
            " 41% 4050/9850 [00:24<00:30, 192.35it/s]\n",
            " 42% 4100/9850 [00:24<00:29, 192.25it/s]\n",
            " 42% 4150/9850 [00:25<00:29, 191.92it/s]\n",
            " 43% 4200/9850 [00:25<00:29, 190.81it/s]\n",
            " 43% 4250/9850 [00:25<00:29, 191.53it/s]\n",
            " 44% 4300/9850 [00:25<00:28, 192.68it/s]\n",
            " 44% 4350/9850 [00:26<00:28, 193.00it/s]\n",
            " 45% 4400/9850 [00:26<00:28, 193.47it/s]\n",
            " 45% 4450/9850 [00:26<00:27, 193.05it/s]\n",
            " 46% 4500/9850 [00:26<00:27, 193.76it/s]\n",
            " 46% 4550/9850 [00:27<00:27, 192.55it/s]\n",
            " 47% 4600/9850 [00:27<00:27, 192.02it/s]\n",
            " 47% 4650/9850 [00:27<00:27, 191.32it/s]\n",
            " 48% 4700/9850 [00:27<00:26, 191.44it/s]\n",
            " 48% 4750/9850 [00:28<00:26, 190.80it/s]\n",
            " 49% 4800/9850 [00:28<00:26, 190.25it/s]\n",
            " 49% 4850/9850 [00:28<00:26, 189.62it/s]\n",
            " 50% 4900/9850 [00:29<00:25, 190.67it/s]\n",
            " 50% 4950/9850 [00:29<00:25, 190.31it/s]\n",
            " 51% 5000/9850 [00:29<00:25, 190.62it/s]\n",
            " 51% 5050/9850 [00:29<00:25, 191.13it/s]\n",
            " 52% 5100/9850 [00:30<00:24, 191.04it/s]\n",
            " 52% 5150/9850 [00:30<00:24, 191.19it/s]\n",
            " 53% 5200/9850 [00:30<00:24, 191.20it/s]\n",
            " 53% 5250/9850 [00:30<00:23, 192.30it/s]\n",
            " 54% 5300/9850 [00:31<00:23, 192.06it/s]\n",
            " 54% 5350/9850 [00:31<00:23, 192.06it/s]\n",
            " 55% 5400/9850 [00:31<00:23, 191.67it/s]\n",
            " 55% 5450/9850 [00:31<00:23, 190.88it/s]\n",
            " 56% 5500/9850 [00:32<00:22, 190.65it/s]\n",
            " 56% 5550/9850 [00:32<00:22, 189.71it/s]\n",
            " 57% 5600/9850 [00:32<00:22, 190.27it/s]\n",
            " 57% 5650/9850 [00:32<00:22, 190.70it/s]\n",
            " 58% 5700/9850 [00:33<00:21, 189.93it/s]\n",
            " 58% 5750/9850 [00:33<00:21, 188.48it/s]\n",
            " 59% 5800/9850 [00:33<00:21, 188.17it/s]\n",
            " 59% 5850/9850 [00:34<00:21, 187.22it/s]\n",
            " 60% 5900/9850 [00:34<00:20, 188.15it/s]\n",
            " 60% 5950/9850 [00:34<00:20, 187.57it/s]\n",
            " 61% 6000/9850 [00:34<00:20, 187.20it/s]\n",
            " 61% 6050/9850 [00:35<00:20, 188.40it/s]\n",
            " 62% 6100/9850 [00:35<00:19, 188.36it/s]\n",
            " 62% 6150/9850 [00:35<00:19, 189.16it/s]\n",
            " 63% 6200/9850 [00:35<00:19, 190.07it/s]\n",
            " 63% 6250/9850 [00:36<00:19, 187.63it/s]\n",
            " 64% 6300/9850 [00:36<00:18, 189.37it/s]\n",
            " 64% 6350/9850 [00:36<00:18, 190.32it/s]\n",
            " 65% 6400/9850 [00:36<00:18, 190.37it/s]\n",
            " 65% 6450/9850 [00:37<00:17, 190.91it/s]\n",
            " 66% 6500/9850 [00:37<00:17, 191.10it/s]\n",
            " 66% 6550/9850 [00:37<00:17, 191.28it/s]\n",
            " 67% 6600/9850 [00:37<00:16, 191.72it/s]\n",
            " 68% 6650/9850 [00:38<00:16, 190.66it/s]\n",
            " 68% 6700/9850 [00:38<00:16, 190.68it/s]\n",
            " 69% 6750/9850 [00:38<00:16, 190.24it/s]\n",
            " 69% 6800/9850 [00:39<00:16, 190.30it/s]\n",
            " 70% 6850/9850 [00:39<00:15, 190.17it/s]\n",
            " 70% 6900/9850 [00:39<00:15, 189.97it/s]\n",
            " 71% 6950/9850 [00:39<00:15, 191.02it/s]\n",
            " 71% 7000/9850 [00:40<00:14, 190.49it/s]\n",
            " 72% 7050/9850 [00:40<00:14, 191.06it/s]\n",
            " 72% 7100/9850 [00:40<00:14, 190.02it/s]\n",
            " 73% 7150/9850 [00:40<00:14, 188.10it/s]\n",
            " 73% 7200/9850 [00:41<00:14, 188.32it/s]\n",
            " 74% 7250/9850 [00:41<00:13, 188.04it/s]\n",
            " 74% 7300/9850 [00:41<00:13, 188.39it/s]\n",
            " 75% 7350/9850 [00:41<00:13, 189.34it/s]\n",
            " 75% 7400/9850 [00:42<00:12, 189.75it/s]\n",
            " 76% 7450/9850 [00:42<00:12, 190.21it/s]\n",
            " 76% 7500/9850 [00:42<00:12, 190.16it/s]\n",
            " 77% 7550/9850 [00:43<00:12, 188.80it/s]\n",
            " 77% 7600/9850 [00:43<00:11, 190.28it/s]\n",
            " 78% 7650/9850 [00:43<00:11, 190.94it/s]\n",
            " 78% 7700/9850 [00:43<00:11, 191.14it/s]\n",
            " 79% 7750/9850 [00:44<00:10, 191.80it/s]\n",
            " 79% 7800/9850 [00:44<00:10, 191.48it/s]\n",
            " 80% 7850/9850 [00:44<00:10, 192.32it/s]\n",
            " 80% 7900/9850 [00:44<00:10, 191.92it/s]\n",
            " 81% 7950/9850 [00:45<00:09, 192.07it/s]\n",
            " 81% 8000/9850 [00:45<00:09, 191.75it/s]\n",
            " 82% 8050/9850 [00:45<00:09, 191.55it/s]\n",
            " 82% 8100/9850 [00:45<00:09, 192.12it/s]\n",
            " 83% 8150/9850 [00:46<00:08, 192.52it/s]\n",
            " 83% 8200/9850 [00:46<00:08, 193.20it/s]\n",
            " 84% 8250/9850 [00:46<00:08, 193.13it/s]\n",
            " 84% 8300/9850 [00:46<00:08, 192.63it/s]\n",
            " 85% 8350/9850 [00:47<00:07, 192.58it/s]\n",
            " 85% 8400/9850 [00:47<00:07, 191.77it/s]\n",
            " 86% 8450/9850 [00:47<00:07, 191.85it/s]\n",
            " 86% 8500/9850 [00:47<00:07, 190.45it/s]\n",
            " 87% 8550/9850 [00:48<00:06, 191.08it/s]\n",
            " 87% 8600/9850 [00:48<00:06, 190.76it/s]\n",
            " 88% 8650/9850 [00:48<00:06, 190.76it/s]\n",
            " 88% 8700/9850 [00:48<00:06, 191.31it/s]\n",
            " 89% 8750/9850 [00:49<00:05, 191.66it/s]\n",
            " 89% 8800/9850 [00:49<00:05, 191.78it/s]\n",
            " 90% 8850/9850 [00:49<00:05, 191.37it/s]\n",
            " 90% 8900/9850 [00:50<00:05, 189.47it/s]\n",
            " 91% 8950/9850 [00:50<00:04, 190.03it/s]\n",
            " 91% 9000/9850 [00:50<00:04, 189.27it/s]\n",
            " 92% 9050/9850 [00:50<00:04, 189.52it/s]\n",
            " 92% 9100/9850 [00:51<00:03, 190.15it/s]\n",
            " 93% 9150/9850 [00:51<00:03, 189.57it/s]\n",
            " 93% 9200/9850 [00:51<00:03, 190.32it/s]\n",
            " 94% 9250/9850 [00:51<00:03, 189.46it/s]\n",
            " 94% 9300/9850 [00:52<00:02, 190.01it/s]\n",
            " 95% 9350/9850 [00:52<00:02, 190.83it/s]\n",
            " 95% 9400/9850 [00:52<00:02, 190.16it/s]\n",
            " 96% 9450/9850 [00:52<00:02, 189.74it/s]\n",
            " 96% 9500/9850 [00:53<00:01, 188.93it/s]\n",
            " 97% 9550/9850 [00:53<00:01, 187.89it/s]\n",
            " 97% 9600/9850 [00:53<00:01, 188.98it/s]\n",
            " 98% 9650/9850 [00:54<00:01, 187.70it/s]\n",
            " 98% 9700/9850 [00:54<00:00, 187.23it/s]\n",
            " 99% 9750/9850 [00:54<00:00, 187.46it/s]\n",
            " 99% 9800/9850 [00:54<00:00, 186.39it/s]\n",
            "100% 9850/9850 [00:55<00:00, 178.80it/s]\n",
            "\n",
            "\n",
            "--- Final result: ---\n",
            "               f_mae =  0.046357           \n",
            "              f_rmse =  0.062583           \n",
            "             H_f_mae =  0.035020           \n",
            "             O_f_mae =  0.069030           \n",
            "         psavg_f_mae =  0.052025           \n",
            "            H_f_rmse =  0.045198           \n",
            "            O_f_rmse =  0.087545           \n",
            "        psavg_f_rmse =  0.066372           \n",
            "               e_mae =  0.059478           \n",
            "             e/N_mae =  0.000620           \n",
            "               f_mae =  0.046357           \n",
            "              f_rmse =  0.062583           \n",
            "             H_f_mae =  0.035020           \n",
            "             O_f_mae =  0.069030           \n",
            "         psavg_f_mae =  0.052025           \n",
            "            H_f_rmse =  0.045198           \n",
            "            O_f_rmse =  0.087545           \n",
            "        psavg_f_rmse =  0.066372           \n",
            "               e_mae =  0.059478           \n",
            "             e/N_mae =  0.000620           \n"
          ]
        }
      ],
      "source": [
        "!nequip-evaluate --train-dir results/water/example-run-water --batch-size 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQHrMMnsPaJO"
      },
      "source": [
        "Again, energy errors of < 1meV/atom (converted from kcal/mol to eV), and force errors of ~45 meV/A 🎉"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4r5FBXaum9n"
      },
      "source": [
        "# LAMMPS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qIYIYyr1B4O"
      },
      "source": [
        "We are now in a position to run MD with our potential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UirNBTlJ1BNZ"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/run.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQs0ijPhvAGb"
      },
      "source": [
        "Set up a simple LAMMPS input file\n",
        "\n",
        "CAUTION: the reference data here are in eV for the energies and eV/A for the forces. The NequIP model will therefore also be predicting outputs in these units. We are therefore using `units metal` in LAMMPS (see [docs](https://docs.lammps.org/units.html)). Time units are also in`ps`)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We now run MD in the NVT ensemble for 10 ps (20,000 steps)"
      ],
      "metadata": {
        "id": "3AXv_y_JD1TJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W090KfMsd2Do",
        "outputId": "0cd7cd0e-8033-4073-8ded-e11596476889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘lammps_run’: File exists\n"
          ]
        }
      ],
      "source": [
        "lammps_input_md = \"\"\"\n",
        "units           metal\n",
        "boundary        p p p\n",
        "atom_style      atomic\n",
        "thermo 1\n",
        "newton off\n",
        "read_data structure.data\n",
        "\n",
        "neighbor        1.0 bin\n",
        "neigh_modify    every 10 delay 0 check no\n",
        "\n",
        "pair_style\tnequip\n",
        "pair_coeff\t* * ../water-deploy.pth H O\n",
        "mass            1 1.00794\n",
        "mass            2 15.9994\n",
        "\n",
        "velocity        all create 300.0 23456789\n",
        "timestep        0.0005\n",
        "fix             1 all nvt temp 300.0 300.0 $(100.0*dt)\n",
        "\n",
        "#print log every X steps\n",
        "thermo          100\n",
        "thermo_style    custom step pe ke etotal temp press vol\n",
        "\n",
        "#print trajectory in xyz every X time units\n",
        "dump              1 all xyz 20 water.xyz \n",
        "dump_modify       1 element H O\n",
        "\n",
        "# dump            2 all custom 1 dump_frc.lamppstrj id type element fx fy fz\n",
        "# dump_modify     2 element O H\n",
        "# dump            3 all custom 1 dump.lammpstrj id type element x y z\n",
        "# dump_modify     3 element O H\n",
        "\n",
        "run             20000\n",
        "\"\"\"\n",
        "\n",
        "!mkdir lammps_run\n",
        "with open(\"lammps_run/water_md.in\", \"w\") as f:\n",
        "    f.write(lammps_input_md)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "Mu4kbiXOt1pI"
      },
      "outputs": [],
      "source": [
        "! cp /content/water-deploy.pth /content/lammps_run/."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvWZCw1zvjRc"
      },
      "source": [
        "We specify the initial water configuration by reading the extxyz file and writing to structure.data file easily parsed by lammps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "eXeHb2ZPvIbU"
      },
      "outputs": [],
      "source": [
        "wat_pos_frc_trj = read('/content/nequip/data/AIMD_data/wat_pos_frc-10k.extxyz')\n",
        "write(\"/content/lammps_run/structure.data\", wat_pos_frc_trj,format='lammps-data')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDuyueY11YBF"
      },
      "source": [
        "### Run the LAMMPS command: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG1LE98LukSO",
        "outputId": "bdf4110c-c0f4-4c17-c805-124da9409fd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "units           metal\n",
            "boundary        p p p\n",
            "atom_style      atomic\n",
            "thermo 1\n",
            "newton off\n",
            "read_data structure.data\n",
            "\n",
            "neighbor        1.0 bin\n",
            "neigh_modify    every 10 delay 0 check no\n",
            "\n",
            "pair_style\tnequip\n",
            "pair_coeff\t* * ../water-deploy.pth H O\n",
            "mass            1 1.00794\n",
            "mass            2 15.9994\n",
            "\n",
            "velocity        all create 300.0 23456789\n",
            "timestep        0.0005\n",
            "fix             1 all nvt temp 300.0 300.0 $(100.0*dt)\n",
            "\n",
            "#print log every X steps\n",
            "thermo          100\n",
            "thermo_style    custom step pe ke etotal temp press vol\n",
            "\n",
            "#print trajectory in xyz every X time units\n",
            "dump              1 all xyz 20 water.xyz \n",
            "dump_modify       1 element H O\n",
            "\n",
            "# dump            2 all custom 1 dump_frc.lamppstrj id type element fx fy fz\n",
            "# dump_modify     2 element O H\n",
            "# dump            3 all custom 1 dump.lammpstrj id type element x y z\n",
            "# dump_modify     3 element O H\n",
            "\n",
            "run             20000\n",
            "/content/lammps_run/structure.data (written by ASE) \n",
            "\n",
            "96 \t atoms \n",
            "2  atom types\n",
            "0.0      9.8499999999999996  xlo xhi\n",
            "0.0      9.8499999999999996  ylo yhi\n",
            "0.0      9.8499999999999996  zlo zhi\n",
            "\n",
            "\n",
            "Atoms \n",
            "\n",
            "     1   1      42.886169670000001   -0.055681660000000001      38.329161120000002\n",
            "     2   1      34.202588720000001              -0.6185484      37.365568080000003\n",
            "     3   1      30.080392589999999     -2.0124176500000002      36.480796079999998\n",
            "     4   1      28.705791179999999     -2.6880392799999999      36.602098329999997\n",
            "     5   1             36.24794267    -0.51634849000000005      34.492359610000001\n",
            "     6   1      37.696472460000003   -0.041087279999999997      35.014073510000003\n",
            "     7   1      27.760669979999999      7.4854206000000003      33.927691950000003\n",
            "     8   1      28.816099959999999              6.49857774             34.21636084\n",
            "     9   1      37.157637200000003      9.0188280800000005      31.926581209999998\n",
            "    10   1      38.606381650000003      9.5820079600000003      32.343597279999997\n",
            "    11   1      34.303195930000001      2.2195014400000002      45.988045190000001\n",
            "    12   1      33.244413940000001              1.30253325      46.469842720000003\n",
            "    13   1      38.728617479999997     -5.0541897699999998      26.074396839999999\n",
            "    14   1      38.348392150000002     -6.2832846900000003      26.986725310000001\n",
            "    15   1      32.864252090000001              3.20606327               30.897116\n",
            "    16   1      31.290408809999999      3.0871834699999998             30.62739775\n",
            "    17   1      33.751986969999997             -3.13832624      39.672760769999996\n",
            "    18   1      34.664297990000001             -3.66438591      38.646602719999997\n",
            "    19   1      42.717321439999999      5.1246883700000003      32.588340119999998\n",
            "    20   1             41.56274552      5.5893544000000004      33.417490280000003\n",
            "    21   1      32.428380089999997      9.1182520999999994             30.54776786\n",
            "    22   1              32.6432407             10.77068308             30.48427787\n",
            "    23   1      31.484867080000001      4.6777144699999997      37.395719499999998\n",
            "    24   1      32.317188299999998     -6.2287496200000003      36.467186439999999\n",
            "    25   1      26.662134099999999      3.1708123800000001      35.682014649999999\n",
            "    26   1             26.52713675              1.60390403      35.488348299999998\n",
            "    27   1      32.023823659999998      16.918208029999999      31.688356989999999\n",
            "    28   1      31.400657970000001      7.0315610800000004      30.239455469999999\n",
            "    29   1             33.52642531     -3.5594808100000002             34.26368308\n",
            "    30   1      34.640485550000001             -3.26538336      35.497148240000001\n",
            "    31   1      40.056437510000002    -0.30543864999999998      29.831207429999999\n",
            "    32   1      39.478446419999997             -1.09483146      38.310114040000002\n",
            "    33   1      39.704076149999999      1.9584631400000001      33.390237599999999\n",
            "    34   1      38.333857080000001              2.69671781             42.92619457\n",
            "    35   1      40.182045549999998     -7.2199289499999999      27.658039049999999\n",
            "    36   1      39.320443179999998     -8.4564252700000004             28.13196589\n",
            "    37   1             36.38769637      8.8117085900000003      38.354536240000002\n",
            "    38   1      36.320563759999999      9.0063075799999996      36.752600139999998\n",
            "    39   1      29.999158309999999     -5.5637817500000004      33.929505059999997\n",
            "    40   1      30.772854550000002     -5.0385870199999996      35.199806719999998\n",
            "    41   1      40.059251779999997      6.3305279499999996      28.257946189999998\n",
            "    42   1      40.239836089999997      5.1745923999999999             29.29629568\n",
            "    43   1             26.33209111      2.4393638599999998      33.565386850000003\n",
            "    44   1      26.960697119999999      1.2711078899999999      32.592388440000001\n",
            "    45   1      34.837269769999999    -0.47227084000000003      30.382436200000001\n",
            "    46   1      35.396881370000003             -1.92684834      30.308183759999999\n",
            "    47   1      32.121760719999997    -0.73334299000000003      36.510438299999997\n",
            "    48   1      32.218084349999998      7.8454304099999996      35.667196779999998\n",
            "    49   1             36.37809987     -4.3048878799999999              36.4539793\n",
            "    50   1      35.811927560000001             -3.00139282      27.034893759999999\n",
            "    51   1      29.645249140000001      1.0652123600000001      35.714365399999998\n",
            "    52   1      30.379465499999998   -0.066814650000000003      34.988246869999998\n",
            "    53   1             34.21493366             -1.65591205      33.887643709999999\n",
            "    54   1      34.784243549999999     -1.0252141100000001      32.503483260000003\n",
            "    55   1      40.464995450000004              1.14678254      31.307350320000001\n",
            "    56   1      41.326246990000001     0.65508034999999998      32.455588290000001\n",
            "    57   1             29.02108599      3.5038194900000001             39.90877029\n",
            "    58   1      29.494542689999999      3.7276637300000002      41.376613800000001\n",
            "    59   1      34.135966400000001     -6.7533422300000003      32.356841029999998\n",
            "    60   1      34.954657009999998     -5.7704242399999996             31.45710669\n",
            "    61   1      33.253235689999997      1.5268048299999999      44.056217189999998\n",
            "    62   1      33.793166939999999     0.50146325999999997      43.059759010000001\n",
            "    63   1      36.820540960000002      2.6214681999999998      40.683400659999997\n",
            "    64   1      37.555270669999999              1.56498329             39.76489351\n",
            "    65   2      43.209908720000001   -0.062845650000000003             47.25931559\n",
            "    66   2      29.394058350000002     -2.3133019500000001      37.140788360000002\n",
            "    67   2      36.741570840000001   -0.083871000000000001      35.259178339999998\n",
            "    68   2             27.94247769      6.7622961500000001      34.564838440000003\n",
            "    69   2      37.681265600000003      9.4216399800000001      32.647864349999999\n",
            "    70   2      33.317129080000001      2.0951401700000001      45.872226509999997\n",
            "    71   2      37.995135589999997      4.3611431200000004             26.55718199\n",
            "    72   2      32.182467090000003      2.6611503399999998      30.457724819999999\n",
            "    73   2      34.653801209999997     -3.4374573100000001      39.588924570000003\n",
            "    74   2      42.292983370000002      5.9471069500000002      32.846099549999998\n",
            "    75   2      32.960469009999997      9.9050313200000009      30.158730670000001\n",
            "    76   2             31.42818866     -5.8338304000000001      36.673874359999999\n",
            "    77   2             26.05637308      2.4973869199999998      35.348687040000002\n",
            "    78   2      32.033492709999997      17.325228939999999      30.811601379999999\n",
            "    79   2      33.825218239999998     -2.9520949600000002      35.022046070000002\n",
            "    80   2      39.456998159999998    -0.30727594000000003      38.934782900000002\n",
            "    81   2             29.48467089      2.8692561099999998      43.006186839999998\n",
            "    82   2      39.286418410000003             -7.62061031      27.627114779999999\n",
            "    83   2      35.879750280000003      8.6515870800000005      37.522173430000002\n",
            "    84   2      30.358254389999999     -4.7607656800000004      34.335564570000003\n",
            "    85   2      40.709895690000003      5.8331250199999998      28.755837589999999\n",
            "    86   2      26.717908300000001      2.2415138300000002      32.657729799999998\n",
            "    87   2      35.658925619999998    -0.99689035000000004      30.574953059999999\n",
            "    88   2      31.585160200000001             -1.31218042      35.901110940000002\n",
            "    89   2      35.548938669999998     -3.9056138900000001      26.821449000000001\n",
            "    90   2      29.565661609999999     0.46817945999999999      34.967071199999999\n",
            "    91   2             34.76151282    -0.95696802000000003      33.489136729999998\n",
            "    92   2      40.485340669999999     0.40236209000000001             31.94254162\n",
            "    93   2      29.672828979999998      4.0134825300000001               40.450578\n",
            "    94   2      34.127228619999997     -5.8796882899999998      31.892542299999999\n",
            "    95   2      33.116888420000002      1.2338084899999999      43.112771199999997\n",
            "    96   2      37.199699350000003      2.5049007099999998      39.791712660000002\n"
          ]
        }
      ],
      "source": [
        "! cat /content/lammps_run/water_md.in\n",
        "! cat /content/lammps_run/structure.data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run MD!"
      ],
      "metadata": {
        "id": "plER-wpaFCCT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gurLjNK5upvq",
        "outputId": "9f202ed9-b1ed-461e-d906-fb8676375fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LAMMPS (29 Sep 2021 - Update 2)\n",
            "OMP_NUM_THREADS environment is not set. Defaulting to 1 thread. (src/comm.cpp:98)\n",
            "  using 1 OpenMP thread(s) per MPI task\n",
            "Reading data file ...\n",
            "  orthogonal box = (0.0000000 0.0000000 0.0000000) to (9.8500000 9.8500000 9.8500000)\n",
            "  1 by 1 by 1 MPI processor grid\n",
            "  reading atoms ...\n",
            "  96 atoms\n",
            "  read_data CPU = 0.001 seconds\n",
            "NEQUIP is using device cuda\n",
            "NequIP Coeff: type 1 is element H\n",
            "NequIP Coeff: type 2 is element O\n",
            "Loading model from ../water-deploy.pth\n",
            "Freezing TorchScript model...\n",
            "Neighbor list info ...\n",
            "  update every 10 steps, delay 0 steps, check no\n",
            "  max neighbors/atom: 2000, page size: 100000\n",
            "  master list distance cutoff = 5\n",
            "  ghost atom cutoff = 5\n",
            "  binsize = 2.5, bins = 4 4 4\n",
            "  1 neighbor lists, perpetual/occasional/extra = 1 0 0\n",
            "  (1) pair nequip, perpetual\n",
            "      attributes: full, newton off\n",
            "      pair build: full/bin/atomonly\n",
            "      stencil: full/bin/3d\n",
            "      bin: standard\n",
            "Setting up Verlet run ...\n",
            "  Unit style    : metal\n",
            "  Current step  : 0\n",
            "  Time step     : 0.0005\n",
            "Per MPI rank memory allocation (min/avg/max) = 3.586 | 3.586 | 3.586 Mbytes\n",
            "Step PotEng KinEng TotEng Temp Press Volume \n",
            "       0   -14985.619    3.6839141   -14981.935          300    4117.3701    955.67162 \n",
            "     100   -14985.152    3.6123147    -14981.54    294.16929    4037.3462    955.67162 \n",
            "     200   -14984.614    3.5535818   -14981.061    289.38637    3971.7026    955.67162 \n",
            "     300   -14984.509    3.7110219   -14980.798    302.20752    4147.6674    955.67162 \n",
            "     400   -14984.445    3.9257441    -14980.52    319.69346    4387.6543    955.67162 \n",
            "     500   -14984.365    4.1371011   -14980.228    336.90534    4623.8799    955.67162 \n",
            "     600   -14983.876    3.9035116   -14979.972    317.88295    4362.8058    955.67162 \n",
            "     700    -14983.34    4.0965478   -14979.243    333.60287     4578.555    955.67162 \n",
            "     800   -14983.483    4.3727181   -14979.111    356.09284      4887.22    955.67162 \n",
            "     900    -14983.76    3.7053192   -14980.054    301.74312    4141.2937    955.67162 \n",
            "    1000   -14983.901    3.6032887   -14980.298    293.43426    4027.2581    955.67162 \n",
            "    1100   -14984.227    3.6791941   -14980.547    299.61562    4112.0947    955.67162 \n",
            "    1200   -14984.434    3.7830052   -14980.651    308.06949    4228.1204    955.67162 \n",
            "    1300   -14984.688     3.615597   -14981.073    294.43658    4041.0146    955.67162 \n",
            "    1400   -14984.774    3.1370967   -14981.637    255.46985    3506.2131    955.67162 \n",
            "    1500   -14985.049    3.5110046   -14981.538     285.9191    3924.1158    955.67162 \n",
            "    1600   -14984.849    3.7096084   -14981.139    302.09242    4146.0876    955.67162 \n",
            "    1700     -14984.5    3.3948661   -14981.105    276.46134    3794.3122    955.67162 \n",
            "    1800   -14985.301    3.2808632    -14982.02     267.1775    3666.8954    955.67162 \n",
            "    1900   -14985.345    3.1324311   -14982.212    255.08991    3500.9985    955.67162 \n",
            "    2000   -14985.119    3.5100035   -14981.609    285.83757    3922.9968    955.67162 \n",
            "    2100   -14984.583    3.4830507     -14981.1    283.64266    3892.8727    955.67162 \n",
            "    2200   -14985.017     4.193287   -14980.823    341.48084    4686.6767    955.67162 \n",
            "    2300   -14984.776    3.8368508    -14980.94    312.45441    4288.3015    955.67162 \n",
            "    2400   -14984.391    3.3015711   -14981.089    268.86385    3690.0399    955.67162 \n",
            "    2500   -14984.069    3.3304502   -14980.739    271.21562    3722.3169    955.67162 \n",
            "    2600   -14984.113    4.0466634   -14980.067    329.54053    4522.8011    955.67162 \n",
            "    2700    -14983.92    3.7362179   -14980.184    304.25936     4175.828    955.67162 \n",
            "    2800   -14984.412    3.5523148    -14980.86    289.28319    3970.2865    955.67162 \n",
            "    2900   -14984.437    3.3691156   -14981.067    274.36434    3765.5318    955.67162 \n",
            "    3000   -14984.715     3.820144   -14980.895     311.0939     4269.629    955.67162 \n",
            "    3100   -14984.799    3.9808415   -14980.818    324.18031    4449.2344    955.67162 \n",
            "    3200   -14985.021    3.8894588   -14981.132    316.73856    4347.0996    955.67162 \n",
            "    3300   -14985.058    3.5726018   -14981.485    290.93527    3992.9606    955.67162 \n",
            "    3400   -14984.501    3.3520046   -14981.149     272.9709    3746.4075    955.67162 \n",
            "    3500   -14984.755    4.1242033   -14980.631      335.855    4609.4645    955.67162 \n",
            "    3600   -14984.447    3.7636186   -14980.684    306.49074    4206.4527    955.67162 \n",
            "    3700   -14985.142    4.2358029   -14980.906    344.94313    4734.1951    955.67162 \n",
            "    3800   -14984.721    3.0913391   -14981.629    251.74358    3455.0717    955.67162 \n",
            "    3900   -14984.785    3.4027512   -14981.382    277.10346     3803.125    955.67162 \n",
            "    4000   -14984.437    3.3760933    -14981.06    274.93257    3773.3305    955.67162 \n",
            "    4100    -14984.68    3.7093461    -14980.97    302.07105    4145.7944    955.67162 \n",
            "    4200   -14984.859    3.7398087    -14981.12    304.55178    4179.8413    955.67162 \n",
            "    4300   -14985.162    3.6533543   -14981.509    297.51135    4083.2145    955.67162 \n",
            "    4400   -14985.078    3.2178246    -14981.86    262.04395    3596.4397    955.67162 \n",
            "    4500   -14984.843    3.4621613   -14981.381    281.94153    3869.5255    955.67162 \n",
            "    4600   -14984.598    3.8383799   -14980.759    312.57894    4290.0106    955.67162 \n",
            "    4700   -14984.033    3.4810049   -14980.552    283.47606    3890.5862    955.67162 \n",
            "    4800    -14983.68    3.3966963   -14980.283    276.61038    3796.3577    955.67162 \n",
            "    4900   -14983.708     3.987327   -14979.721    324.70846    4456.4831    955.67162 \n",
            "    5000   -14983.732    4.1006646   -14979.632    333.93813    4583.1562    955.67162 \n",
            "    5100   -14984.018    3.3989704   -14980.619    276.79557    3798.8993    955.67162 \n",
            "    5200   -14984.354    3.5687863   -14980.785    290.62455    3988.6962    955.67162 \n",
            "    5300   -14984.163     3.404379   -14980.759    277.23602    3804.9443    955.67162 \n",
            "    5400   -14984.107    3.8818357   -14980.226    316.11777    4338.5794    955.67162 \n",
            "    5500   -14983.468    3.5721245   -14979.896     290.8964    3992.4271    955.67162 \n",
            "    5600   -14983.605    3.6360221   -14979.969    296.09991     4063.843    955.67162 \n",
            "    5700   -14983.463    3.6542421   -14979.809    297.58366    4084.2069    955.67162 \n",
            "    5800   -14982.815    3.8244295   -14978.991    311.44289    4274.4187    955.67162 \n",
            "    5900   -14982.454    3.7205496   -14978.734    302.98341    4158.3162    955.67162 \n",
            "    6000    -14983.65    4.1069687   -14979.543     334.4515    4590.2021    955.67162 \n",
            "    6100   -14983.564    3.2995337   -14980.265    268.69793    3687.7627    955.67162 \n",
            "    6200   -14984.204    3.6929704   -14980.511     300.7375     4127.492    955.67162 \n",
            "    6300   -14983.767    3.3618782   -14980.405    273.77496    3757.4428    955.67162 \n",
            "    6400   -14983.793    3.9093968   -14979.884    318.36221    4369.3835    955.67162 \n",
            "    6500   -14983.847    4.1978427   -14979.649    341.85183    4691.7684    955.67162 \n",
            "    6600   -14983.749    3.4144193   -14980.335    278.05365     3816.166    955.67162 \n",
            "    6700   -14984.101    3.3933676   -14980.707    276.33931    3792.6373    955.67162 \n",
            "    6800   -14984.053     3.697826   -14980.355    301.13292    4132.9189    955.67162 \n",
            "    6900    -14983.67    3.7632888   -14979.907    306.46389    4206.0842    955.67162 \n",
            "    7000   -14983.857    4.0994389   -14979.758    333.83831    4581.7863    955.67162 \n",
            "    7100   -14983.567    3.7222109   -14979.845     303.1187    4160.1729    955.67162 \n",
            "    7200   -14983.694    3.6579771   -14980.036    297.88781    4088.3812    955.67162 \n",
            "    7300   -14983.807     3.635486   -14980.171    296.05625    4063.2438    955.67162 \n",
            "    7400    -14983.73    3.4584908   -14980.272    281.64262    3865.4231    955.67162 \n",
            "    7500   -14984.219    3.9094904   -14980.309    318.36983    4369.4881    955.67162 \n",
            "    7600   -14984.241    3.7677347   -14980.473    306.82594    4211.0531    955.67162 \n",
            "    7700   -14984.371    3.5665811   -14980.805    290.44497    3986.2314    955.67162 \n",
            "    7800   -14984.202    3.3696339   -14980.833    274.40655    3766.1111    955.67162 \n",
            "    7900   -14983.929    3.9750881   -14979.954    323.71179    4442.8041    955.67162 \n",
            "    8000   -14983.909      4.36712   -14979.542    355.63696    4880.9632    955.67162 \n",
            "    8100   -14983.694    3.7435273   -14979.951    304.85461    4183.9975    955.67162 \n",
            "    8200   -14984.033    3.6285395   -14980.405    295.49056      4055.48    955.67162 \n",
            "    8300   -14984.184    3.8249586   -14980.359    311.48597    4275.0101    955.67162 \n",
            "    8400   -14983.934    3.7508097   -14980.183    305.44765    4192.1367    955.67162 \n",
            "    8500    -14983.59    3.7607802   -14979.829     306.2596    4203.2804    955.67162 \n",
            "    8600    -14983.44    4.5117109   -14978.929    367.41173    5042.5669    955.67162 \n",
            "    8700   -14983.646    4.2382152   -14979.408    345.13958    4736.8913    955.67162 \n",
            "    8800   -14983.537    3.3692461   -14980.168    274.37497    3765.6777    955.67162 \n",
            "    8900   -14983.653    3.2397247   -14980.414    263.82738    3620.9166    955.67162 \n",
            "    9000   -14983.503     3.306651   -14980.196    269.27753    3695.7175    955.67162 \n",
            "    9100   -14983.613    4.0483287   -14979.565    329.67615    4524.6624    955.67162 \n",
            "    9200    -14983.17    3.6341065   -14979.536    295.94391     4061.702    955.67162 \n",
            "    9300   -14983.522    3.5139375   -14980.009    286.15793    3927.3937    955.67162 \n",
            "    9400   -14984.178    3.7627265   -14980.415    306.41809    4205.4556    955.67162 \n",
            "    9500   -14983.926    3.3968206   -14980.529     276.6205    3796.4966    955.67162 \n",
            "    9600   -14984.111    3.8772347   -14980.234    315.74308    4333.4371    955.67162 \n",
            "    9700   -14983.479    3.8232715   -14979.656    311.34858    4273.1245    955.67162 \n",
            "    9800   -14983.479    3.7502589   -14979.729    305.40279    4191.5211    955.67162 \n",
            "    9900   -14984.164    3.8904825   -14980.274    316.82192    4348.2436    955.67162 \n",
            "   10000   -14984.505    3.5651816    -14980.94      290.331    3984.6673    955.67162 \n",
            "   10100   -14984.254    3.5173352   -14980.737    286.43463    3931.1912    955.67162 \n",
            "   10200   -14984.208    3.8709899   -14980.337    315.23453    4326.4575    955.67162 \n",
            "   10300   -14984.136     4.036681   -14980.099    328.72761    4511.6442    955.67162 \n",
            "   10400   -14984.415    4.1073929   -14980.308    334.48605    4590.6762    955.67162 \n",
            "   10500   -14984.397     3.435084   -14980.962    279.73649    3839.2622    955.67162 \n",
            "   10600   -14984.475    3.6012832   -14980.873    293.27094    4025.0167    955.67162 \n",
            "   10700   -14984.321    3.6861761   -14980.635    300.18421    4119.8982    955.67162 \n",
            "   10800   -14984.059    3.7507447   -14980.308    305.44235    4192.0641    955.67162 \n",
            "   10900    -14983.51    4.0930338   -14979.417    333.31671    4574.6276    955.67162 \n",
            "   11000   -14983.581    3.9005661    -14979.68    317.64308    4359.5137    955.67162 \n",
            "   11100   -14984.144    3.8955441   -14980.248    317.23411    4353.9008    955.67162 \n",
            "   11200   -14984.401    3.9172403   -14980.484    319.00095    4378.1499    955.67162 \n",
            "   11300    -14984.03    3.4725514   -14980.558    282.78765    3881.1381    955.67162 \n",
            "   11400   -14984.536    3.8477527   -14980.688    313.34222    4300.4862    955.67162 \n",
            "   11500   -14984.381    3.2361279   -14981.145    263.53448    3616.8966    955.67162 \n",
            "   11600   -14984.388    3.0256515   -14981.362     246.3943     3381.655    955.67162 \n",
            "   11700   -14984.296    3.5525906   -14980.743    289.30565    3970.5948    955.67162 \n",
            "   11800   -14984.438     4.046251   -14980.391    329.50695    4522.3402    955.67162 \n",
            "   11900   -14984.091    3.5903761     -14980.5    292.38272    4012.8262    955.67162 \n",
            "   12000   -14984.277    3.5089342   -14980.768    285.75049    3921.8017    955.67162 \n",
            "   12100   -14984.624    3.3293102   -14981.295    271.12279    3721.0428    955.67162 \n",
            "   12200    -14984.63    3.1499792    -14981.48    256.51895    3520.6115    955.67162 \n",
            "   12300   -14984.574    3.8784821   -14980.696    315.84467    4334.8313    955.67162 \n",
            "   12400   -14984.281    3.8586804   -14980.423    314.23211    4312.6997    955.67162 \n",
            "   12500   -14984.249    3.5916626   -14980.657    292.48749    4014.2641    955.67162 \n",
            "   12600   -14984.327    3.4209311   -14980.906    278.58394     3823.444    955.67162 \n",
            "   12700   -14984.646    3.6613661   -14980.985     298.1638    4092.1691    955.67162 \n",
            "   12800   -14983.802    3.3560919   -14980.446    273.30376    3750.9757    955.67162 \n",
            "   12900    -14983.91    4.0911801   -14979.819    333.16576    4572.5557    955.67162 \n",
            "   13000   -14983.637    3.8347958   -14979.802    312.28707    4286.0048    955.67162 \n",
            "   13100   -14983.837    3.9550256   -14979.882    322.07799     4420.381    955.67162 \n",
            "   13200   -14983.856    3.6783746   -14980.178    299.54889    4111.1788    955.67162 \n",
            "   13300   -14984.503    3.9146404   -14980.588    318.78923    4375.2441    955.67162 \n",
            "   13400   -14984.171    3.3869699   -14980.784    275.81831    3785.4868    955.67162 \n",
            "   13500   -14984.269    3.6177323   -14980.651    294.61047    4043.4011    955.67162 \n",
            "   13600   -14984.177    3.6513322   -14980.525    297.34668    4080.9545    955.67162 \n",
            "   13700   -14984.276    4.2718778   -14980.004    347.88089    4774.5146    955.67162 \n",
            "   13800   -14983.827    3.9539588   -14979.873    321.99112    4419.1887    955.67162 \n",
            "   13900   -14984.305    3.8307621   -14980.474    311.95858    4281.4964    955.67162 \n",
            "   14000   -14984.788    3.8296964   -14980.958     311.8718    4280.3054    955.67162 \n",
            "   14100    -14984.66    3.7704488    -14980.89    307.04696    4214.0866    955.67162 \n",
            "   14200   -14984.205    3.4925462   -14980.713    284.41593    3903.4855    955.67162 \n",
            "   14300   -14984.616    4.3537613   -14980.262    354.54909    4866.0327    955.67162 \n",
            "   14400   -14984.039      4.52036   -14979.519    368.11607    5052.2337    955.67162 \n",
            "   14500   -14983.228    3.5957279   -14979.632    292.81855    4018.8078    955.67162 \n",
            "   14600    -14984.26    3.6443416   -14980.615    296.77741    4073.1414    955.67162 \n",
            "   14700   -14984.379    3.4847266   -14980.894    283.77914    3894.7458    955.67162 \n",
            "   14800    -14984.16    3.4346925   -14980.725    279.70461    3838.8246    955.67162 \n",
            "   14900   -14984.229     4.052256   -14980.177    329.99597    4529.0518    955.67162 \n"
          ]
        }
      ],
      "source": [
        "!cd /content/lammps_run/ && ../lammps/build/lmp -in water_md.in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGEqNBu2fmKQ"
      },
      "outputs": [],
      "source": [
        "from ase.visualize import view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYxvyvyK2ctM"
      },
      "outputs": [],
      "source": [
        "wat_traj = read(\"/content/lammps_run/water.xyz\",index='::10')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G_nFd7dgWhc"
      },
      "outputs": [],
      "source": [
        "for i in range(len(wat_traj)):\n",
        "  wat_traj[i].cell = cell_vec_abc\n",
        "  wat_traj[i].pbc = np.array([True,True,True])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZLtkZ7bdszJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-P5s2IbvfocL"
      },
      "outputs": [],
      "source": [
        "view(wat_traj, viewer='ngl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install MDAnalysis"
      ],
      "metadata": {
        "id": "QOo4fyaB8jAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import MDAnalysis as mda\n",
        "from MDAnalysis.analysis import rdf"
      ],
      "metadata": {
        "id": "RV4F1NHf9WUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = mda.coordinates.XYZ.XYZReader(\"/content/lammps_run/water.xyz\")\n",
        "topology = mda.topology.XYZParser.XYZParser(\"/content/lammps_run/water.xyz\")\n",
        "\n",
        "u = mda.Universe(\"/content/lammps_run/water.xyz\")\n",
        "\n",
        "u.dimensions = [cell_vec_abc[0], cell_vec_abc[1],cell_vec_abc[2], 90., 90., 90. ]"
      ],
      "metadata": {
        "id": "LtqT9im4AC15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "O_at = u.select_atoms('name O')\n",
        "H_at = u.select_atoms('name H')\n",
        "\n",
        "irdf = rdf.InterRDF(O_at, O_at,\n",
        "                    nbins=75,  # default\n",
        "                    range=(0.00001, 4.9),  # distance in angstroms\n",
        "                   )\n",
        "irdf.run()\n"
      ],
      "metadata": {
        "id": "GkoMQR8ZA0Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(irdf.bins, irdf.rdf)\n",
        "plt.xlabel('Radius (angstrom)')\n",
        "plt.ylabel('Radial distribution')"
      ],
      "metadata": {
        "id": "Og07Lts5C1q_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "my-short-nequip-tutorial.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "c9be9acec9edbd902b751bf46a8fbd7b71bbc5f0438c72d3ebaee4bffeb5e5e4"
    },
    "kernelspec": {
      "display_name": "Python 3.7.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "24568af79df546058302ee1cc7ff62d5": {
          "model_module": "nglview-js-widgets",
          "model_name": "ColormakerRegistryModel",
          "model_module_version": "3.0.1",
          "state": {
            "_dom_classes": [],
            "_model_module": "nglview-js-widgets",
            "_model_module_version": "3.0.1",
            "_model_name": "ColormakerRegistryModel",
            "_msg_ar": [],
            "_msg_q": [],
            "_ready": false,
            "_view_count": null,
            "_view_module": "nglview-js-widgets",
            "_view_module_version": "3.0.1",
            "_view_name": "ColormakerRegistryView",
            "layout": "IPY_MODEL_51e7935017174a5bae1ae6ad2cfbef1c"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}