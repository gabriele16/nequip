{"cells":[{"cell_type":"markdown","metadata":{"id":"CFpAi8g9XmUU"},"source":["# Molecular Dynamics with NequIP \n","\n","### Authors: Simon Batzner, Albert Musaelian, Lixin Sun, Anders Johansson, Boris Kozinsky"]},{"cell_type":"markdown","metadata":{},"source":["### Tutorial Modified by Gabriele Tocci"]},{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://colab.research.google.com/github/gabriele16/nequip/blob/main/colab/my-short-nequip-tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":97013,"status":"ok","timestamp":1657098603498,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"K7bW4JWmmuyD","outputId":"7ed0ee13-7e83-4bc2-8d83-63955a2771da"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.10\n","  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:42tcmalloc: large alloc 1147494400 bytes == 0x66628000 @  0x7f3142468615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |████████████████████████████████| 881.9 MB 18 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10) (4.1.1)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.11.0+cu113\n","    Uninstalling torch-1.11.0+cu113:\n","      Successfully uninstalled torch-1.11.0+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\n","torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\n","torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.10.0\n"]}],"source":["!pip install torch==1.10"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22108,"status":"ok","timestamp":1657098625592,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"ZOLIFOJZaeZ5","outputId":"7ce5f0f8-f16a-4168-e3f6-5a341915b4cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import warnings\n","import os\n","\n","USE_COLAB = True\n","if USE_COLAB == True:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    work_dir = '/content/drive/MyDrive/Colab Notebooks/nequip/'\n","    data_dir = '/content/nequip/data'\n","else:\n","    work_dir = '/Users/gabrieletocci/Google Drive/My Drive/Colab Notebooks/nequip/'\n","    data_dir = '/Users/gabrieletocci/Documents/projects/MD_DFT/'\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":529,"status":"ok","timestamp":1657098626116,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"uZpOvFtImsy2","outputId":"a49df79f-b8af-4c44-88a8-397e5dc8c225"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.10.0+cu102\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","print(torch. __version__)\n","torch.cuda.is_available()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1657098626116,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"t7Uh7nyHnR-w","outputId":"735872f3-90d8-4eaf-d499-8e48b8aa3620"},"outputs":[{"name":"stdout","output_type":"stream","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2020 NVIDIA Corporation\n","Built on Mon_Oct_12_20:09:46_PDT_2020\n","Cuda compilation tools, release 11.1, V11.1.105\n","Build cuda_11.1.TC455_06.29190527_0\n"]}],"source":["!nvcc --version"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19205,"status":"ok","timestamp":1657098645319,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"uIMyrDOEm2IB","outputId":"a10729bc-2527-4e27-c620-81b7468f00cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["rm: cannot remove 'lammps': No such file or directory\n","Cloning into 'lammps'...\n","remote: Enumerating objects: 11732, done.\u001b[K\n","remote: Counting objects: 100% (11732/11732), done.\u001b[K\n","remote: Compressing objects: 100% (8609/8609), done.\u001b[K\n","remote: Total 11732 (delta 3932), reused 6311 (delta 2924), pack-reused 0\u001b[K\n","Receiving objects: 100% (11732/11732), 110.00 MiB | 14.95 MiB/s, done.\n","Resolving deltas: 100% (3932/3932), done.\n","Note: checking out '7586adbb6a61254125992709ef2fda9134cfca6c'.\n","\n","You are in 'detached HEAD' state. You can look around, make experimental\n","changes and commit them, and you can discard any commits you make in this\n","state without impacting any branches by performing another checkout.\n","\n","If you want to create a new branch to retain commits you create, you may\n","do so (now or later) by using -b with the checkout command again. Example:\n","\n","  git checkout -b <new-branch-name>\n","\n","Checking out files: 100% (11058/11058), done.\n","Cloning into 'pair_nequip'...\n","remote: Enumerating objects: 418, done.\u001b[K\n","remote: Counting objects: 100% (116/116), done.\u001b[K\n","remote: Compressing objects: 100% (43/43), done.\u001b[K\n","remote: Total 418 (delta 79), reused 85 (delta 65), pack-reused 302\u001b[K\n","Receiving objects: 100% (418/418), 427.14 KiB | 22.48 MiB/s, done.\n","Resolving deltas: 100% (208/208), done.\n","Copying files...\n","Updating CMakeLists.txt...\n","Done!\n"]}],"source":["!rm -r lammps\n","!git clone -b \"stable_29Sep2021_update2\" --depth 1 \"https://github.com/lammps/lammps.git\"\n","!git clone https://github.com/mir-group/pair_nequip\n","!cd pair_nequip && ./patch_lammps.sh /content/lammps/\n","!cd .."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1657098645319,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"100Be8B6m5am"},"outputs":[],"source":["!cp /content/pair_nequip/*.cpp /content/lammps/src/\n","!cp /content/pair_nequip/*.h /content/lammps/src/\n","! sed -i 's/CMAKE_CXX_STANDARD 11/CMAKE_CXX_STANDARD 14/g'  /content/lammps/cmake/CMakeLists.txt"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3788,"status":"ok","timestamp":1657098649096,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"KlgRSmyom9VQ","outputId":"1bf07769-a1f7-4794-8239-b5f57a2ca4c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mkl in /usr/local/lib/python3.7/dist-packages (2019.0)\n","Collecting mkl-include\n","  Downloading mkl_include-2022.1.0-py2.py3-none-manylinux1_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 20.1 MB/s \n","\u001b[?25hRequirement already satisfied: intel-openmp in /usr/local/lib/python3.7/dist-packages (from mkl) (2022.1.0)\n","Installing collected packages: mkl-include\n","Successfully installed mkl-include-2022.1.0\n"]}],"source":["!pip install mkl mkl-include"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155776,"status":"ok","timestamp":1657098804867,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"t8vY6rwbnEgK","outputId":"2d46df06-4105-4fc5-f08a-38231895be8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["-- The CXX compiler identification is GNU 7.5.0\n","-- Detecting CXX compiler ABI info\n","-- Detecting CXX compiler ABI info - done\n","-- Check for working CXX compiler: /usr/bin/c++ - skipped\n","-- Detecting CXX compile features\n","-- Detecting CXX compile features - done\n","-- Found Git: /usr/bin/git (found version \"2.17.1\") \n","-- Appending /usr/local/cuda/lib64/stubs to CMAKE_LIBRARY_PATH: /usr/local/cuda/lib64/stubs\n","-- Running check for auto-generated files from make-based build system\n","-- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so (found version \"3.1\") \n","-- Found MPI: TRUE (found version \"3.1\")  \n","-- Looking for C++ include omp.h\n","-- Looking for C++ include omp.h - found\n","-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n","-- Found OpenMP: TRUE (found version \"4.5\")  \n","-- Found JPEG: /usr/lib/x86_64-linux-gnu/libjpeg.so (found version \"80\") \n","-- Found PNG: /usr/lib/x86_64-linux-gnu/libpng.so (found version \"1.6.34\") \n","-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n","-- Found GZIP: /bin/gzip  \n","-- Found FFMPEG: /usr/bin/ffmpeg  \n","-- Looking for C++ include cmath\n","-- Looking for C++ include cmath - found\n","-- Generating style headers...\n","-- Generating package headers...\n","-- Generating lmpinstalledpkgs.h...\n","-- Could NOT find ClangFormat (missing: ClangFormat_EXECUTABLE) (Required is at least version \"8.0\")\n","-- The following tools and libraries have been found and configured:\n"," * Git\n"," * MPI\n"," * OpenMP\n"," * JPEG\n"," * PNG\n"," * ZLIB\n","\n","-- <<< Build configuration >>>\n","   Operating System: Linux Ubuntu 18.04\n","   Build type:       RelWithDebInfo\n","   Install path:     /root/.local\n","   Generator:        Unix Makefiles using /usr/bin/make\n","-- Enabled packages: <None>\n","-- <<< Compilers and Flags: >>>\n","-- C++ Compiler:     /usr/bin/c++\n","      Type:          GNU\n","      Version:       7.5.0\n","      C++ Flags:     -O2 -g -DNDEBUG\n","      Defines:       LAMMPS_SMALLBIG;LAMMPS_MEMALIGN=64;LAMMPS_OMP_COMPAT=3;LAMMPS_JPEG;LAMMPS_PNG;LAMMPS_GZIP;LAMMPS_FFMPEG\n","-- <<< Linker flags: >>>\n","-- Executable name:  lmp\n","-- Static library flags:    \n","-- <<< MPI flags >>>\n","-- MPI_defines:      MPICH_SKIP_MPICXX;OMPI_SKIP_MPICXX;_MPICC_H\n","-- MPI includes:     /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include;/usr/lib/x86_64-linux-gnu/openmpi/include\n","-- MPI libraries:    /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so;\n","-- Looking for C++ include pthread.h\n","-- Looking for C++ include pthread.h - found\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n","-- Looking for pthread_create in pthreads\n","-- Looking for pthread_create in pthreads - not found\n","-- Looking for pthread_create in pthread\n","-- Looking for pthread_create in pthread - found\n","-- Found Threads: TRUE  \n","-- Found CUDA: /usr/local/cuda (found version \"11.1\") \n","-- Caffe2: CUDA detected: 11.1\n","-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\n","-- Caffe2: CUDA toolkit directory: /usr/local/cuda\n","-- Caffe2: Header version is: 11.1\n","-- Found CUDNN: /usr/lib/x86_64-linux-gnu/libcudnn.so  \n","-- Found cuDNN: v7.6.5  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)\n","-- /usr/local/cuda/lib64/libnvrtc.so shorthash is 3a20f2b6\n","-- Autodetected CUDA architecture(s):  7.5\n","-- Added CUDA NVCC flags for: -gencode;arch=compute_75,code=sm_75\n","\u001b[33mCMake Warning at /usr/local/lib/python3.7/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\n","  static library kineto_LIBRARY-NOTFOUND not found.\n","Call Stack (most recent call first):\n","  /usr/local/lib/python3.7/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:127 (append_torchlib_if_found)\n","  CMakeLists.txt:922 (find_package)\n","\n","\u001b[0m\n","-- Found Torch: /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so  \n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/lammps/build\n","[  0%] \u001b[34m\u001b[1mGenerating includes/lammps/atom.h\u001b[0m\n","[  0%] \u001b[34m\u001b[1mGenerating includes/lammps/angle.h\u001b[0m\n","[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/variable.h\u001b[0m\n","-- Git Directory: /content/lammps/.git\n","[  1%] Built target atom.h\n","[  1%] Built target angle.h\n","[  1%] Built target variable.h\n","[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/comm.h\u001b[0m\n","[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/citeme.h\u001b[0m\n","[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/bond.h\u001b[0m\n","[  1%] Built target comm.h\n","[  1%] Built target bond.h\n","[  1%] Built target citeme.h\n","[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/compute.h\u001b[0m\n","[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/domain.h\u001b[0m\n","[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/dihedral.h\u001b[0m\n","[  2%] Built target compute.h\n","[  2%] Built target dihedral.h\n","[  2%] Built target domain.h\n","[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/error.h\u001b[0m\n","[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/fix.h\u001b[0m\n","[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/force.h\u001b[0m\n","[  2%] Built target error.h\n","[  2%] Built target force.h\n","[  2%] Built target fix.h\n","[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/group.h\u001b[0m\n","[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/improper.h\u001b[0m\n","[  3%] Built target group.h\n","[  3%] Built target improper.h\n","[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/input.h\u001b[0m\n","[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/kspace.h\u001b[0m\n","[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/info.h\u001b[0m\n","-- Generating lmpgitversion.h...\n","[  4%] Built target input.h\n","[  4%] Built target kspace.h\n","[  4%] Built target info.h\n","[  4%] Built target gitversion\n","[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lammps.h\u001b[0m\n","[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lattice.h\u001b[0m\n","[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/library.h\u001b[0m\n","[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/lmppython.h\u001b[0m\n","[  5%] Built target lammps.h\n","[  5%] Built target lattice.h\n","[  5%] Built target library.h\n","[  5%] Built target lmppython.h\n","[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/lmptype.h\u001b[0m\n","[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/memory.h\u001b[0m\n","[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/modify.h\u001b[0m\n","[  6%] Built target memory.h\n","[  6%] Built target lmptype.h\n","[  6%] Built target modify.h\n","[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/neighbor.h\u001b[0m\n","[  6%] Built target neighbor.h\n","[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/output.h\u001b[0m\n","[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/neigh_list.h\u001b[0m\n","[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/pair.h\u001b[0m\n","[  7%] Built target neigh_list.h\n","[  7%] Built target output.h\n","[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/pointers.h\u001b[0m\n","[  7%] Built target pair.h\n","[  7%] Built target pointers.h\n","[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/timer.h\u001b[0m\n","[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/region.h\u001b[0m\n","[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/universe.h\u001b[0m\n","[  8%] Built target timer.h\n","[  8%] Built target region.h\n","[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/update.h\u001b[0m\n","[  8%] Built target universe.h\n","[  8%] Built target update.h\n","[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/utils.h\u001b[0m\n","[  8%] Built target utils.h\n","[  8%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle.cpp.o\u001b[0m\n","[  8%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_hybrid.cpp.o\u001b[0m\n","[  8%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_deprecated.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_zero.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/arg_info.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_map.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec.cpp.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_atomic.cpp.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_body.cpp.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_charge.cpp.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_ellipsoid.cpp.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_hybrid.cpp.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_line.cpp.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_sphere.cpp.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_tri.cpp.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/balance.cpp.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/body.cpp.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond.cpp.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_deprecated.cpp.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_hybrid.cpp.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_zero.cpp.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/change_box.cpp.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/citeme.cpp.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_brick.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_tiled.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_aggregate_atom.cpp.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle.cpp.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle_local.cpp.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angmom_chunk.cpp.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond_local.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centro_atom.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centroid_stress_atom.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_atom.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_spread_atom.cpp.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cluster_atom.cpp.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cna_atom.cpp.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com.cpp.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com_chunk.cpp.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_coord_atom.cpp.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_deprecated.cpp.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral.cpp.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral_local.cpp.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole.cpp.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole_chunk.cpp.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_displace_atom.cpp.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere_atom.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_fragment_atom.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_global_atom.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_group_group.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration.cpp.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration_chunk.cpp.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_heat_flux.cpp.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper.cpp.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper_local.cpp.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_inertia_chunk.cpp.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke.cpp.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke_atom.cpp.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd.cpp.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd_chunk.cpp.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_omega_chunk.cpp.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_orientorder_atom.cpp.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair.cpp.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair_local.cpp.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe.cpp.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe_atom.cpp.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pressure.cpp.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_atom.cpp.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_chunk.cpp.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_local.cpp.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_rdf.cpp.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce.cpp.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_chunk.cpp.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_region.cpp.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_slice.cpp.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_stress_atom.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_chunk.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_com.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_deform.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_partial.cpp.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_profile.cpp.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_ramp.cpp.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_region.cpp.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_sphere.cpp.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_torque_chunk.cpp.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vacf.cpp.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vcm_chunk.cpp.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_atoms.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_bonds.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_box.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_atoms.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_bonds.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/deprecated.cpp.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral.cpp.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_deprecated.cpp.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_hybrid.cpp.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_zero.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/displace_atoms.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/domain.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_atom.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_cfg.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_custom.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_deprecated.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_image.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_local.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_movie.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_xyz.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/error.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/finish.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_adapt.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_addforce.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_atom.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_chunk.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_correlate.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo_weight.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_time.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_aveforce.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_balance.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_box_relax.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deform.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deposit.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deprecated.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dt_reset.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dummy.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_efield.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_enforce2d.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_evaporate.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_external.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_gravity.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_group.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_halt.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_heat.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_indent.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_langevin.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_lineforce.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_minimize.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_momentum.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_move.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_neigh_history.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh_sphere.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph_sphere.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt_sphere.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_limit.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_noforce.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_sphere.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sllod.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sphere.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_planeforce.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_press_berendsen.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_print.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_property_atom.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_read_restart.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_recenter.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_respa.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_restrain.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_setforce.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_chunk.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_self.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_force.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_state.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_berendsen.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_rescale.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_thermal_conductivity.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_vector.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_viscous.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_harmonic.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj1043.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj126.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj93.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_morse.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_reflect.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_region.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_format.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_os.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/force.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/gridcomm.cpp.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/group.cpp.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/hashlittle.cpp.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/image.cpp.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_group.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_neigh.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_store.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_time.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_var.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_deprecated.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_hybrid.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_zero.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/info.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/input.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/integrate.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/irregular.cpp.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace.cpp.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace_deprecated.cpp.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lammps.cpp.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lattice.cpp.o\u001b[0m\n","[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/library.cpp.o\u001b[0m\n","[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lmppython.cpp.o\u001b[0m\n","[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_eigen.cpp.o\u001b[0m\n","[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_extra.cpp.o\u001b[0m\n","[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_special.cpp.o\u001b[0m\n","[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/memory.cpp.o\u001b[0m\n","[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min.cpp.o\u001b[0m\n","[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_cg.cpp.o\u001b[0m\n","[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_fire.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_fire_old.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_hftn.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_linesearch.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_quickmin.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_sd.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/minimize.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/modify.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/molecule.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_page.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_pool_chunk.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_multi.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_standard.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_list.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_request.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neighbor.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_copy.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin_atomonly.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin_ghost.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_multi.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_multi_old.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_nsq.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_nsq_ghost.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_atomonly_newton.cpp.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newtoff.cpp.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newtoff_ghost.cpp.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newton.cpp.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newton_tri.cpp.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newtoff.cpp.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newton.cpp.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newton_tri.cpp.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newtoff.cpp.o\u001b[0m\n","[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newton.cpp.o\u001b[0m\n","[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newton_tri.cpp.o\u001b[0m\n","[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newtoff.cpp.o\u001b[0m\n","[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newtoff_ghost.cpp.o\u001b[0m\n","[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newton.cpp.o\u001b[0m\n","[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newtoff.cpp.o\u001b[0m\n","[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newton.cpp.o\u001b[0m\n","[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newton_tri.cpp.o\u001b[0m\n","[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_nsq_newtoff.cpp.o\u001b[0m\n","[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_nsq_newton.cpp.o\u001b[0m\n","[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newtoff.cpp.o\u001b[0m\n","[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newton.cpp.o\u001b[0m\n","[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newton_tri.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newtoff.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newton.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newton_tri.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newtoff.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newton.cpp.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newton_tri.cpp.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_nsq_newtoff.cpp.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_nsq_newton.cpp.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_halffull_newtoff.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_halffull_newton.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_respa.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size.cpp.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on.cpp.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on_oneside.cpp.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil.cpp.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_bin_2d.cpp.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_bin_3d.cpp.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_ghost_bin_2d.cpp.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_ghost_bin_3d.cpp.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_2d.cpp.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_3d.cpp.o\u001b[0m\n","[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_old_2d.cpp.o\u001b[0m\n","[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_old_3d.cpp.o\u001b[0m\n","[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_2d.cpp.o\u001b[0m\n","[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_2d_tri.cpp.o\u001b[0m\n","[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_3d.cpp.o\u001b[0m\n","[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_3d_tri.cpp.o\u001b[0m\n","[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_2d.cpp.o\u001b[0m\n","[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_2d_tri.cpp.o\u001b[0m\n","[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_3d.cpp.o\u001b[0m\n","[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_3d_tri.cpp.o\u001b[0m\n","[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_2d.cpp.o\u001b[0m\n","[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_2d_tri.cpp.o\u001b[0m\n","[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_3d.cpp.o\u001b[0m\n","[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_3d_tri.cpp.o\u001b[0m\n","[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo.cpp.o\u001b[0m\n","[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_all.cpp.o\u001b[0m\n","[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_partial.cpp.o\u001b[0m\n","[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_template.cpp.o\u001b[0m\n","[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_all.cpp.o\u001b[0m\n","[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_partial.cpp.o\u001b[0m\n","[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_template.cpp.o\u001b[0m\n","[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_all.cpp.o\u001b[0m\n","[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_partial.cpp.o\u001b[0m\n","[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_template.cpp.o\u001b[0m\n","[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_all.cpp.o\u001b[0m\n","[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_partial.cpp.o\u001b[0m\n","[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_template.cpp.o\u001b[0m\n","[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/output.cpp.o\u001b[0m\n","[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair.cpp.o\u001b[0m\n","[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_born.cpp.o\u001b[0m\n","[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck.cpp.o\u001b[0m\n","[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck_coul_cut.cpp.o\u001b[0m\n","[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_cut.cpp.o\u001b[0m\n","[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_debye.cpp.o\u001b[0m\n","[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_dsf.cpp.o\u001b[0m\n","[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_wolf.cpp.o\u001b[0m\n","[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_deprecated.cpp.o\u001b[0m\n","[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid.cpp.o\u001b[0m\n","[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_overlay.cpp.o\u001b[0m\n","[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_scaled.cpp.o\u001b[0m\n","[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut.cpp.o\u001b[0m\n","[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut_coul_cut.cpp.o\u001b[0m\n","[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_expand.cpp.o\u001b[0m\n","[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_morse.cpp.o\u001b[0m\n","[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_nequip.cpp.o\u001b[0m\n","[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_soft.cpp.o\u001b[0m\n","[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_table.cpp.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_yukawa.cpp.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zbl.cpp.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zero.cpp.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/potential_file_reader.cpp.o\u001b[0m\n","[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/procmap.cpp.o\u001b[0m\n","[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_mars.cpp.o\u001b[0m\n","[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_park.cpp.o\u001b[0m\n","[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rcb.cpp.o\u001b[0m\n","[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_data.cpp.o\u001b[0m\n","[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_dump.cpp.o\u001b[0m\n","[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_restart.cpp.o\u001b[0m\n","[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader.cpp.o\u001b[0m\n","[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_native.cpp.o\u001b[0m\n","[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_xyz.cpp.o\u001b[0m\n","[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region.cpp.o\u001b[0m\n","[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_block.cpp.o\u001b[0m\n","[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cone.cpp.o\u001b[0m\n","[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cylinder.cpp.o\u001b[0m\n","[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_deprecated.cpp.o\u001b[0m\n","[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_intersect.cpp.o\u001b[0m\n","[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_plane.cpp.o\u001b[0m\n","[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_prism.cpp.o\u001b[0m\n","[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_sphere.cpp.o\u001b[0m\n","[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_union.cpp.o\u001b[0m\n","[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/replicate.cpp.o\u001b[0m\n","[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rerun.cpp.o\u001b[0m\n","[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atom_ids.cpp.o\u001b[0m\n","[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_mol_ids.cpp.o\u001b[0m\n","[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/respa.cpp.o\u001b[0m\n","[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/run.cpp.o\u001b[0m\n","[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/set.cpp.o\u001b[0m\n","[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/special.cpp.o\u001b[0m\n","[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/table_file_reader.cpp.o\u001b[0m\n","[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tabular_function.cpp.o\u001b[0m\n","[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/text_file_reader.cpp.o\u001b[0m\n","[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/thermo.cpp.o\u001b[0m\n","[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/timer.cpp.o\u001b[0m\n","[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tokenizer.cpp.o\u001b[0m\n","[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/universe.cpp.o\u001b[0m\n","[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/update.cpp.o\u001b[0m\n","[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/utils.cpp.o\u001b[0m\n","[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/variable.cpp.o\u001b[0m\n","\u001b[01m\u001b[K/content/lammps/src/variable.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint LAMMPS_NS::Variable::next(int, char**)\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/lammps/src/variable.cpp:714:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n","         \u001b[01;35m\u001b[Kfread(buf,1,64,fp)\u001b[m\u001b[K;\n","         \u001b[01;35m\u001b[K~~~~~^~~~~~~~~~~~~\u001b[m\u001b[K\n","[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/velocity.cpp.o\u001b[0m\n","[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/verlet.cpp.o\u001b[0m\n","[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_coeff.cpp.o\u001b[0m\n","[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_data.cpp.o\u001b[0m\n","[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_dump.cpp.o\u001b[0m\n","[100%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_restart.cpp.o\u001b[0m\n","[100%] \u001b[32m\u001b[1mLinking CXX static library liblammps.a\u001b[0m\n","[100%] Built target lammps\n","[100%] \u001b[32mBuilding CXX object CMakeFiles/lmp.dir/content/lammps/src/main.cpp.o\u001b[0m\n","[100%] \u001b[32m\u001b[1mLinking CXX executable lmp\u001b[0m\n","[100%] Built target lmp\n"]}],"source":["!cd lammps && mkdir -p build && cd build && cmake ../cmake -DCMAKE_PREFIX_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` && make -j4"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3671,"status":"ok","timestamp":1657098808525,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"PHCh2aC7WfKj","outputId":"d2c4bfb2-7366-48a4-acc9-b5a3504a236e"},"outputs":[{"name":"stdout","output_type":"stream","text":["._AIMD_data\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n","AIMD_data/\n","AIMD_data/._WATER-frc-10k-1.xyz\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n","AIMD_data/WATER-frc-10k-1.xyz\n","AIMD_data/._celldata.dat\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n","AIMD_data/celldata.dat\n","AIMD_data/._WATER-pos-10k-1.xyz\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n","AIMD_data/WATER-pos-10k-1.xyz\n"]}],"source":["! git clone --depth 1 \"https://github.com/gabriele16/nequip.git\"\n","! tar -xzvf  /content/nequip/data/AIMD_data.tar.gz -C /content/nequip/data/"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26562,"status":"ok","timestamp":1657098835084,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"J51CA0Bod1Jv","outputId":"9745a751-1031-4a6e-c8b6-c56d022fa871"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wandb\n","  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 32.9 MB/s \n","\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 55.4 MB/s \n","\u001b[?25hCollecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.6.0-py2.py3-none-any.whl (145 kB)\n","\u001b[K     |████████████████████████████████| 145 kB 69.8 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n","Collecting setproctitle\n","  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n","Building wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=6af8a0ef84e3104d0b5ac92c201d58989f18cc233a190597a641d6c37a3e87ef\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built pathtools\n","Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n","Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.6.0 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.21\n","Cloning into 'nequip'...\n","remote: Enumerating objects: 183, done.\u001b[K\n","remote: Counting objects: 100% (183/183), done.\u001b[K\n","remote: Compressing objects: 100% (178/178), done.\u001b[K\n","remote: Total 183 (delta 8), reused 84 (delta 0), pack-reused 0\u001b[K\n","Receiving objects: 100% (183/183), 422.36 KiB | 7.68 MiB/s, done.\n","Resolving deltas: 100% (8/8), done.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing ./nequip\n","\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (1.21.6)\n","Collecting ase\n","  Downloading ase-3.22.1-py3-none-any.whl (2.2 MB)\n","\u001b[K     |████████████████████████████████| 2.2 MB 29.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (4.64.0)\n","Requirement already satisfied: torch!=1.9.0,<=1.12,>=1.8 in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (1.10.0)\n","Collecting e3nn<0.6.0,>=0.3.5\n","  Downloading e3nn-0.5.0-py3-none-any.whl (117 kB)\n","\u001b[K     |████████████████████████████████| 117 kB 96.6 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (3.13)\n","Collecting torch-runstats>=0.2.0\n","  Downloading torch_runstats-0.2.0-py3-none-any.whl (8.1 kB)\n","Collecting torch-ema>=0.3.0\n","  Downloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\n","Collecting scikit_learn<=1.0.1\n","  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n","\u001b[K     |████████████████████████████████| 23.2 MB 1.1 MB/s \n","\u001b[?25hRequirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (4.1.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.7.1)\n","Collecting opt-einsum-fx>=0.1.4\n","  Downloading opt_einsum_fx-0.1.4-py3-none-any.whl (13 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.4.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (21.3)\n","Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (3.3.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn<=1.0.1->nequip==0.5.5) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn<=1.0.1->nequip==0.5.5) (1.1.0)\n","Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from ase->nequip==0.5.5) (3.2.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (1.4.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1.0->ase->nequip==0.5.5) (1.15.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.2.1)\n","Building wheels for collected packages: nequip\n","  Building wheel for nequip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nequip: filename=nequip-0.5.5-py3-none-any.whl size=138681 sha256=9e58da4c0a94d5d9c1d2fb41040ae9899ff2c2feb1b897d31ac50cb40a51b534\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-_07lzu9b/wheels/a8/8f/18/b30c4402c2d6ab52853310650b85822afe26aaae5f6d00356b\n","Successfully built nequip\n","Installing collected packages: opt-einsum-fx, torch-runstats, torch-ema, scikit-learn, e3nn, ase, nequip\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.0.2\n","    Uninstalling scikit-learn-1.0.2:\n","      Successfully uninstalled scikit-learn-1.0.2\n","Successfully installed ase-3.22.1 e3nn-0.5.0 nequip-0.5.5 opt-einsum-fx-0.1.4 scikit-learn-1.0.1 torch-ema-0.3 torch-runstats-0.2.0\n"]},{"data":{"text/plain":["<torch._C.Generator at 0x7f68387085b0>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","import pandas as pd\n","# install wandb\n","!pip install wandb\n","# install nequip\n","#uncomment if running on google colab\n","!git clone --depth 1 \"https://github.com/gabriele16/nequip.git\"\n","!pip install nequip/\n","# fix colab imports\n","import site\n","site.main()\n","# set to allow anonymous WandB\n","import os\n","os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n","import numpy as np\n","from ase.io import read, write\n","\n","np.random.seed(0)\n","torch.manual_seed(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZixXbCiPcMGg"},"outputs":[],"source":["!pip3 install nglview\n","!jupyter-nbextension enable nglview --py --sys-prefix\n","import nglview as nv"]},{"cell_type":"code","execution_count":63,"metadata":{"executionInfo":{"elapsed":554,"status":"ok","timestamp":1657104213972,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"jDJ9Re0arOb0"},"outputs":[],"source":["def MD_reader_xyz(f, data_dir, no_skip = 0):\n","  filename = os.path.join(data_dir, f)\n","  fo = open(filename, 'r')\n","  natoms_str = fo.read().rsplit(' i = ')[0]\n","  natoms = int(natoms_str.split('\\n')[0])\n","  fo.close()  \n","  fo = open(filename, 'r')\n","  samples = fo.read().split(natoms_str)[1:]\n","  steps = []\n","  xyz = []\n","  temperatures = []\n","  energies = []\n","  for sample in samples[::no_skip]:\n","     entries = sample.split('\\n')[:-1]\n","     energies.append(float(entries[0].split(\"=\")[-1]))\n","     temp = np.array([list(map(float, lv.split()[1:])) for lv in entries[1:]])\n","     xyz.append(temp[:,:])\n","  return natoms_str, np.array(xyz), np.array(energies)     "]},{"cell_type":"code","execution_count":64,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1657104214459,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"DziQT_zjMKS4"},"outputs":[],"source":["from ase.build import sort\n","def MD_writer_xyz(positions,forces,cell_vec_abc,energies,\n","                  data_dir,f,  conv_frc = 1.0 , conv_ener = 1.0 ):\n","\n","  filename = os.path.join(data_dir, f)\n","  fo = open(filename, 'w')\n","\n","  for it, frame in enumerate(positions):\n","    natoms = len(frame)\n","    fo.write(\"{:5d}\\n\".format(natoms))\n","    fo.write('Lattice=\"{:.5f} 0.0 0.0 0.0 {:.5f} 0.0 0.0 0.0 {:.5f}\" \\\n","    Properties=\"species:S:1:pos:R:3:forces:R:3\" \\\n","    energy={:.10f} pbc=\"T T T\"\\n'.format(cell_vec_abc[0],cell_vec_abc[1],cell_vec_abc[2],energies[it]*conv_ener)    \n","    )\n","    if it%100 == 0.0:\n","      print(it)\n","    \n","    sorted_frame = sort(frame)\n","    sorted_forces = sort(forces[it])\n","\n","    fo.write(\"\".join(\"{:8s} {:.8f} {:16.8f} {:16.8f}\\\n","     {:16.8f} {:16.8f} {:16.8f}\\n\".format(sorted_frame[iat].symbol,\n","                                          sorted_frame[iat].position[0],\n","                                          sorted_frame[iat].position[1],\n","                                          sorted_frame[iat].position[2],\n","                                          sorted_forces[iat].position[0]*conv_frc,\n","                                          sorted_forces[iat].position[1]*conv_frc,\n","                                          sorted_forces[iat].position[2]*conv_frc)\n","                                          for iat in range(len(frame))))"]},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1657104214459,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"Z2U5i3IZqLBI","outputId":"e7c19798-178d-4e9b-994d-4900245b702d"},"outputs":[{"data":{"text/plain":["array([9.85, 9.85, 9.85])"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["def read_cell(f,data_dir):\n","  filename = os.path.join(data_dir,f)\n","  fo = open(filename,'r')\n","  cell_list_abc = fo.read().split('\\n')[:-1]\n","  cell_vec_abc = np.array([list(map(float, lv.split())) for lv in cell_list_abc]).squeeze()\n","  return(cell_vec_abc)\n","\n","cell_vec_abc = read_cell('celldata.dat',data_dir + '/AIMD_data')\n","cell_vec_abc"]},{"cell_type":"code","execution_count":66,"metadata":{"executionInfo":{"elapsed":8488,"status":"ok","timestamp":1657104222944,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"X9fcRMYnu5-V"},"outputs":[],"source":["wat_traj = read(data_dir +'AIMD_data/WATER-pos-10k-1.xyz',index=':')\n","wat_frc = read(data_dir + 'AIMD_data/WATER-frc-10k-1.xyz', index=':')"]},{"cell_type":"code","execution_count":67,"metadata":{"executionInfo":{"elapsed":1945,"status":"ok","timestamp":1657104224876,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"jT7B4ryYu82t"},"outputs":[],"source":["natoms, positions, energies = MD_reader_xyz('WATER-pos-10k-1.xyz', data_dir + '/AIMD_data/', no_skip=1)"]},{"cell_type":"code","execution_count":68,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25962,"status":"ok","timestamp":1657104250832,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"xgmmJAU5l5F2","outputId":"37e49f4a-f2e1-4a80-fd00-b686d1b7d77f"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","100\n","200\n","300\n","400\n","500\n","600\n","700\n","800\n","900\n","1000\n","1100\n","1200\n","1300\n","1400\n","1500\n","1600\n","1700\n","1800\n","1900\n","2000\n","2100\n","2200\n","2300\n","2400\n","2500\n","2600\n","2700\n","2800\n","2900\n","3000\n","3100\n","3200\n","3300\n","3400\n","3500\n","3600\n","3700\n","3800\n","3900\n","4000\n","4100\n","4200\n","4300\n","4400\n","4500\n","4600\n","4700\n","4800\n","4900\n","5000\n","5100\n","5200\n","5300\n","5400\n","5500\n","5600\n","5700\n","5800\n","5900\n","6000\n","6100\n","6200\n","6300\n","6400\n","6500\n","6600\n","6700\n","6800\n","6900\n","7000\n","7100\n","7200\n","7300\n","7400\n","7500\n","7600\n","7700\n","7800\n","7900\n","8000\n","8100\n","8200\n","8300\n","8400\n","8500\n","8600\n","8700\n","8800\n","8900\n","9000\n","9100\n","9200\n","9300\n","9400\n","9500\n","9600\n","9700\n","9800\n","9900\n"]}],"source":["MD_writer_xyz(wat_traj, wat_frc, cell_vec_abc, energies, data_dir + '/AIMD_data/', 'wat_pos_frc-10k.extxyz',conv_frc = 1.0, conv_ener = 27.211399)\n"]},{"cell_type":"markdown","metadata":{"id":"ZPqnt-SAXyvL"},"source":["### Turn on GPU\n","\n","Make sure Runtime --> Change runtime type is set to GPU"]},{"cell_type":"markdown","metadata":{"id":"-HDmxkn3z8_m"},"source":["## 3 Steps: \n","* Train: using a data set, train the neural network 🧠 \n","* Deploy: convert the Python-based model into a stand-alone potential file for fast execution ⚡\n","* Run: run Molecular Dynamics, Monte Carlo, Structural Minimization, ...  with it in LAMMPS 🏃"]},{"cell_type":"markdown","metadata":{"id":"6OD71eeDz7dA"},"source":["<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/all.png?raw=true\" width=\"60%\">"]},{"cell_type":"markdown","metadata":{"id":"62aEgq6QYFIn"},"source":["### Train a model"]},{"cell_type":"markdown","metadata":{"id":"ELdBzH_8z4_2"},"source":["<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/train.png?raw=true\" width=\"60%\">"]},{"cell_type":"markdown","metadata":{"id":"1KuOIippfVfd"},"source":["Here, we will train a NequIP potential on the following system\n","\n","* Water\n","* sampled at T=300K from AIMD\n","* Using 1000 training configurations\n","* The units of the reference data are in eV and A."]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1657104250833,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"mgoydrJW5lg0","outputId":"05aa85b6-60e9-4f66-d5f4-5da8f957468f"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'0.5.5'"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["import nequip\n","nequip.__version__"]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1657104250833,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"fFCszShRk2RP","outputId":"e6099dda-0d3a-42bb-a053-dfa78ddc7e51"},"outputs":[{"data":{"text/plain":["{'root': 'results/water', 'run_name': 'example-run-water', 'seed': 123, 'dataset_seed': 456, 'append': True, 'default_dtype': 'float32', 'allow_tf32': False, 'r_max': 4.0, 'num_layers': 4, 'l_max': 1, 'parity': True, 'num_features': 32, 'nonlinearity_type': 'gate', 'resnet': False, 'nonlinearity_scalars': {'e': 'silu', 'o': 'tanh'}, 'nonlinearity_gates': {'e': 'silu', 'o': 'tanh'}, 'num_basis': 8, 'BesselBasis_trainable': True, 'PolynomialCutoff_p': 6, 'invariant_layers': 2, 'invariant_neurons': 64, 'avg_num_neighbors': 'auto', 'use_sc': True, 'dataset': 'ase', 'dataset_file_name': './AIMD_data/wat_pos_frc-10k.extxyz', 'ase_args': {'format': 'extxyz'}, 'include_keys': ['user_label'], 'key_mapping': {'user_label': 'label0'}, 'chemical_symbols': ['H', 'O'], 'verbose': 'info', 'log_batch_freq': 1, 'log_epoch_freq': 1, 'save_checkpoint_freq': -1, 'save_ema_checkpoint_freq': -1, 'n_train': 100, 'n_val': 50, 'learning_rate': 0.005, 'batch_size': 5, 'validation_batch_size': 10, 'max_epochs': 100000, 'train_val_split': 'random', 'shuffle': True, 'metrics_key': 'validation_loss', 'use_ema': True, 'ema_decay': 0.99, 'ema_use_num_updates': True, 'report_init_validation': True, 'early_stopping_patiences': {'validation_loss': 50}, 'early_stopping_delta': {'validation_loss': 0.005}, 'early_stopping_cumulative_delta': False, 'early_stopping_lower_bounds': {'LR': 1e-05}, 'early_stopping_upper_bounds': {'cumulative_wall': 1e+100}, 'loss_coeffs': {'forces': 1, 'total_energy': [1, 'PerAtomMSELoss']}, 'metrics_components': [['forces', 'mae'], ['forces', 'rmse'], ['forces', 'mae', {'PerSpecies': True, 'report_per_component': False}], ['forces', 'rmse', {'PerSpecies': True, 'report_per_component': False}], ['total_energy', 'mae'], ['total_energy', 'mae', {'PerAtom': True}]], 'optimizer_name': 'Adam', 'optimizer_amsgrad': False, 'optimizer_betas': (0.9, 0.999), 'optimizer_eps': 1e-08, 'optimizer_weight_decay': 0, 'max_gradient_norm': None, 'lr_scheduler_name': 'ReduceLROnPlateau', 'lr_scheduler_patience': 100, 'lr_scheduler_factor': 0.5, 'per_species_rescale_scales_trainable': False, 'per_species_rescale_shifts_trainable': False, 'per_species_rescale_shifts': 'dataset_per_atom_total_energy_mean', 'per_species_rescale_scales': 'dataset_forces_rms', 'global_rescale_shift': None, 'global_rescale_scale': 'dataset_forces_rms', 'global_rescale_shift_trainable': False, 'global_rescale_scale_trainable': False}"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["from nequip.utils import Config\n","config = Config.from_file(data_dir + 'configs/my-full-example.yaml')\n","config"]},{"cell_type":"code","execution_count":71,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":679810,"status":"ok","timestamp":1657104930638,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"ukSnt_QD5avu","outputId":"205d1a03-0af0-4e9f-97fb-857f1e330a75"},"outputs":[{"name":"stdout","output_type":"stream","text":["'/content/drive/MyDrive/Colab Notebooks/nequip/configs/my-full-example.yaml'\n","Torch device: cuda\n","Processing dataset...\n","Loaded data: Batch(atomic_numbers=[960000, 1], batch=[960000], cell=[10000, 3, 3], edge_cell_shift=[25556838, 3], edge_index=[2, 25556838], forces=[960000, 3], pbc=[10000, 3], pos=[960000, 3], ptr=[10001], total_energy=[10000, 1])\n","Cached processed data to disk\n","Done!\n","Successfully loaded the data set of type ASEDataset(10000)...\n","Replace string dataset_forces_rms to 0.7736424803733826\n","Replace string dataset_per_atom_total_energy_mean to -156.0919189453125\n","Atomic outputs are scaled by: [H, O: 0.773642], shifted by [H, O: -156.091919].\n","Replace string dataset_forces_rms to 0.7736424803733826\n","Initially outputs are globally scaled by: 0.7736424803733826, total_energy are globally shifted by None.\n","Successfully built the network...\n","Number of weights: 154200\n","! Starting training ...\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      0     1          1.1         1.09       0.0142        0.606        0.807        0.515        0.787        0.651        0.688            1        0.846         8.82       0.0919\n","      0     2         1.02         1.01       0.0135        0.577        0.776        0.491        0.749         0.62        0.662        0.965        0.813         8.61       0.0896\n","      0     3        0.992        0.979       0.0132        0.574        0.765        0.494        0.734        0.614        0.659        0.943        0.801         8.52       0.0888\n","      0     4        0.973         0.96       0.0134        0.564        0.758         0.48        0.733        0.606        0.653        0.933        0.793         8.58       0.0894\n","      0     5        0.969        0.956       0.0131        0.571        0.756        0.491        0.732        0.612         0.65        0.934        0.792          8.5       0.0885\n","\n","\n","  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Initial Validation          0    3.602    0.005        0.998       0.0135         1.01        0.578        0.773        0.494        0.747        0.621        0.663        0.956        0.809         8.61       0.0896\n","Wall time: 3.602406453000185\n","! Best model        0    1.011\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      1     1        0.886        0.873       0.0135         0.55        0.723        0.481        0.687        0.584        0.633        0.875        0.754          8.6       0.0896\n","      1     2         1.02         1.01       0.0064        0.576        0.778        0.498        0.733        0.616        0.664        0.968        0.816         5.93       0.0618\n","      1     3         1.05         1.05      0.00212        0.586        0.792        0.499        0.759        0.629        0.675        0.986         0.83         3.37       0.0351\n","      1     4         1.01         1.01     0.000403        0.573        0.777        0.492        0.735        0.613        0.672        0.954        0.813          1.4       0.0146\n","      1     5        0.962        0.962        5e-05        0.576        0.759        0.499         0.73        0.615        0.655        0.933        0.794        0.481      0.00501\n","      1     6        0.956        0.956      2.6e-05        0.573        0.756        0.491        0.737        0.614        0.647        0.938        0.792         0.33      0.00344\n","      1     7        0.904        0.904     6.71e-05        0.553        0.736        0.472        0.715        0.594        0.632        0.908         0.77        0.529      0.00552\n","      1     8        0.891        0.891     3.62e-05         0.53         0.73        0.457        0.677        0.567        0.626        0.903        0.764        0.413       0.0043\n","      1     9        0.826        0.826     0.000137        0.533        0.703        0.457        0.685        0.571        0.597        0.878        0.738        0.817      0.00851\n","      1    10        0.893        0.893     0.000659        0.558        0.731        0.474        0.726          0.6        0.627        0.903        0.765         1.89       0.0197\n","      1    11        0.769        0.767      0.00131        0.505        0.678        0.435        0.646        0.541        0.581        0.839         0.71         2.69        0.028\n","      1    12        0.654        0.651        0.003        0.474        0.624        0.403        0.615        0.509        0.526        0.785        0.655         4.05       0.0422\n","      1    13        0.596        0.591      0.00545        0.458        0.595        0.394        0.586         0.49         0.51        0.735        0.623         5.47        0.057\n","      1    14         0.59        0.581      0.00867        0.445         0.59        0.381        0.574        0.477          0.5        0.737        0.619          6.9       0.0719\n","      1    15        0.528        0.516       0.0123         0.41        0.556        0.343        0.544        0.444        0.463        0.705        0.584         8.23       0.0858\n","      1    16         0.46        0.445       0.0148        0.394        0.516        0.331         0.52        0.425        0.434         0.65        0.542         9.04       0.0941\n","      1    17        0.346        0.333       0.0131        0.345        0.446        0.303        0.428        0.365        0.388        0.545        0.466         8.49       0.0884\n","      1    18        0.335        0.325      0.00993         0.34        0.441        0.302        0.415        0.359        0.386        0.535         0.46         7.38       0.0769\n","      1    19        0.295        0.293      0.00247         0.33        0.419        0.287        0.416        0.351         0.36        0.516        0.438         3.67       0.0382\n","      1    20        0.273        0.272     0.000177        0.313        0.404        0.279        0.381         0.33        0.355        0.487        0.421        0.965       0.0101\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      1     1        0.298        0.298     0.000639        0.326        0.422        0.281        0.415        0.348        0.357        0.529        0.443         1.84       0.0191\n","      1     2        0.273        0.272     0.000526        0.312        0.404        0.272        0.391        0.332        0.348        0.497        0.422         1.65       0.0172\n","      1     3        0.253        0.253     0.000496        0.304        0.389        0.272        0.368         0.32        0.343        0.467        0.405         1.62       0.0169\n","      1     4        0.264        0.263     0.000611        0.311        0.397        0.274        0.385        0.329        0.344        0.487        0.415         1.79       0.0187\n","      1     5        0.265        0.264     0.000581        0.308        0.398        0.266        0.392        0.329        0.338        0.496        0.417         1.77       0.0184\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               1    7.919    0.005        0.707      0.00473        0.712        0.481        0.651        0.414        0.616        0.515        0.558        0.805        0.681         4.03        0.042\n","! Validation          1    7.919    0.005         0.27     0.000571        0.271        0.312        0.402        0.273         0.39        0.332        0.346        0.495        0.421         1.73       0.0181\n","Wall time: 7.919866944000205\n","! Best model        1    0.271\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      2     1         0.27        0.266      0.00439        0.312        0.399        0.277        0.382         0.33         0.35        0.482        0.416         4.87       0.0508\n","      2     2        0.263        0.254      0.00901        0.299         0.39        0.269        0.358        0.313        0.351        0.457        0.404         7.02       0.0731\n","      2     3        0.212          0.2       0.0126        0.265        0.346         0.23        0.335        0.283        0.297        0.426        0.362         8.31       0.0866\n","      2     4        0.207        0.199       0.0088        0.266        0.345        0.233        0.333        0.283        0.298        0.424        0.361         6.94       0.0723\n","      2     5        0.204        0.202      0.00223        0.268        0.347        0.232        0.338        0.285        0.299        0.428        0.364         3.49       0.0364\n","      2     6        0.161        0.161     0.000149        0.239         0.31         0.21        0.299        0.254        0.269        0.379        0.324        0.696      0.00725\n","      2     7        0.167        0.166     0.000341        0.246        0.315        0.213        0.312        0.263        0.272        0.389         0.33         1.21       0.0126\n","      2     8        0.152        0.152     0.000398         0.23        0.301        0.197        0.295        0.246        0.253         0.38        0.316         1.25        0.013\n","      2     9        0.174        0.173     0.000924        0.246        0.322        0.211        0.317        0.264        0.272        0.404        0.338         2.24       0.0233\n","      2    10        0.154        0.154     0.000707        0.236        0.303        0.204          0.3        0.252         0.26        0.375        0.317         1.93       0.0201\n","      2    11        0.146        0.145     0.000939        0.231        0.295        0.204        0.284        0.244        0.256         0.36        0.308         2.23       0.0232\n","      2    12        0.163        0.162     0.000592        0.246        0.312        0.215        0.309        0.262        0.271         0.38        0.325         1.78       0.0186\n","      2    13        0.142        0.141     0.000747        0.226         0.29        0.195        0.289        0.242        0.245        0.364        0.305         1.99       0.0207\n","      2    14         0.13        0.128      0.00154        0.214        0.277        0.184        0.274        0.229        0.233        0.349        0.291         2.88         0.03\n","      2    15        0.138        0.136      0.00201         0.22        0.286        0.194        0.273        0.233        0.247        0.351        0.299          3.3       0.0344\n","      2    16        0.147        0.146      0.00121        0.224        0.295        0.189        0.292        0.241        0.242         0.38        0.311         2.54       0.0264\n","      2    17        0.137        0.136     0.000426        0.222        0.286        0.194        0.279        0.236        0.244        0.355        0.299         1.48       0.0155\n","      2    18        0.137        0.137     3.24e-05         0.22        0.286         0.19        0.278        0.234        0.244        0.356          0.3        0.353      0.00367\n","      2    19        0.122        0.122     4.68e-05        0.208         0.27        0.182        0.262        0.222        0.233        0.332        0.282        0.402      0.00419\n","      2    20        0.127        0.126     0.000242        0.211        0.275        0.179        0.274        0.227        0.231        0.347        0.289         1.14       0.0119\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      2     1        0.128        0.128     4.02e-05        0.211        0.277        0.181        0.272        0.226        0.229        0.353        0.291        0.433      0.00451\n","      2     2        0.132        0.132      3.8e-05        0.215        0.281        0.186        0.275         0.23        0.239         0.35        0.294         0.42      0.00437\n","      2     3        0.114        0.114     2.33e-05        0.203        0.261        0.178        0.253        0.216        0.226        0.319        0.272         0.29      0.00302\n","      2     4         0.12         0.12     4.85e-05        0.205        0.268        0.179        0.257        0.218        0.229        0.333        0.281        0.392      0.00408\n","      2     5        0.128        0.127     3.44e-05        0.211        0.276        0.179        0.274        0.227        0.229        0.353        0.291        0.366      0.00381\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               2   11.618    0.005        0.165      0.00237        0.168        0.241        0.314         0.21        0.304        0.257        0.271        0.388        0.329          2.8       0.0292\n","! Validation          2   11.618    0.005        0.124     3.69e-05        0.124        0.209        0.273        0.181        0.266        0.223         0.23        0.342        0.286         0.38      0.00396\n","Wall time: 11.618556000999888\n","! Best model        2    0.124\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      3     1        0.123        0.122      0.00115        0.205        0.271        0.169        0.278        0.223        0.221        0.349        0.285         2.48       0.0258\n","      3     2        0.126        0.124      0.00129        0.209        0.273        0.181        0.265        0.223        0.231        0.341        0.286         2.63       0.0274\n","      3     3        0.128        0.127     0.000888        0.212        0.275        0.186        0.265        0.225        0.239        0.336        0.288         2.16       0.0225\n","      3     4        0.104        0.104      0.00017        0.192        0.249        0.165        0.245        0.205        0.212        0.311        0.261        0.896      0.00933\n","      3     5        0.122        0.122     0.000148        0.204         0.27        0.176        0.261        0.218        0.233        0.332        0.283        0.863      0.00898\n","      3     6       0.0961       0.0959     0.000121        0.184         0.24        0.157        0.238        0.198        0.199        0.305        0.252        0.734      0.00764\n","      3     7        0.104        0.103      0.00029        0.193        0.249        0.166        0.246        0.206        0.211        0.311        0.261         1.15        0.012\n","      3     8        0.114        0.114     0.000186        0.199        0.261        0.171        0.256        0.214        0.221        0.325        0.273        0.973       0.0101\n","      3     9        0.109        0.109     4.67e-05        0.196        0.255        0.169        0.252         0.21         0.22        0.314        0.267        0.334      0.00348\n","      3    10       0.0941       0.0939     0.000195        0.184        0.237        0.162        0.228        0.195        0.205         0.29        0.248         1.01       0.0105\n","      3    11        0.106        0.106     0.000142        0.189        0.252        0.164        0.239        0.201        0.213        0.314        0.264        0.786      0.00818\n","      3    12       0.0905       0.0905     1.81e-05        0.181        0.233        0.157        0.228        0.193        0.199        0.289        0.244        0.252      0.00262\n","      3    13        0.102        0.102     4.52e-05        0.193        0.247        0.169         0.24        0.204        0.214        0.302        0.258        0.485      0.00505\n","      3    14       0.0898       0.0896     0.000191         0.18        0.232         0.16        0.221        0.191        0.205        0.277        0.241        0.887      0.00924\n","      3    15       0.0931        0.093     3.22e-05        0.185        0.236        0.159        0.236        0.197        0.201        0.294        0.247        0.389      0.00405\n","      3    16       0.0853       0.0851     0.000143        0.174        0.226        0.146         0.23        0.188        0.188        0.287        0.237        0.841      0.00876\n","      3    17       0.0794       0.0791     0.000293        0.169        0.218        0.149         0.21        0.179        0.194        0.259        0.226         1.21       0.0126\n","      3    18       0.0847       0.0844     0.000328        0.174        0.225        0.149        0.226        0.187        0.191         0.28        0.236         1.25       0.0131\n","      3    19        0.098        0.098     3.49e-05        0.187        0.242        0.161        0.238          0.2        0.206        0.302        0.254         0.38      0.00396\n","      3    20       0.0859       0.0858     8.84e-05        0.175        0.227         0.15        0.226        0.188        0.193        0.282        0.238        0.653      0.00681\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      3     1       0.0955       0.0955     2.49e-05        0.182        0.239        0.155        0.234        0.195        0.199        0.304        0.251        0.297      0.00309\n","      3     2       0.0993       0.0992     3.77e-05        0.186        0.244        0.161        0.238        0.199         0.21          0.3        0.255        0.396      0.00412\n","      3     3       0.0819       0.0819     2.86e-05        0.171        0.221        0.151        0.212        0.182        0.194        0.268        0.231        0.308       0.0032\n","      3     4       0.0904       0.0904     1.67e-05        0.177        0.233        0.154        0.224        0.189        0.199        0.289        0.244        0.283      0.00295\n","      3     5       0.0939       0.0939     1.97e-05        0.181        0.237        0.154        0.235        0.195        0.198        0.301        0.249        0.247      0.00258\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               3   15.329    0.005        0.101      0.00029        0.102        0.189        0.246        0.163        0.241        0.202         0.21        0.306        0.258         1.02       0.0106\n","! Validation          3   15.329    0.005       0.0922     2.55e-05       0.0922         0.18        0.235        0.155        0.228        0.192          0.2        0.292        0.246        0.306      0.00319\n","Wall time: 15.3300363620001\n","! Best model        3    0.092\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      4     1       0.0771        0.077     4.66e-05        0.165        0.215        0.142        0.211        0.177        0.183        0.268        0.225        0.425      0.00443\n","      4     2       0.0834       0.0834     5.56e-05        0.175        0.223         0.15        0.227        0.188        0.187        0.282        0.235        0.492      0.00512\n","      4     3       0.0764       0.0763     8.12e-05        0.169        0.214        0.146        0.215        0.181        0.183        0.265        0.224        0.609      0.00634\n","      4     4       0.0777       0.0775      0.00013        0.166        0.215        0.148        0.202        0.175        0.195        0.252        0.223        0.795      0.00828\n","      4     5       0.0776       0.0775     9.66e-05        0.166        0.215        0.144        0.208        0.176        0.184        0.267        0.226         0.69      0.00719\n","      4     6       0.0768       0.0762     0.000654        0.163        0.214        0.142        0.205        0.174        0.183        0.264        0.223         1.89       0.0196\n","      4     7       0.0787       0.0782     0.000551        0.166        0.216        0.144        0.211        0.177        0.185        0.268        0.226         1.72       0.0179\n","      4     8       0.0756       0.0756     3.23e-05        0.165        0.213        0.143        0.209        0.176        0.183        0.262        0.223        0.362      0.00377\n","      4     9        0.074       0.0738     0.000186        0.161         0.21        0.138        0.207        0.172        0.177        0.265        0.221        0.937      0.00976\n","      4    10       0.0778       0.0773     0.000587        0.166        0.215        0.142        0.215        0.178        0.185        0.265        0.225         1.77       0.0184\n","      4    11       0.0726       0.0725     7.92e-05        0.159        0.208        0.135        0.207        0.171        0.175        0.263        0.219        0.619      0.00645\n","      4    12       0.0609       0.0609     6.43e-05        0.148        0.191        0.128        0.189        0.158        0.164        0.235          0.2        0.546      0.00569\n","      4    13       0.0593       0.0592     6.22e-05        0.146        0.188        0.127        0.185        0.156        0.162        0.231        0.197        0.561      0.00584\n","      4    14       0.0751       0.0751     9.74e-06        0.163        0.212        0.141        0.207        0.174        0.182        0.262        0.222        0.196      0.00205\n","      4    15       0.0636       0.0634     0.000251        0.152        0.195        0.131        0.195        0.163        0.168         0.24        0.204         1.16       0.0121\n","      4    16        0.078       0.0776     0.000349        0.163        0.216        0.142        0.206        0.174        0.185        0.267        0.226         1.32       0.0138\n","      4    17        0.053        0.053     5.15e-05        0.137        0.178         0.12         0.17        0.145        0.158        0.213        0.185        0.434      0.00452\n","      4    18        0.065       0.0648     0.000273        0.151        0.197         0.13        0.195        0.162        0.168        0.244        0.206         1.17       0.0122\n","      4    19       0.0543       0.0542     0.000137         0.14         0.18        0.124        0.172        0.148        0.158        0.218        0.188        0.864        0.009\n","      4    20       0.0703       0.0702     9.22e-05         0.16        0.205         0.14        0.201         0.17         0.18        0.248        0.214        0.629      0.00656\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      4     1       0.0703       0.0703     1.68e-05        0.156        0.205        0.135          0.2        0.167        0.174        0.257        0.215        0.258      0.00268\n","      4     2       0.0727       0.0727     2.65e-05        0.159        0.209         0.14        0.199        0.169        0.184        0.251        0.217        0.314      0.00327\n","      4     3       0.0591       0.0591     1.29e-05        0.146        0.188         0.13        0.178        0.154        0.166        0.225        0.196        0.227      0.00237\n","      4     4       0.0644       0.0644     1.17e-05        0.151        0.196        0.131        0.191        0.161        0.168        0.243        0.206        0.206      0.00214\n","      4     5       0.0637       0.0637     1.16e-05        0.149        0.195        0.127        0.192         0.16        0.167        0.243        0.205        0.193      0.00201\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               4   19.031    0.005       0.0712     0.000189       0.0714        0.159        0.206        0.138        0.202         0.17        0.178        0.254        0.216         0.86      0.00895\n","! Validation          4   19.031    0.005        0.066     1.59e-05        0.066        0.152        0.199        0.132        0.192        0.162        0.172        0.244        0.208        0.239      0.00249\n","Wall time: 19.03149556199969\n","! Best model        4    0.066\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      5     1        0.066       0.0657     0.000242        0.151        0.198        0.133        0.186        0.159        0.174         0.24        0.207          1.1       0.0114\n","      5     2        0.068        0.068     5.87e-05        0.155        0.202        0.136        0.193        0.164        0.175        0.246        0.211         0.52      0.00542\n","      5     3       0.0584       0.0581     0.000315        0.142        0.186        0.122         0.18        0.151        0.161         0.23        0.195         1.24       0.0129\n","      5     4        0.055       0.0545     0.000554        0.139        0.181         0.12        0.177        0.148        0.154        0.224        0.189         1.73        0.018\n","      5     5       0.0494       0.0493      0.00014        0.133        0.172        0.117        0.166        0.141        0.149        0.209        0.179        0.837      0.00872\n","      5     6       0.0516       0.0515     8.55e-05        0.136        0.176        0.118        0.173        0.146        0.153        0.214        0.183        0.619      0.00645\n","      5     7       0.0525       0.0522     0.000236        0.138        0.177        0.119        0.178        0.148        0.149        0.222        0.186         1.11       0.0116\n","      5     8       0.0561       0.0557     0.000401        0.142        0.183        0.125        0.175         0.15        0.161         0.22         0.19         1.47       0.0154\n","      5     9       0.0497       0.0496     1.22e-05        0.133        0.172        0.116        0.166        0.141        0.152        0.208         0.18        0.225      0.00235\n","      5    10       0.0543       0.0541      0.00018         0.14         0.18        0.124        0.171        0.148        0.159        0.216        0.187        0.971       0.0101\n","      5    11       0.0636       0.0634     0.000142         0.15        0.195         0.13        0.189        0.159        0.169        0.239        0.204        0.874       0.0091\n","      5    12       0.0526       0.0526     8.18e-06        0.138        0.177        0.122        0.171        0.146        0.157        0.213        0.185         0.16      0.00167\n","      5    13       0.0531       0.0529     0.000262        0.137        0.178        0.122        0.167        0.145        0.158        0.212        0.185         1.18       0.0122\n","      5    14       0.0441       0.0438     0.000332        0.127        0.162        0.112        0.158        0.135        0.142        0.196        0.169         1.35       0.0141\n","      5    15       0.0456       0.0454     0.000246        0.127        0.165        0.111         0.16        0.136        0.142        0.203        0.172         1.16       0.0121\n","      5    16       0.0502       0.0502     7.74e-06        0.135        0.173        0.123        0.158        0.141         0.16        0.198        0.179        0.159      0.00166\n","      5    17       0.0464       0.0457     0.000677        0.126        0.165        0.108        0.162        0.135        0.137        0.211        0.174         1.92         0.02\n","      5    18       0.0435       0.0423      0.00118        0.122        0.159        0.108        0.149        0.129        0.142        0.188        0.165         2.55       0.0265\n","      5    19       0.0496       0.0493     0.000295        0.134        0.172        0.117        0.169        0.143        0.151        0.208        0.179         1.26       0.0131\n","      5    20       0.0523       0.0523     1.44e-05        0.135        0.177        0.121        0.163        0.142        0.157        0.211        0.184        0.276      0.00287\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      5     1       0.0568       0.0568     2.65e-05        0.141        0.184        0.123        0.177         0.15        0.159        0.227        0.193        0.315      0.00328\n","      5     2        0.058       0.0579     3.86e-05        0.143        0.186        0.127        0.175        0.151        0.167         0.22        0.193        0.408      0.00425\n","      5     3       0.0471       0.0471     5.02e-05         0.13        0.168        0.117        0.156        0.136        0.151        0.198        0.174        0.469      0.00488\n","      5     4        0.051        0.051     3.31e-05        0.135        0.175        0.117         0.17        0.144        0.151        0.214        0.183        0.369      0.00385\n","      5     5       0.0494       0.0494     4.74e-05        0.132        0.172        0.114        0.167        0.141        0.149         0.21         0.18        0.459      0.00479\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               5   22.742    0.005       0.0528     0.000269       0.0531        0.137        0.178         0.12        0.171        0.145        0.155        0.216        0.186         1.04       0.0108\n","! Validation          5   22.742    0.005       0.0524     3.92e-05       0.0525        0.136        0.177        0.119        0.169        0.144        0.155        0.214        0.185        0.404      0.00421\n","Wall time: 22.742360530999576\n","! Best model        5    0.052\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      6     1       0.0443       0.0442     0.000107        0.125        0.163        0.108         0.16        0.134         0.14        0.201         0.17        0.749       0.0078\n","      6     2       0.0422        0.042     0.000277        0.124        0.158        0.109        0.152        0.131        0.138        0.193        0.165         1.23       0.0129\n","      6     3       0.0457       0.0455     0.000197        0.128        0.165        0.115        0.155        0.135        0.149        0.192        0.171         1.02       0.0107\n","      6     4       0.0522       0.0522     8.34e-06        0.136        0.177        0.121        0.164        0.143        0.157        0.211        0.184         0.21      0.00219\n","      6     5       0.0449       0.0448     7.61e-05        0.127        0.164        0.113        0.156        0.135        0.144        0.197        0.171        0.576        0.006\n","      6     6       0.0492       0.0492     9.71e-06         0.13        0.172        0.115         0.16        0.137        0.153        0.203        0.178        0.212      0.00221\n","      6     7       0.0431       0.0431     1.13e-05        0.126        0.161        0.113        0.151        0.132        0.143        0.191        0.167        0.225      0.00234\n","      6     8       0.0443        0.044     0.000307        0.125        0.162        0.108        0.157        0.133        0.139          0.2         0.17         1.27       0.0133\n","      6     9       0.0378       0.0377     6.48e-05        0.118         0.15        0.103        0.148        0.126        0.132        0.181        0.157        0.595       0.0062\n","      6    10       0.0475       0.0474     1.04e-05        0.129        0.169        0.112        0.162        0.137        0.147        0.205        0.176        0.183      0.00191\n","      6    11       0.0531        0.053     5.87e-05        0.136        0.178        0.119        0.169        0.144        0.157        0.215        0.186        0.452      0.00471\n","      6    12       0.0462       0.0458     0.000342        0.129        0.166        0.115        0.157        0.136        0.146        0.199        0.173         1.36       0.0141\n","      6    13       0.0472       0.0468     0.000329         0.13        0.167        0.115        0.158        0.137        0.149          0.2        0.174         1.29       0.0134\n","      6    14        0.045        0.045     2.37e-05        0.128        0.164        0.114        0.156        0.135        0.147        0.193         0.17        0.322      0.00335\n","      6    15       0.0397       0.0397     9.63e-06        0.118        0.154        0.104        0.146        0.125        0.133        0.189        0.161        0.192        0.002\n","      6    16       0.0443       0.0442     7.91e-05        0.126        0.163        0.111        0.156        0.134        0.142        0.197         0.17         0.65      0.00677\n","      6    17       0.0428       0.0425     0.000282        0.122         0.16        0.112        0.144        0.128        0.147        0.182        0.164         1.24       0.0129\n","      6    18       0.0357       0.0357     1.29e-05        0.115        0.146        0.101        0.141        0.121        0.129        0.175        0.152         0.23       0.0024\n","      6    19       0.0489       0.0487     0.000193        0.131        0.171        0.116        0.161        0.139        0.151        0.205        0.178         1.01       0.0105\n","      6    20       0.0429       0.0429     1.11e-05        0.125         0.16        0.111        0.153        0.132        0.143         0.19        0.167        0.215      0.00224\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      6     1       0.0488       0.0488     8.03e-06        0.131        0.171        0.115        0.163        0.139        0.149        0.208        0.179        0.175      0.00182\n","      6     2        0.049        0.049      1.6e-05        0.132        0.171        0.118         0.16        0.139        0.154        0.202        0.178        0.246      0.00256\n","      6     3       0.0404       0.0404     1.72e-05        0.121        0.156        0.109        0.144        0.127         0.14        0.182        0.161        0.265      0.00276\n","      6     4       0.0433       0.0433     1.15e-05        0.125        0.161        0.108        0.157        0.133         0.14        0.197        0.168        0.189      0.00197\n","      6     5        0.042        0.042     1.92e-05        0.121        0.158        0.106        0.152        0.129        0.139        0.191        0.165        0.269       0.0028\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               6   26.461    0.005       0.0447     0.000121       0.0448        0.126        0.164        0.112        0.155        0.134        0.145        0.196         0.17        0.662      0.00689\n","! Validation          6   26.461    0.005       0.0447     1.44e-05       0.0447        0.126        0.164        0.111        0.155        0.133        0.144        0.196         0.17        0.229      0.00238\n","Wall time: 26.46136679099982\n","! Best model        6    0.045\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      7     1       0.0443       0.0442     0.000101        0.125        0.163        0.112        0.152        0.132        0.146        0.192        0.169        0.732      0.00762\n","      7     2       0.0428       0.0428      5.5e-05        0.123         0.16        0.107        0.156        0.131        0.139        0.196        0.167        0.526      0.00548\n","      7     3       0.0455       0.0453     0.000155        0.127        0.165         0.11        0.161        0.135        0.141        0.204        0.172        0.921      0.00959\n","      7     4       0.0437       0.0437     5.95e-05        0.125        0.162        0.115        0.146         0.13        0.147        0.187        0.167        0.521      0.00543\n","      7     5       0.0417       0.0414     0.000279         0.12        0.157        0.108        0.145        0.127        0.143        0.183        0.163         1.19       0.0124\n","      7     6       0.0383       0.0377     0.000554        0.118         0.15        0.105        0.142        0.124        0.136        0.176        0.156         1.75       0.0182\n","      7     7       0.0407       0.0406     2.55e-05        0.121        0.156        0.109        0.145        0.127        0.138        0.186        0.162        0.339      0.00353\n","      7     8       0.0393       0.0393     5.54e-06        0.118        0.153        0.105        0.143        0.124        0.137        0.182         0.16        0.132      0.00137\n","      7     9       0.0413       0.0413     1.74e-05         0.12        0.157        0.109        0.143        0.126        0.139        0.188        0.164        0.264      0.00275\n","      7    10       0.0417       0.0415     0.000165        0.122        0.158        0.106        0.154         0.13        0.135        0.195        0.165        0.926      0.00964\n","      7    11       0.0487       0.0484     0.000285        0.131         0.17         0.12        0.153        0.137        0.159        0.191        0.175         1.25        0.013\n","      7    12       0.0407       0.0407     2.26e-05         0.12        0.156        0.107        0.147        0.127         0.14        0.185        0.162        0.315      0.00328\n","      7    13       0.0411       0.0408     0.000251         0.12        0.156        0.109        0.143        0.126        0.142        0.182        0.162         1.17       0.0122\n","      7    14       0.0446       0.0446     6.93e-06        0.127        0.163        0.114        0.154        0.134        0.146        0.193         0.17        0.159      0.00166\n","      7    15       0.0349       0.0349     5.36e-05        0.112        0.144       0.0978         0.14        0.119        0.125        0.177        0.151        0.529      0.00551\n","      7    16       0.0436       0.0436     7.98e-06        0.124        0.162        0.115        0.143        0.129        0.151         0.18        0.166        0.167      0.00174\n","      7    17       0.0332       0.0331     4.74e-05        0.109        0.141       0.0959        0.135        0.115        0.122        0.172        0.147        0.471      0.00491\n","      7    18       0.0403       0.0402     6.03e-05        0.121        0.155         0.11        0.142        0.126         0.14        0.181        0.161        0.563      0.00587\n","      7    19       0.0379       0.0378     0.000113        0.116         0.15        0.101        0.145        0.123        0.131        0.183        0.157        0.777      0.00809\n","      7    20       0.0397       0.0395     0.000182        0.118        0.154        0.104        0.146        0.125        0.134        0.187         0.16         0.97       0.0101\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      7     1       0.0437       0.0437     5.73e-06        0.124        0.162        0.109        0.155        0.132        0.141        0.197        0.169        0.154      0.00161\n","      7     2        0.043        0.043     1.14e-05        0.124         0.16         0.11        0.151        0.131        0.143         0.19        0.167        0.199      0.00207\n","      7     3       0.0362       0.0362     1.29e-05        0.114        0.147        0.103        0.138         0.12        0.132        0.174        0.153        0.232      0.00242\n","      7     4       0.0385       0.0385     8.21e-06        0.117        0.152        0.102        0.148        0.125        0.132        0.185        0.159        0.156      0.00162\n","      7     5       0.0376       0.0376     1.46e-05        0.115         0.15        0.101        0.143        0.122        0.132        0.181        0.156        0.234      0.00244\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               7   30.183    0.005       0.0411     0.000122       0.0412        0.121        0.157        0.108        0.147        0.127         0.14        0.186        0.163        0.683      0.00712\n","! Validation          7   30.183    0.005       0.0398     1.06e-05       0.0398        0.119        0.154        0.105        0.147        0.126        0.136        0.185        0.161        0.195      0.00203\n","Wall time: 30.183670796000115\n","! Best model        7    0.040\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      8     1       0.0302       0.0302     4.92e-07        0.104        0.134       0.0922        0.127         0.11        0.119        0.162         0.14       0.0445     0.000464\n","      8     2       0.0361       0.0361     4.16e-05        0.112        0.147        0.101        0.134        0.117        0.133        0.172        0.152        0.462      0.00482\n","      8     3       0.0392       0.0391     5.96e-05        0.119        0.153        0.108        0.141        0.125        0.141        0.175        0.158        0.502      0.00523\n","      8     4       0.0396       0.0395     5.85e-05         0.12        0.154        0.106        0.147        0.127        0.137        0.183         0.16        0.547       0.0057\n","      8     5       0.0413       0.0413     5.72e-05         0.12        0.157        0.105         0.15        0.128        0.137        0.191        0.164        0.498      0.00518\n","      8     6       0.0398       0.0397     4.95e-05        0.117        0.154        0.105        0.142        0.123        0.136        0.185        0.161        0.459      0.00479\n","      8     7       0.0334       0.0333     0.000187         0.11        0.141        0.095        0.139        0.117        0.122        0.174        0.148            1       0.0105\n","      8     8        0.039       0.0389     5.88e-05        0.116        0.153        0.103        0.142        0.122        0.134        0.184        0.159        0.525      0.00546\n","      8     9       0.0291        0.029     7.09e-05        0.103        0.132       0.0926        0.125        0.109        0.117        0.158        0.137        0.622      0.00648\n","      8    10       0.0377       0.0376     8.19e-06        0.116         0.15        0.104        0.141        0.123        0.134        0.178        0.156        0.121      0.00127\n","      8    11       0.0377       0.0375     0.000189        0.116         0.15        0.103        0.144        0.123        0.131        0.181        0.156        0.999       0.0104\n","      8    12       0.0327       0.0324      0.00035        0.108        0.139       0.0938        0.135        0.114         0.12        0.171        0.146         1.38       0.0144\n","      8    13       0.0296       0.0295     0.000109        0.104        0.133       0.0933        0.124        0.109        0.122        0.153        0.137        0.755      0.00786\n","      8    14       0.0333       0.0333     1.16e-05        0.109        0.141       0.0955        0.135        0.115        0.124         0.17        0.147         0.23       0.0024\n","      8    15       0.0361        0.036     0.000134        0.112        0.147       0.0994        0.136        0.118        0.132        0.173        0.152         0.85      0.00886\n","      8    16       0.0339       0.0336      0.00029         0.11        0.142       0.0984        0.134        0.116        0.128        0.166        0.147         1.26       0.0131\n","      8    17       0.0305       0.0305     4.11e-05        0.106        0.135        0.092        0.133        0.113        0.117        0.166        0.141        0.412       0.0043\n","      8    18       0.0375       0.0374     0.000108        0.115         0.15       0.0992        0.146        0.122        0.129        0.184        0.156        0.765      0.00797\n","      8    19       0.0355       0.0354     9.54e-05        0.112        0.146       0.0989        0.139        0.119        0.129        0.175        0.152        0.707      0.00736\n","      8    20       0.0295       0.0295     4.83e-06        0.104        0.133       0.0906        0.131        0.111        0.115        0.162        0.139         0.13      0.00135\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      8     1       0.0394       0.0394     4.67e-06        0.118        0.154        0.103        0.147        0.125        0.134        0.186         0.16        0.135      0.00141\n","      8     2       0.0382       0.0382     8.06e-06        0.117        0.151        0.104        0.144        0.124        0.135         0.18        0.157        0.162      0.00169\n","      8     3       0.0328       0.0327     9.33e-06        0.109         0.14       0.0975        0.132        0.115        0.125        0.166        0.146        0.201       0.0021\n","      8     4       0.0347       0.0347     5.59e-06        0.111        0.144       0.0969         0.14        0.118        0.126        0.175         0.15        0.116      0.00121\n","      8     5        0.034        0.034     1.02e-05         0.11        0.143       0.0963        0.136        0.116        0.126        0.171        0.149        0.187      0.00194\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               8   33.916    0.005        0.035     9.62e-05       0.0351        0.112        0.145       0.0988        0.137        0.118        0.128        0.173        0.151        0.614      0.00639\n","! Validation          8   33.916    0.005       0.0358     7.57e-06       0.0358        0.113        0.146       0.0997         0.14         0.12        0.129        0.176        0.153         0.16      0.00167\n","Wall time: 33.91656781399979\n","! Best model        8    0.036\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      9     1       0.0371        0.037     0.000153        0.113        0.149          0.1        0.139         0.12        0.131        0.179        0.155        0.886      0.00922\n","      9     2       0.0337       0.0332     0.000567        0.109        0.141       0.0963        0.134        0.115        0.123        0.171        0.147         1.76       0.0183\n","      9     3        0.031       0.0309     0.000145        0.106        0.136        0.095        0.129        0.112        0.121        0.162        0.141        0.866      0.00903\n","      9     4        0.034       0.0338     0.000212        0.109        0.142          0.1        0.127        0.114        0.132        0.161        0.146         1.07       0.0112\n","      9     5       0.0257       0.0253     0.000391       0.0966        0.123       0.0859        0.118        0.102        0.111        0.145        0.128         1.46       0.0152\n","      9     6       0.0362       0.0361     0.000175        0.114        0.147        0.103        0.138         0.12        0.132        0.173        0.152        0.956      0.00996\n","      9     7       0.0319       0.0318     9.92e-05        0.107        0.138       0.0944        0.132        0.113        0.122        0.166        0.144        0.732      0.00763\n","      9     8       0.0346       0.0346      4.6e-06        0.112        0.144       0.0976        0.141        0.119        0.126        0.175         0.15        0.133      0.00138\n","      9     9       0.0312        0.031     0.000217        0.104        0.136       0.0928        0.128         0.11        0.121        0.162        0.142         1.08       0.0112\n","      9    10       0.0322       0.0322     1.43e-05        0.108        0.139       0.0937        0.136        0.115        0.119        0.171        0.145        0.242      0.00252\n","      9    11        0.033        0.033     1.38e-05        0.109        0.141        0.097        0.133        0.115        0.123         0.17        0.147        0.253      0.00263\n","      9    12       0.0354       0.0353     4.76e-05        0.113        0.145          0.1        0.138        0.119        0.129        0.174        0.151        0.503      0.00524\n","      9    13       0.0302       0.0299     0.000234        0.103        0.134       0.0908        0.126        0.109        0.117        0.163         0.14         1.12       0.0116\n","      9    14       0.0348       0.0348     2.58e-05        0.113        0.144        0.104        0.131        0.118        0.132        0.166        0.149         0.35      0.00365\n","      9    15       0.0305       0.0305     6.21e-06        0.104        0.135       0.0932        0.127         0.11        0.119        0.162        0.141        0.144       0.0015\n","      9    16       0.0369       0.0369     4.71e-05        0.114        0.149       0.0991        0.143        0.121        0.129        0.181        0.155        0.502      0.00522\n","      9    17       0.0306       0.0302     0.000393        0.105        0.134       0.0943        0.125         0.11        0.121        0.157        0.139         1.47       0.0153\n","      9    18       0.0338       0.0336     0.000177        0.109        0.142       0.0964        0.135        0.116        0.125        0.171        0.148        0.979       0.0102\n","      9    19       0.0325       0.0325     9.86e-06        0.106        0.139       0.0961        0.126        0.111        0.128         0.16        0.144        0.181      0.00188\n","      9    20       0.0244       0.0244     5.28e-05       0.0944        0.121       0.0814        0.121        0.101        0.103         0.15        0.127        0.531      0.00553\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      9     1       0.0357       0.0357     5.26e-06        0.112        0.146       0.0984         0.14        0.119        0.128        0.178        0.153        0.139      0.00145\n","      9     2       0.0344       0.0344     6.35e-06        0.111        0.143       0.0985        0.137        0.118        0.127        0.171        0.149        0.136      0.00142\n","      9     3       0.0296       0.0296     7.23e-06        0.104        0.133       0.0925        0.126        0.109        0.118        0.159        0.139        0.171      0.00178\n","      9     4       0.0313       0.0313      4.6e-06        0.105        0.137       0.0922        0.132        0.112         0.12        0.166        0.143        0.115      0.00119\n","      9     5       0.0308       0.0308     6.52e-06        0.105        0.136       0.0916        0.131        0.111         0.12        0.164        0.142        0.147      0.00154\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               9   37.664    0.005       0.0323     0.000149       0.0325        0.107        0.139       0.0956        0.131        0.113        0.123        0.166        0.145        0.761      0.00793\n","! Validation          9   37.664    0.005       0.0324     5.99e-06       0.0324        0.107        0.139       0.0946        0.133        0.114        0.123        0.168        0.145        0.142      0.00148\n","Wall time: 37.665100072000314\n","! Best model        9    0.032\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     10     1       0.0302       0.0302     1.31e-05        0.104        0.134       0.0914        0.128         0.11        0.121        0.158        0.139         0.24       0.0025\n","     10     2       0.0306       0.0305     2.54e-05        0.105        0.135       0.0935        0.128        0.111        0.121         0.16         0.14        0.333      0.00347\n","     10     3       0.0313       0.0313     1.33e-05        0.104        0.137       0.0909        0.129         0.11         0.12        0.166        0.143        0.215      0.00224\n","     10     4       0.0314       0.0314     9.08e-06        0.105        0.137       0.0924        0.131        0.112        0.121        0.165        0.143        0.192        0.002\n","     10     5        0.029       0.0289     3.62e-05        0.104        0.132       0.0911        0.129         0.11        0.116        0.158        0.137        0.412      0.00429\n","     10     6       0.0286       0.0285     9.94e-05        0.101        0.131       0.0918        0.119        0.105        0.118        0.152        0.135        0.736      0.00767\n","     10     7       0.0252       0.0252     4.42e-06       0.0951        0.123       0.0853        0.115          0.1        0.109        0.146        0.128        0.142      0.00148\n","     10     8       0.0291       0.0291     2.75e-05        0.101        0.132       0.0849        0.133        0.109        0.108         0.17        0.139        0.352      0.00366\n","     10     9       0.0265       0.0264     5.68e-05       0.0961        0.126       0.0839         0.12        0.102        0.109        0.154        0.131        0.516      0.00538\n","     10    10         0.03       0.0299     7.54e-05        0.102        0.134       0.0901        0.127        0.108        0.118         0.16        0.139        0.626      0.00652\n","     10    11       0.0288       0.0288     1.04e-05       0.0998        0.131       0.0868        0.126        0.106        0.114         0.16        0.137        0.228      0.00237\n","     10    12       0.0254       0.0253     2.64e-05       0.0951        0.123        0.083        0.119        0.101        0.106        0.151        0.129        0.346       0.0036\n","     10    13       0.0268       0.0268      2.6e-05       0.0985        0.127       0.0874        0.121        0.104        0.112        0.152        0.132        0.367      0.00382\n","     10    14        0.029        0.029     7.46e-06        0.102        0.132       0.0883        0.129        0.109        0.113        0.162        0.138        0.158      0.00165\n","     10    15       0.0252       0.0252     1.86e-05       0.0963        0.123       0.0824        0.124        0.103        0.104        0.153        0.129        0.298       0.0031\n","     10    16       0.0324       0.0324     2.86e-05        0.107        0.139       0.0925        0.135        0.114        0.121        0.169        0.145         0.38      0.00396\n","     10    17        0.024        0.024     1.87e-05       0.0931         0.12       0.0817        0.116       0.0989        0.103        0.148        0.125        0.271      0.00282\n","     10    18       0.0243       0.0241     0.000286       0.0951         0.12       0.0873        0.111       0.0991        0.111        0.136        0.123         1.25       0.0131\n","     10    19       0.0259       0.0257     0.000132       0.0952        0.124       0.0858        0.114          0.1         0.11        0.149        0.129         0.85      0.00885\n","     10    20       0.0258       0.0258     1.11e-05       0.0951        0.124        0.084        0.117        0.101        0.109        0.151         0.13        0.205      0.00213\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     10     1       0.0324       0.0324     4.92e-06        0.107        0.139       0.0934        0.133        0.113        0.121         0.17        0.145        0.132      0.00138\n","     10     2       0.0309       0.0309     5.37e-06        0.105        0.136       0.0928        0.131        0.112         0.12        0.164        0.142        0.136      0.00141\n","     10     3       0.0268       0.0268     6.78e-06       0.0985        0.127       0.0873        0.121        0.104        0.112        0.153        0.132        0.164      0.00171\n","     10     4       0.0283       0.0283     3.85e-06          0.1         0.13       0.0876        0.126        0.107        0.114        0.158        0.136        0.106       0.0011\n","     10     5       0.0279       0.0279      5.6e-06       0.0996        0.129       0.0869        0.125        0.106        0.113        0.156        0.135        0.138      0.00144\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              10   41.404    0.005       0.0279     4.63e-05        0.028       0.0997        0.129       0.0877        0.124        0.106        0.113        0.156        0.135        0.406      0.00423\n","! Validation         10   41.404    0.005       0.0293      5.3e-06       0.0293        0.102        0.132       0.0896        0.127        0.108        0.116         0.16        0.138        0.135      0.00141\n","Wall time: 41.40420898299999\n","! Best model       10    0.029\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     11     1       0.0268       0.0267     0.000115       0.0954        0.126       0.0817        0.123        0.102        0.107        0.159        0.133        0.777      0.00809\n","     11     2       0.0264       0.0263     0.000102       0.0978        0.126       0.0852        0.123        0.104        0.108        0.155        0.131        0.719      0.00748\n","     11     3       0.0248       0.0248     1.54e-05       0.0943        0.122       0.0825        0.118          0.1        0.106        0.148        0.127        0.238      0.00248\n","     11     4       0.0266       0.0266      1.3e-05       0.0978        0.126        0.088        0.117        0.103        0.113        0.149        0.131        0.257      0.00268\n","     11     5       0.0234       0.0233     5.23e-06       0.0917        0.118       0.0811        0.113        0.097        0.105        0.142        0.123        0.139      0.00145\n","     11     6       0.0257       0.0256     0.000103       0.0959        0.124       0.0832        0.122        0.102        0.107        0.152         0.13        0.747      0.00778\n","     11     7       0.0256       0.0255     0.000119       0.0962        0.124        0.085        0.119        0.102         0.11        0.148        0.129        0.798      0.00832\n","     11     8        0.022        0.022     3.09e-05        0.087        0.115       0.0766        0.108       0.0922          0.1         0.14         0.12          0.4      0.00417\n","     11     9       0.0242       0.0238     0.000418       0.0919        0.119       0.0799        0.116       0.0978        0.103        0.147        0.125         1.51       0.0158\n","     11    10       0.0251       0.0246     0.000468       0.0928        0.121       0.0809        0.117       0.0988        0.106        0.147        0.127          1.6       0.0167\n","     11    11        0.023       0.0229     0.000151       0.0916        0.117       0.0815        0.112       0.0967        0.104        0.139        0.122        0.906      0.00944\n","     11    12       0.0225       0.0223     0.000167       0.0897        0.116       0.0778        0.113       0.0956       0.0994        0.142        0.121        0.941       0.0098\n","     11    13       0.0255       0.0249     0.000655       0.0953        0.122       0.0833        0.119        0.101        0.107        0.147        0.127          1.9       0.0198\n","     11    14       0.0276       0.0274     0.000138       0.0978        0.128       0.0846        0.124        0.104        0.111        0.156        0.134        0.857      0.00893\n","     11    15       0.0231       0.0231     2.26e-05       0.0898        0.118       0.0807        0.108       0.0943        0.105        0.139        0.122        0.348      0.00363\n","     11    16       0.0278       0.0277     6.45e-05       0.0972        0.129       0.0856         0.12        0.103        0.113        0.155        0.134        0.584      0.00609\n","     11    17       0.0278       0.0277     8.93e-05       0.0969        0.129       0.0849        0.121        0.103        0.113        0.156        0.134        0.692      0.00721\n","     11    18       0.0268       0.0266     0.000122       0.0966        0.126       0.0831        0.124        0.103        0.109        0.156        0.132        0.813      0.00847\n","     11    19       0.0237       0.0235     0.000167       0.0916        0.119       0.0794        0.116       0.0977        0.101        0.147        0.124        0.951       0.0099\n","     11    20       0.0211       0.0211     1.01e-05       0.0868        0.112       0.0747        0.111       0.0928       0.0966        0.139        0.118        0.159      0.00165\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     11     1       0.0292       0.0292        6e-06        0.101        0.132       0.0883        0.127        0.108        0.115        0.162        0.138        0.139      0.00145\n","     11     2       0.0278       0.0278     5.08e-06       0.0997        0.129       0.0872        0.125        0.106        0.113        0.156        0.135        0.146      0.00152\n","     11     3       0.0242       0.0242     6.16e-06       0.0935         0.12       0.0822        0.116       0.0991        0.105        0.146        0.126        0.157      0.00164\n","     11     4       0.0255       0.0255     4.21e-06        0.095        0.124       0.0829        0.119        0.101        0.108         0.15        0.129        0.125       0.0013\n","     11     5       0.0253       0.0253      4.1e-06       0.0949        0.123       0.0824         0.12        0.101        0.107         0.15        0.129        0.122      0.00127\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              11   45.141    0.005       0.0248     0.000149        0.025       0.0937        0.122        0.082        0.117       0.0996        0.106        0.148        0.127        0.767      0.00799\n","! Validation         11   45.141    0.005       0.0264     5.11e-06       0.0264       0.0968        0.126       0.0846        0.121        0.103         0.11        0.153        0.131        0.138      0.00144\n","Wall time: 45.14188299000034\n","! Best model       11    0.026\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     12     1       0.0248       0.0245     0.000266       0.0951        0.121       0.0815        0.122        0.102        0.103        0.151        0.127         1.21       0.0126\n","     12     2       0.0345       0.0339     0.000634        0.112        0.142        0.102        0.131        0.116         0.13        0.164        0.147         1.86       0.0194\n","     12     3       0.0285       0.0285     2.93e-05        0.101        0.131       0.0907        0.123        0.107        0.117        0.154        0.136        0.389      0.00405\n","     12     4         0.02       0.0199     0.000129        0.085        0.109       0.0748        0.105       0.0901       0.0949        0.133        0.114        0.816       0.0085\n","     12     5       0.0297       0.0293      0.00041        0.102        0.132       0.0882        0.129        0.108        0.113        0.164        0.139          1.5       0.0156\n","     12     6       0.0348       0.0344     0.000388        0.112        0.143        0.105        0.127        0.116        0.132        0.163        0.148         1.46       0.0152\n","     12     7       0.0294       0.0294     4.56e-05        0.104        0.133       0.0935        0.124        0.109        0.119        0.157        0.138        0.465      0.00484\n","     12     8       0.0224       0.0221     0.000211       0.0895        0.115       0.0782        0.112       0.0952       0.0995        0.141         0.12         1.08       0.0112\n","     12     9       0.0334       0.0334     9.15e-06        0.111        0.141        0.105        0.123        0.114        0.135        0.154        0.144         0.18      0.00187\n","     12    10       0.0279       0.0278     3.69e-05          0.1        0.129       0.0855        0.129        0.107        0.109        0.162        0.135        0.439      0.00457\n","     12    11       0.0208       0.0207      0.00012       0.0856        0.111       0.0749        0.107        0.091       0.0957        0.137        0.116        0.809      0.00842\n","     12    12        0.028        0.028     3.35e-05        0.102        0.129       0.0881        0.129        0.108        0.112        0.158        0.135        0.351      0.00366\n","     12    13       0.0273       0.0272     0.000118       0.0967        0.128       0.0826        0.125        0.104        0.108         0.16        0.134        0.772      0.00804\n","     12    14       0.0215       0.0213     0.000152       0.0857        0.113       0.0755        0.106       0.0908       0.0991        0.136        0.118        0.905      0.00942\n","     12    15       0.0217       0.0217     7.97e-05       0.0884        0.114       0.0764        0.112       0.0944       0.0988        0.139        0.119        0.656      0.00684\n","     12    16       0.0207       0.0207     8.27e-06       0.0866        0.111       0.0786        0.103       0.0906        0.101        0.129        0.115        0.209      0.00218\n","     12    17       0.0226       0.0226     7.77e-05       0.0893        0.116       0.0774        0.113       0.0952          0.1        0.143        0.122        0.646      0.00673\n","     12    18       0.0249       0.0247     0.000177       0.0925        0.122       0.0811        0.115       0.0983        0.109        0.144        0.126        0.988       0.0103\n","     12    19       0.0213       0.0213     1.08e-05       0.0873        0.113       0.0753        0.111       0.0933       0.0976        0.139        0.118        0.207      0.00215\n","     12    20       0.0239       0.0239     4.01e-05       0.0911        0.119       0.0802        0.113       0.0965        0.105        0.144        0.125        0.459      0.00478\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     12     1       0.0266       0.0266     7.63e-06       0.0964        0.126       0.0837        0.122        0.103        0.109        0.155        0.132        0.163       0.0017\n","     12     2       0.0254       0.0254     5.45e-06       0.0947        0.123       0.0822         0.12        0.101        0.107        0.151        0.129        0.157      0.00164\n","     12     3       0.0222       0.0221     6.37e-06       0.0891        0.115       0.0777        0.112       0.0948       0.0997        0.141         0.12        0.154      0.00161\n","     12     4       0.0234       0.0234     5.75e-06       0.0908        0.118       0.0789        0.115       0.0967        0.103        0.144        0.123        0.146      0.00152\n","     12     5       0.0231       0.0231     3.56e-06       0.0908        0.118       0.0784        0.116        0.097        0.102        0.145        0.123        0.116      0.00121\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              12   48.872    0.005       0.0258     0.000149       0.0259       0.0958        0.124       0.0847        0.118        0.101         0.11        0.149        0.129        0.769      0.00801\n","! Validation         12   48.872    0.005       0.0241     5.75e-06       0.0241       0.0923         0.12       0.0802        0.117       0.0984        0.104        0.147        0.126        0.147      0.00154\n","Wall time: 48.87277193099999\n","! Best model       12    0.024\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     13     1        0.024        0.024     1.04e-06        0.092         0.12       0.0791        0.118       0.0985        0.104        0.147        0.125       0.0662      0.00069\n","     13     2       0.0206       0.0205     6.28e-05       0.0865        0.111       0.0742        0.111       0.0926       0.0958        0.136        0.116        0.583      0.00608\n","     13     3       0.0239       0.0239     4.24e-05       0.0912         0.12       0.0767         0.12       0.0984        0.101         0.15        0.125        0.456      0.00475\n","     13     4       0.0197       0.0197     8.27e-06       0.0849        0.109       0.0745        0.106       0.0901       0.0943        0.133        0.113        0.199      0.00207\n","     13     5         0.02         0.02     4.94e-05       0.0848        0.109       0.0731        0.108       0.0907        0.093        0.136        0.115        0.513      0.00534\n","     13     6       0.0272        0.027     0.000157       0.0999        0.127        0.092        0.116        0.104        0.117        0.146        0.131        0.921      0.00959\n","     13     7       0.0232       0.0231     5.45e-05       0.0926        0.118       0.0795        0.119       0.0991          0.1        0.146        0.123        0.542      0.00565\n","     13     8       0.0202       0.0202     1.03e-05       0.0837         0.11       0.0731        0.105        0.089       0.0962        0.133        0.115         0.22      0.00229\n","     13     9       0.0249       0.0249     4.61e-05       0.0938        0.122       0.0804         0.12          0.1        0.105        0.151        0.128        0.484      0.00504\n","     13    10       0.0209       0.0207     0.000104       0.0849        0.111       0.0764        0.102       0.0891       0.0999        0.132        0.116        0.746      0.00777\n","     13    11       0.0234       0.0234      1.8e-05       0.0916        0.118       0.0793        0.116       0.0978        0.103        0.145        0.124        0.251      0.00261\n","     13    12       0.0252       0.0251     7.74e-05       0.0957        0.123       0.0856        0.116        0.101         0.11        0.145        0.127        0.651      0.00679\n","     13    13       0.0207       0.0206     5.45e-05       0.0855        0.111       0.0741        0.108       0.0913       0.0951        0.138        0.116        0.541      0.00563\n","     13    14       0.0247       0.0247     2.33e-06        0.092        0.122       0.0785        0.119       0.0988        0.106        0.149        0.127         0.11      0.00115\n","     13    15       0.0221       0.0221     6.22e-06       0.0876        0.115       0.0764         0.11       0.0932       0.0989        0.142         0.12        0.179      0.00187\n","     13    16       0.0191        0.019      2.3e-05       0.0827        0.107       0.0717        0.105       0.0882       0.0921        0.131        0.112         0.29      0.00302\n","     13    17       0.0219       0.0219     1.22e-05        0.087        0.114       0.0743        0.113       0.0934        0.098        0.142         0.12        0.229      0.00238\n","     13    18       0.0218       0.0217     5.08e-05       0.0864        0.114       0.0767        0.106       0.0913       0.0992        0.139        0.119        0.503      0.00523\n","     13    19       0.0203       0.0203     1.21e-05       0.0854         0.11       0.0743        0.108       0.0909       0.0951        0.135        0.115        0.228      0.00237\n","     13    20       0.0231        0.023     9.24e-05       0.0905        0.117       0.0792        0.113       0.0961        0.103        0.141        0.122        0.709      0.00739\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     13     1       0.0245       0.0244     9.96e-06       0.0923        0.121       0.0798        0.117       0.0986        0.104         0.15        0.127         0.19      0.00198\n","     13     2       0.0236       0.0236     6.26e-06       0.0908        0.119       0.0781        0.116       0.0971        0.102        0.146        0.124        0.166      0.00173\n","     13     3       0.0205       0.0205     7.23e-06       0.0855        0.111       0.0739        0.109       0.0913       0.0952        0.137        0.116        0.161      0.00167\n","     13     4       0.0216       0.0216     8.03e-06       0.0872        0.114       0.0756         0.11        0.093       0.0987        0.139        0.119        0.173      0.00181\n","     13     5       0.0214       0.0214     4.21e-06       0.0872        0.113       0.0748        0.112       0.0934       0.0967         0.14        0.118        0.123      0.00128\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              13   52.613    0.005       0.0223     4.43e-05       0.0223       0.0889        0.116       0.0775        0.112       0.0947          0.1        0.141        0.121        0.421      0.00439\n","! Validation         13   52.613    0.005       0.0223     7.14e-06       0.0223       0.0886        0.115       0.0764        0.113       0.0947       0.0993        0.142        0.121        0.163      0.00169\n","Wall time: 52.61340102400027\n","! Best model       13    0.022\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     14     1       0.0214       0.0213     8.33e-05       0.0866        0.113        0.075         0.11       0.0924       0.0962        0.141        0.118        0.674      0.00702\n","     14     2       0.0196       0.0195     7.53e-05        0.084        0.108       0.0742        0.103       0.0889       0.0945        0.131        0.113        0.616      0.00642\n","     14     3       0.0184       0.0184     2.44e-06       0.0816        0.105       0.0706        0.104       0.0871       0.0894        0.131         0.11         0.11      0.00114\n","     14     4       0.0242        0.024     0.000163       0.0931         0.12       0.0812        0.117        0.099        0.105        0.146        0.125         0.93      0.00969\n","     14     5       0.0218       0.0216     0.000201       0.0887        0.114       0.0749        0.116       0.0956       0.0963        0.143        0.119         1.03       0.0107\n","     14     6       0.0209       0.0209     6.58e-05       0.0876        0.112       0.0794        0.104       0.0917        0.101        0.131        0.116        0.588      0.00612\n","     14     7       0.0187       0.0187     5.12e-06       0.0814        0.106       0.0689        0.106       0.0876       0.0886        0.134        0.111        0.139      0.00145\n","     14     8       0.0218       0.0217     5.54e-05       0.0878        0.114       0.0782        0.107       0.0926        0.101        0.136        0.119        0.536      0.00558\n","     14     9       0.0253       0.0253     8.44e-05       0.0938        0.123       0.0795        0.122        0.101        0.101        0.158         0.13        0.663       0.0069\n","     14    10       0.0209       0.0208     5.41e-05       0.0839        0.112       0.0725        0.107       0.0896       0.0975        0.135        0.116        0.542      0.00564\n","     14    11       0.0189       0.0189     3.22e-05       0.0817        0.106       0.0696        0.106       0.0878       0.0893        0.134        0.112        0.403       0.0042\n","     14    12       0.0175       0.0175      2.5e-06       0.0795        0.102       0.0693          0.1       0.0847       0.0894        0.124        0.107        0.116      0.00121\n","     14    13       0.0165       0.0164     5.68e-05       0.0758       0.0991        0.065       0.0976       0.0813       0.0852        0.122        0.104        0.554      0.00577\n","     14    14       0.0223       0.0222     3.66e-05       0.0861        0.115       0.0727        0.113       0.0928       0.0981        0.144        0.121        0.444      0.00462\n","     14    15       0.0201       0.0201     1.81e-06       0.0834         0.11       0.0719        0.107       0.0892        0.094        0.136        0.115       0.0924     0.000962\n","     14    16       0.0177       0.0177     1.34e-05       0.0791        0.103       0.0692       0.0988        0.084         0.09        0.125        0.107        0.243      0.00253\n","     14    17       0.0212       0.0212     3.13e-06       0.0867        0.113       0.0738        0.112       0.0931       0.0956        0.141        0.118        0.111      0.00116\n","     14    18       0.0197       0.0196     6.55e-06       0.0837        0.108       0.0729        0.105       0.0891       0.0937        0.133        0.113        0.172      0.00179\n","     14    19       0.0196       0.0195     3.88e-05       0.0821        0.108       0.0703        0.106        0.088       0.0919        0.135        0.113        0.456      0.00475\n","     14    20       0.0169       0.0169     1.91e-05       0.0785        0.101       0.0675          0.1        0.084        0.085        0.126        0.105        0.308      0.00321\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     14     1       0.0228       0.0227     8.55e-06       0.0889        0.117       0.0764        0.114       0.0952       0.0992        0.145        0.122        0.177      0.00184\n","     14     2        0.022        0.022     5.16e-06       0.0875        0.115       0.0747        0.113       0.0939       0.0981        0.142         0.12        0.157      0.00164\n","     14     3       0.0191       0.0191     6.01e-06       0.0825        0.107       0.0708        0.106       0.0884       0.0911        0.133        0.112        0.148      0.00154\n","     14     4       0.0201       0.0201     6.59e-06        0.084         0.11       0.0725        0.107       0.0897       0.0948        0.135        0.115         0.15      0.00156\n","     14     5         0.02         0.02     3.45e-06       0.0842        0.109       0.0717        0.109       0.0904       0.0925        0.137        0.115         0.11      0.00115\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              14   56.360    0.005       0.0201     5.01e-05       0.0202       0.0843         0.11       0.0728        0.107         0.09       0.0943        0.135        0.115        0.436      0.00454\n","! Validation         14   56.360    0.005       0.0208     5.95e-06       0.0208       0.0854        0.112       0.0732         0.11       0.0915       0.0952        0.139        0.117        0.148      0.00154\n","Wall time: 56.36067863900007\n","! Best model       14    0.021\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     15     1       0.0193       0.0193     1.54e-05        0.082        0.108       0.0702        0.106       0.0879       0.0922        0.133        0.113        0.274      0.00286\n","     15     2       0.0182       0.0181     0.000114       0.0801        0.104       0.0689        0.103       0.0857        0.089        0.129        0.109        0.777      0.00809\n","     15     3       0.0174       0.0173     2.25e-05       0.0786        0.102       0.0669        0.102       0.0844       0.0854        0.129        0.107        0.331      0.00345\n","     15     4       0.0201       0.0201     9.22e-06       0.0846         0.11       0.0739        0.106       0.0899       0.0949        0.135        0.115        0.191      0.00199\n","     15     5       0.0197       0.0197     2.21e-05       0.0834        0.109       0.0713        0.108       0.0895       0.0921        0.136        0.114        0.324      0.00338\n","     15     6       0.0182       0.0182     1.59e-06       0.0817        0.104       0.0721        0.101       0.0865       0.0913        0.126        0.109       0.0822     0.000857\n","     15     7       0.0202       0.0202     3.27e-06       0.0837         0.11       0.0721        0.107       0.0895       0.0938        0.136        0.115        0.114      0.00119\n","     15     8       0.0194       0.0194     2.68e-06        0.083        0.108       0.0713        0.106       0.0888       0.0914        0.134        0.113        0.118      0.00123\n","     15     9       0.0215       0.0215     1.39e-05       0.0879        0.113       0.0776        0.109        0.093        0.101        0.135        0.118        0.241      0.00251\n","     15    10       0.0215       0.0214     0.000101       0.0867        0.113       0.0773        0.105       0.0913          0.1        0.136        0.118        0.739       0.0077\n","     15    11       0.0197       0.0197     2.49e-05       0.0848        0.109       0.0731        0.108       0.0906       0.0923        0.135        0.114        0.329      0.00343\n","     15    12       0.0221        0.022     6.33e-05        0.088        0.115       0.0771         0.11       0.0934          0.1         0.14         0.12        0.583      0.00608\n","     15    13       0.0268       0.0267     0.000138        0.098        0.126        0.088        0.118        0.103        0.112         0.15        0.131        0.867      0.00903\n","     15    14       0.0189       0.0188     0.000149       0.0809        0.106       0.0705        0.102       0.0861       0.0904        0.132        0.111        0.904      0.00941\n","     15    15        0.021        0.021     1.27e-06       0.0854        0.112       0.0727        0.111       0.0918       0.0966        0.138        0.117       0.0709     0.000739\n","     15    16       0.0218       0.0217     0.000128       0.0904        0.114       0.0812        0.109        0.095        0.101        0.135        0.118         0.82      0.00854\n","     15    17       0.0168       0.0168     8.31e-05       0.0774          0.1       0.0659          0.1       0.0832       0.0854        0.124        0.105         0.67      0.00698\n","     15    18       0.0222       0.0222     8.79e-07       0.0867        0.115       0.0738        0.113       0.0932       0.0981        0.143        0.121         0.05     0.000521\n","     15    19        0.019        0.019     1.33e-05       0.0833        0.107       0.0748          0.1       0.0875       0.0951        0.126        0.111        0.248      0.00259\n","     15    20       0.0198       0.0198     2.19e-05       0.0816        0.109       0.0675         0.11       0.0887       0.0884        0.141        0.115        0.315      0.00328\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     15     1       0.0213       0.0213     8.34e-06        0.086        0.113       0.0735        0.111       0.0922       0.0954        0.142        0.119        0.176      0.00184\n","     15     2       0.0207       0.0207     4.99e-06       0.0846        0.111       0.0717         0.11        0.091       0.0945        0.139        0.117        0.155      0.00162\n","     15     3       0.0179       0.0179      5.7e-06       0.0799        0.104        0.068        0.104       0.0858       0.0876         0.13        0.109        0.145      0.00151\n","     15     4        0.019        0.019     6.68e-06       0.0814        0.107         0.07        0.104       0.0871       0.0916        0.132        0.112        0.151      0.00157\n","     15     5       0.0188       0.0188     3.42e-06       0.0815        0.106        0.069        0.107       0.0878       0.0889        0.134        0.111        0.109      0.00114\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              15   60.117    0.005       0.0201     4.64e-05       0.0202       0.0844         0.11       0.0733        0.107         0.09       0.0948        0.135        0.115        0.402      0.00419\n","! Validation         15   60.117    0.005       0.0195     5.82e-06       0.0195       0.0827        0.108       0.0704        0.107       0.0888       0.0916        0.135        0.113        0.148      0.00154\n","Wall time: 60.117828788000224\n","! Best model       15    0.020\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     16     1       0.0185       0.0185     4.48e-06        0.083        0.105        0.074        0.101       0.0874       0.0939        0.125        0.109        0.153      0.00159\n","     16     2       0.0212       0.0212      5.7e-05       0.0863        0.113       0.0728        0.113       0.0931        0.094        0.143        0.118        0.547       0.0057\n","     16     3       0.0212       0.0211     3.26e-06       0.0849        0.113       0.0702        0.114       0.0923       0.0934        0.143        0.118        0.108      0.00113\n","     16     4       0.0186       0.0186      3.2e-05       0.0813        0.105       0.0684        0.107       0.0877       0.0872        0.135        0.111        0.414      0.00431\n","     16     5       0.0181        0.018     3.59e-05       0.0796        0.104       0.0694          0.1       0.0847       0.0891        0.128        0.109        0.427      0.00445\n","     16     6       0.0177       0.0177     2.34e-05       0.0789        0.103       0.0677        0.101       0.0845       0.0871        0.129        0.108        0.325      0.00339\n","     16     7       0.0183       0.0183     2.59e-05       0.0811        0.105       0.0719       0.0994       0.0857        0.093        0.124        0.109        0.371      0.00387\n","     16     8       0.0207       0.0206     0.000102       0.0858        0.111        0.074        0.109       0.0917       0.0942        0.138        0.116        0.746      0.00778\n","     16     9       0.0208       0.0208     1.92e-05       0.0837        0.112        0.071        0.109       0.0901       0.0956        0.138        0.117        0.272      0.00284\n","     16    10       0.0175       0.0174     9.45e-06       0.0774        0.102        0.065        0.102       0.0836       0.0853         0.13        0.107        0.212      0.00221\n","     16    11        0.024       0.0239        9e-05       0.0916         0.12       0.0778        0.119       0.0985       0.0999        0.152        0.126        0.664      0.00692\n","     16    12       0.0174       0.0174     5.44e-05       0.0782        0.102       0.0691       0.0964       0.0827       0.0896        0.123        0.106        0.531      0.00553\n","     16    13        0.017        0.017     1.35e-06       0.0768        0.101       0.0649          0.1       0.0827        0.086        0.126        0.106       0.0701      0.00073\n","     16    14       0.0182       0.0182     4.41e-06        0.079        0.104       0.0675        0.102       0.0848       0.0882        0.131        0.109        0.133      0.00138\n","     16    15       0.0154       0.0154     1.99e-06       0.0738       0.0959       0.0622       0.0971       0.0796       0.0812         0.12        0.101       0.0971      0.00101\n","     16    16       0.0166       0.0166     1.71e-06       0.0768       0.0995        0.066       0.0983       0.0822       0.0856        0.123        0.104       0.0742     0.000773\n","     16    17       0.0176       0.0176     4.02e-06       0.0787        0.103       0.0653        0.105       0.0853       0.0851        0.131        0.108        0.135      0.00141\n","     16    18       0.0157       0.0157     4.75e-06       0.0741       0.0969       0.0622       0.0978         0.08        0.081        0.123        0.102        0.124      0.00129\n","     16    19       0.0139       0.0138     3.53e-05       0.0707        0.091       0.0611       0.0899       0.0755       0.0765        0.115       0.0956        0.433      0.00451\n","     16    20       0.0165       0.0165     7.21e-06       0.0759       0.0994       0.0648       0.0981       0.0815       0.0838        0.125        0.104        0.179      0.00187\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     16     1       0.0202       0.0202     7.03e-06       0.0836         0.11        0.071        0.109       0.0898       0.0923        0.138        0.115         0.16      0.00167\n","     16     2       0.0196       0.0196      4.1e-06       0.0821        0.108       0.0691        0.108       0.0886       0.0913        0.136        0.114        0.142      0.00148\n","     16     3        0.017        0.017     4.84e-06       0.0776        0.101       0.0658        0.101       0.0835       0.0848        0.127        0.106        0.135      0.00141\n","     16     4        0.018        0.018     5.28e-06       0.0793        0.104        0.068        0.102       0.0849       0.0888        0.129        0.109        0.138      0.00144\n","     16     5       0.0178       0.0178     2.85e-06        0.079        0.103       0.0665        0.104       0.0853       0.0858        0.131        0.108        0.106       0.0011\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              16   63.861    0.005       0.0182     2.59e-05       0.0182       0.0799        0.104       0.0683        0.103       0.0857       0.0887         0.13        0.109        0.301      0.00313\n","! Validation         16   63.861    0.005       0.0185     4.82e-06       0.0185       0.0803        0.105       0.0681        0.105       0.0864       0.0887        0.132         0.11        0.136      0.00142\n","Wall time: 63.862243307000426\n","! Best model       16    0.019\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     17     1       0.0154       0.0154     4.33e-06       0.0743       0.0961       0.0623       0.0984       0.0804       0.0796        0.123        0.101        0.122      0.00127\n","     17     2       0.0151        0.015     2.18e-05       0.0734       0.0949       0.0626       0.0949       0.0788       0.0797         0.12       0.0997        0.331      0.00344\n","     17     3       0.0167       0.0167        6e-05       0.0775       0.0999       0.0662          0.1       0.0831       0.0844        0.125        0.105        0.563      0.00587\n","     17     4       0.0188       0.0187     3.31e-05       0.0802        0.106       0.0685        0.103        0.086       0.0906        0.131        0.111        0.383      0.00399\n","     17     5       0.0159       0.0159     9.71e-06       0.0742       0.0975       0.0642       0.0943       0.0793       0.0843         0.12        0.102        0.219      0.00228\n","     17     6       0.0159       0.0158     5.56e-05        0.074       0.0972       0.0618       0.0984       0.0801       0.0795        0.125        0.102        0.523      0.00545\n","     17     7       0.0164       0.0163      6.2e-05       0.0761       0.0988       0.0635        0.101       0.0825       0.0818        0.126        0.104        0.575      0.00599\n","     17     8       0.0192       0.0192     5.66e-06       0.0808        0.107       0.0676        0.107       0.0874       0.0895        0.136        0.113        0.143      0.00149\n","     17     9       0.0151        0.015     0.000129       0.0719       0.0947       0.0601       0.0957       0.0779       0.0797        0.119       0.0994        0.842      0.00877\n","     17    10       0.0157       0.0155     0.000231       0.0748       0.0962       0.0639       0.0967       0.0803       0.0815         0.12        0.101         1.13       0.0117\n","     17    11       0.0183       0.0182     1.24e-05        0.078        0.104       0.0643        0.105       0.0848       0.0852        0.135         0.11        0.205      0.00213\n","     17    12       0.0164       0.0161      0.00022       0.0763       0.0983       0.0645       0.0998       0.0822       0.0821        0.125        0.103         1.09       0.0113\n","     17    13       0.0164       0.0161     0.000381        0.074        0.098       0.0636       0.0949       0.0792       0.0838        0.122        0.103         1.44        0.015\n","     17    14       0.0153       0.0153     1.48e-06       0.0745       0.0958       0.0636       0.0961       0.0799       0.0798        0.122        0.101       0.0805     0.000838\n","     17    15       0.0178       0.0173     0.000497       0.0774        0.102       0.0671       0.0981       0.0826       0.0878        0.125        0.106         1.65       0.0172\n","     17    16       0.0154       0.0151     0.000377       0.0733        0.095       0.0599          0.1       0.0801       0.0755        0.125          0.1         1.43       0.0149\n","     17    17       0.0163       0.0163      6.4e-06       0.0756       0.0988       0.0647       0.0974        0.081       0.0833        0.124        0.104         0.16      0.00167\n","     17    18       0.0174        0.017     0.000414       0.0766        0.101       0.0652       0.0996       0.0824       0.0856        0.126        0.106         1.51       0.0157\n","     17    19       0.0187       0.0184     0.000326       0.0799        0.105       0.0667        0.106       0.0865       0.0887        0.132         0.11         1.34        0.014\n","     17    20       0.0154       0.0154     7.52e-06       0.0738       0.0961       0.0627        0.096       0.0793       0.0808        0.121        0.101         0.18      0.00187\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     17     1       0.0191       0.0191      7.2e-06       0.0813        0.107       0.0687        0.106       0.0875       0.0895        0.135        0.112        0.165      0.00172\n","     17     2       0.0185       0.0185     3.95e-06       0.0797        0.105       0.0667        0.106       0.0862       0.0883        0.133        0.111        0.137      0.00143\n","     17     3       0.0161       0.0161     4.89e-06       0.0754       0.0982       0.0635       0.0992       0.0814        0.082        0.124        0.103        0.138      0.00143\n","     17     4       0.0171       0.0171     5.55e-06       0.0772        0.101        0.066       0.0996       0.0828       0.0861        0.126        0.106        0.141      0.00147\n","     17     5       0.0169       0.0169     2.91e-06       0.0769          0.1       0.0643        0.102       0.0831       0.0831        0.128        0.106       0.0997      0.00104\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              17   67.625    0.005       0.0164     0.000143       0.0166       0.0758       0.0992       0.0641       0.0992       0.0817       0.0832        0.125        0.104        0.696      0.00725\n","! Validation         17   67.625    0.005       0.0176      4.9e-06       0.0176       0.0781        0.102       0.0658        0.103       0.0842       0.0858         0.13        0.108        0.136      0.00142\n","Wall time: 67.62575232899962\n","! Best model       17    0.018\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     18     1       0.0173        0.017      0.00027       0.0756        0.101       0.0662       0.0945       0.0804       0.0879        0.123        0.105         1.22       0.0127\n","     18     2       0.0149       0.0148     0.000147       0.0734        0.094       0.0622       0.0958        0.079       0.0782         0.12       0.0989        0.898      0.00935\n","     18     3       0.0187       0.0187     3.43e-06       0.0803        0.106       0.0656         0.11       0.0876       0.0865        0.136        0.111       0.0984      0.00103\n","     18     4       0.0169       0.0168      0.00015       0.0765          0.1       0.0633        0.103       0.0831       0.0816         0.13        0.106        0.887      0.00924\n","     18     5       0.0156       0.0155     0.000134       0.0723       0.0963       0.0595       0.0979       0.0787       0.0795        0.123        0.101        0.854      0.00889\n","     18     6       0.0149       0.0149     7.09e-06       0.0714       0.0945       0.0602       0.0938        0.077       0.0792        0.119       0.0993        0.149      0.00156\n","     18     7        0.016       0.0159     0.000112       0.0743       0.0976       0.0615          0.1       0.0808       0.0804        0.125        0.103         0.77      0.00802\n","     18     8       0.0166       0.0165      2.7e-05       0.0772       0.0995       0.0666       0.0984       0.0825       0.0847        0.124        0.104        0.379      0.00395\n","     18     9       0.0158       0.0158     2.06e-06       0.0755       0.0972       0.0654       0.0958       0.0806        0.083        0.121        0.102       0.0883      0.00092\n","     18    10       0.0169       0.0169     2.64e-05       0.0772          0.1       0.0662       0.0993       0.0828       0.0864        0.124        0.105        0.364      0.00379\n","     18    11        0.015        0.015     1.14e-06       0.0738       0.0947       0.0648       0.0917       0.0782       0.0822        0.116       0.0989       0.0676     0.000704\n","     18    12       0.0147       0.0147     2.76e-06       0.0726       0.0938       0.0612       0.0953       0.0783        0.077        0.121       0.0988        0.109      0.00114\n","     18    13        0.016        0.016     9.85e-06       0.0762       0.0978       0.0661       0.0965       0.0813       0.0855        0.119        0.102        0.222      0.00231\n","     18    14        0.017       0.0169     5.34e-06       0.0784        0.101       0.0681       0.0992       0.0836       0.0856        0.126        0.106        0.138      0.00144\n","     18    15       0.0178       0.0178     6.33e-06       0.0794        0.103       0.0664        0.105       0.0858       0.0845        0.133        0.109        0.178      0.00185\n","     18    16       0.0154       0.0153     3.97e-05       0.0747       0.0958       0.0655       0.0931       0.0793       0.0829        0.117          0.1        0.459      0.00478\n","     18    17       0.0144       0.0143     6.05e-05       0.0704       0.0926       0.0591        0.093        0.076       0.0766        0.118       0.0974        0.563      0.00587\n","     18    18       0.0152       0.0152     6.18e-06       0.0725       0.0955       0.0635       0.0906        0.077       0.0829        0.117       0.0997        0.152      0.00158\n","     18    19        0.019        0.019     8.55e-06       0.0824        0.107       0.0691        0.109       0.0891        0.088        0.136        0.112        0.199      0.00208\n","     18    20       0.0177       0.0176     3.07e-05        0.079        0.103        0.069       0.0991        0.084       0.0899        0.125        0.107        0.409      0.00426\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     18     1       0.0181       0.0181     7.04e-06       0.0791        0.104       0.0665        0.104       0.0853       0.0867        0.132         0.11        0.164      0.00171\n","     18     2       0.0176       0.0176     3.63e-06       0.0776        0.103       0.0647        0.103        0.084       0.0855         0.13        0.108         0.13      0.00136\n","     18     3       0.0153       0.0153     4.75e-06       0.0734       0.0958       0.0614       0.0973       0.0794       0.0794        0.122        0.101        0.136      0.00141\n","     18     4       0.0164       0.0164     5.45e-06       0.0754        0.099       0.0642       0.0977       0.0809       0.0837        0.124        0.104        0.141      0.00147\n","     18     5        0.016        0.016     2.84e-06       0.0748        0.098       0.0622       0.0999       0.0811       0.0806        0.126        0.103       0.0969      0.00101\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              18   71.403    0.005       0.0162     5.25e-05       0.0163       0.0757       0.0986       0.0645        0.098       0.0813       0.0832        0.124        0.103         0.41      0.00427\n","! Validation         18   71.403    0.005       0.0167     4.74e-06       0.0167        0.076          0.1       0.0638        0.101       0.0822       0.0832        0.127        0.105        0.134      0.00139\n","Wall time: 71.40362453299986\n","! Best model       18    0.017\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     19     1       0.0156       0.0155     3.94e-05       0.0732       0.0965       0.0646       0.0906       0.0776       0.0856        0.115          0.1         0.46      0.00479\n","     19     2       0.0133       0.0133     4.33e-05       0.0674       0.0891       0.0555       0.0912       0.0734       0.0726        0.115        0.094        0.481      0.00501\n","     19     3       0.0135       0.0135     9.15e-07       0.0695         0.09       0.0613       0.0861       0.0737       0.0784         0.11        0.094        0.051     0.000531\n","     19     4       0.0143       0.0142      8.9e-05       0.0709       0.0923       0.0591       0.0946       0.0769       0.0755        0.119       0.0973        0.699      0.00728\n","     19     5       0.0158       0.0158      7.7e-05       0.0742       0.0972        0.063       0.0965       0.0798       0.0829        0.121        0.102        0.646      0.00673\n","     19     6       0.0162       0.0162     2.43e-05       0.0758       0.0985       0.0642        0.099       0.0816       0.0825        0.124        0.103        0.326      0.00339\n","     19     7       0.0148       0.0148     1.58e-05       0.0718        0.094       0.0583        0.099       0.0786       0.0757        0.123       0.0992        0.283      0.00295\n","     19     8       0.0132        0.013     0.000199       0.0675       0.0882       0.0579       0.0867       0.0723       0.0747         0.11       0.0926         1.04       0.0108\n","     19     9       0.0142       0.0141     0.000142       0.0703       0.0918       0.0584        0.094       0.0762       0.0755        0.118       0.0966         0.88      0.00917\n","     19    10       0.0188       0.0188     5.77e-06       0.0804        0.106       0.0668        0.108       0.0872       0.0874        0.136        0.112        0.155      0.00161\n","     19    11       0.0241       0.0241     3.28e-05       0.0932         0.12       0.0816        0.116        0.099        0.105        0.146        0.125        0.404      0.00421\n","     19    12       0.0244       0.0241     0.000252       0.0959         0.12       0.0882        0.111       0.0997         0.11        0.139        0.124         1.17       0.0122\n","     19    13       0.0173       0.0173     2.32e-06       0.0783        0.102       0.0693       0.0961       0.0827       0.0902        0.122        0.106       0.0916     0.000954\n","     19    14       0.0173       0.0172     1.91e-05       0.0783        0.102       0.0675          0.1       0.0838       0.0876        0.125        0.106        0.318      0.00332\n","     19    15       0.0242       0.0242     6.32e-05       0.0958         0.12       0.0913        0.105       0.0981        0.114        0.133        0.123        0.589      0.00614\n","     19    16       0.0214       0.0214     3.13e-06       0.0889        0.113       0.0799        0.107       0.0934       0.0996        0.136        0.118        0.112      0.00116\n","     19    17       0.0157       0.0157      1.5e-06       0.0741        0.097        0.062       0.0984       0.0802       0.0815        0.122        0.102       0.0764     0.000795\n","     19    18       0.0158       0.0157      3.9e-05       0.0746       0.0971       0.0635       0.0967       0.0801       0.0826        0.121        0.102        0.437      0.00455\n","     19    19       0.0164       0.0163     0.000106       0.0749       0.0987       0.0645       0.0956       0.0801       0.0843        0.122        0.103        0.763      0.00795\n","     19    20       0.0174       0.0174     5.71e-06       0.0768        0.102       0.0645        0.101       0.0829       0.0856        0.129        0.107        0.168      0.00175\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     19     1       0.0173       0.0173     5.44e-06       0.0772        0.102       0.0646        0.102       0.0834       0.0844         0.13        0.107         0.14      0.00146\n","     19     2       0.0168       0.0168     2.91e-06       0.0757          0.1       0.0629        0.101       0.0821       0.0832        0.128        0.105        0.118      0.00123\n","     19     3       0.0146       0.0146     3.71e-06       0.0716       0.0935       0.0597       0.0953       0.0775       0.0773        0.119       0.0984        0.123      0.00129\n","     19     4       0.0157       0.0157     3.71e-06       0.0737       0.0969       0.0626        0.096       0.0793       0.0816        0.122        0.102        0.119      0.00124\n","     19     5       0.0154       0.0154     2.22e-06        0.073       0.0959       0.0606       0.0979       0.0792       0.0785        0.123        0.101        0.094      0.00098\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              19   75.181    0.005       0.0171      5.8e-05       0.0172       0.0776        0.101       0.0671       0.0986       0.0828       0.0873        0.125        0.106        0.458      0.00477\n","! Validation         19   75.181    0.005        0.016      3.6e-06        0.016       0.0742       0.0977       0.0621       0.0985       0.0803        0.081        0.125        0.103        0.119      0.00124\n","Wall time: 75.18205545399996\n","! Best model       19    0.016\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     20     1       0.0157       0.0157     6.65e-06       0.0746       0.0968       0.0635       0.0968       0.0802       0.0824        0.121        0.102        0.139      0.00145\n","     20     2       0.0134       0.0134     6.45e-06       0.0687       0.0895       0.0578       0.0904       0.0741        0.074        0.114       0.0942        0.174      0.00182\n","     20     3       0.0169       0.0169      2.4e-05        0.076          0.1       0.0652       0.0977       0.0814       0.0857        0.125        0.105        0.336       0.0035\n","     20     4       0.0175       0.0175     4.61e-06       0.0777        0.102       0.0651        0.103        0.084       0.0855         0.13        0.108        0.139      0.00145\n","     20     5       0.0144       0.0144      1.8e-05       0.0704       0.0927       0.0585       0.0943       0.0764       0.0777        0.117       0.0974        0.301      0.00314\n","     20     6       0.0139       0.0139     2.34e-05       0.0705       0.0911       0.0594       0.0927       0.0761       0.0762        0.115       0.0958        0.351      0.00366\n","     20     7       0.0141       0.0141     9.46e-06       0.0696       0.0918       0.0578       0.0933       0.0755       0.0751        0.118       0.0967        0.208      0.00216\n","     20     8       0.0125       0.0125     4.14e-06       0.0665       0.0866       0.0543       0.0908       0.0726       0.0694        0.113       0.0914        0.118      0.00123\n","     20     9       0.0132       0.0132     5.56e-06        0.068       0.0887       0.0581       0.0878       0.0729       0.0753        0.111        0.093        0.161      0.00167\n","     20    10       0.0178       0.0177     9.33e-05       0.0803        0.103       0.0675        0.106       0.0866       0.0849        0.132        0.108          0.7      0.00729\n","     20    11       0.0169       0.0169     1.34e-05       0.0768          0.1       0.0662        0.098       0.0821       0.0869        0.123        0.105        0.247      0.00257\n","     20    12       0.0136       0.0136     3.99e-06       0.0693       0.0903       0.0582       0.0916       0.0749       0.0752        0.115        0.095        0.124      0.00129\n","     20    13       0.0131       0.0131     2.77e-05        0.067       0.0885       0.0549       0.0912       0.0731       0.0703        0.117       0.0934        0.387      0.00403\n","     20    14       0.0133       0.0132     3.15e-05       0.0687        0.089        0.058       0.0901        0.074       0.0735        0.114       0.0937        0.393      0.00409\n","     20    15       0.0137       0.0137     1.12e-05       0.0695       0.0904       0.0578       0.0928       0.0753        0.075        0.115       0.0951        0.233      0.00242\n","     20    16       0.0136       0.0136     7.72e-06       0.0678       0.0903       0.0556       0.0924        0.074       0.0731        0.117       0.0953        0.199      0.00208\n","     20    17       0.0162       0.0162     1.04e-05       0.0752       0.0985       0.0639       0.0978       0.0808       0.0823        0.125        0.104        0.238      0.00248\n","     20    18       0.0176       0.0176     6.63e-06       0.0807        0.103       0.0721       0.0979        0.085       0.0917        0.122        0.107        0.128      0.00133\n","     20    19       0.0199       0.0199     1.09e-05       0.0828        0.109       0.0702        0.108        0.089       0.0937        0.135        0.114        0.222      0.00231\n","     20    20       0.0132       0.0132     3.38e-05       0.0683       0.0889       0.0578       0.0893       0.0735       0.0749        0.112       0.0934        0.421      0.00438\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     20     1       0.0166       0.0166     5.02e-06       0.0755       0.0997       0.0631          0.1       0.0817       0.0824        0.127        0.105        0.133      0.00139\n","     20     2       0.0161       0.0161     2.71e-06        0.074       0.0981       0.0613       0.0993       0.0803        0.081        0.125        0.103        0.112      0.00116\n","     20     3        0.014        0.014     3.47e-06         0.07       0.0916       0.0582       0.0937       0.0759       0.0754        0.118       0.0965         0.12      0.00125\n","     20     4       0.0151       0.0151     3.43e-06       0.0722        0.095       0.0612       0.0944       0.0778       0.0797         0.12       0.0998        0.115       0.0012\n","     20     5       0.0147       0.0147     2.14e-06       0.0713       0.0938        0.059        0.096       0.0775       0.0766        0.121       0.0989        0.093     0.000968\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              20   78.967    0.005        0.015     1.76e-05        0.015       0.0724       0.0948       0.0611       0.0951       0.0781       0.0794         0.12       0.0996        0.261      0.00272\n","! Validation         20   78.967    0.005       0.0153     3.35e-06       0.0153       0.0726       0.0957       0.0606       0.0967       0.0786       0.0791        0.122        0.101        0.114      0.00119\n","Wall time: 78.96805578200019\n","! Best model       20    0.015\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     21     1       0.0124       0.0123     7.42e-05       0.0662       0.0858       0.0559       0.0867       0.0713       0.0707         0.11       0.0903        0.634      0.00661\n","     21     2       0.0171       0.0171      1.7e-06        0.078        0.101       0.0656        0.103       0.0842       0.0861        0.126        0.106       0.0762     0.000793\n","     21     3        0.017       0.0169     4.75e-05       0.0767        0.101       0.0655       0.0992       0.0823       0.0848        0.126        0.106         0.49       0.0051\n","     21     4       0.0175       0.0174     0.000152       0.0796        0.102       0.0698       0.0991       0.0844       0.0888        0.124        0.107        0.912       0.0095\n","     21     5       0.0151       0.0151     5.98e-06       0.0736       0.0949       0.0617       0.0974       0.0796       0.0789        0.121       0.0999        0.164      0.00171\n","     21     6       0.0147       0.0147     4.99e-06       0.0717       0.0937       0.0607       0.0938       0.0773       0.0787        0.118       0.0984        0.153       0.0016\n","     21     7        0.016       0.0159     7.57e-05       0.0729       0.0975       0.0609        0.097       0.0789       0.0788        0.127        0.103        0.634       0.0066\n","     21     8       0.0189       0.0188     0.000146       0.0833        0.106       0.0742        0.102       0.0879       0.0932        0.128        0.111        0.894      0.00931\n","     21     9       0.0121       0.0121     1.94e-06       0.0663       0.0853       0.0567       0.0855       0.0711       0.0711        0.108       0.0896       0.0809     0.000842\n","     21    10       0.0124       0.0123     5.68e-05       0.0669       0.0859       0.0569        0.087       0.0719       0.0717        0.109       0.0903        0.549      0.00572\n","     21    11       0.0156       0.0155     8.48e-05       0.0752       0.0964       0.0675       0.0906       0.0791       0.0849        0.116          0.1        0.674      0.00702\n","     21    12       0.0152       0.0151     7.27e-05       0.0722        0.095       0.0597       0.0972       0.0784       0.0789        0.121       0.0999         0.62      0.00646\n","     21    13       0.0138       0.0138     3.01e-06       0.0685        0.091       0.0576       0.0903       0.0739       0.0755        0.116       0.0957        0.125       0.0013\n","     21    14       0.0161       0.0161     4.43e-05       0.0751       0.0981       0.0608        0.104       0.0823       0.0795        0.127        0.103        0.491      0.00511\n","     21    15       0.0148       0.0147     2.56e-05       0.0718       0.0939       0.0625       0.0902       0.0764       0.0801        0.117       0.0984        0.362      0.00377\n","     21    16        0.015        0.015     8.39e-06       0.0728       0.0947       0.0607        0.097       0.0788       0.0786        0.121       0.0996         0.19      0.00198\n","     21    17       0.0129       0.0129     3.44e-06       0.0666       0.0877       0.0538       0.0923       0.0731       0.0693        0.116       0.0927        0.122      0.00127\n","     21    18       0.0148       0.0148     1.56e-06       0.0727       0.0942       0.0629       0.0922       0.0775       0.0799        0.118       0.0987       0.0895     0.000932\n","     21    19       0.0163       0.0163     2.75e-05       0.0753       0.0987       0.0652       0.0956       0.0804       0.0837        0.123        0.104        0.381      0.00397\n","     21    20        0.015        0.015     2.49e-05       0.0727       0.0947       0.0632       0.0916       0.0774       0.0821        0.116        0.099        0.365       0.0038\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     21     1        0.016        0.016     5.25e-06        0.074       0.0977       0.0616       0.0986       0.0801       0.0806        0.125        0.103        0.139      0.00145\n","     21     2       0.0154       0.0154      2.5e-06       0.0724       0.0961       0.0598       0.0976       0.0787       0.0791        0.123        0.101        0.107      0.00111\n","     21     3       0.0135       0.0135     3.59e-06       0.0686       0.0898       0.0569       0.0921       0.0745       0.0736        0.116       0.0946        0.121      0.00126\n","     21     4       0.0146       0.0146     3.67e-06       0.0709       0.0933       0.0598       0.0929       0.0764       0.0779        0.118       0.0981        0.117      0.00122\n","     21     5       0.0142       0.0142     2.25e-06         0.07       0.0921       0.0578       0.0943       0.0761        0.075        0.119       0.0971        0.092     0.000958\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              21   82.759    0.005       0.0151     4.31e-05       0.0151       0.0729        0.095       0.0621       0.0945       0.0783         0.08         0.12       0.0998          0.4      0.00417\n","! Validation         21   82.759    0.005       0.0147     3.45e-06       0.0147       0.0712       0.0939       0.0592       0.0951       0.0771       0.0773         0.12       0.0988        0.115       0.0012\n","Wall time: 82.76006219899955\n","! Best model       21    0.015\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     22     1       0.0139       0.0138     8.55e-05       0.0687       0.0909       0.0574       0.0911       0.0743       0.0751        0.116       0.0957        0.678      0.00706\n","     22     2       0.0168       0.0168     6.23e-05       0.0764          0.1       0.0655       0.0981       0.0818       0.0849        0.125        0.105        0.574      0.00598\n","     22     3       0.0183       0.0183     4.93e-06       0.0796        0.105       0.0703       0.0982       0.0842       0.0937        0.123        0.109        0.112      0.00117\n","     22     4       0.0134       0.0134     8.83e-05       0.0695       0.0894       0.0597        0.089       0.0744       0.0752        0.113       0.0939        0.695      0.00724\n","     22     5       0.0129       0.0128     9.49e-05       0.0679       0.0875       0.0581       0.0875       0.0728       0.0739         0.11       0.0919        0.714      0.00744\n","     22     6       0.0148       0.0148     4.12e-06       0.0718       0.0942       0.0596       0.0964        0.078       0.0759        0.123       0.0993        0.121      0.00126\n","     22     7       0.0157       0.0157     8.23e-07       0.0737        0.097       0.0605          0.1       0.0803        0.078        0.127        0.102       0.0615     0.000641\n","     22     8       0.0154       0.0153     5.73e-05       0.0736       0.0958        0.062       0.0967       0.0794       0.0786        0.123        0.101        0.561      0.00584\n","     22     9        0.013        0.013     3.17e-05       0.0682       0.0881       0.0564       0.0917       0.0741       0.0706        0.115        0.093         0.41      0.00427\n","     22    10       0.0153       0.0153     5.62e-06        0.073       0.0958       0.0613       0.0964       0.0788       0.0802        0.121        0.101        0.136      0.00142\n","     22    11       0.0127       0.0127     2.35e-06       0.0657       0.0871       0.0548       0.0876       0.0712        0.072        0.111       0.0916       0.0938     0.000977\n","     22    12       0.0137       0.0137     5.08e-06        0.069       0.0906       0.0575        0.092       0.0747        0.075        0.116       0.0953        0.162      0.00169\n","     22    13       0.0121       0.0121     3.03e-05       0.0638        0.085       0.0524       0.0866       0.0695       0.0695         0.11       0.0895        0.403       0.0042\n","     22    14        0.012        0.012     2.67e-06        0.066       0.0849       0.0551       0.0877       0.0714       0.0703        0.108       0.0893       0.0939     0.000979\n","     22    15       0.0135       0.0135     1.23e-05       0.0693       0.0899       0.0565       0.0949       0.0757       0.0735        0.116       0.0947        0.252      0.00262\n","     22    16       0.0117       0.0117     5.64e-06        0.064       0.0835       0.0538       0.0842        0.069       0.0692        0.107       0.0879        0.166      0.00173\n","     22    17       0.0126       0.0126     3.21e-05       0.0657       0.0869       0.0544       0.0883       0.0713       0.0718        0.111       0.0915        0.406      0.00423\n","     22    18       0.0118       0.0118      1.7e-05       0.0634       0.0841       0.0526       0.0849       0.0688       0.0688        0.108       0.0887        0.292      0.00304\n","     22    19        0.012        0.012     9.81e-06       0.0651       0.0847       0.0561       0.0832       0.0697        0.071        0.107        0.089        0.221       0.0023\n","     22    20        0.013        0.013     4.42e-05       0.0669       0.0881       0.0537       0.0933       0.0735       0.0685        0.118       0.0932        0.489      0.00509\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     22     1       0.0153       0.0153     4.76e-06       0.0725       0.0958       0.0602        0.097       0.0786       0.0788        0.123        0.101         0.13      0.00136\n","     22     2       0.0148       0.0148     2.19e-06        0.071       0.0942       0.0584        0.096       0.0772       0.0772        0.121       0.0992       0.0995      0.00104\n","     22     3        0.013        0.013     3.21e-06       0.0673       0.0882       0.0557       0.0907       0.0732        0.072        0.114        0.093        0.116      0.00121\n","     22     4       0.0141       0.0141      3.2e-06       0.0695       0.0917       0.0586       0.0915        0.075       0.0762        0.117       0.0965        0.111      0.00116\n","     22     5       0.0137       0.0137     2.12e-06       0.0686       0.0904       0.0566       0.0927       0.0746       0.0735        0.117       0.0953       0.0903     0.000941\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              22   86.549    0.005       0.0137     2.98e-05       0.0137       0.0691       0.0906       0.0579       0.0914       0.0746        0.075        0.116       0.0953        0.332      0.00346\n","! Validation         22   86.549    0.005       0.0142      3.1e-06       0.0142       0.0698       0.0921       0.0579       0.0936       0.0757       0.0756        0.118        0.097        0.109      0.00114\n","Wall time: 86.5498736769996\n","! Best model       22    0.014\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     23     1       0.0115       0.0115     3.91e-07       0.0641        0.083       0.0533       0.0855       0.0694       0.0693        0.105       0.0873       0.0453     0.000472\n","     23     2       0.0127       0.0126     2.54e-05       0.0683        0.087       0.0583       0.0883       0.0733       0.0735        0.109       0.0912        0.367      0.00382\n","     23     3       0.0134       0.0134     5.12e-05       0.0682       0.0894       0.0562       0.0922       0.0742       0.0735        0.115       0.0942        0.529      0.00551\n","     23     4       0.0141       0.0141     1.55e-05       0.0693       0.0918       0.0566       0.0946       0.0756       0.0747        0.119       0.0968        0.279      0.00291\n","     23     5       0.0128       0.0127     9.13e-05       0.0658       0.0872       0.0547       0.0881       0.0714       0.0713        0.113       0.0919        0.701       0.0073\n","     23     6       0.0142       0.0142     1.81e-05       0.0702        0.092       0.0583        0.094       0.0762       0.0758        0.118       0.0969        0.311      0.00324\n","     23     7       0.0127       0.0127     6.21e-05       0.0664        0.087       0.0554       0.0885       0.0719       0.0726         0.11       0.0915        0.569      0.00593\n","     23     8       0.0141       0.0141     7.54e-05       0.0697       0.0917       0.0571       0.0951       0.0761       0.0747        0.119       0.0967        0.625      0.00651\n","     23     9       0.0132       0.0132     6.18e-06        0.067        0.089       0.0561       0.0887       0.0724       0.0728        0.115       0.0938        0.158      0.00165\n","     23    10       0.0125       0.0124     8.11e-05       0.0656       0.0862        0.055       0.0869        0.071        0.072        0.109       0.0906        0.652      0.00679\n","     23    11       0.0135       0.0135     1.14e-05       0.0691         0.09       0.0587       0.0898       0.0743       0.0747        0.115       0.0947        0.233      0.00242\n","     23    12       0.0132       0.0132     5.65e-06       0.0686       0.0887       0.0583       0.0892       0.0738       0.0741        0.112       0.0933        0.152      0.00159\n","     23    13       0.0116       0.0115     0.000104       0.0635        0.083       0.0528       0.0849       0.0688       0.0688        0.106       0.0874        0.753      0.00785\n","     23    14       0.0127       0.0127     3.67e-06       0.0671       0.0871       0.0569       0.0876       0.0722       0.0729         0.11       0.0916        0.125      0.00131\n","     23    15       0.0166       0.0165      7.5e-05       0.0778       0.0994       0.0717         0.09       0.0808       0.0898        0.116        0.103        0.634       0.0066\n","     23    16        0.016        0.016     5.23e-06       0.0773       0.0979       0.0702       0.0915       0.0809       0.0884        0.115        0.101        0.145      0.00151\n","     23    17       0.0144       0.0144     4.36e-06       0.0714       0.0927       0.0595       0.0953       0.0774       0.0748        0.121       0.0978        0.153       0.0016\n","     23    18       0.0146       0.0146     4.32e-05       0.0701       0.0934       0.0582       0.0939        0.076       0.0777        0.119       0.0982         0.48        0.005\n","     23    19       0.0141       0.0141      1.2e-06       0.0708        0.092       0.0572       0.0979       0.0776       0.0723        0.122       0.0973       0.0619     0.000645\n","     23    20       0.0187       0.0186     9.52e-05       0.0828        0.106       0.0733        0.102       0.0875       0.0914        0.129         0.11        0.718      0.00748\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     23     1       0.0148       0.0148     4.51e-06       0.0712       0.0941        0.059       0.0955       0.0772       0.0772        0.121       0.0991         0.13      0.00135\n","     23     2       0.0143       0.0143     1.94e-06       0.0695       0.0924       0.0571       0.0945       0.0758       0.0755        0.119       0.0974       0.0923     0.000961\n","     23     3       0.0126       0.0126     3.05e-06       0.0661       0.0867       0.0545       0.0892       0.0719       0.0705        0.112       0.0914        0.113      0.00117\n","     23     4       0.0136       0.0136     2.98e-06       0.0683       0.0902       0.0574       0.0901       0.0738       0.0747        0.115       0.0949        0.108      0.00112\n","     23     5       0.0132       0.0132     2.04e-06       0.0673       0.0888       0.0554       0.0911       0.0733       0.0721        0.115       0.0937       0.0877     0.000913\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              23   90.346    0.005       0.0138     3.88e-05       0.0138       0.0697       0.0909       0.0589       0.0912        0.075        0.076        0.115       0.0955        0.385      0.00401\n","! Validation         23   90.346    0.005       0.0137      2.9e-06       0.0137       0.0685       0.0905       0.0567       0.0921       0.0744        0.074        0.117       0.0953        0.106       0.0011\n","Wall time: 90.34655927200038\n","! Best model       23    0.014\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     24     1       0.0197       0.0197      1.2e-06       0.0858        0.109       0.0782        0.101       0.0896       0.0979        0.127        0.113       0.0672       0.0007\n","     24     2       0.0143       0.0143     1.59e-05       0.0711       0.0926       0.0613       0.0907        0.076       0.0781        0.116       0.0972        0.288        0.003\n","     24     3       0.0117       0.0117     4.47e-05       0.0631       0.0836       0.0519       0.0854       0.0687       0.0676        0.109       0.0882        0.481      0.00501\n","     24     4       0.0149       0.0149     4.96e-06       0.0737       0.0944       0.0634       0.0945       0.0789       0.0795        0.119       0.0991        0.161      0.00167\n","     24     5       0.0137       0.0136     9.42e-05       0.0688       0.0901       0.0583       0.0897        0.074       0.0749        0.115       0.0947         0.72       0.0075\n","     24     6       0.0123       0.0122     2.73e-05       0.0642       0.0855       0.0526       0.0874         0.07       0.0701         0.11       0.0901        0.373      0.00389\n","     24     7       0.0127       0.0126      2.2e-05       0.0673        0.087        0.057       0.0878       0.0724        0.073         0.11       0.0914        0.338      0.00352\n","     24     8       0.0138       0.0138     2.54e-05       0.0699       0.0908       0.0594       0.0911       0.0752       0.0762        0.115       0.0954         0.36      0.00375\n","     24     9       0.0137       0.0136      1.9e-05        0.068       0.0903       0.0555       0.0931       0.0743       0.0742        0.116       0.0951         0.31      0.00322\n","     24    10       0.0132       0.0132     7.57e-06        0.069        0.089       0.0581       0.0908       0.0744       0.0745        0.112       0.0935        0.196      0.00204\n","     24    11       0.0145       0.0145     7.74e-05       0.0717        0.093       0.0581       0.0987       0.0784       0.0732        0.123       0.0983        0.647      0.00674\n","     24    12       0.0126       0.0126     4.61e-06       0.0656        0.087        0.056       0.0847       0.0703       0.0733        0.109       0.0913        0.156      0.00163\n","     24    13       0.0126       0.0126      1.6e-05       0.0679       0.0868        0.059       0.0856       0.0723       0.0747        0.107       0.0908        0.278       0.0029\n","     24    14       0.0128       0.0128     1.38e-05       0.0671       0.0874       0.0556       0.0901       0.0728       0.0712        0.113       0.0922        0.255      0.00266\n","     24    15       0.0164       0.0164     7.06e-06       0.0758        0.099       0.0648       0.0978       0.0813       0.0835        0.124        0.104        0.193      0.00201\n","     24    16       0.0134       0.0133     6.98e-05       0.0687       0.0893       0.0557       0.0946       0.0752       0.0702        0.119       0.0944        0.612      0.00638\n","     24    17        0.014        0.014     4.52e-06       0.0707       0.0916       0.0575       0.0971       0.0773       0.0733         0.12       0.0967        0.124      0.00129\n","     24    18       0.0125       0.0124     3.86e-05       0.0671       0.0862       0.0572       0.0867        0.072       0.0733        0.108       0.0904        0.461       0.0048\n","     24    19       0.0122       0.0121     2.78e-05       0.0642       0.0852       0.0532        0.086       0.0696       0.0696         0.11       0.0898        0.385      0.00401\n","     24    20       0.0111       0.0111     6.08e-07       0.0626       0.0817       0.0531       0.0816       0.0673       0.0684        0.103       0.0858       0.0443     0.000462\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     24     1       0.0143       0.0143     3.96e-06         0.07       0.0925       0.0579       0.0941        0.076       0.0758        0.119       0.0974        0.122      0.00127\n","     24     2       0.0138       0.0138     1.81e-06       0.0684        0.091        0.056       0.0932       0.0746       0.0741        0.118       0.0959       0.0883      0.00092\n","     24     3       0.0122       0.0122     2.65e-06        0.065       0.0853       0.0535       0.0879       0.0707       0.0692        0.111         0.09        0.106      0.00111\n","     24     4       0.0132       0.0132     2.43e-06       0.0672       0.0889       0.0564       0.0889       0.0726       0.0733        0.114       0.0936          0.1      0.00104\n","     24     5       0.0128       0.0128     1.87e-06       0.0662       0.0875       0.0545       0.0897       0.0721       0.0709        0.114       0.0922       0.0853     0.000888\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              24   94.142    0.005       0.0136     2.61e-05       0.0136       0.0691       0.0901       0.0583       0.0907       0.0745       0.0751        0.114       0.0948        0.322      0.00336\n","! Validation         24   94.142    0.005       0.0133     2.54e-06       0.0133       0.0674       0.0891       0.0557       0.0907       0.0732       0.0727        0.115       0.0939          0.1      0.00105\n","Wall time: 94.14256863999981\n","! Best model       24    0.013\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     25     1       0.0149       0.0149     4.56e-05       0.0716       0.0944       0.0598       0.0951       0.0775        0.079        0.119       0.0992        0.493      0.00514\n","     25     2       0.0139       0.0139     6.69e-06       0.0697       0.0913       0.0574       0.0944       0.0759       0.0725         0.12       0.0964        0.135      0.00141\n","     25     3       0.0133       0.0133     2.41e-06       0.0691       0.0891       0.0599       0.0874       0.0737        0.075        0.112       0.0936       0.0789     0.000822\n","     25     4        0.012       0.0119      5.2e-05       0.0659       0.0844       0.0563       0.0852       0.0707       0.0714        0.106       0.0886        0.507      0.00528\n","     25     5       0.0109       0.0109     1.19e-06       0.0606       0.0807       0.0503       0.0813       0.0658       0.0659        0.104        0.085        0.068     0.000708\n","     25     6        0.012       0.0118     0.000193       0.0651       0.0841       0.0559       0.0835       0.0697       0.0715        0.105       0.0882         1.03       0.0107\n","     25     7       0.0135       0.0135     6.66e-05       0.0657       0.0898       0.0523       0.0923       0.0723       0.0722        0.117       0.0948        0.605      0.00631\n","     25     8       0.0117       0.0116     3.85e-05       0.0639       0.0835       0.0537       0.0843        0.069       0.0685        0.107       0.0879        0.453      0.00472\n","     25     9       0.0111        0.011     0.000102       0.0626       0.0812        0.052       0.0839       0.0679        0.066        0.105       0.0856        0.749       0.0078\n","     25    10       0.0121       0.0121     7.77e-05       0.0653       0.0849       0.0544       0.0871       0.0708       0.0692         0.11       0.0895        0.649      0.00676\n","     25    11       0.0127       0.0127     2.03e-05       0.0676       0.0873       0.0587       0.0854        0.072       0.0743        0.109       0.0915        0.331      0.00345\n","     25    12       0.0121        0.012     6.92e-05        0.065       0.0847       0.0537       0.0876       0.0706       0.0684         0.11       0.0894        0.614       0.0064\n","     25    13       0.0134       0.0134     6.81e-07       0.0677       0.0895       0.0567       0.0898       0.0732       0.0748        0.113       0.0941       0.0516     0.000537\n","     25    14       0.0114       0.0113      2.3e-05       0.0632       0.0824       0.0536       0.0823       0.0679       0.0682        0.105       0.0867        0.341      0.00355\n","     25    15        0.012       0.0119     1.44e-05       0.0645       0.0845       0.0539       0.0857       0.0698       0.0701        0.108       0.0889         0.26      0.00271\n","     25    16       0.0116       0.0116     2.07e-05        0.062       0.0834       0.0492       0.0876       0.0684       0.0652        0.111       0.0882        0.322      0.00335\n","     25    17       0.0125       0.0124     4.77e-05       0.0653       0.0862       0.0528       0.0904       0.0716       0.0674        0.115       0.0912        0.502      0.00523\n","     25    18        0.012        0.012     2.15e-06       0.0643       0.0846       0.0526       0.0879       0.0702       0.0687         0.11       0.0892       0.0916     0.000954\n","     25    19       0.0125       0.0125     2.87e-05       0.0661       0.0865        0.054       0.0904       0.0722       0.0711        0.111       0.0911        0.388      0.00404\n","     25    20       0.0106       0.0106     9.45e-06       0.0616       0.0795       0.0502       0.0842       0.0672       0.0628        0.105       0.0841        0.227      0.00236\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     25     1       0.0138       0.0138     3.64e-06       0.0689        0.091       0.0569       0.0928       0.0749       0.0745        0.117       0.0959        0.116      0.00121\n","     25     2       0.0134       0.0134     1.65e-06       0.0672       0.0895       0.0549       0.0919       0.0734       0.0726        0.116       0.0944       0.0853     0.000888\n","     25     3       0.0118       0.0118     2.51e-06        0.064       0.0841       0.0526       0.0867       0.0697        0.068        0.109       0.0887        0.104      0.00108\n","     25     4       0.0128       0.0128     1.98e-06       0.0662       0.0877       0.0553       0.0878       0.0716       0.0719        0.113       0.0923       0.0914     0.000952\n","     25     5       0.0124       0.0124     1.86e-06       0.0652       0.0861       0.0536       0.0883       0.0709       0.0698        0.112       0.0908       0.0845      0.00088\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              25   97.946    0.005       0.0123     4.11e-05       0.0123       0.0653       0.0857       0.0544       0.0873       0.0708       0.0702         0.11       0.0903        0.395      0.00411\n","! Validation         25   97.946    0.005       0.0129     2.33e-06       0.0129       0.0663       0.0877       0.0547       0.0895       0.0721       0.0714        0.114       0.0925       0.0962        0.001\n","Wall time: 97.94706556100027\n","! Best model       25    0.013\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     26     1       0.0135       0.0135     1.19e-05       0.0681       0.0899       0.0562       0.0919        0.074       0.0739        0.116       0.0947        0.186      0.00193\n","     26     2       0.0127       0.0127     5.24e-06       0.0676       0.0873       0.0595       0.0836       0.0716       0.0759        0.107       0.0912        0.151      0.00158\n","     26     3       0.0121       0.0121     8.07e-06       0.0659       0.0851       0.0558       0.0861        0.071       0.0703        0.109       0.0895        0.205      0.00213\n","     26     4       0.0124       0.0124     1.59e-05       0.0654       0.0862       0.0536        0.089       0.0713       0.0703        0.112       0.0909        0.285      0.00297\n","     26     5         0.01      0.00999     2.17e-05       0.0587       0.0773       0.0469       0.0824       0.0647       0.0606        0.103       0.0818        0.343      0.00357\n","     26     6       0.0103       0.0103     1.92e-06       0.0601       0.0787       0.0483       0.0837        0.066        0.062        0.104       0.0832       0.0852     0.000887\n","     26     7       0.0119       0.0119      1.7e-05       0.0631       0.0844       0.0517       0.0859       0.0688       0.0683         0.11        0.089        0.303      0.00315\n","     26     8       0.0126       0.0126     4.17e-05       0.0666       0.0868       0.0567       0.0864       0.0716       0.0717        0.111       0.0914        0.475      0.00495\n","     26     9       0.0137       0.0137     2.33e-06       0.0695       0.0904       0.0597       0.0891       0.0744       0.0762        0.114       0.0949       0.0965      0.00101\n","     26    10        0.014       0.0139     1.79e-05        0.069       0.0913       0.0569       0.0931        0.075        0.075        0.117       0.0962        0.302      0.00315\n","     26    11       0.0109       0.0109     1.97e-05       0.0612       0.0806       0.0501       0.0835       0.0668       0.0642        0.106       0.0852        0.327      0.00341\n","     26    12       0.0106       0.0105     9.98e-06       0.0618       0.0794       0.0518       0.0816       0.0667       0.0654        0.102       0.0836        0.204      0.00212\n","     26    13       0.0135       0.0135     5.12e-06        0.068       0.0899       0.0565       0.0911       0.0738       0.0722        0.118       0.0949        0.156      0.00162\n","     26    14       0.0169       0.0169     1.88e-05       0.0787          0.1       0.0711        0.094       0.0826       0.0902        0.118        0.104        0.312      0.00325\n","     26    15        0.016        0.016     2.23e-06        0.077       0.0978       0.0713       0.0886       0.0799       0.0895        0.113        0.101       0.0854     0.000889\n","     26    16       0.0115       0.0115     1.27e-05        0.064       0.0831       0.0538       0.0844       0.0691       0.0685        0.106       0.0875         0.26       0.0027\n","     26    17       0.0118       0.0118     4.38e-05       0.0643       0.0839       0.0546       0.0838       0.0692       0.0708        0.105        0.088         0.48        0.005\n","     26    18       0.0168       0.0168      1.3e-05       0.0775          0.1       0.0679       0.0968       0.0824       0.0884         0.12        0.104        0.261      0.00272\n","     26    19        0.019       0.0188     0.000185       0.0843        0.106       0.0769        0.099        0.088       0.0954        0.125         0.11         1.01       0.0105\n","     26    20       0.0188       0.0187     7.72e-05       0.0831        0.106       0.0713        0.107        0.089       0.0888        0.133        0.111        0.647      0.00674\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     26     1       0.0134       0.0134     3.42e-06       0.0678       0.0896        0.056       0.0915       0.0737       0.0732        0.116       0.0944         0.11      0.00115\n","     26     2        0.013        0.013     1.62e-06       0.0662       0.0881       0.0539       0.0907       0.0723       0.0713        0.115        0.093       0.0839     0.000874\n","     26     3       0.0115       0.0115     2.23e-06        0.063       0.0829       0.0517       0.0855       0.0686       0.0668        0.108       0.0875       0.0977      0.00102\n","     26     4       0.0125       0.0125     1.66e-06       0.0652       0.0864       0.0544       0.0868       0.0706       0.0707        0.111       0.0911        0.084     0.000875\n","     26     5        0.012        0.012     1.85e-06       0.0641       0.0849       0.0527       0.0869       0.0698       0.0688         0.11       0.0895       0.0837     0.000872\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              26  101.758    0.005       0.0134     2.66e-05       0.0135       0.0687       0.0896       0.0585        0.089       0.0738       0.0755        0.113       0.0941        0.308      0.00321\n","! Validation         26  101.758    0.005       0.0125     2.15e-06       0.0125       0.0653       0.0864       0.0537       0.0883        0.071       0.0702        0.112       0.0911       0.0919     0.000957\n","Wall time: 101.75893772299969\n","! Best model       26    0.012\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     27     1        0.014        0.014     6.78e-06       0.0705       0.0916       0.0599       0.0918       0.0759       0.0753        0.117       0.0964        0.189      0.00197\n","     27     2       0.0112        0.011     0.000202       0.0604        0.081       0.0505       0.0802       0.0654        0.065        0.106       0.0854         1.06        0.011\n","     27     3       0.0142       0.0139     0.000218       0.0705       0.0913       0.0587       0.0941       0.0764       0.0748        0.118       0.0962         1.09       0.0114\n","     27     4       0.0226       0.0225     5.92e-05       0.0927        0.116       0.0866        0.105       0.0958        0.109         0.13        0.119        0.567      0.00591\n","     27     5       0.0165       0.0165     1.88e-05       0.0771       0.0992       0.0691        0.093       0.0811       0.0876        0.119        0.103        0.311      0.00324\n","     27     6       0.0142       0.0142     3.12e-05        0.068       0.0921       0.0542       0.0955       0.0748       0.0746        0.119       0.0971        0.413      0.00431\n","     27     7       0.0142       0.0142     2.22e-06       0.0721       0.0922       0.0641        0.088       0.0761       0.0813        0.111       0.0961        0.098      0.00102\n","     27     8       0.0193       0.0193     5.27e-05       0.0851        0.107       0.0755        0.104       0.0899        0.093        0.132        0.112        0.529      0.00551\n","     27     9        0.017       0.0169     1.43e-05       0.0777        0.101       0.0671        0.099        0.083       0.0852        0.126        0.106        0.237      0.00247\n","     27    10        0.011        0.011     4.32e-06       0.0617       0.0813        0.051       0.0831       0.0671       0.0661        0.105       0.0857        0.118      0.00122\n","     27    11       0.0175       0.0175     4.15e-05       0.0807        0.102       0.0716       0.0988       0.0852         0.09        0.123        0.107        0.465      0.00485\n","     27    12        0.015        0.015     1.64e-06       0.0735       0.0948       0.0659       0.0885       0.0772       0.0847        0.112       0.0985       0.0828     0.000863\n","     27    13       0.0116       0.0116     2.31e-05       0.0643       0.0831        0.052        0.089       0.0705       0.0659         0.11       0.0878        0.339      0.00354\n","     27    14       0.0177       0.0176     4.02e-05       0.0809        0.103       0.0751       0.0924       0.0838       0.0945        0.118        0.106        0.463      0.00483\n","     27    15       0.0176       0.0175     6.37e-05       0.0806        0.102        0.073       0.0958       0.0844       0.0907        0.123        0.107        0.589      0.00614\n","     27    16       0.0111       0.0111     4.91e-05       0.0616       0.0815       0.0516       0.0818       0.0667       0.0668        0.105       0.0858        0.514      0.00536\n","     27    17       0.0118       0.0118     8.45e-06       0.0659       0.0841        0.057       0.0836       0.0703       0.0725        0.104        0.088        0.208      0.00217\n","     27    18       0.0155       0.0155     6.76e-06       0.0766       0.0963       0.0651       0.0997       0.0824       0.0804        0.122        0.101        0.185      0.00193\n","     27    19       0.0142       0.0142     2.25e-05       0.0701       0.0922       0.0597        0.091       0.0753       0.0789        0.114       0.0965        0.352      0.00366\n","     27    20       0.0139       0.0138     4.56e-05       0.0691        0.091       0.0567        0.094       0.0753       0.0741        0.118       0.0959        0.485      0.00506\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     27     1       0.0131       0.0131     3.39e-06        0.067       0.0885       0.0552       0.0906       0.0729       0.0721        0.115       0.0933        0.112      0.00117\n","     27     2       0.0127       0.0127     1.41e-06       0.0653       0.0871       0.0531       0.0896       0.0714       0.0703        0.113       0.0919        0.079     0.000823\n","     27     3       0.0112       0.0112     2.19e-06       0.0622       0.0819       0.0511       0.0845       0.0678       0.0659        0.107       0.0864       0.0979      0.00102\n","     27     4       0.0122       0.0122     1.77e-06       0.0645       0.0855       0.0536       0.0862       0.0699       0.0697        0.111       0.0902       0.0872     0.000908\n","     27     5       0.0118       0.0118     1.81e-06       0.0633       0.0839        0.052       0.0857       0.0689        0.068        0.109       0.0884       0.0802     0.000835\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              27  105.569    0.005        0.015     4.56e-05        0.015        0.073       0.0946       0.0632       0.0924       0.0778       0.0813        0.117        0.099        0.415      0.00432\n","! Validation         27  105.569    0.005       0.0122     2.11e-06       0.0122       0.0644       0.0854        0.053       0.0873       0.0702       0.0692        0.111       0.0901       0.0913     0.000951\n","Wall time: 105.56926507299977\n","! Best model       27    0.012\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     28     1       0.0164       0.0164     1.52e-06       0.0752       0.0992       0.0634       0.0989       0.0811       0.0839        0.124        0.104        0.084     0.000875\n","     28     2       0.0135       0.0134     3.96e-05       0.0685       0.0897       0.0578       0.0901       0.0739       0.0755        0.113       0.0942        0.445      0.00463\n","     28     3       0.0126       0.0126     1.19e-06       0.0656       0.0869       0.0556       0.0855       0.0705       0.0728         0.11       0.0913        0.066     0.000688\n","     28     4       0.0151        0.015     4.71e-05       0.0741       0.0948       0.0639       0.0943       0.0791       0.0811        0.117       0.0993        0.496      0.00516\n","     28     5       0.0158       0.0158      1.9e-05       0.0756       0.0971        0.065       0.0968       0.0809       0.0822        0.122        0.102        0.314      0.00327\n","     28     6       0.0129       0.0129     7.52e-06       0.0668       0.0879       0.0557       0.0888       0.0723       0.0721        0.113       0.0926        0.184      0.00192\n","     28     7       0.0117       0.0117     3.47e-05       0.0652       0.0836       0.0541       0.0875       0.0708       0.0686        0.108       0.0881        0.428      0.00446\n","     28     8       0.0134       0.0133     8.16e-05       0.0686       0.0893       0.0575        0.091       0.0742       0.0729        0.115       0.0941        0.663      0.00691\n","     28     9       0.0122       0.0122     1.24e-06       0.0654       0.0854       0.0523       0.0917        0.072       0.0661        0.115       0.0904       0.0699     0.000728\n","     28    10       0.0103       0.0103     2.58e-05       0.0589       0.0784        0.049       0.0786       0.0638       0.0647          0.1       0.0825        0.366      0.00381\n","     28    11       0.0108       0.0108     1.26e-05       0.0611       0.0805       0.0519       0.0795       0.0657       0.0677        0.101       0.0845        0.245      0.00255\n","     28    12        0.012        0.012     2.01e-06       0.0653       0.0847       0.0543       0.0873       0.0708        0.068        0.111       0.0894       0.0879     0.000916\n","     28    13       0.0101       0.0101     2.44e-05       0.0588       0.0776       0.0491       0.0784       0.0637       0.0638       0.0997       0.0817        0.358      0.00373\n","     28    14      0.00913      0.00912      6.7e-06       0.0564       0.0739       0.0461       0.0769       0.0615       0.0585       0.0976       0.0781        0.183      0.00191\n","     28    15       0.0114       0.0113     7.51e-05       0.0633       0.0823        0.052       0.0861        0.069        0.067        0.106       0.0867        0.638      0.00665\n","     28    16       0.0113       0.0113     6.12e-06       0.0628       0.0824       0.0529       0.0827       0.0678       0.0682        0.105       0.0867        0.166      0.00173\n","     28    17      0.00975      0.00969      5.9e-05       0.0587       0.0762       0.0489       0.0781       0.0635       0.0628       0.0976       0.0802        0.565      0.00588\n","     28    18       0.0127       0.0127     7.82e-05       0.0668       0.0871       0.0558       0.0888       0.0723       0.0718        0.112       0.0916        0.656      0.00684\n","     28    19       0.0136       0.0136     1.81e-06       0.0681       0.0903       0.0569       0.0905       0.0737       0.0749        0.115        0.095        0.082     0.000854\n","     28    20       0.0111       0.0109     0.000153       0.0619       0.0808       0.0513       0.0831       0.0672        0.066        0.104       0.0851        0.915      0.00953\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     28     1       0.0128       0.0128      3.1e-06       0.0662       0.0875       0.0545       0.0897       0.0721       0.0712        0.113       0.0922        0.104      0.00108\n","     28     2       0.0124       0.0124     1.66e-06       0.0646       0.0861       0.0525       0.0888       0.0707       0.0693        0.113       0.0909       0.0822     0.000857\n","     28     3        0.011        0.011     2.07e-06       0.0615       0.0811       0.0505       0.0837       0.0671       0.0651        0.106       0.0856       0.0914     0.000952\n","     28     4        0.012        0.012     1.34e-06       0.0638       0.0847       0.0529       0.0854       0.0692       0.0688         0.11       0.0893        0.077     0.000802\n","     28     5       0.0115       0.0115     1.98e-06       0.0625       0.0829       0.0514       0.0847       0.0681       0.0672        0.108       0.0874       0.0858     0.000894\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              28  109.374    0.005       0.0123     3.39e-05       0.0123       0.0654       0.0857       0.0547       0.0867       0.0707       0.0707         0.11       0.0902        0.351      0.00365\n","! Validation         28  109.374    0.005       0.0119     2.03e-06       0.0119       0.0637       0.0845       0.0524       0.0865       0.0694       0.0684         0.11       0.0891        0.088     0.000917\n","Wall time: 109.37473888000022\n","! Best model       28    0.012\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     29     1       0.0108       0.0107      6.9e-05       0.0611       0.0802       0.0506        0.082       0.0663       0.0655        0.104       0.0845        0.615      0.00641\n","     29     2       0.0139       0.0139     3.27e-06       0.0707       0.0911       0.0605        0.091       0.0757       0.0771        0.114       0.0956        0.114      0.00118\n","     29     3       0.0117       0.0116     3.35e-05       0.0631       0.0834       0.0528       0.0838       0.0683       0.0685        0.107       0.0878        0.415      0.00432\n","     29     4       0.0098      0.00969     0.000111       0.0582       0.0762       0.0474       0.0798       0.0636       0.0611       0.0996       0.0804        0.775      0.00808\n","     29     5       0.0123       0.0123     2.84e-05       0.0661       0.0858       0.0551       0.0881       0.0716       0.0686        0.113       0.0906         0.37      0.00386\n","     29     6       0.0118       0.0118     6.02e-05       0.0634       0.0839       0.0529       0.0845       0.0687       0.0683        0.108       0.0884        0.558      0.00582\n","     29     7       0.0094      0.00933     7.19e-05        0.057       0.0747       0.0463       0.0783       0.0623        0.059        0.099        0.079        0.627      0.00653\n","     29     8       0.0103       0.0103     1.06e-06       0.0601       0.0785        0.049       0.0822       0.0656       0.0624        0.103       0.0829       0.0537     0.000559\n","     29     9       0.0146       0.0145     1.77e-05       0.0716       0.0933       0.0613       0.0922       0.0767       0.0782        0.118        0.098         0.29      0.00302\n","     29    10       0.0131       0.0131     1.64e-05       0.0665       0.0886       0.0554       0.0886        0.072       0.0728        0.114       0.0933        0.296      0.00309\n","     29    11       0.0128       0.0128     5.54e-06       0.0657       0.0875       0.0542       0.0889       0.0715       0.0734         0.11        0.092        0.164      0.00171\n","     29    12        0.015       0.0149     4.69e-05        0.073       0.0945       0.0668       0.0856       0.0762       0.0864        0.109       0.0977        0.507      0.00528\n","     29    13       0.0118       0.0118     2.61e-06       0.0663       0.0839       0.0583       0.0821       0.0702       0.0722        0.103       0.0878       0.0826     0.000861\n","     29    14      0.00957      0.00956     9.74e-06       0.0574       0.0756       0.0476       0.0772       0.0624       0.0605       0.0992       0.0799        0.209      0.00218\n","     29    15       0.0166       0.0166     3.62e-05       0.0777       0.0997       0.0671        0.099       0.0831        0.085        0.124        0.104        0.441      0.00459\n","     29    16       0.0182       0.0182     7.17e-05       0.0812        0.104       0.0718          0.1       0.0859       0.0911        0.127        0.109        0.626      0.00652\n","     29    17       0.0123       0.0123     4.98e-05       0.0654       0.0858        0.055       0.0862       0.0706       0.0711        0.109       0.0902        0.518       0.0054\n","     29    18       0.0109       0.0108     0.000113       0.0607       0.0802       0.0492       0.0838       0.0665       0.0635        0.106       0.0848        0.781      0.00814\n","     29    19        0.013        0.013     8.55e-07       0.0697       0.0881       0.0637       0.0818       0.0727       0.0793        0.104       0.0914       0.0646     0.000673\n","     29    20       0.0138       0.0138     2.13e-05       0.0696       0.0908        0.059       0.0908       0.0749       0.0747        0.117       0.0956        0.339      0.00354\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     29     1       0.0125       0.0125     3.04e-06       0.0655       0.0865       0.0538       0.0888       0.0713       0.0704        0.112       0.0912        0.105      0.00109\n","     29     2       0.0121       0.0121     1.38e-06       0.0639       0.0852       0.0518        0.088       0.0699       0.0683        0.111       0.0899       0.0763     0.000794\n","     29     3       0.0108       0.0108     2.06e-06       0.0609       0.0803       0.0499       0.0829       0.0664       0.0643        0.105       0.0847       0.0933     0.000971\n","     29     4       0.0117       0.0117     1.48e-06       0.0631       0.0838       0.0523       0.0847       0.0685       0.0679        0.109       0.0884       0.0797      0.00083\n","     29     5       0.0112       0.0112     1.82e-06       0.0618        0.082       0.0508       0.0838       0.0673       0.0664        0.107       0.0865         0.08     0.000833\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              29  113.202    0.005       0.0125     3.85e-05       0.0126       0.0662       0.0866       0.0562       0.0863       0.0712       0.0725         0.11       0.0911        0.392      0.00409\n","! Validation         29  113.202    0.005       0.0117     1.96e-06       0.0117        0.063       0.0836       0.0517       0.0856       0.0687       0.0675        0.109       0.0882       0.0869     0.000905\n","Wall time: 113.20244314400043\n","! Best model       29    0.012\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     30     1       0.0129       0.0129     6.86e-05       0.0673       0.0878       0.0551       0.0916       0.0733        0.071        0.114       0.0926        0.612      0.00638\n","     30     2       0.0111       0.0111      7.3e-06       0.0627       0.0814       0.0549       0.0785       0.0667       0.0695        0.101       0.0853        0.184      0.00192\n","     30     3       0.0137       0.0137     4.17e-05       0.0698       0.0904       0.0593       0.0908       0.0751        0.074        0.117       0.0952        0.475      0.00494\n","     30     4       0.0129       0.0129     1.29e-05       0.0656       0.0879       0.0526       0.0916       0.0721         0.07        0.116       0.0929        0.252      0.00262\n","     30     5      0.00992      0.00992     2.08e-07       0.0584       0.0771       0.0475       0.0802       0.0639       0.0611        0.102       0.0814       0.0297     0.000309\n","     30     6       0.0108       0.0108     1.17e-05       0.0618       0.0803       0.0517       0.0819       0.0668       0.0668        0.102       0.0845        0.243      0.00253\n","     30     7      0.00942      0.00942      2.7e-06        0.058       0.0751       0.0488       0.0762       0.0625       0.0613       0.0969       0.0791       0.0898     0.000936\n","     30     8        0.011        0.011     2.29e-06       0.0621       0.0811       0.0533       0.0799       0.0666       0.0684        0.102       0.0851       0.0938     0.000977\n","     30     9       0.0116       0.0116     1.38e-06       0.0634       0.0833       0.0519       0.0864       0.0692       0.0671        0.109       0.0879       0.0691      0.00072\n","     30    10       0.0119       0.0118     5.36e-05       0.0625       0.0841       0.0509       0.0859       0.0684       0.0671        0.111       0.0888        0.543      0.00566\n","     30    11       0.0123       0.0123      3.8e-05       0.0655       0.0857       0.0539       0.0885       0.0712       0.0698        0.111       0.0903        0.455      0.00474\n","     30    12       0.0113       0.0113     4.09e-06       0.0619       0.0821       0.0515       0.0827       0.0671       0.0679        0.105       0.0864         0.13      0.00135\n","     30    13       0.0106       0.0106     1.72e-05       0.0607       0.0795        0.049       0.0841       0.0666        0.063        0.105        0.084        0.274      0.00285\n","     30    14       0.0102       0.0102     6.99e-05         0.06        0.078       0.0515        0.077       0.0642       0.0664       0.0971       0.0818         0.62      0.00646\n","     30    15       0.0111       0.0111      6.1e-06       0.0623       0.0815        0.052        0.083       0.0675       0.0671        0.105       0.0858        0.159      0.00165\n","     30    16       0.0121       0.0121     7.69e-07       0.0648       0.0851        0.056       0.0822       0.0691        0.072        0.107       0.0893       0.0592     0.000616\n","     30    17       0.0163       0.0163     1.24e-06       0.0785       0.0987        0.069       0.0975       0.0832        0.085        0.122        0.103       0.0701      0.00073\n","     30    18       0.0147       0.0147     5.91e-05        0.072       0.0937         0.06        0.096        0.078       0.0755        0.122       0.0989        0.569      0.00593\n","     30    19       0.0105       0.0105     1.05e-05       0.0604       0.0793       0.0493       0.0827        0.066       0.0637        0.104       0.0837        0.226      0.00235\n","     30    20       0.0138       0.0138     6.67e-06       0.0706       0.0907       0.0592       0.0934       0.0763       0.0745        0.117       0.0956        0.149      0.00155\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     30     1       0.0122       0.0122     3.01e-06       0.0648       0.0856       0.0531        0.088       0.0706       0.0695        0.111       0.0902        0.105      0.00109\n","     30     2       0.0119       0.0119     1.28e-06       0.0633       0.0843       0.0513       0.0872       0.0693       0.0675        0.111        0.089       0.0728     0.000758\n","     30     3       0.0106       0.0106     2.01e-06       0.0603       0.0796       0.0493       0.0822       0.0658       0.0636        0.104        0.084       0.0918     0.000956\n","     30     4       0.0115       0.0115      1.4e-06       0.0624       0.0831       0.0516       0.0841       0.0679       0.0671        0.108       0.0876       0.0776     0.000809\n","     30     5        0.011        0.011     1.89e-06       0.0611       0.0812       0.0503       0.0828       0.0666       0.0658        0.105       0.0856       0.0815     0.000849\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              30  117.024    0.005       0.0119     2.08e-05       0.0119       0.0644       0.0843       0.0539       0.0855       0.0697       0.0693        0.108       0.0888        0.265      0.00276\n","! Validation         30  117.024    0.005       0.0114     1.92e-06       0.0114       0.0624       0.0828       0.0511       0.0849        0.068       0.0667        0.108       0.0873       0.0857     0.000893\n","Wall time: 117.02514514700033\n","! Best model       30    0.011\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     31     1       0.0143       0.0143      4.2e-06       0.0716       0.0926        0.063       0.0888       0.0759         0.08        0.114       0.0968        0.148      0.00154\n","     31     2       0.0123       0.0122     2.32e-05       0.0664       0.0856       0.0574       0.0844       0.0709       0.0739        0.105       0.0895        0.347      0.00362\n","     31     3       0.0101       0.0101     3.35e-05       0.0587       0.0778       0.0485       0.0793       0.0639       0.0623        0.102       0.0821        0.427      0.00445\n","     31     4       0.0112       0.0112     1.09e-05       0.0631       0.0819       0.0551       0.0793       0.0672       0.0704        0.101       0.0858        0.239      0.00249\n","     31     5       0.0107       0.0107     9.25e-06       0.0613       0.0799       0.0499        0.084        0.067        0.064        0.105       0.0843        0.188      0.00196\n","     31     6       0.0097       0.0097     3.29e-06       0.0593       0.0762       0.0503       0.0772       0.0638       0.0632       0.0971       0.0802        0.106      0.00111\n","     31     7       0.0121       0.0121     5.55e-05       0.0648        0.085        0.054       0.0864       0.0702       0.0701        0.109       0.0895        0.551      0.00574\n","     31     8       0.0106       0.0106     4.18e-05       0.0607       0.0795       0.0486        0.085       0.0668       0.0623        0.106        0.084        0.464      0.00484\n","     31     9       0.0106       0.0106     3.83e-06       0.0606       0.0797       0.0497       0.0824       0.0661       0.0639        0.104       0.0842         0.14      0.00146\n","     31    10       0.0121       0.0121     2.48e-06       0.0656       0.0852       0.0559        0.085       0.0705       0.0703        0.109       0.0897          0.1      0.00105\n","     31    11         0.01      0.00999     3.95e-05        0.059       0.0773       0.0491       0.0789        0.064       0.0625        0.101       0.0816        0.461       0.0048\n","     31    12       0.0119       0.0119     3.66e-05       0.0638       0.0844       0.0527       0.0859       0.0693       0.0694        0.108       0.0889        0.446      0.00464\n","     31    13       0.0108       0.0108     9.14e-07        0.061       0.0803       0.0508       0.0813       0.0661       0.0657        0.103       0.0846        0.065     0.000677\n","     31    14       0.0101       0.0101        1e-05       0.0575       0.0778       0.0448        0.083       0.0639       0.0601        0.105       0.0823        0.208      0.00216\n","     31    15       0.0099      0.00988      1.8e-05       0.0581       0.0769       0.0466       0.0813       0.0639       0.0606        0.102       0.0813        0.308      0.00321\n","     31    16      0.00976      0.00975     2.65e-06       0.0575       0.0764       0.0476       0.0774       0.0625       0.0614       0.0998       0.0806       0.0971      0.00101\n","     31    17       0.0107       0.0107     2.21e-05       0.0603       0.0801        0.049        0.083        0.066       0.0644        0.105       0.0845        0.341      0.00355\n","     31    18      0.00931      0.00931     4.69e-07       0.0572       0.0746       0.0475       0.0767       0.0621       0.0605       0.0969       0.0787       0.0363     0.000378\n","     31    19        0.011       0.0109     2.41e-05       0.0613       0.0809       0.0499        0.084       0.0669       0.0646        0.106       0.0855        0.347      0.00362\n","     31    20         0.01         0.01     1.48e-06        0.059       0.0775       0.0479       0.0811       0.0645       0.0614        0.102       0.0819        0.065     0.000677\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     31     1        0.012        0.012     2.89e-06       0.0641       0.0847       0.0525       0.0873       0.0699       0.0686         0.11       0.0893        0.101      0.00105\n","     31     2       0.0116       0.0116     1.28e-06       0.0626       0.0835       0.0506       0.0865       0.0686       0.0666         0.11       0.0881       0.0716     0.000746\n","     31     3       0.0104       0.0104     1.92e-06       0.0597       0.0789       0.0488       0.0816       0.0652       0.0629        0.104       0.0833       0.0882     0.000919\n","     31     4       0.0113       0.0113     1.22e-06       0.0618       0.0823        0.051       0.0835       0.0672       0.0662        0.107       0.0868        0.074     0.000771\n","     31     5       0.0108       0.0108     1.92e-06       0.0605       0.0803       0.0497        0.082       0.0659       0.0651        0.104       0.0847       0.0799     0.000832\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              31  120.843    0.005       0.0109     1.72e-05       0.0109       0.0613       0.0806       0.0509       0.0822       0.0666       0.0658        0.104       0.0849        0.254      0.00265\n","! Validation         31  120.843    0.005       0.0112     1.85e-06       0.0112       0.0617       0.0819       0.0505       0.0842       0.0674       0.0659        0.107       0.0865       0.0829     0.000863\n","Wall time: 120.84330340100041\n","! Best model       31    0.011\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     32     1       0.0108       0.0108     1.21e-05       0.0609       0.0803       0.0504       0.0818       0.0661        0.067        0.102       0.0844        0.247      0.00257\n","     32     2      0.00862       0.0086     1.96e-05       0.0556       0.0717       0.0443       0.0783       0.0613       0.0558        0.096       0.0759        0.322      0.00336\n","     32     3        0.013        0.013     1.26e-06       0.0678       0.0882       0.0581        0.087       0.0726       0.0745         0.11       0.0925       0.0705     0.000734\n","     32     4       0.0112       0.0111     5.66e-05       0.0642       0.0816       0.0567        0.079       0.0679        0.071       0.0994       0.0852        0.547       0.0057\n","     32     5      0.00981      0.00979     2.38e-05       0.0581       0.0765       0.0464       0.0814       0.0639       0.0592        0.103        0.081        0.361      0.00376\n","     32     6       0.0105       0.0105     5.09e-06        0.061       0.0793       0.0513       0.0805       0.0659       0.0651        0.102       0.0835        0.145      0.00151\n","     32     7       0.0128       0.0127     8.33e-05       0.0654       0.0872       0.0534       0.0893       0.0713       0.0708        0.113        0.092        0.674      0.00702\n","     32     8        0.011       0.0109     0.000127       0.0619       0.0807       0.0513       0.0831       0.0672       0.0648        0.106       0.0852        0.836       0.0087\n","     32     9       0.0112       0.0111     2.62e-05       0.0625       0.0816       0.0526       0.0824       0.0675       0.0675        0.104       0.0859        0.379      0.00395\n","     32    10       0.0119       0.0117     0.000177        0.064       0.0836       0.0528       0.0864       0.0696       0.0674        0.109       0.0882        0.982       0.0102\n","     32    11       0.0122       0.0122     4.59e-05       0.0656       0.0854        0.056       0.0847       0.0703       0.0735        0.105       0.0893        0.493      0.00514\n","     32    12       0.0098       0.0098     3.18e-06       0.0586       0.0766       0.0475       0.0809       0.0642       0.0608        0.101       0.0809        0.117      0.00122\n","     32    13      0.00984      0.00976      7.7e-05       0.0583       0.0764       0.0506       0.0737       0.0622       0.0656       0.0945         0.08        0.646      0.00673\n","     32    14       0.0103       0.0103     3.85e-05       0.0605       0.0783       0.0509       0.0796       0.0652        0.064        0.101       0.0825        0.448      0.00466\n","     32    15       0.0107       0.0107     1.52e-05         0.06         0.08       0.0493       0.0813       0.0653       0.0648        0.104       0.0844        0.287      0.00299\n","     32    16      0.00999      0.00994     5.13e-05       0.0586       0.0771       0.0488       0.0781       0.0635       0.0617        0.101       0.0814        0.521      0.00542\n","     32    17       0.0114       0.0114     8.54e-06       0.0619       0.0825       0.0498       0.0859       0.0679       0.0641        0.111       0.0873        0.211       0.0022\n","     32    18        0.013        0.013     2.74e-05       0.0681       0.0881         0.06       0.0842       0.0721       0.0765        0.108       0.0921        0.385      0.00401\n","     32    19       0.0106       0.0105     3.87e-05       0.0614       0.0794       0.0527       0.0788       0.0658       0.0679       0.0983       0.0831        0.448      0.00467\n","     32    20       0.0118       0.0118     9.92e-06       0.0627       0.0839       0.0508       0.0865       0.0686       0.0673         0.11       0.0886        0.222      0.00231\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     32     1       0.0117       0.0117     2.92e-06       0.0634       0.0838       0.0518       0.0865       0.0692       0.0678        0.109       0.0883        0.103      0.00107\n","     32     2       0.0114       0.0114     1.19e-06        0.062       0.0826       0.0501       0.0858       0.0679       0.0658        0.109       0.0873       0.0696     0.000725\n","     32     3       0.0102       0.0102     1.92e-06       0.0592       0.0782       0.0483        0.081       0.0646       0.0622        0.103       0.0826        0.088     0.000917\n","     32     4       0.0111       0.0111      1.3e-06       0.0612       0.0815       0.0504       0.0829       0.0666       0.0655        0.107       0.0861       0.0762     0.000793\n","     32     5       0.0106       0.0106     1.83e-06       0.0599       0.0796       0.0492       0.0812       0.0652       0.0645        0.103       0.0839        0.077     0.000802\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              32  124.665    0.005        0.011     4.24e-05        0.011       0.0618        0.081       0.0517       0.0821       0.0669       0.0666        0.104       0.0853        0.417      0.00435\n","! Validation         32  124.665    0.005        0.011     1.83e-06        0.011       0.0611       0.0812         0.05       0.0835       0.0667       0.0652        0.106       0.0857       0.0828     0.000862\n","Wall time: 124.66536703600013\n","! Best model       32    0.011\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     33     1       0.0105       0.0103     0.000176       0.0614       0.0785       0.0532       0.0778       0.0655       0.0671       0.0974       0.0823        0.983       0.0102\n","     33     2       0.0132       0.0132      2.6e-06       0.0667        0.089       0.0519       0.0964       0.0742       0.0661        0.123       0.0943        0.109      0.00113\n","     33     3       0.0109       0.0109     3.01e-07       0.0624       0.0806       0.0518       0.0835       0.0676       0.0641        0.106       0.0852       0.0371     0.000387\n","     33     4      0.00956      0.00949     6.49e-05       0.0577       0.0754       0.0477       0.0776       0.0626       0.0604       0.0988       0.0796        0.594      0.00619\n","     33     5       0.0134       0.0134      1.8e-05       0.0673       0.0896       0.0534        0.095       0.0742       0.0695         0.12       0.0948         0.31      0.00323\n","     33     6        0.017       0.0169     5.69e-05       0.0795        0.101       0.0726       0.0933       0.0829       0.0907        0.118        0.104        0.558      0.00581\n","     33     7       0.0147       0.0147     8.81e-06       0.0739       0.0939       0.0675       0.0868       0.0771       0.0845         0.11       0.0974        0.212      0.00221\n","     33     8         0.01         0.01     4.96e-06       0.0591       0.0774       0.0485       0.0804       0.0644       0.0617        0.102       0.0818        0.152      0.00158\n","     33     9       0.0127       0.0127      3.5e-06       0.0676       0.0873       0.0596       0.0835       0.0716       0.0756        0.107       0.0913        0.137      0.00143\n","     33    10       0.0144       0.0144     1.07e-05       0.0709       0.0928       0.0592       0.0943       0.0768       0.0761        0.119       0.0977        0.233      0.00243\n","     33    11       0.0113       0.0112     5.87e-05       0.0613       0.0819       0.0498       0.0842        0.067       0.0642        0.109       0.0866        0.568      0.00591\n","     33    12        0.011        0.011     2.41e-05       0.0617       0.0811       0.0486       0.0878       0.0682       0.0628        0.109       0.0858        0.358      0.00373\n","     33    13       0.0132       0.0131     5.71e-05       0.0683       0.0886       0.0574       0.0899       0.0737       0.0739        0.112       0.0931        0.559      0.00582\n","     33    14       0.0123       0.0123     1.27e-05       0.0667       0.0857        0.059       0.0821       0.0706       0.0741        0.105       0.0896         0.24       0.0025\n","     33    15      0.00947      0.00947     2.92e-06       0.0574       0.0753       0.0478       0.0767       0.0623       0.0614       0.0973       0.0793         0.11      0.00114\n","     33    16      0.00966      0.00965     9.61e-06       0.0588        0.076       0.0498       0.0767       0.0633       0.0638       0.0958       0.0798        0.222      0.00232\n","     33    17       0.0111        0.011      2.4e-05       0.0621       0.0813       0.0502       0.0858        0.068       0.0662        0.105       0.0857        0.338      0.00352\n","     33    18       0.0113       0.0113     4.36e-06        0.062       0.0821       0.0507       0.0848       0.0677       0.0668        0.106       0.0866        0.133      0.00138\n","     33    19       0.0113       0.0113     6.06e-06       0.0631       0.0823       0.0525       0.0843       0.0684       0.0681        0.105       0.0866        0.158      0.00165\n","     33    20      0.00992      0.00992     5.39e-07       0.0575        0.077       0.0477       0.0771       0.0624       0.0633        0.099       0.0811         0.04     0.000417\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     33     1       0.0115       0.0115     2.81e-06       0.0628        0.083       0.0512       0.0859       0.0685        0.067        0.108       0.0875          0.1      0.00104\n","     33     2       0.0112       0.0112      1.2e-06       0.0614       0.0819       0.0496       0.0851       0.0673       0.0652        0.108       0.0865       0.0683     0.000711\n","     33     3         0.01         0.01     1.83e-06       0.0587       0.0775       0.0478       0.0804       0.0641       0.0616        0.102       0.0819       0.0858     0.000894\n","     33     4       0.0109       0.0109     1.23e-06       0.0607       0.0809       0.0498       0.0824       0.0661       0.0647        0.106       0.0854       0.0749      0.00078\n","     33     5       0.0104       0.0104     1.75e-06       0.0593       0.0789       0.0488       0.0805       0.0646       0.0639        0.103       0.0832       0.0753     0.000784\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              33  128.515    0.005       0.0118     2.73e-05       0.0118       0.0643       0.0841       0.0539       0.0849       0.0694       0.0695        0.108       0.0885        0.303      0.00315\n","! Validation         33  128.515    0.005       0.0108     1.77e-06       0.0108       0.0606       0.0805       0.0494       0.0828       0.0661       0.0645        0.105       0.0849       0.0809     0.000842\n","Wall time: 128.51567819899992\n","! Best model       33    0.011\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     34     1       0.0102       0.0102     1.63e-06       0.0596       0.0782       0.0497       0.0794       0.0646        0.063        0.102       0.0826       0.0693     0.000722\n","     34     2       0.0124       0.0124     4.18e-06       0.0659       0.0861       0.0547       0.0883       0.0715       0.0697        0.112       0.0908        0.142      0.00148\n","     34     3      0.00927      0.00927     1.22e-06       0.0583       0.0745       0.0498       0.0752       0.0625       0.0622       0.0944       0.0783       0.0562     0.000586\n","     34     4       0.0113       0.0113     6.61e-06       0.0618       0.0823       0.0502        0.085       0.0676       0.0669        0.107       0.0868        0.171      0.00178\n","     34     5      0.00994      0.00994     2.09e-06       0.0578       0.0771       0.0484       0.0767       0.0626       0.0644       0.0977       0.0811       0.0904     0.000942\n","     34     6       0.0102       0.0102     3.37e-05       0.0601       0.0782       0.0511       0.0781       0.0646       0.0654       0.0989       0.0821         0.43      0.00448\n","     34     7      0.00904      0.00904     4.74e-06       0.0555       0.0735       0.0443       0.0778       0.0611       0.0575       0.0981       0.0778        0.147      0.00153\n","     34     8      0.00976      0.00975     6.81e-06       0.0583       0.0764       0.0478       0.0793       0.0636       0.0619       0.0993       0.0806        0.181      0.00189\n","     34     9      0.00992      0.00991     1.34e-06       0.0587        0.077       0.0475       0.0811       0.0643       0.0607        0.102       0.0814       0.0822     0.000857\n","     34    10      0.00897      0.00896     8.14e-06        0.056       0.0732       0.0463       0.0755       0.0609       0.0594       0.0951       0.0772        0.191      0.00199\n","     34    11       0.0105       0.0105     6.13e-06       0.0606       0.0792       0.0511       0.0795       0.0653       0.0644        0.103       0.0835        0.165      0.00172\n","     34    12       0.0102       0.0102     3.08e-06       0.0592        0.078       0.0471       0.0833       0.0652       0.0615        0.103       0.0824       0.0959     0.000999\n","     34    13       0.0106       0.0106     4.54e-07       0.0598       0.0795       0.0484       0.0825       0.0654       0.0635        0.104        0.084       0.0436     0.000454\n","     34    14       0.0106       0.0106     1.07e-06        0.059       0.0798       0.0469       0.0832        0.065       0.0616        0.107       0.0844       0.0678     0.000706\n","     34    15      0.00892      0.00892     1.34e-06       0.0566       0.0731       0.0466       0.0765       0.0615       0.0593       0.0948        0.077        0.066     0.000688\n","     34    16      0.00966      0.00966     4.03e-06       0.0571        0.076       0.0451       0.0812       0.0631        0.058        0.103       0.0805        0.137      0.00142\n","     34    17       0.0104       0.0104     2.41e-05       0.0591        0.079       0.0486         0.08       0.0643       0.0618        0.105       0.0835        0.362      0.00377\n","     34    18       0.0121       0.0121     1.15e-06       0.0653        0.085       0.0562       0.0836       0.0699       0.0715        0.107       0.0893         0.05     0.000521\n","     34    19        0.012        0.012     3.31e-06       0.0646       0.0847       0.0546       0.0845       0.0695       0.0701        0.108       0.0891        0.114      0.00119\n","     34    20      0.00991      0.00986     4.62e-05       0.0587       0.0768       0.0479       0.0802       0.0641       0.0612        0.101       0.0811        0.504      0.00525\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     34     1       0.0113       0.0113     2.72e-06       0.0622       0.0822       0.0507       0.0852       0.0679       0.0662        0.107       0.0867       0.0986      0.00103\n","     34     2        0.011        0.011     1.19e-06       0.0608       0.0812       0.0491       0.0844       0.0667       0.0644        0.107       0.0857       0.0661     0.000689\n","     34     3      0.00989      0.00989     1.79e-06       0.0582       0.0769       0.0473         0.08       0.0636        0.061        0.102       0.0813       0.0831     0.000866\n","     34     4       0.0107       0.0107     1.13e-06       0.0601       0.0802       0.0493       0.0818       0.0656        0.064        0.105       0.0847       0.0716     0.000746\n","     34     5       0.0102       0.0102     1.85e-06       0.0588       0.0782       0.0483       0.0797        0.064       0.0633        0.102       0.0824        0.076     0.000791\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              34  132.347    0.005       0.0103     8.07e-06       0.0103       0.0596       0.0785       0.0491       0.0805       0.0648       0.0633        0.102       0.0828        0.158      0.00165\n","! Validation         34  132.347    0.005       0.0106     1.74e-06       0.0106         0.06       0.0798       0.0489       0.0822       0.0656       0.0638        0.105       0.0842       0.0791     0.000824\n","Wall time: 132.3475579369997\n","! Best model       34    0.011\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     35     1      0.00981      0.00975     6.57e-05       0.0575       0.0764       0.0462       0.0803       0.0632       0.0604        0.101       0.0807        0.597      0.00622\n","     35     2      0.00997      0.00996     1.97e-06       0.0591       0.0772       0.0509       0.0756       0.0632        0.065       0.0972       0.0811       0.0818     0.000852\n","     35     3       0.0103       0.0103     2.49e-05        0.059       0.0786       0.0477       0.0816       0.0646       0.0632        0.103       0.0829        0.367      0.00382\n","     35     4      0.00981      0.00976     5.24e-05       0.0578       0.0764       0.0477        0.078       0.0628       0.0619       0.0992       0.0806        0.532      0.00554\n","     35     5       0.0115       0.0115     8.66e-06       0.0628       0.0829       0.0512        0.086       0.0686       0.0664        0.109       0.0876        0.195      0.00203\n","     35     6        0.011       0.0109     7.97e-05       0.0613       0.0807       0.0504        0.083       0.0667       0.0648        0.106       0.0852        0.661      0.00688\n","     35     7       0.0109       0.0109     1.35e-06       0.0609       0.0809       0.0502       0.0823       0.0663       0.0651        0.106       0.0854        0.075     0.000781\n","     35     8       0.0101       0.0101     1.52e-05       0.0583       0.0776       0.0471       0.0807       0.0639       0.0617        0.102        0.082        0.281      0.00293\n","     35     9       0.0103       0.0103     1.28e-06       0.0609       0.0786       0.0513       0.0801       0.0657       0.0653          0.1       0.0827       0.0814     0.000848\n","     35    10       0.0132       0.0132      2.1e-06       0.0683       0.0889       0.0577       0.0894       0.0736       0.0744        0.112       0.0935       0.0975      0.00102\n","     35    11       0.0132       0.0132      2.6e-05       0.0684       0.0888       0.0551       0.0949        0.075       0.0696        0.118       0.0939        0.363      0.00378\n","     35    12       0.0129       0.0129     1.36e-05       0.0673       0.0879       0.0563       0.0894       0.0729       0.0716        0.114       0.0927        0.253      0.00263\n","     35    13       0.0108       0.0108     1.09e-05       0.0607       0.0803       0.0497       0.0827       0.0662       0.0645        0.105       0.0847        0.239      0.00249\n","     35    14       0.0082      0.00819     6.42e-06       0.0543         0.07       0.0444       0.0741       0.0593       0.0564       0.0914       0.0739        0.183      0.00191\n","     35    15      0.00997      0.00997     1.95e-07        0.058       0.0773       0.0476       0.0787       0.0632       0.0613        0.102       0.0816       0.0293     0.000305\n","     35    16       0.0107       0.0106     6.91e-05       0.0619       0.0797       0.0523       0.0813       0.0668       0.0658        0.102       0.0839        0.611      0.00636\n","     35    17       0.0124       0.0124     1.71e-06       0.0664       0.0863       0.0585       0.0821       0.0703       0.0747        0.106       0.0902       0.0811     0.000844\n","     35    18      0.00933      0.00931     2.15e-05       0.0577       0.0746       0.0485       0.0761       0.0623       0.0615       0.0957       0.0786        0.341      0.00355\n","     35    19      0.00814      0.00808     5.74e-05       0.0535       0.0695       0.0449       0.0707       0.0578       0.0569       0.0897       0.0733        0.554      0.00578\n","     35    20       0.0108       0.0108     1.09e-05       0.0614       0.0802       0.0521       0.0802       0.0661       0.0666        0.102       0.0844        0.221      0.00231\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     35     1       0.0111       0.0111     2.76e-06       0.0616       0.0814       0.0502       0.0846       0.0674       0.0655        0.106       0.0859        0.101      0.00105\n","     35     2       0.0108       0.0108     1.11e-06       0.0603       0.0805       0.0486       0.0837       0.0662       0.0637        0.106        0.085        0.065     0.000677\n","     35     3      0.00975      0.00975     1.72e-06       0.0577       0.0764       0.0469       0.0795       0.0632       0.0604        0.101       0.0807       0.0823     0.000858\n","     35     4       0.0106       0.0106     1.22e-06       0.0596       0.0796       0.0487       0.0813        0.065       0.0633        0.105        0.084       0.0741     0.000772\n","     35     5         0.01         0.01     1.78e-06       0.0583       0.0775       0.0479       0.0791       0.0635       0.0628        0.101       0.0818       0.0737     0.000768\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              35  136.174    0.005       0.0106     2.36e-05       0.0107       0.0608       0.0798       0.0505       0.0814       0.0659        0.065        0.103       0.0841        0.292      0.00304\n","! Validation         35  136.174    0.005       0.0105     1.72e-06       0.0105       0.0595       0.0791       0.0485       0.0816       0.0651       0.0632        0.104       0.0835       0.0793     0.000826\n","Wall time: 136.17474903499988\n","! Best model       35    0.010\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     36     1       0.0112       0.0111     0.000106       0.0634       0.0815       0.0531       0.0841       0.0686       0.0689        0.102       0.0855        0.764      0.00796\n","     36     2      0.00975      0.00969     6.13e-05       0.0589       0.0761       0.0502       0.0763       0.0633       0.0641       0.0958       0.0799        0.579      0.00603\n","     36     3      0.00913      0.00913     2.27e-06       0.0564       0.0739        0.047       0.0752       0.0611       0.0599        0.096       0.0779        0.104      0.00108\n","     36     4       0.0125       0.0124     2.09e-05       0.0662       0.0863       0.0584       0.0817       0.0701       0.0755        0.104         0.09        0.329      0.00342\n","     36     5       0.0112       0.0112     2.13e-05       0.0627       0.0819       0.0544       0.0795       0.0669       0.0699        0.102       0.0858        0.333      0.00346\n","     36     6      0.00946      0.00946     3.01e-06       0.0571       0.0752       0.0457       0.0799       0.0628       0.0591          0.1       0.0796       0.0852     0.000887\n","     36     7       0.0109       0.0109     3.74e-06       0.0621       0.0806       0.0524       0.0815        0.067       0.0681        0.101       0.0846        0.116      0.00121\n","     36     8       0.0106       0.0106     3.15e-05       0.0612       0.0797       0.0501       0.0835       0.0668        0.064        0.104       0.0841        0.408      0.00425\n","     36     9      0.00897      0.00897     4.12e-06       0.0554       0.0733       0.0426       0.0808       0.0617       0.0537        0.102       0.0777        0.138      0.00143\n","     36    10      0.00885      0.00885     9.35e-07       0.0557       0.0728       0.0455       0.0761       0.0608       0.0572       0.0967        0.077        0.065     0.000677\n","     36    11      0.00848      0.00848     7.03e-07        0.054       0.0712       0.0439       0.0742       0.0591       0.0556       0.0951       0.0753       0.0469     0.000488\n","     36    12      0.00966      0.00965     7.39e-06       0.0575        0.076       0.0476       0.0773       0.0625       0.0623       0.0979       0.0801        0.195      0.00203\n","     36    13      0.00954      0.00954     1.15e-06       0.0576       0.0756       0.0468       0.0792        0.063       0.0588        0.101       0.0799       0.0594     0.000618\n","     36    14      0.00942      0.00942     3.23e-06       0.0562       0.0751       0.0448       0.0791       0.0619       0.0575        0.102       0.0795        0.104      0.00109\n","     36    15      0.00891      0.00889     1.29e-05       0.0558        0.073        0.046       0.0754       0.0607       0.0573        0.097       0.0771        0.252      0.00263\n","     36    16       0.0117       0.0117     2.51e-06       0.0627       0.0837        0.051       0.0859       0.0685       0.0677        0.109       0.0882        0.103      0.00107\n","     36    17       0.0118       0.0118      3.3e-05       0.0638        0.084       0.0533       0.0847        0.069       0.0692        0.108       0.0884        0.424      0.00441\n","     36    18       0.0103       0.0103     2.03e-05       0.0596       0.0785       0.0502       0.0784       0.0643       0.0643        0.101       0.0827        0.316      0.00329\n","     36    19       0.0089      0.00889     1.42e-05       0.0555       0.0729       0.0459       0.0749       0.0604       0.0586       0.0953        0.077        0.272      0.00284\n","     36    20       0.0105       0.0105     2.71e-06       0.0607       0.0792       0.0521       0.0778       0.0649        0.067       0.0991       0.0831        0.101      0.00106\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     36     1       0.0109       0.0109     2.76e-06       0.0611       0.0807       0.0497        0.084       0.0668       0.0649        0.106       0.0852        0.102      0.00107\n","     36     2       0.0106       0.0106     1.02e-06       0.0598       0.0798       0.0481       0.0832       0.0656       0.0631        0.106       0.0843       0.0614      0.00064\n","     36     3      0.00961      0.00961     1.72e-06       0.0573       0.0758       0.0464        0.079       0.0627       0.0598          0.1       0.0801       0.0824     0.000859\n","     36     4       0.0104       0.0104     1.22e-06       0.0591       0.0789       0.0483       0.0808       0.0645       0.0627        0.104       0.0834       0.0725     0.000755\n","     36     5      0.00988      0.00988     1.72e-06       0.0579       0.0769       0.0475       0.0786        0.063       0.0622       0.0999       0.0811         0.07     0.000729\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              36  140.000    0.005       0.0101     1.77e-05       0.0101       0.0591       0.0776       0.0491       0.0793       0.0642       0.0632          0.1       0.0818         0.24       0.0025\n","! Validation         36  140.000    0.005       0.0103     1.69e-06       0.0103        0.059       0.0785        0.048       0.0811       0.0645       0.0626        0.103       0.0829       0.0778      0.00081\n","Wall time: 140.00076468899988\n","! Best model       36    0.010\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     37     1       0.0112       0.0112     3.89e-06       0.0632        0.082       0.0525       0.0847       0.0686       0.0665        0.106       0.0864         0.14      0.00146\n","     37     2       0.0107       0.0107     2.09e-05        0.061       0.0801       0.0479       0.0871       0.0675       0.0618        0.108       0.0847        0.318      0.00332\n","     37     3       0.0103       0.0102     9.18e-06       0.0583       0.0783       0.0467       0.0816       0.0642       0.0612        0.104       0.0828         0.22      0.00229\n","     37     4       0.0105       0.0105     5.24e-06       0.0589       0.0794       0.0453        0.086       0.0656       0.0604        0.108       0.0841        0.164      0.00171\n","     37     5       0.0097      0.00965     4.89e-05       0.0571        0.076       0.0464       0.0785       0.0625       0.0606       0.0999       0.0802        0.514      0.00536\n","     37     6       0.0097      0.00968     2.32e-05       0.0574       0.0761       0.0471       0.0781       0.0626       0.0594        0.102       0.0805        0.354      0.00368\n","     37     7       0.0101       0.0101     4.82e-05       0.0585       0.0776       0.0475       0.0806       0.0641       0.0604        0.104       0.0821         0.51      0.00532\n","     37     8      0.00962      0.00951     0.000104       0.0567       0.0755       0.0464       0.0775       0.0619       0.0611        0.098       0.0796        0.749      0.00781\n","     37     9       0.0101         0.01     2.86e-05       0.0591       0.0775       0.0501       0.0771       0.0636       0.0636       0.0997       0.0817        0.396      0.00413\n","     37    10         0.01         0.01     2.44e-05        0.059       0.0774       0.0482       0.0807       0.0645        0.062        0.101       0.0817        0.361      0.00376\n","     37    11       0.0091      0.00895     0.000151       0.0561       0.0732       0.0457       0.0769       0.0613       0.0587       0.0958       0.0773        0.913      0.00951\n","     37    12       0.0101       0.0101      8.4e-06       0.0597       0.0776       0.0484       0.0825       0.0654       0.0627        0.101       0.0819        0.207      0.00215\n","     37    13      0.00878      0.00877     7.93e-06       0.0564       0.0724       0.0477       0.0738       0.0608       0.0593       0.0934       0.0763        0.175      0.00182\n","     37    14      0.00871      0.00869     1.86e-05        0.055       0.0721       0.0438       0.0773       0.0605       0.0558       0.0969       0.0763        0.306      0.00319\n","     37    15      0.00859      0.00858     1.01e-05       0.0548       0.0717       0.0447        0.075       0.0599       0.0575       0.0939       0.0757         0.22      0.00229\n","     37    16       0.0107       0.0107     9.03e-06       0.0605         0.08       0.0483        0.085       0.0667        0.063        0.106       0.0846        0.203      0.00211\n","     37    17      0.00856      0.00855     6.18e-06       0.0542       0.0715       0.0455       0.0718       0.0586       0.0591       0.0915       0.0753        0.161      0.00168\n","     37    18      0.00855      0.00854     1.34e-05       0.0546       0.0715       0.0442       0.0753       0.0598       0.0563       0.0949       0.0756        0.259       0.0027\n","     37    19      0.00932      0.00932     2.84e-06       0.0569       0.0747       0.0463       0.0781       0.0622       0.0594       0.0983       0.0789        0.105      0.00109\n","     37    20      0.00936      0.00933     3.68e-05       0.0567       0.0747       0.0458       0.0787       0.0622       0.0588       0.0992        0.079        0.448      0.00466\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     37     1       0.0107       0.0107     2.87e-06       0.0606       0.0801       0.0493       0.0834       0.0663       0.0642        0.105       0.0845        0.106       0.0011\n","     37     2       0.0105       0.0105     9.28e-07       0.0592       0.0792       0.0476       0.0825       0.0651       0.0624        0.105       0.0837        0.059     0.000614\n","     37     3      0.00948      0.00948     1.72e-06       0.0569       0.0753        0.046       0.0785       0.0623       0.0593       0.0999       0.0796       0.0821     0.000856\n","     37     4       0.0103       0.0103     1.31e-06       0.0587       0.0784       0.0478       0.0804       0.0641        0.062        0.104       0.0828       0.0755     0.000786\n","     37     5      0.00972      0.00972     1.69e-06       0.0574       0.0763       0.0471        0.078       0.0626       0.0617       0.0992       0.0804       0.0696     0.000725\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              37  143.820    0.005      0.00966     2.91e-05      0.00969       0.0577        0.076       0.0469       0.0793       0.0631       0.0604          0.1       0.0803        0.336       0.0035\n","! Validation         37  143.820    0.005       0.0101      1.7e-06       0.0101       0.0586       0.0779       0.0476       0.0806       0.0641        0.062        0.103       0.0822       0.0784     0.000816\n","Wall time: 143.82096384099987\n","! Best model       37    0.010\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     38     1       0.0086      0.00856     3.54e-05       0.0548       0.0716       0.0444       0.0756         0.06       0.0565       0.0948       0.0756        0.438      0.00456\n","     38     2       0.0102       0.0102     5.37e-06       0.0586       0.0782       0.0458       0.0842        0.065       0.0597        0.106       0.0828        0.149      0.00155\n","     38     3      0.00958      0.00955     3.47e-05       0.0579       0.0756       0.0475       0.0786        0.063       0.0602       0.0995       0.0798        0.433      0.00451\n","     38     4       0.0101         0.01     2.89e-05       0.0586       0.0775       0.0487       0.0784       0.0636       0.0637       0.0994       0.0816        0.392      0.00409\n","     38     5      0.00873      0.00873     1.38e-06       0.0556       0.0723        0.046       0.0748       0.0604        0.059       0.0934       0.0762        0.083     0.000865\n","     38     6       0.0103       0.0103     1.16e-06       0.0605       0.0786       0.0519       0.0776       0.0648       0.0664       0.0985       0.0825       0.0725     0.000755\n","     38     7      0.00859      0.00859     3.31e-06       0.0541       0.0717       0.0447       0.0728       0.0588       0.0572       0.0942       0.0757        0.133      0.00138\n","     38     8      0.00908      0.00907     1.46e-05       0.0562       0.0737       0.0458       0.0771       0.0614       0.0587       0.0969       0.0778        0.259       0.0027\n","     38     9       0.0104       0.0103      4.2e-05       0.0596       0.0785       0.0493         0.08       0.0647       0.0634        0.102       0.0829        0.465      0.00484\n","     38    10       0.0108       0.0108     1.51e-05       0.0602       0.0803       0.0483       0.0841       0.0662       0.0634        0.106       0.0849        0.279      0.00291\n","     38    11       0.0105       0.0104     5.57e-05       0.0614        0.079       0.0516       0.0812       0.0664       0.0663       0.0996        0.083         0.55      0.00573\n","     38    12      0.00996      0.00993     2.16e-05       0.0597       0.0771       0.0515       0.0763       0.0639       0.0645       0.0976        0.081        0.339      0.00354\n","     38    13       0.0105       0.0105     7.35e-07       0.0607       0.0793       0.0492       0.0838       0.0665       0.0638        0.104       0.0837        0.049     0.000511\n","     38    14      0.00941      0.00938     3.67e-05       0.0572       0.0749       0.0465       0.0788       0.0626       0.0594       0.0989       0.0792        0.447      0.00465\n","     38    15       0.0111        0.011     7.71e-06       0.0614       0.0813        0.049       0.0861       0.0676       0.0635        0.109        0.086        0.192        0.002\n","     38    16      0.00886      0.00876     0.000102       0.0549       0.0724       0.0438       0.0769       0.0604       0.0552       0.0982       0.0767         0.75      0.00781\n","     38    17      0.00866      0.00863     3.12e-05       0.0543       0.0719       0.0441       0.0748       0.0595       0.0563       0.0957        0.076         0.41      0.00427\n","     38    18      0.00954      0.00952     1.96e-05       0.0555       0.0755       0.0453        0.076       0.0606       0.0597       0.0998       0.0798        0.324      0.00337\n","     38    19      0.00819      0.00808      0.00012       0.0528       0.0695       0.0435       0.0713       0.0574       0.0552       0.0917       0.0734        0.808      0.00842\n","     38    20      0.00885      0.00882     3.02e-05       0.0558       0.0727       0.0461       0.0752       0.0606       0.0592        0.094       0.0766        0.392      0.00408\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     38     1       0.0105       0.0105     2.76e-06       0.0602       0.0794       0.0488       0.0829       0.0658       0.0636        0.104       0.0839        0.104      0.00108\n","     38     2       0.0103       0.0103     9.41e-07       0.0587       0.0785       0.0472       0.0819       0.0645       0.0618        0.104        0.083        0.059     0.000614\n","     38     3      0.00936      0.00935     1.65e-06       0.0565       0.0748       0.0456       0.0781       0.0619       0.0588       0.0994       0.0791       0.0796     0.000829\n","     38     4       0.0101       0.0101     1.23e-06       0.0582       0.0778       0.0474         0.08       0.0637       0.0614        0.103       0.0822       0.0721     0.000751\n","     38     5      0.00956      0.00956      1.7e-06        0.057       0.0756       0.0467       0.0774       0.0621       0.0612       0.0984       0.0798       0.0691      0.00072\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              38  147.641    0.005      0.00956     3.04e-05      0.00959       0.0575       0.0757       0.0471       0.0782       0.0627       0.0607       0.0991       0.0799        0.348      0.00363\n","! Validation         38  147.641    0.005      0.00997     1.66e-06      0.00998       0.0581       0.0773       0.0471       0.0801       0.0636       0.0614        0.102       0.0816       0.0767     0.000799\n","Wall time: 147.64124243300012\n","! Best model       38    0.010\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     39     1       0.0106       0.0106     8.19e-05       0.0607       0.0795       0.0502       0.0817        0.066        0.065        0.103       0.0838        0.667      0.00695\n","     39     2       0.0109       0.0109     5.89e-05       0.0612       0.0807       0.0501       0.0834       0.0668       0.0644        0.106       0.0852        0.567      0.00591\n","     39     3       0.0108       0.0108     2.12e-06       0.0608       0.0805       0.0493       0.0838       0.0665       0.0639        0.106        0.085       0.0861     0.000897\n","     39     4       0.0106       0.0104     0.000106       0.0606       0.0791       0.0492       0.0835       0.0663       0.0619        0.105       0.0836        0.764      0.00796\n","     39     5        0.009        0.009     3.58e-06       0.0556       0.0734       0.0445        0.078       0.0612       0.0574       0.0978       0.0776        0.114      0.00119\n","     39     6       0.0102       0.0102     3.54e-05       0.0592        0.078       0.0502       0.0773       0.0637       0.0657       0.0982       0.0819        0.436      0.00454\n","     39     7      0.00876      0.00876      3.2e-06       0.0554       0.0724       0.0479       0.0705       0.0592       0.0608       0.0913        0.076        0.122      0.00127\n","     39     8       0.0104       0.0104     1.39e-05       0.0602       0.0789       0.0495       0.0816       0.0655       0.0625        0.104       0.0833        0.268      0.00279\n","     39     9       0.0114       0.0113     9.07e-05       0.0625       0.0822       0.0528       0.0821       0.0674       0.0679        0.105       0.0865        0.706      0.00735\n","     39    10       0.0104       0.0103     5.34e-05       0.0614       0.0785       0.0537       0.0768       0.0653       0.0678       0.0964       0.0821        0.542      0.00564\n","     39    11       0.0104       0.0104     2.78e-05       0.0606       0.0787       0.0514        0.079       0.0652       0.0665       0.0988       0.0826        0.381      0.00397\n","     39    12      0.00735      0.00735     2.08e-06       0.0501       0.0663        0.041       0.0683       0.0546       0.0522       0.0879       0.0701       0.0961        0.001\n","     39    13       0.0102       0.0101     3.79e-05       0.0588       0.0778       0.0495       0.0774       0.0635       0.0639          0.1       0.0819        0.446      0.00464\n","     39    14         0.01         0.01     2.11e-05       0.0594       0.0775       0.0489       0.0805       0.0647       0.0626        0.101       0.0817        0.314      0.00327\n","     39    15       0.0097      0.00961     9.43e-05       0.0572       0.0758       0.0453       0.0809       0.0631       0.0588        0.102       0.0802         0.72       0.0075\n","     39    16      0.00783      0.00776     7.14e-05       0.0518       0.0681       0.0424       0.0707       0.0565       0.0544       0.0895       0.0719         0.62      0.00645\n","     39    17       0.0106       0.0106     4.48e-05       0.0611       0.0796       0.0516       0.0802       0.0659        0.066        0.101       0.0837        0.487      0.00507\n","     39    18         0.01      0.00991     0.000123       0.0577        0.077       0.0465       0.0799       0.0632       0.0602        0.103       0.0814        0.824      0.00858\n","     39    19      0.00945      0.00943     2.47e-05        0.057       0.0751       0.0458       0.0794       0.0626       0.0596       0.0991       0.0794         0.35      0.00364\n","     39    20      0.00987       0.0098      6.5e-05        0.058       0.0766       0.0472       0.0795       0.0633       0.0609        0.101       0.0809        0.593      0.00618\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     39     1       0.0104       0.0104     2.75e-06       0.0597       0.0788       0.0484       0.0823       0.0653       0.0629        0.103       0.0832        0.103      0.00107\n","     39     2       0.0102       0.0102     9.65e-07       0.0583        0.078       0.0468       0.0814       0.0641       0.0613        0.104       0.0825       0.0589     0.000613\n","     39     3      0.00924      0.00924     1.63e-06       0.0561       0.0744       0.0452       0.0778       0.0615       0.0583       0.0989       0.0786         0.08     0.000833\n","     39     4      0.00997      0.00997      1.2e-06       0.0578       0.0772       0.0469       0.0796       0.0632       0.0608        0.102       0.0816       0.0726     0.000756\n","     39     5      0.00941      0.00941     1.71e-06       0.0565       0.0751       0.0463        0.077       0.0616       0.0607       0.0977       0.0792       0.0695     0.000724\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              39  151.472    0.005      0.00988     4.81e-05      0.00993       0.0585       0.0769       0.0483       0.0787       0.0635       0.0623       0.0999       0.0811        0.455      0.00474\n","! Validation         39  151.472    0.005      0.00983     1.65e-06      0.00983       0.0577       0.0767       0.0467       0.0796       0.0632       0.0608        0.101        0.081       0.0768       0.0008\n","Wall time: 151.47307099699992\n","! Best model       39    0.010\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     40     1      0.00887      0.00881     5.89e-05       0.0558       0.0726       0.0456       0.0761       0.0608       0.0577       0.0957       0.0767        0.564      0.00588\n","     40     2      0.00922      0.00921     1.02e-05       0.0558       0.0743       0.0454       0.0765       0.0609       0.0583       0.0987       0.0785        0.222      0.00231\n","     40     3       0.0102       0.0101      2.6e-05       0.0582       0.0779       0.0464       0.0816        0.064       0.0605        0.104       0.0824        0.376      0.00392\n","     40     4         0.01         0.01     9.61e-07       0.0607       0.0775       0.0528       0.0765       0.0646       0.0669       0.0954       0.0811       0.0725     0.000755\n","     40     5       0.0148       0.0148     1.82e-06       0.0753       0.0941        0.069       0.0878       0.0784       0.0857        0.109       0.0973       0.0881     0.000918\n","     40     6       0.0148       0.0148      1.6e-05       0.0725       0.0941       0.0594       0.0988       0.0791       0.0768        0.121       0.0991        0.291      0.00303\n","     40     7       0.0117       0.0116     9.46e-05       0.0635       0.0834       0.0522       0.0863       0.0692       0.0661         0.11       0.0881        0.717      0.00746\n","     40     8      0.00988      0.00987     4.25e-06       0.0584       0.0769       0.0488       0.0777       0.0632       0.0628       0.0992        0.081        0.125      0.00131\n","     40     9       0.0104       0.0103     7.53e-05       0.0588       0.0786       0.0477       0.0809       0.0643       0.0623        0.104        0.083        0.643      0.00669\n","     40    10      0.00978      0.00969     9.75e-05       0.0569       0.0761       0.0449       0.0809       0.0629       0.0591        0.102       0.0806        0.731      0.00762\n","     40    11      0.00984      0.00984     6.09e-06       0.0574       0.0767       0.0456       0.0811       0.0633       0.0595        0.103       0.0812        0.177      0.00184\n","     40    12      0.00998      0.00989     8.82e-05       0.0582       0.0769       0.0468       0.0811       0.0639       0.0605        0.102       0.0813        0.696      0.00725\n","     40    13       0.0115       0.0115     4.33e-05       0.0655       0.0829       0.0607        0.075       0.0678       0.0762        0.095       0.0856        0.483      0.00503\n","     40    14       0.0111       0.0111     1.23e-05       0.0625       0.0815       0.0543       0.0789       0.0666       0.0696        0.101       0.0854        0.245      0.00255\n","     40    15       0.0104       0.0104     3.75e-06       0.0593       0.0791       0.0467       0.0846       0.0656       0.0592        0.108       0.0838        0.139      0.00145\n","     40    16      0.00892       0.0089      2.5e-05       0.0556        0.073       0.0438       0.0793       0.0615       0.0551       0.0995       0.0773        0.341      0.00355\n","     40    17      0.00914      0.00914      3.3e-06       0.0563        0.074        0.046        0.077       0.0615       0.0592        0.097       0.0781       0.0912      0.00095\n","     40    18      0.00933      0.00932      5.1e-06       0.0573       0.0747       0.0463       0.0793       0.0628       0.0594       0.0984       0.0789        0.146      0.00152\n","     40    19       0.0095       0.0095     5.85e-06        0.058       0.0754         0.05        0.074        0.062       0.0627       0.0958       0.0793        0.175      0.00182\n","     40    20      0.00943      0.00943     2.31e-06       0.0581       0.0751       0.0492       0.0759       0.0626       0.0627       0.0953        0.079        0.111      0.00115\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     40     1       0.0102       0.0102     2.67e-06       0.0593       0.0782       0.0479        0.082       0.0649       0.0624        0.103       0.0826        0.102      0.00106\n","     40     2         0.01         0.01     9.38e-07       0.0579       0.0774       0.0463       0.0809       0.0636       0.0607        0.103       0.0819       0.0574     0.000598\n","     40     3      0.00913      0.00913     1.65e-06       0.0557       0.0739       0.0449       0.0774       0.0611       0.0579       0.0985       0.0782       0.0791     0.000824\n","     40     4      0.00984      0.00984      1.2e-06       0.0574       0.0767       0.0465       0.0792       0.0628       0.0602        0.102       0.0811       0.0731     0.000762\n","     40     5      0.00928      0.00928      1.7e-06       0.0562       0.0745        0.046       0.0765       0.0612       0.0602       0.0971       0.0786       0.0684     0.000712\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              40  155.286    0.005       0.0104      2.9e-05       0.0104       0.0602        0.079       0.0501       0.0805       0.0653       0.0645        0.102       0.0832        0.322      0.00335\n","! Validation         40  155.286    0.005       0.0097     1.63e-06       0.0097       0.0573       0.0762       0.0463       0.0792       0.0628       0.0603        0.101       0.0805        0.076     0.000792\n","Wall time: 155.28700739800024\n","! Best model       40    0.010\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     41     1      0.00965      0.00961     3.06e-05       0.0577       0.0759       0.0462       0.0806       0.0634       0.0592        0.101       0.0802        0.408      0.00425\n","     41     2      0.00915      0.00914     1.73e-05       0.0561        0.074       0.0455       0.0772       0.0613       0.0586       0.0977       0.0781        0.291      0.00304\n","     41     3      0.00833      0.00829     3.75e-05       0.0536       0.0705       0.0437       0.0735       0.0586       0.0553       0.0937       0.0745        0.452      0.00471\n","     41     4      0.00947      0.00947     4.91e-07       0.0589       0.0753       0.0511       0.0745       0.0628       0.0641       0.0937       0.0789       0.0469     0.000488\n","     41     5       0.0106       0.0106     1.72e-06       0.0619       0.0798       0.0532       0.0793       0.0663       0.0666        0.101       0.0838       0.0859     0.000895\n","     41     6      0.00887      0.00885     1.94e-05       0.0546       0.0728       0.0435       0.0768       0.0602       0.0556       0.0986       0.0771        0.325      0.00338\n","     41     7      0.00806      0.00806      2.3e-06       0.0532       0.0695       0.0429       0.0738       0.0584        0.055       0.0918       0.0734        0.102      0.00106\n","     41     8      0.00844      0.00844     4.43e-06        0.054       0.0711       0.0453       0.0712       0.0583       0.0584       0.0913       0.0748        0.139      0.00145\n","     41     9      0.00853      0.00853     1.12e-06        0.054       0.0715       0.0446       0.0728       0.0587       0.0567       0.0943       0.0755       0.0699     0.000728\n","     41    10      0.00922      0.00922        6e-06       0.0569       0.0743       0.0481       0.0747       0.0614       0.0617       0.0946       0.0781         0.16      0.00167\n","     41    11      0.00946      0.00944     1.12e-05       0.0552       0.0752       0.0424       0.0809       0.0617       0.0557        0.104       0.0797        0.238      0.00248\n","     41    12         0.01         0.01     3.63e-05       0.0586       0.0774        0.049       0.0779       0.0634       0.0633       0.0998       0.0815        0.445      0.00464\n","     41    13      0.00905      0.00905     1.05e-06       0.0557       0.0736       0.0457       0.0757       0.0607       0.0601        0.095       0.0776       0.0539     0.000562\n","     41    14       0.0087      0.00868     1.41e-05       0.0541       0.0721       0.0419       0.0784       0.0602       0.0531       0.0998       0.0764        0.263      0.00274\n","     41    15      0.00866      0.00866     7.42e-07        0.055        0.072       0.0447       0.0756       0.0601       0.0566       0.0956       0.0761        0.058     0.000604\n","     41    16       0.0139       0.0139      1.8e-06       0.0692       0.0913       0.0581       0.0914       0.0747       0.0756        0.116        0.096       0.0883      0.00092\n","     41    17       0.0145       0.0144     0.000122       0.0734       0.0929       0.0649       0.0903       0.0776       0.0818        0.112       0.0968        0.813      0.00847\n","     41    18       0.0127       0.0127      2.2e-06       0.0694       0.0873       0.0617       0.0847       0.0732       0.0766        0.105        0.091       0.0867     0.000903\n","     41    19         0.01      0.00998     5.04e-05       0.0593       0.0773       0.0478       0.0822        0.065       0.0602        0.103       0.0817        0.524      0.00546\n","     41    20      0.00952      0.00941     0.000118       0.0562        0.075       0.0461       0.0763       0.0612       0.0597       0.0988       0.0793        0.804      0.00837\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     41     1       0.0101       0.0101     2.74e-06       0.0589       0.0777       0.0475       0.0816       0.0646       0.0618        0.102       0.0821        0.103      0.00108\n","     41     2       0.0099       0.0099      9.2e-07       0.0575        0.077        0.046       0.0805       0.0632       0.0602        0.103       0.0814       0.0559     0.000582\n","     41     3      0.00904      0.00904     1.58e-06       0.0554       0.0736       0.0446       0.0771       0.0608       0.0575       0.0981       0.0778       0.0785     0.000818\n","     41     4      0.00972      0.00972     1.23e-06        0.057       0.0763       0.0461       0.0789       0.0625       0.0597        0.102       0.0806       0.0733     0.000764\n","     41     5      0.00916      0.00916     1.71e-06       0.0558        0.074       0.0456       0.0761       0.0609       0.0597       0.0965       0.0781       0.0694     0.000723\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              41  159.112    0.005      0.00983     2.39e-05      0.00985       0.0583       0.0767       0.0483       0.0784       0.0634       0.0622       0.0996       0.0809        0.273      0.00284\n","! Validation         41  159.112    0.005      0.00958     1.64e-06      0.00958       0.0569       0.0757        0.046       0.0788       0.0624       0.0598          0.1         0.08       0.0761     0.000792\n","Wall time: 159.11300975599988\n","! Best model       41    0.010\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     42     1       0.0105       0.0105     2.56e-05       0.0591       0.0792       0.0485       0.0803       0.0644       0.0641        0.103       0.0835        0.364      0.00379\n","     42     2       0.0101      0.00981     0.000283       0.0576       0.0766       0.0463       0.0801       0.0632       0.0599        0.102        0.081         1.25        0.013\n","     42     3        0.012        0.012     1.33e-05       0.0664       0.0847       0.0594       0.0805         0.07       0.0751        0.101       0.0881         0.26       0.0027\n","     42     4       0.0123       0.0123      7.7e-05       0.0675       0.0857       0.0615       0.0795       0.0705       0.0763        0.102       0.0891         0.65      0.00677\n","     42     5       0.0112        0.011      0.00019       0.0619       0.0812       0.0504       0.0849       0.0677       0.0646        0.107       0.0858         1.02       0.0106\n","     42     6      0.00962       0.0096     1.83e-05       0.0573       0.0758       0.0463       0.0791       0.0627         0.06          0.1       0.0801        0.315      0.00328\n","     42     7      0.00826      0.00815     0.000108       0.0532       0.0699       0.0428       0.0738       0.0583       0.0542       0.0936       0.0739         0.77      0.00802\n","     42     8      0.00892      0.00892     7.04e-07       0.0545       0.0731       0.0429       0.0776       0.0603       0.0549       0.0999       0.0774       0.0543     0.000566\n","     42     9      0.00886      0.00879      6.8e-05       0.0542       0.0725       0.0443        0.074       0.0591       0.0585       0.0946       0.0765        0.609      0.00635\n","     42    10      0.00802      0.00799        3e-05       0.0525       0.0692       0.0419       0.0736       0.0578       0.0536       0.0928       0.0732        0.402      0.00418\n","     42    11      0.00828      0.00825     2.95e-05       0.0537       0.0703       0.0444       0.0721       0.0583       0.0563        0.092       0.0742        0.401      0.00417\n","     42    12      0.00902      0.00893     8.81e-05       0.0554       0.0731       0.0444       0.0776        0.061       0.0569       0.0978       0.0773        0.681      0.00709\n","     42    13      0.00855      0.00855     2.06e-06       0.0542       0.0715       0.0436       0.0753       0.0595       0.0562       0.0951       0.0756          0.1      0.00104\n","     42    14      0.00853       0.0085     2.99e-05       0.0539       0.0713       0.0439       0.0739       0.0589       0.0562       0.0945       0.0754        0.398      0.00415\n","     42    15      0.00886      0.00884     1.95e-05       0.0545       0.0727       0.0435       0.0764         0.06        0.057       0.0967       0.0769        0.322      0.00335\n","     42    16      0.00856      0.00855     9.05e-07       0.0543       0.0716       0.0417       0.0794       0.0606       0.0527        0.099       0.0759       0.0643     0.000669\n","     42    17       0.0102       0.0102     4.05e-05       0.0602        0.078       0.0505       0.0794        0.065       0.0641          0.1       0.0821        0.469      0.00489\n","     42    18       0.0133       0.0132      1.9e-05       0.0696        0.089        0.061       0.0867       0.0738       0.0768        0.109       0.0931        0.313      0.00326\n","     42    19       0.0131       0.0131     1.77e-06       0.0699       0.0884       0.0617       0.0862       0.0739       0.0759        0.109       0.0925       0.0936     0.000975\n","     42    20       0.0097       0.0097     5.41e-07       0.0581       0.0762       0.0478       0.0787       0.0633       0.0606          0.1       0.0805       0.0422     0.000439\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     42     1      0.00996      0.00996     2.83e-06       0.0585       0.0772       0.0472       0.0812       0.0642       0.0613        0.102       0.0816        0.104      0.00109\n","     42     2      0.00978      0.00978     9.04e-07       0.0571       0.0765       0.0457         0.08       0.0628       0.0598        0.102       0.0809       0.0558     0.000581\n","     42     3      0.00895      0.00895     1.63e-06       0.0551       0.0732       0.0443       0.0768       0.0605       0.0571       0.0977       0.0774       0.0801     0.000834\n","     42     4       0.0096       0.0096     1.29e-06       0.0567       0.0758       0.0457       0.0786       0.0621       0.0592        0.101       0.0802        0.074     0.000771\n","     42     5      0.00904      0.00904     1.65e-06       0.0555       0.0736       0.0453       0.0758       0.0606       0.0593       0.0959       0.0776       0.0688     0.000716\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              42  162.922    0.005      0.00984     5.23e-05      0.00989       0.0584       0.0767       0.0484       0.0785       0.0634       0.0622       0.0997       0.0809        0.429      0.00447\n","! Validation         42  162.922    0.005      0.00947     1.66e-06      0.00947       0.0566       0.0753       0.0456       0.0785        0.062       0.0593       0.0998       0.0796       0.0766     0.000798\n","Wall time: 162.9231094050001\n","! Best model       42    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     43     1      0.00831      0.00831     2.13e-06       0.0524       0.0705       0.0418       0.0738       0.0578       0.0548       0.0944       0.0746        0.103      0.00107\n","     43     2       0.0123       0.0123     1.33e-05       0.0651       0.0858       0.0566       0.0821       0.0693       0.0739        0.106       0.0898        0.258      0.00268\n","     43     3        0.015       0.0149     5.65e-05       0.0748       0.0945       0.0661       0.0921       0.0791       0.0819        0.116       0.0988        0.549      0.00572\n","     43     4       0.0141       0.0141     3.72e-06       0.0722       0.0919       0.0628       0.0912        0.077       0.0786        0.114       0.0963        0.138      0.00143\n","     43     5      0.00988      0.00985     3.01e-05       0.0588       0.0768       0.0493       0.0777       0.0635       0.0629       0.0988       0.0809        0.403       0.0042\n","     43     6       0.0085      0.00849      5.9e-06       0.0532       0.0713       0.0417       0.0762       0.0589       0.0537       0.0973       0.0755        0.158      0.00164\n","     43     7      0.00711      0.00706     4.83e-05       0.0491        0.065       0.0393       0.0688        0.054       0.0494       0.0884       0.0689        0.514      0.00535\n","     43     8      0.00859      0.00859     9.58e-06       0.0542       0.0717       0.0442        0.074       0.0591       0.0574       0.0939       0.0757        0.207      0.00216\n","     43     9      0.00877      0.00877     2.81e-06       0.0548       0.0724       0.0447       0.0752       0.0599       0.0579        0.095       0.0765       0.0873     0.000909\n","     43    10      0.00804      0.00803     1.13e-05       0.0524       0.0693       0.0422        0.073       0.0576       0.0541       0.0925       0.0733        0.244      0.00254\n","     43    11      0.00869      0.00868      1.9e-06        0.055       0.0721       0.0449       0.0751         0.06       0.0573        0.095       0.0761       0.0832     0.000867\n","     43    12      0.00831      0.00829     2.15e-05       0.0528       0.0704       0.0429       0.0726       0.0577       0.0551       0.0939       0.0745        0.342      0.00356\n","     43    13      0.00949      0.00949     4.83e-06       0.0567       0.0753       0.0463       0.0776        0.062       0.0601       0.0991       0.0796        0.152      0.00159\n","     43    14      0.00825      0.00816     8.34e-05       0.0532       0.0699       0.0438       0.0721       0.0579       0.0554       0.0923       0.0738        0.671      0.00699\n","     43    15      0.00933      0.00926     6.24e-05       0.0562       0.0745       0.0456       0.0773       0.0615       0.0593        0.098       0.0786        0.582      0.00606\n","     43    16      0.00807      0.00801     5.91e-05       0.0519       0.0692       0.0425       0.0708       0.0566       0.0545       0.0919       0.0732        0.569      0.00592\n","     43    17      0.00878      0.00861      0.00017       0.0544       0.0718       0.0446       0.0741       0.0594       0.0568        0.095       0.0759        0.966       0.0101\n","     43    18      0.00821      0.00821      4.9e-06       0.0529       0.0701       0.0419       0.0748       0.0584       0.0535        0.095       0.0742        0.156      0.00162\n","     43    19      0.00892      0.00887      5.5e-05       0.0553       0.0728       0.0435       0.0789       0.0612        0.055       0.0993       0.0772        0.549      0.00572\n","     43    20      0.00872      0.00867     5.48e-05       0.0548        0.072       0.0441       0.0763       0.0602       0.0563       0.0961       0.0762        0.547       0.0057\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     43     1      0.00984      0.00984     2.76e-06       0.0581       0.0767       0.0468       0.0808       0.0638       0.0608        0.101       0.0811        0.104      0.00108\n","     43     2      0.00968      0.00967     9.17e-07       0.0568       0.0761       0.0453       0.0796       0.0625       0.0593        0.102       0.0805       0.0554     0.000577\n","     43     3      0.00886      0.00886     1.61e-06       0.0548       0.0728        0.044       0.0765       0.0602       0.0567       0.0973        0.077       0.0787      0.00082\n","     43     4      0.00949      0.00949     1.21e-06       0.0563       0.0754       0.0453       0.0782       0.0618       0.0587        0.101       0.0797       0.0728     0.000758\n","     43     5      0.00893      0.00893     1.65e-06       0.0552       0.0731        0.045       0.0754       0.0602       0.0589       0.0954       0.0771       0.0676     0.000704\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              43  166.727    0.005      0.00933     3.51e-05      0.00937       0.0565       0.0747       0.0464       0.0767       0.0616         0.06       0.0978       0.0789        0.364      0.00379\n","! Validation         43  166.727    0.005      0.00936     1.63e-06      0.00936       0.0562       0.0748       0.0453       0.0781       0.0617       0.0589       0.0993       0.0791       0.0757     0.000789\n","Wall time: 166.7280538490004\n","! Best model       43    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     44     1      0.00792      0.00791     5.01e-06       0.0521       0.0688        0.042       0.0723       0.0572        0.054       0.0915       0.0728        0.161      0.00167\n","     44     2       0.0093      0.00927     2.26e-05       0.0558       0.0745       0.0457        0.076       0.0608         0.06       0.0972       0.0786        0.345      0.00359\n","     44     3      0.00923      0.00923     1.13e-07       0.0571       0.0743       0.0472       0.0769       0.0621         0.06       0.0968       0.0784       0.0207     0.000216\n","     44     4      0.00878      0.00872     5.51e-05       0.0547       0.0723       0.0435        0.077       0.0603       0.0567       0.0961       0.0764        0.541      0.00563\n","     44     5      0.00822      0.00822        3e-06       0.0531       0.0701       0.0425       0.0744       0.0585       0.0545       0.0939       0.0742        0.114      0.00119\n","     44     6       0.0103       0.0103     7.98e-05       0.0598       0.0784       0.0501       0.0792       0.0646       0.0649          0.1       0.0825        0.658      0.00686\n","     44     7      0.00921      0.00916     4.37e-05       0.0568       0.0741       0.0471       0.0762       0.0617         0.06       0.0961       0.0781        0.487      0.00507\n","     44     8      0.00946      0.00946     2.66e-06       0.0583       0.0752         0.05       0.0747       0.0624       0.0628       0.0953       0.0791       0.0955     0.000995\n","     44     9      0.00875      0.00865     0.000106        0.054       0.0719       0.0428       0.0765       0.0597       0.0545       0.0979       0.0762        0.757      0.00788\n","     44    10      0.00892      0.00888      4.1e-05       0.0551       0.0729       0.0436        0.078       0.0608       0.0556       0.0987       0.0772        0.462      0.00481\n","     44    11      0.00934      0.00934     1.82e-06       0.0556       0.0748        0.044       0.0788       0.0614       0.0573        0.101       0.0792       0.0945     0.000985\n","     44    12       0.0114       0.0114     9.27e-06       0.0635       0.0827        0.051       0.0884       0.0697       0.0646         0.11       0.0875        0.219      0.00228\n","     44    13       0.0117       0.0116     0.000117       0.0648       0.0834       0.0568       0.0806       0.0687       0.0713        0.103       0.0873        0.804      0.00838\n","     44    14       0.0108       0.0108     1.22e-05       0.0622       0.0803       0.0544        0.078       0.0662         0.07       0.0979       0.0839        0.239      0.00249\n","     44    15      0.00868      0.00863     4.33e-05       0.0543       0.0719       0.0436       0.0758       0.0597       0.0555       0.0966       0.0761        0.481      0.00502\n","     44    16      0.00952      0.00952     5.23e-07       0.0575       0.0755       0.0488       0.0749       0.0618       0.0626       0.0962       0.0794       0.0475     0.000494\n","     44    17       0.0103       0.0103     6.48e-07       0.0602       0.0786        0.052       0.0766       0.0643       0.0659       0.0992       0.0825       0.0523     0.000545\n","     44    18       0.0089      0.00889     4.68e-06       0.0553        0.073       0.0463       0.0734       0.0598       0.0583       0.0957        0.077        0.155      0.00161\n","     44    19      0.00855      0.00855     2.54e-06       0.0536       0.0715       0.0426       0.0756       0.0591       0.0543       0.0972       0.0758       0.0875     0.000911\n","     44    20      0.00997      0.00996     6.39e-06       0.0578       0.0772       0.0468       0.0798       0.0633       0.0614        0.102       0.0816        0.163       0.0017\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     44     1      0.00972      0.00972     2.69e-06       0.0578       0.0763       0.0465       0.0804       0.0634       0.0604        0.101       0.0806        0.102      0.00107\n","     44     2      0.00957      0.00957        9e-07       0.0564       0.0757        0.045       0.0793       0.0621       0.0589        0.101         0.08       0.0542     0.000565\n","     44     3      0.00877      0.00877     1.57e-06       0.0545       0.0725       0.0437       0.0762       0.0599       0.0563        0.097       0.0767       0.0787      0.00082\n","     44     4      0.00939      0.00939     1.21e-06        0.056        0.075        0.045       0.0779       0.0615       0.0582          0.1       0.0793       0.0729     0.000759\n","     44     5      0.00883      0.00883      1.6e-06       0.0549       0.0727       0.0448       0.0751       0.0599       0.0585        0.095       0.0767       0.0672       0.0007\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              44  170.548    0.005      0.00944     2.79e-05      0.00947       0.0571       0.0752        0.047       0.0772       0.0621       0.0604       0.0982       0.0793        0.299      0.00312\n","! Validation         44  170.548    0.005      0.00926     1.59e-06      0.00926       0.0559       0.0744        0.045       0.0778       0.0614       0.0585       0.0989       0.0787        0.075     0.000782\n","Wall time: 170.54899106500034\n","! Best model       44    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     45     1      0.00941       0.0094     1.16e-05       0.0575        0.075       0.0478       0.0769       0.0624       0.0603        0.098       0.0791        0.235      0.00245\n","     45     2       0.0103       0.0103     2.29e-06       0.0606       0.0787       0.0516       0.0786       0.0651       0.0659       0.0995       0.0827       0.0881     0.000918\n","     45     3      0.00888      0.00888     1.34e-06       0.0554       0.0729       0.0462       0.0737       0.0599       0.0585       0.0954       0.0769       0.0695     0.000724\n","     45     4       0.0104       0.0104     4.83e-06         0.06        0.079       0.0507       0.0785       0.0646       0.0654        0.101       0.0831        0.111      0.00116\n","     45     5       0.0146       0.0146     1.99e-05       0.0741       0.0934       0.0669       0.0883       0.0776       0.0828        0.112       0.0972        0.321      0.00334\n","     45     6        0.013       0.0129     1.25e-05       0.0679        0.088       0.0574       0.0888       0.0731       0.0722        0.113       0.0927        0.253      0.00263\n","     45     7      0.00904      0.00904     6.11e-07       0.0564       0.0735        0.045       0.0791       0.0621       0.0571       0.0985       0.0778       0.0553     0.000576\n","     45     8      0.00731      0.00731     2.75e-06       0.0497       0.0661       0.0393       0.0704       0.0548       0.0499       0.0903       0.0701        0.112      0.00117\n","     45     9      0.00948      0.00947     6.49e-06        0.057       0.0753       0.0463       0.0782       0.0623       0.0591          0.1       0.0796        0.184      0.00191\n","     45    10       0.0111       0.0111     1.24e-06       0.0633       0.0817       0.0534        0.083       0.0682       0.0673        0.105        0.086       0.0678     0.000706\n","     45    11       0.0118       0.0118     2.71e-05       0.0657       0.0841       0.0583       0.0804       0.0694       0.0743        0.101       0.0876        0.384        0.004\n","     45    12      0.00903      0.00903     3.92e-06       0.0558       0.0735       0.0436       0.0803        0.062       0.0556          0.1       0.0779        0.118      0.00122\n","     45    13      0.00875      0.00871     3.08e-05       0.0544       0.0722       0.0436       0.0761       0.0598       0.0575       0.0951       0.0763        0.398      0.00415\n","     45    14      0.00959      0.00957     2.14e-05        0.057       0.0757       0.0464       0.0782       0.0623       0.0594        0.101         0.08        0.337      0.00351\n","     45    15      0.00932      0.00931     5.69e-06       0.0565       0.0746       0.0455       0.0785        0.062       0.0588       0.0991       0.0789        0.168      0.00175\n","     45    16       0.0105       0.0104     8.45e-05       0.0595        0.079       0.0485       0.0816        0.065       0.0636        0.103       0.0834        0.679      0.00708\n","     45    17      0.00821      0.00814     6.38e-05       0.0528       0.0698       0.0423        0.074       0.0581        0.053       0.0948       0.0739        0.593      0.00617\n","     45    18       0.0103       0.0103     2.69e-05       0.0602       0.0784       0.0537       0.0732       0.0635       0.0687       0.0948       0.0817        0.382      0.00398\n","     45    19      0.00921      0.00913      8.5e-05       0.0564       0.0739       0.0466        0.076       0.0613       0.0597       0.0963        0.078        0.675      0.00704\n","     45    20      0.00996      0.00996      6.5e-06        0.059       0.0772       0.0482       0.0806       0.0644        0.062        0.101       0.0815        0.163       0.0017\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     45     1      0.00961      0.00961     2.74e-06       0.0574       0.0758       0.0461       0.0801       0.0631       0.0599          0.1       0.0801        0.103      0.00107\n","     45     2      0.00948      0.00947     9.07e-07       0.0561       0.0753       0.0447       0.0789       0.0618       0.0585        0.101       0.0797       0.0537     0.000559\n","     45     3       0.0087       0.0087     1.56e-06       0.0543       0.0721       0.0434       0.0759       0.0597        0.056       0.0967       0.0763       0.0776     0.000809\n","     45     4      0.00929      0.00929     1.26e-06       0.0557       0.0746       0.0447       0.0777       0.0612       0.0578          0.1       0.0789       0.0743     0.000774\n","     45     5      0.00873      0.00872     1.58e-06       0.0546       0.0723       0.0444       0.0748       0.0596        0.058       0.0945       0.0763       0.0663     0.000691\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              45  174.352    0.005      0.00999      2.1e-05         0.01        0.059       0.0773       0.0491       0.0787       0.0639        0.063          0.1       0.0815         0.27      0.00281\n","! Validation         45  174.352    0.005      0.00916     1.61e-06      0.00916       0.0556        0.074       0.0447       0.0775       0.0611       0.0581       0.0985       0.0783        0.075     0.000781\n","Wall time: 174.35246231600013\n","! Best model       45    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     46     1      0.00805      0.00794     0.000108       0.0519       0.0689       0.0413       0.0731       0.0572       0.0534       0.0925       0.0729        0.767      0.00799\n","     46     2       0.0114       0.0113     8.84e-05       0.0623       0.0824       0.0508       0.0854       0.0681        0.065        0.109       0.0871        0.693      0.00722\n","     46     3       0.0109       0.0108     9.56e-05       0.0614       0.0802       0.0517       0.0807       0.0662       0.0668        0.102       0.0844        0.719      0.00749\n","     46     4      0.00976      0.00968     8.34e-05       0.0596       0.0761       0.0533       0.0723       0.0628       0.0665       0.0923       0.0794        0.675      0.00703\n","     46     5       0.0093       0.0093     4.03e-07        0.057       0.0746       0.0456         0.08       0.0628        0.058       0.0998       0.0789       0.0395     0.000411\n","     46     6      0.00816      0.00804     0.000114       0.0537       0.0694       0.0431       0.0749        0.059       0.0543       0.0925       0.0734        0.794      0.00827\n","     46     7      0.00886      0.00885     9.24e-06       0.0552       0.0728        0.045       0.0755       0.0603       0.0581       0.0957       0.0769        0.219      0.00228\n","     46     8      0.00994      0.00983     0.000105       0.0579       0.0767       0.0478       0.0783        0.063       0.0618          0.1       0.0809        0.761      0.00792\n","     46     9         0.01         0.01      7.4e-06       0.0607       0.0774       0.0524       0.0774       0.0649        0.066       0.0964       0.0812        0.179      0.00187\n","     46    10       0.0106       0.0106     2.45e-05       0.0607       0.0795       0.0512       0.0797       0.0654       0.0656        0.102       0.0837        0.365       0.0038\n","     46    11      0.00784      0.00775     9.05e-05       0.0516       0.0681       0.0392       0.0765       0.0578       0.0499       0.0945       0.0722          0.7       0.0073\n","     46    12      0.00723      0.00723     1.09e-06       0.0488       0.0658        0.039       0.0683       0.0537       0.0498       0.0896       0.0697       0.0691      0.00072\n","     46    13         0.01      0.00993     6.98e-05       0.0577       0.0771       0.0458       0.0815       0.0637       0.0593        0.104       0.0816        0.615      0.00641\n","     46    14      0.00728      0.00728     3.39e-06       0.0507        0.066       0.0412       0.0698       0.0555       0.0522       0.0873       0.0698        0.112      0.00117\n","     46    15      0.00924      0.00923     6.25e-06       0.0566       0.0743       0.0474        0.075       0.0612       0.0617       0.0947       0.0782        0.158      0.00164\n","     46    16      0.00874      0.00874     1.01e-06       0.0546       0.0723       0.0445       0.0747       0.0596       0.0563       0.0967       0.0765       0.0617     0.000643\n","     46    17      0.00974      0.00973     4.84e-06       0.0584       0.0763       0.0486        0.078       0.0633        0.062        0.099       0.0805         0.16      0.00167\n","     46    18       0.0101       0.0101     7.46e-06       0.0594       0.0776       0.0486        0.081       0.0648       0.0625        0.101       0.0819         0.17      0.00177\n","     46    19      0.00961      0.00956     4.22e-05       0.0571       0.0757       0.0456       0.0803       0.0629       0.0585        0.102       0.0801        0.464      0.00483\n","     46    20       0.0084      0.00839     9.73e-06       0.0536       0.0709        0.043       0.0749       0.0589       0.0557       0.0942       0.0749        0.218      0.00227\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     46     1      0.00951       0.0095     2.69e-06       0.0571       0.0754       0.0458       0.0797       0.0627       0.0595       0.0999       0.0797        0.102      0.00106\n","     46     2      0.00939      0.00939     9.79e-07       0.0559        0.075       0.0445       0.0787       0.0616       0.0582          0.1       0.0793       0.0559     0.000582\n","     46     3      0.00863      0.00863     1.53e-06        0.054       0.0719       0.0432       0.0757       0.0594       0.0557       0.0964        0.076       0.0765     0.000797\n","     46     4       0.0092       0.0092     1.22e-06       0.0554       0.0742       0.0443       0.0774       0.0609       0.0574       0.0997       0.0785       0.0725     0.000755\n","     46     5      0.00863      0.00863     1.59e-06       0.0542       0.0719       0.0441       0.0745       0.0593       0.0576       0.0941       0.0758       0.0665     0.000693\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              46  178.150    0.005      0.00921     4.36e-05      0.00926       0.0564       0.0743       0.0462       0.0769       0.0615       0.0594       0.0974       0.0784        0.397      0.00414\n","! Validation         46  178.150    0.005      0.00907      1.6e-06      0.00907       0.0553       0.0737       0.0444       0.0772       0.0608       0.0577       0.0981       0.0779       0.0747     0.000778\n","Wall time: 178.15076411399968\n","! Best model       46    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     47     1       0.0102       0.0102     2.85e-06       0.0578       0.0781       0.0465       0.0804       0.0634       0.0611        0.104       0.0826         0.11      0.00115\n","     47     2       0.0108       0.0108     1.09e-05       0.0621       0.0804       0.0534       0.0795       0.0665       0.0673        0.102       0.0845        0.214      0.00223\n","     47     3      0.00865      0.00863     1.66e-05        0.055       0.0719        0.045        0.075         0.06       0.0576       0.0941       0.0759        0.301      0.00313\n","     47     4       0.0087       0.0087     5.45e-06       0.0545       0.0722       0.0443        0.075       0.0596       0.0566       0.0959       0.0763        0.161      0.00167\n","     47     5      0.00976      0.00976     1.79e-06       0.0582       0.0764       0.0455       0.0835       0.0645        0.057        0.105        0.081       0.0709     0.000739\n","     47     6       0.0108       0.0108     1.24e-05       0.0607       0.0804       0.0488       0.0846       0.0667       0.0633        0.107        0.085        0.259       0.0027\n","     47     7      0.00969      0.00965     3.67e-05       0.0595        0.076       0.0526       0.0734        0.063        0.066       0.0928       0.0794        0.443      0.00462\n","     47     8      0.00748      0.00746     2.15e-05       0.0509       0.0668       0.0417       0.0693       0.0555       0.0532       0.0879       0.0706        0.338      0.00352\n","     47     9      0.00913      0.00912        9e-06       0.0559       0.0739       0.0467       0.0742       0.0605       0.0588       0.0972        0.078         0.21      0.00219\n","     47    10       0.0121       0.0121     1.63e-05       0.0659       0.0852       0.0563       0.0852       0.0707       0.0708        0.108       0.0896        0.296      0.00308\n","     47    11       0.0107       0.0106     3.16e-05       0.0618       0.0797       0.0522        0.081       0.0666       0.0663        0.101       0.0838        0.399      0.00416\n","     47    12      0.00879      0.00876     3.04e-05       0.0549       0.0724        0.043       0.0785       0.0608       0.0565       0.0966       0.0766        0.394      0.00411\n","     47    13      0.00963      0.00956        7e-05        0.059       0.0757       0.0507       0.0754       0.0631       0.0647       0.0938       0.0793        0.616      0.00642\n","     47    14       0.0118       0.0118     1.11e-05       0.0649       0.0839       0.0557       0.0832       0.0694       0.0706        0.106       0.0881        0.245      0.00255\n","     47    15       0.0113       0.0113     3.37e-05        0.064       0.0821       0.0544       0.0833       0.0688        0.068        0.105       0.0864        0.425      0.00443\n","     47    16      0.00918      0.00917     5.11e-06       0.0559       0.0741       0.0437       0.0804       0.0621       0.0558        0.101       0.0785        0.166      0.00173\n","     47    17      0.00849      0.00847     2.29e-05        0.054       0.0712       0.0426       0.0769       0.0597       0.0555       0.0951       0.0753        0.342      0.00356\n","     47    18       0.0124       0.0124     2.62e-05       0.0666       0.0861        0.058       0.0838       0.0709       0.0726        0.108       0.0904        0.379      0.00395\n","     47    19       0.0138       0.0137      3.6e-05       0.0721       0.0907       0.0655       0.0853       0.0754       0.0802        0.109       0.0945        0.439      0.00458\n","     47    20      0.00846      0.00844     1.71e-05       0.0553       0.0711       0.0473       0.0712       0.0593       0.0597       0.0896       0.0746        0.304      0.00317\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     47     1       0.0094       0.0094     2.62e-06       0.0568        0.075       0.0455       0.0794       0.0624        0.059       0.0995       0.0793        0.101      0.00105\n","     47     2      0.00929      0.00929     9.69e-07       0.0556       0.0746       0.0442       0.0783       0.0612       0.0578          0.1       0.0789       0.0565     0.000589\n","     47     3      0.00855      0.00854     1.47e-06       0.0537       0.0715       0.0429       0.0754       0.0592       0.0553       0.0961       0.0757       0.0741     0.000772\n","     47     4      0.00911       0.0091     1.19e-06       0.0551       0.0738        0.044       0.0771       0.0606       0.0569       0.0993       0.0781       0.0721     0.000751\n","     47     5      0.00855      0.00855     1.55e-06        0.054       0.0715       0.0439       0.0742       0.0591       0.0573       0.0937       0.0755       0.0646     0.000673\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              47  181.966    0.005       0.0101     2.09e-05       0.0101       0.0595       0.0776       0.0497       0.0789       0.0643       0.0635          0.1       0.0818        0.306      0.00318\n","! Validation         47  181.966    0.005      0.00898     1.56e-06      0.00898        0.055       0.0733       0.0441       0.0769       0.0605       0.0573       0.0978       0.0775       0.0737     0.000767\n","Wall time: 181.96662384699994\n","! Best model       47    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     48     1      0.00929      0.00929     2.36e-06        0.057       0.0745       0.0473       0.0764       0.0619         0.06       0.0974       0.0787        0.108      0.00113\n","     48     2       0.0151       0.0151     2.69e-06       0.0761       0.0951       0.0693       0.0898       0.0795       0.0856        0.112       0.0987       0.0961        0.001\n","     48     3       0.0134       0.0134     8.66e-06       0.0703       0.0897       0.0645       0.0818       0.0732       0.0805        0.106        0.093        0.198      0.00206\n","     48     4      0.00944      0.00944     1.89e-06       0.0551       0.0752       0.0434       0.0785       0.0609       0.0582        0.101       0.0795       0.0879     0.000916\n","     48     5      0.00934      0.00932     2.25e-05       0.0568       0.0747        0.047       0.0763       0.0616       0.0596        0.098       0.0788        0.333      0.00347\n","     48     6       0.0116       0.0115     9.05e-05       0.0642       0.0831       0.0544       0.0837       0.0691       0.0689        0.106       0.0875        0.701       0.0073\n","     48     7      0.00898      0.00896     2.21e-05       0.0553       0.0732        0.046        0.074         0.06        0.059       0.0955       0.0773        0.344      0.00358\n","     48     8        0.008      0.00787     0.000127       0.0519       0.0686       0.0411       0.0737       0.0574       0.0526       0.0927       0.0727        0.835       0.0087\n","     48     9      0.00942       0.0094      2.3e-05       0.0577        0.075       0.0485       0.0759       0.0622       0.0617       0.0963        0.079        0.352      0.00366\n","     48    10      0.00992      0.00992     1.22e-06       0.0598       0.0771       0.0515       0.0763       0.0639       0.0653       0.0964       0.0808       0.0525     0.000547\n","     48    11       0.0082      0.00812     7.92e-05       0.0525       0.0697       0.0424       0.0727       0.0575       0.0549       0.0925       0.0737        0.658      0.00685\n","     48    12       0.0088      0.00877      2.5e-05       0.0558       0.0725       0.0457        0.076       0.0609       0.0573       0.0959       0.0766        0.369      0.00384\n","     48    13      0.00858      0.00858     1.33e-06       0.0548       0.0717       0.0437        0.077       0.0603       0.0558       0.0958       0.0758       0.0719     0.000749\n","     48    14      0.00958      0.00958     3.35e-06       0.0567       0.0757       0.0442       0.0817       0.0629        0.059        0.101       0.0801        0.128      0.00134\n","     48    15      0.00892      0.00891     9.58e-07       0.0554        0.073       0.0452       0.0756       0.0604        0.058       0.0963       0.0772       0.0666     0.000694\n","     48    16      0.00874      0.00873     1.44e-06       0.0552       0.0723       0.0438        0.078       0.0609        0.056        0.097       0.0765       0.0711     0.000741\n","     48    17      0.00804      0.00803     5.43e-06       0.0529       0.0693        0.043       0.0728       0.0579        0.055       0.0915       0.0732        0.149      0.00156\n","     48    18      0.00815      0.00812     2.86e-05       0.0533       0.0697       0.0425        0.075       0.0587       0.0542       0.0933       0.0738        0.377      0.00392\n","     48    19      0.00787      0.00786     5.07e-06       0.0514       0.0686       0.0412       0.0719       0.0565       0.0528       0.0924       0.0726        0.149      0.00155\n","     48    20      0.00806      0.00806     2.07e-06       0.0515       0.0695       0.0409       0.0727       0.0568       0.0534       0.0936       0.0735       0.0932      0.00097\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     48     1      0.00932      0.00932     2.72e-06       0.0565       0.0747       0.0452       0.0791       0.0621       0.0587       0.0992       0.0789        0.104      0.00108\n","     48     2      0.00922      0.00922      8.9e-07       0.0553       0.0743       0.0439        0.078        0.061       0.0575       0.0997       0.0786       0.0533     0.000555\n","     48     3      0.00848      0.00848     1.45e-06       0.0535       0.0712       0.0426       0.0752       0.0589        0.055       0.0958       0.0754       0.0743     0.000774\n","     48     4      0.00902      0.00902     1.26e-06       0.0548       0.0735       0.0438       0.0769       0.0603       0.0566        0.099       0.0778       0.0737     0.000768\n","     48     5      0.00845      0.00845     1.56e-06       0.0537       0.0711       0.0436       0.0739       0.0588       0.0569       0.0933       0.0751       0.0645     0.000671\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              48  185.774    0.005      0.00945     2.27e-05      0.00948       0.0572       0.0752       0.0473        0.077       0.0621        0.061       0.0976       0.0793        0.262      0.00273\n","! Validation         48  185.774    0.005       0.0089     1.57e-06       0.0089       0.0548        0.073       0.0438       0.0766       0.0602       0.0569       0.0974       0.0772       0.0739      0.00077\n","Wall time: 185.7744450959999\n","! Best model       48    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     49     1      0.00795      0.00791     4.37e-05       0.0516       0.0688       0.0425         0.07       0.0562       0.0542       0.0913       0.0727        0.478      0.00498\n","     49     2       0.0082      0.00818     1.87e-05       0.0525         0.07       0.0424       0.0727       0.0576        0.055       0.0929        0.074        0.318      0.00332\n","     49     3      0.00815      0.00814     2.43e-06       0.0527       0.0698       0.0426       0.0728       0.0577       0.0549       0.0927       0.0738        0.106       0.0011\n","     49     4      0.00861       0.0086     4.85e-06        0.054       0.0718       0.0436       0.0749       0.0593       0.0569       0.0947       0.0758        0.144       0.0015\n","     49     5       0.0101      0.00999     5.97e-05       0.0597       0.0773       0.0503       0.0783       0.0643       0.0636       0.0992       0.0814        0.569      0.00592\n","     49     6       0.0109       0.0109     9.83e-06       0.0622       0.0806       0.0531       0.0805       0.0668       0.0681        0.101       0.0846        0.221       0.0023\n","     49     7      0.00808      0.00808     9.77e-07       0.0533       0.0695       0.0431       0.0736       0.0584       0.0548       0.0922       0.0735       0.0514     0.000535\n","     49     8      0.00886      0.00886     3.29e-06       0.0555       0.0728       0.0453       0.0759       0.0606       0.0571       0.0969        0.077        0.125       0.0013\n","     49     9      0.00927      0.00921     5.48e-05        0.059       0.0743       0.0531       0.0709        0.062       0.0651       0.0897       0.0774        0.545      0.00568\n","     49    10      0.00792      0.00791     6.94e-06       0.0524       0.0688       0.0425       0.0724       0.0574       0.0542       0.0913       0.0727        0.177      0.00185\n","     49    11        0.009      0.00899     2.33e-06        0.055       0.0734       0.0458       0.0735       0.0596       0.0593       0.0955       0.0774       0.0916     0.000954\n","     49    12       0.0111       0.0111     1.24e-05       0.0636       0.0816       0.0546       0.0815        0.068       0.0682        0.103       0.0857        0.256      0.00267\n","     49    13       0.0138       0.0137     6.06e-05       0.0699       0.0906       0.0572       0.0953       0.0763        0.072        0.119       0.0957        0.575      0.00599\n","     49    14       0.0108       0.0108     9.84e-06       0.0617       0.0804       0.0501       0.0847       0.0674       0.0632        0.107        0.085        0.229      0.00238\n","     49    15       0.0082       0.0082     5.92e-06       0.0543         0.07        0.045       0.0728       0.0589       0.0568        0.091       0.0739        0.162      0.00169\n","     49    16      0.00981       0.0098     1.49e-05       0.0573       0.0766        0.048        0.076        0.062       0.0631       0.0981       0.0806        0.285      0.00297\n","     49    17       0.0129       0.0129     1.39e-05       0.0695        0.088       0.0636       0.0813       0.0725       0.0797        0.103       0.0911        0.261      0.00272\n","     49    18      0.00934      0.00932     1.62e-05       0.0574       0.0747       0.0476       0.0771       0.0624       0.0601       0.0975       0.0788         0.28      0.00292\n","     49    19      0.00752      0.00749     2.28e-05       0.0504        0.067       0.0399       0.0714       0.0556       0.0497       0.0923        0.071        0.348      0.00362\n","     49    20      0.00861       0.0086     9.08e-06       0.0544       0.0717       0.0434       0.0764       0.0599       0.0558        0.096       0.0759         0.22      0.00229\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     49     1      0.00923      0.00922     2.68e-06       0.0562       0.0743       0.0449       0.0788       0.0618       0.0583       0.0988       0.0785        0.103      0.00107\n","     49     2      0.00913      0.00913     9.75e-07        0.055       0.0739       0.0437       0.0778       0.0607       0.0571       0.0994       0.0782       0.0558     0.000581\n","     49     3      0.00842      0.00842     1.49e-06       0.0533        0.071       0.0424       0.0751       0.0587       0.0547       0.0956       0.0751        0.074     0.000771\n","     49     4      0.00894      0.00894      1.2e-06       0.0545       0.0732       0.0435       0.0766         0.06       0.0562       0.0987       0.0774       0.0727     0.000757\n","     49     5      0.00837      0.00836     1.56e-06       0.0534       0.0708       0.0433       0.0736       0.0585       0.0565       0.0929       0.0747       0.0655     0.000683\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              49  189.603    0.005      0.00944     1.87e-05      0.00945       0.0573       0.0751       0.0477       0.0766       0.0621        0.061       0.0975       0.0792        0.272      0.00283\n","! Validation         49  189.603    0.005      0.00882     1.58e-06      0.00882       0.0545       0.0726       0.0436       0.0764         0.06       0.0566       0.0971       0.0768       0.0742     0.000773\n","Wall time: 189.6040167049996\n","! Best model       49    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     50     1      0.00875      0.00869     5.99e-05       0.0541       0.0721       0.0433       0.0758       0.0596       0.0548        0.098       0.0764        0.572      0.00596\n","     50     2      0.00815      0.00814     3.98e-06       0.0528       0.0698        0.043       0.0722       0.0576       0.0544       0.0933       0.0738        0.144       0.0015\n","     50     3      0.00835      0.00834      1.1e-05        0.053       0.0707       0.0429       0.0732        0.058       0.0565       0.0927       0.0746        0.232      0.00242\n","     50     4      0.00785      0.00783     1.47e-05       0.0521       0.0685       0.0412       0.0739       0.0576       0.0517       0.0934       0.0725        0.279       0.0029\n","     50     5      0.00758      0.00758     1.16e-06       0.0513       0.0674       0.0419       0.0702        0.056       0.0529       0.0895       0.0712       0.0789     0.000822\n","     50     6      0.00724      0.00724     2.02e-06       0.0503       0.0658       0.0401       0.0706       0.0554       0.0502       0.0893       0.0697       0.0928     0.000966\n","     50     7      0.00875      0.00874     1.16e-05       0.0557       0.0723       0.0483       0.0705       0.0594       0.0612       0.0906       0.0759        0.241      0.00251\n","     50     8      0.00945      0.00943      1.5e-05       0.0571       0.0751       0.0484       0.0746       0.0615       0.0628       0.0951        0.079        0.263      0.00274\n","     50     9      0.00779      0.00779     2.02e-06       0.0522       0.0683       0.0421       0.0724       0.0572       0.0536       0.0908       0.0722       0.0965      0.00101\n","     50    10         0.01      0.00998     2.06e-05       0.0594       0.0773       0.0506        0.077       0.0638        0.065       0.0973       0.0812        0.335      0.00349\n","     50    11       0.0106       0.0106      7.3e-07       0.0612       0.0796       0.0507       0.0822       0.0664       0.0638        0.104        0.084       0.0531     0.000553\n","     50    12      0.00799      0.00799     7.82e-06       0.0517       0.0691       0.0408       0.0736       0.0572       0.0532       0.0932       0.0732        0.195      0.00203\n","     50    13      0.00878      0.00877     1.18e-05       0.0547       0.0724       0.0432       0.0777       0.0604       0.0554       0.0979       0.0767        0.222      0.00232\n","     50    14      0.00744      0.00742     2.32e-05       0.0511       0.0666       0.0414       0.0704       0.0559       0.0526       0.0882       0.0704        0.353      0.00368\n","     50    15      0.00942      0.00942     4.47e-07       0.0559       0.0751       0.0434       0.0808       0.0621       0.0577        0.101       0.0795       0.0344     0.000358\n","     50    16      0.00838      0.00838     6.57e-06       0.0531       0.0708       0.0417        0.076       0.0589       0.0546       0.0953       0.0749         0.15      0.00156\n","     50    17      0.00986      0.00986     7.45e-07       0.0582       0.0768       0.0465       0.0815        0.064       0.0585        0.104       0.0813       0.0535     0.000557\n","     50    18      0.00934      0.00933     1.15e-06       0.0571       0.0747       0.0447       0.0818       0.0632       0.0568        0.102       0.0792       0.0654     0.000682\n","     50    19      0.00745      0.00745     1.75e-06       0.0502       0.0668       0.0404       0.0699       0.0551        0.052       0.0893       0.0706       0.0955     0.000995\n","     50    20      0.00969      0.00969     1.53e-06       0.0568       0.0761       0.0447       0.0811       0.0629       0.0584        0.103       0.0806       0.0621     0.000647\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     50     1      0.00913      0.00913     2.71e-06       0.0559       0.0739       0.0446       0.0784       0.0615       0.0579       0.0984       0.0782        0.103      0.00107\n","     50     2      0.00906      0.00905     1.02e-06       0.0548       0.0736       0.0435       0.0775       0.0605       0.0568       0.0991       0.0779       0.0562     0.000585\n","     50     3      0.00836      0.00836      1.5e-06       0.0531       0.0707       0.0422       0.0748       0.0585       0.0544       0.0953       0.0749       0.0728     0.000758\n","     50     4      0.00886      0.00886     1.18e-06       0.0542       0.0728       0.0432       0.0763       0.0598       0.0558       0.0983       0.0771       0.0715     0.000745\n","     50     5      0.00829      0.00829     1.53e-06       0.0532       0.0704       0.0431       0.0734       0.0582       0.0562       0.0925       0.0744       0.0644      0.00067\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              50  193.420    0.005      0.00863     9.89e-06      0.00864       0.0544       0.0719        0.044       0.0753       0.0596       0.0565       0.0955        0.076        0.181      0.00188\n","! Validation         50  193.420    0.005      0.00874     1.59e-06      0.00874       0.0542       0.0723       0.0433       0.0761       0.0597       0.0562       0.0968       0.0765       0.0735     0.000766\n","Wall time: 193.420842042\n","! Best model       50    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     51     1       0.0113       0.0113     4.33e-05       0.0644       0.0822       0.0556       0.0821       0.0688       0.0713        0.101       0.0859        0.483      0.00503\n","     51     2      0.00958      0.00958     7.92e-07       0.0587       0.0757       0.0493       0.0773       0.0633       0.0613       0.0984       0.0799       0.0635     0.000661\n","     51     3      0.00813      0.00813     9.56e-07       0.0519       0.0698       0.0403       0.0751       0.0577       0.0512       0.0967        0.074       0.0641     0.000667\n","     51     4      0.00981      0.00979     1.39e-05        0.058       0.0766       0.0464       0.0811       0.0638       0.0595        0.103        0.081        0.265      0.00276\n","     51     5       0.0137       0.0136     0.000104       0.0703       0.0902       0.0618       0.0872       0.0745       0.0769        0.112       0.0945        0.748       0.0078\n","     51     6        0.012       0.0119     4.08e-05       0.0663       0.0845       0.0595       0.0798       0.0696        0.075        0.101        0.088        0.474      0.00493\n","     51     7      0.00842       0.0084     2.31e-05       0.0537       0.0709       0.0438       0.0735       0.0587       0.0562       0.0936       0.0749        0.355      0.00369\n","     51     8      0.00873      0.00873     5.57e-06       0.0553       0.0723       0.0452       0.0755       0.0603       0.0578       0.0948       0.0763        0.124      0.00129\n","     51     9      0.00942      0.00942     5.05e-06       0.0587       0.0751       0.0503       0.0754       0.0629       0.0635        0.094       0.0788        0.162      0.00169\n","     51    10      0.00983      0.00982     7.28e-06       0.0588       0.0767       0.0481       0.0802       0.0641       0.0612        0.101        0.081        0.189      0.00197\n","     51    11      0.00952      0.00948     4.35e-05       0.0577       0.0753       0.0461        0.081       0.0636       0.0584        0.101       0.0797        0.489      0.00509\n","     51    12      0.00842      0.00842      8.4e-06       0.0537        0.071       0.0433       0.0746       0.0589       0.0549       0.0953       0.0751        0.206      0.00215\n","     51    13      0.00824      0.00815      8.8e-05       0.0526       0.0699       0.0423       0.0733       0.0578       0.0545       0.0933       0.0739        0.696      0.00724\n","     51    14       0.0094      0.00938     1.81e-05       0.0571       0.0749       0.0477       0.0758       0.0617       0.0615       0.0964       0.0789        0.302      0.00314\n","     51    15       0.0089      0.00885     5.73e-05        0.055       0.0728       0.0441       0.0766       0.0604       0.0563       0.0977        0.077        0.559      0.00582\n","     51    16       0.0085      0.00842     8.09e-05       0.0536        0.071       0.0431       0.0745       0.0588       0.0545       0.0959       0.0752        0.663      0.00691\n","     51    17       0.0073       0.0073     2.01e-06        0.052       0.0661       0.0453       0.0654       0.0554       0.0567       0.0816       0.0692       0.0789     0.000822\n","     51    18      0.00832      0.00824     8.15e-05       0.0531       0.0702       0.0422       0.0748       0.0585       0.0538       0.0949       0.0744         0.67      0.00697\n","     51    19       0.0116       0.0116     6.49e-06       0.0623       0.0832       0.0492       0.0885       0.0689       0.0641        0.112       0.0881        0.185      0.00193\n","     51    20      0.00921      0.00921     1.81e-06       0.0576       0.0742       0.0495       0.0738       0.0617       0.0633       0.0923       0.0778       0.0799     0.000832\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     51     1      0.00905      0.00905     2.67e-06       0.0556       0.0736       0.0443       0.0782       0.0612       0.0576       0.0981       0.0778        0.103      0.00107\n","     51     2      0.00898      0.00898     9.87e-07       0.0546       0.0733       0.0432       0.0773       0.0603       0.0564       0.0987       0.0776       0.0542     0.000565\n","     51     3       0.0083       0.0083     1.43e-06       0.0529       0.0705        0.042       0.0746       0.0583       0.0542       0.0951       0.0746       0.0734     0.000765\n","     51     4      0.00878      0.00878     1.22e-06        0.054       0.0725       0.0429       0.0761       0.0595       0.0555        0.098       0.0767       0.0729      0.00076\n","     51     5      0.00821      0.00821     1.53e-06       0.0529       0.0701       0.0429       0.0731        0.058       0.0559       0.0922        0.074       0.0641     0.000667\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              51  197.251    0.005      0.00948     3.16e-05      0.00952       0.0575       0.0753       0.0476       0.0773       0.0625        0.061        0.098       0.0795        0.343      0.00357\n","! Validation         51  197.251    0.005      0.00866     1.57e-06      0.00866        0.054        0.072       0.0431       0.0759       0.0595       0.0559       0.0965       0.0762       0.0735     0.000765\n","Wall time: 197.25214046600013\n","! Best model       51    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     52     1      0.00831      0.00831     7.22e-06       0.0528       0.0705       0.0414       0.0756       0.0585       0.0527       0.0968       0.0747        0.177      0.00185\n","     52     2       0.0104       0.0104     1.01e-05       0.0622       0.0789       0.0559       0.0748       0.0653       0.0694       0.0951       0.0822        0.221      0.00231\n","     52     3      0.00891      0.00891     1.53e-06       0.0559        0.073       0.0477       0.0721       0.0599       0.0614        0.092       0.0767        0.083     0.000865\n","     52     4      0.00804      0.00804     9.52e-07       0.0526       0.0694       0.0411       0.0755       0.0583       0.0532       0.0937       0.0734       0.0598     0.000623\n","     52     5      0.00778      0.00776     2.17e-05        0.052       0.0682       0.0418       0.0723       0.0571       0.0536       0.0905       0.0721        0.327      0.00341\n","     52     6      0.00916      0.00915     2.28e-06       0.0564        0.074       0.0453       0.0785       0.0619       0.0584       0.0981       0.0782        0.104      0.00108\n","     52     7       0.0085      0.00848     1.73e-05       0.0545       0.0712       0.0432       0.0773       0.0602       0.0542       0.0967       0.0754        0.305      0.00318\n","     52     8      0.00953      0.00953     3.62e-06       0.0567       0.0755       0.0459       0.0782        0.062       0.0598       0.0998       0.0798       0.0943     0.000983\n","     52     9      0.00787      0.00783     3.48e-05       0.0516       0.0685        0.042       0.0708       0.0564       0.0535       0.0913       0.0724        0.437      0.00455\n","     52    10      0.00908      0.00908     1.19e-06       0.0567       0.0737        0.048       0.0743       0.0611       0.0607       0.0946       0.0776       0.0744     0.000775\n","     52    11      0.00902      0.00896     6.35e-05       0.0565       0.0732       0.0487        0.072       0.0603       0.0625       0.0909       0.0767        0.587      0.00611\n","     52    12       0.0076      0.00757     3.14e-05       0.0514       0.0673       0.0426       0.0689       0.0558       0.0536       0.0885       0.0711        0.412      0.00429\n","     52    13      0.00824      0.00822     2.06e-05       0.0525       0.0701       0.0427       0.0721       0.0574       0.0554       0.0928       0.0741        0.335      0.00349\n","     52    14      0.00938      0.00931     6.75e-05       0.0576       0.0747       0.0475       0.0778       0.0627         0.06       0.0976       0.0788        0.608      0.00634\n","     52    15       0.0101       0.0101     4.62e-06       0.0584       0.0779       0.0454       0.0843       0.0649       0.0582        0.107       0.0826        0.133      0.00139\n","     52    16      0.00734      0.00733     8.28e-06        0.051       0.0663       0.0421       0.0688       0.0554       0.0525       0.0875         0.07        0.199      0.00208\n","     52    17      0.00879      0.00879     1.26e-06       0.0535       0.0725       0.0415       0.0774       0.0595       0.0542       0.0995       0.0769       0.0717     0.000747\n","     52    18      0.00874      0.00874     3.93e-06        0.056       0.0723       0.0457       0.0768       0.0612       0.0577        0.095       0.0764        0.144       0.0015\n","     52    19      0.00815      0.00815     7.04e-07       0.0521       0.0698        0.041       0.0743       0.0576       0.0532       0.0948        0.074       0.0613     0.000639\n","     52    20       0.0094      0.00938     1.57e-05       0.0578       0.0749        0.051       0.0714       0.0612       0.0658       0.0904       0.0781        0.273      0.00285\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     52     1      0.00896      0.00896      2.7e-06       0.0553       0.0732       0.0441       0.0778       0.0609       0.0572       0.0977       0.0775        0.103      0.00107\n","     52     2       0.0089       0.0089     1.07e-06       0.0543        0.073        0.043        0.077         0.06       0.0561       0.0984       0.0772        0.057     0.000594\n","     52     3      0.00824      0.00824     1.44e-06       0.0527       0.0702       0.0418       0.0744       0.0581       0.0539       0.0949       0.0744       0.0718     0.000748\n","     52     4       0.0087       0.0087     1.24e-06       0.0537       0.0721       0.0427       0.0758       0.0593       0.0551       0.0977       0.0764       0.0737     0.000768\n","     52     5      0.00813      0.00813     1.48e-06       0.0527       0.0697       0.0426       0.0728       0.0577       0.0555       0.0918       0.0737       0.0635     0.000661\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              52  201.064    0.005       0.0087     1.59e-05      0.00872       0.0549       0.0722        0.045       0.0747       0.0598       0.0577       0.0947       0.0762        0.235      0.00245\n","! Validation         52  201.064    0.005      0.00858     1.59e-06      0.00859       0.0538       0.0717       0.0428       0.0756       0.0592       0.0556       0.0961       0.0758       0.0738     0.000769\n","Wall time: 201.06426175399974\n","! Best model       52    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     53     1      0.00923       0.0092     2.59e-05       0.0575       0.0742       0.0474       0.0777       0.0626       0.0604       0.0961       0.0782        0.371      0.00387\n","     53     2       0.0102       0.0102     1.17e-05       0.0604       0.0782       0.0476       0.0859       0.0668       0.0599        0.106       0.0828        0.242      0.00252\n","     53     3      0.00793      0.00791     1.85e-05       0.0517       0.0688        0.041       0.0732       0.0571       0.0518        0.094       0.0729        0.311      0.00323\n","     53     4      0.00806      0.00804     1.87e-05       0.0536       0.0694       0.0442       0.0723       0.0582       0.0557       0.0907       0.0732        0.301      0.00314\n","     53     5      0.00844      0.00844     8.67e-07       0.0545       0.0711       0.0454       0.0726        0.059       0.0567       0.0934        0.075       0.0645     0.000671\n","     53     6      0.00977      0.00973      4.8e-05       0.0565       0.0763       0.0447       0.0801       0.0624       0.0584        0.103       0.0808        0.511      0.00533\n","     53     7      0.00742       0.0074      2.2e-05       0.0512       0.0665       0.0414       0.0709       0.0561       0.0519       0.0889       0.0704        0.344      0.00358\n","     53     8      0.00977      0.00973      4.6e-05       0.0588       0.0763       0.0499       0.0765       0.0632       0.0638       0.0966       0.0802        0.495      0.00516\n","     53     9      0.00707      0.00703     3.65e-05       0.0491       0.0649       0.0382       0.0711       0.0546       0.0484       0.0891       0.0687        0.445      0.00463\n","     53    10       0.0088      0.00879     8.69e-06        0.057       0.0725       0.0509       0.0692         0.06       0.0638       0.0874       0.0756        0.208      0.00217\n","     53    11      0.00823      0.00821     1.91e-05       0.0546       0.0701       0.0471       0.0696       0.0584       0.0586       0.0888       0.0737        0.317       0.0033\n","     53    12      0.00786      0.00786     2.52e-06       0.0513       0.0686        0.039       0.0759       0.0574       0.0503       0.0951       0.0727        0.103      0.00107\n","     53    13      0.00811      0.00804     6.96e-05       0.0519       0.0694       0.0395       0.0765        0.058       0.0502        0.097       0.0736        0.619      0.00644\n","     53    14      0.00853      0.00849      3.5e-05       0.0538       0.0713       0.0423       0.0768       0.0595        0.054        0.097       0.0755        0.436      0.00455\n","     53    15       0.0073      0.00721     8.04e-05       0.0487       0.0657       0.0375       0.0712       0.0543       0.0491       0.0901       0.0696        0.662      0.00689\n","     53    16      0.00821      0.00813     7.44e-05       0.0525       0.0698       0.0433        0.071       0.0571       0.0558       0.0916       0.0737        0.637      0.00663\n","     53    17      0.00784      0.00783     3.18e-06       0.0519       0.0685       0.0411       0.0734       0.0572       0.0525       0.0924       0.0725        0.111      0.00115\n","     53    18      0.00788      0.00783     5.51e-05       0.0519       0.0685       0.0424       0.0709       0.0567       0.0542       0.0904       0.0723        0.543      0.00565\n","     53    19         0.01         0.01     1.44e-06       0.0584       0.0774       0.0465       0.0821       0.0643       0.0599        0.104       0.0819       0.0721     0.000751\n","     53    20      0.00804      0.00804     1.15e-06        0.052       0.0694       0.0419        0.072        0.057       0.0539       0.0928       0.0734       0.0664     0.000692\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     53     1      0.00888      0.00888     2.75e-06       0.0551       0.0729       0.0438       0.0775       0.0607       0.0569       0.0973       0.0771        0.103      0.00107\n","     53     2      0.00882      0.00882     1.03e-06       0.0541       0.0727       0.0428       0.0768       0.0598       0.0558       0.0981       0.0769       0.0562     0.000585\n","     53     3      0.00818      0.00818     1.48e-06       0.0525         0.07       0.0416       0.0742       0.0579       0.0536       0.0946       0.0741       0.0732     0.000763\n","     53     4      0.00862      0.00862     1.31e-06       0.0535       0.0718       0.0424       0.0756        0.059       0.0548       0.0974       0.0761       0.0763     0.000794\n","     53     5      0.00806      0.00806     1.47e-06       0.0524       0.0694       0.0424       0.0726       0.0575       0.0552       0.0915       0.0733       0.0643     0.000669\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              53  204.864    0.005      0.00841     2.89e-05      0.00844       0.0539       0.0709       0.0436       0.0744        0.059       0.0557       0.0943        0.075        0.343      0.00357\n","! Validation         53  204.864    0.005      0.00851     1.61e-06      0.00851       0.0535       0.0714       0.0426       0.0753        0.059       0.0552       0.0958       0.0755       0.0746     0.000777\n","Wall time: 204.8651380379997\n","! Best model       53    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     54     1      0.00731      0.00731     1.17e-06       0.0502       0.0662       0.0409       0.0688       0.0549       0.0526       0.0872       0.0699       0.0752     0.000783\n","     54     2      0.00891      0.00891     6.82e-06       0.0548        0.073       0.0426       0.0792       0.0609       0.0547          0.1       0.0774        0.189      0.00197\n","     54     3      0.00783      0.00782     1.01e-05       0.0532       0.0684       0.0457        0.068       0.0569       0.0577       0.0859       0.0718        0.235      0.00245\n","     54     4      0.00841      0.00838     2.77e-05       0.0539       0.0708       0.0433        0.075       0.0592       0.0546       0.0953        0.075        0.376      0.00392\n","     54     5      0.00803      0.00796      6.8e-05       0.0526        0.069       0.0409       0.0759       0.0584       0.0518       0.0944       0.0731         0.61      0.00635\n","     54     6      0.00706      0.00704     2.25e-05         0.05       0.0649       0.0407       0.0687       0.0547       0.0511       0.0861       0.0686        0.347      0.00362\n","     54     7       0.0103       0.0103      1.5e-05       0.0603       0.0785       0.0476       0.0856       0.0666       0.0609        0.105       0.0831        0.272      0.00283\n","     54     8       0.0118       0.0116     0.000183        0.064       0.0835       0.0528       0.0865       0.0696       0.0682        0.108        0.088            1       0.0104\n","     54     9      0.00915      0.00915     2.96e-06       0.0574        0.074       0.0509       0.0703       0.0606       0.0644       0.0901       0.0773       0.0957     0.000997\n","     54    10      0.00837      0.00834     2.75e-05       0.0528       0.0707       0.0415       0.0754       0.0584       0.0536       0.0961       0.0748        0.387      0.00403\n","     54    11      0.00874      0.00873     4.27e-06       0.0552       0.0723       0.0461       0.0735       0.0598       0.0589       0.0935       0.0762        0.153      0.00159\n","     54    12      0.00959      0.00959     3.71e-06       0.0589       0.0758       0.0493       0.0781       0.0637       0.0617        0.098       0.0798        0.139      0.00145\n","     54    13      0.00895      0.00894     1.25e-06       0.0555       0.0732       0.0439       0.0785       0.0612        0.056       0.0989       0.0775       0.0619     0.000645\n","     54    14      0.00793      0.00785     7.38e-05       0.0513       0.0686       0.0404       0.0731       0.0568       0.0526       0.0926       0.0726        0.633       0.0066\n","     54    15      0.00772      0.00772     6.99e-06       0.0515        0.068       0.0418       0.0708       0.0563       0.0527       0.0911       0.0719        0.196      0.00204\n","     54    16       0.0084      0.00834     6.14e-05       0.0528       0.0706       0.0423       0.0738        0.058       0.0548       0.0947       0.0747        0.576        0.006\n","     54    17      0.00932      0.00925     7.26e-05        0.055       0.0744        0.042       0.0808       0.0614       0.0547        0.103       0.0789        0.629      0.00655\n","     54    18      0.00791       0.0079     7.95e-06       0.0522       0.0688       0.0416       0.0735       0.0576       0.0533       0.0922       0.0728        0.198      0.00206\n","     54    19      0.00915      0.00906     9.84e-05       0.0559       0.0736       0.0473       0.0733       0.0603       0.0604       0.0947       0.0775        0.737      0.00767\n","     54    20       0.0084      0.00835     5.31e-05       0.0547       0.0707       0.0465       0.0711       0.0588       0.0583       0.0905       0.0744        0.537      0.00559\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     54     1      0.00879      0.00878     2.63e-06       0.0548       0.0725       0.0435       0.0772       0.0604       0.0564        0.097       0.0767        0.101      0.00105\n","     54     2      0.00874      0.00874     1.13e-06       0.0538       0.0723       0.0425       0.0765       0.0595       0.0554       0.0977       0.0766        0.058     0.000604\n","     54     3      0.00812      0.00812     1.43e-06       0.0523       0.0697       0.0414        0.074       0.0577       0.0533       0.0943       0.0738         0.07     0.000729\n","     54     4      0.00854      0.00854     1.23e-06       0.0532       0.0715       0.0422       0.0753       0.0587       0.0544        0.097       0.0757       0.0736     0.000767\n","     54     5      0.00798      0.00798     1.46e-06       0.0522       0.0691       0.0421       0.0724       0.0573       0.0549       0.0911        0.073        0.062     0.000646\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              54  208.666    0.005      0.00863     3.74e-05      0.00867       0.0546       0.0719       0.0444        0.075       0.0597       0.0568        0.095       0.0759        0.372      0.00388\n","! Validation         54  208.666    0.005      0.00843     1.58e-06      0.00843       0.0533        0.071       0.0424       0.0751       0.0587       0.0549       0.0955       0.0752       0.0729      0.00076\n","Wall time: 208.66664952599967\n","! Best model       54    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     55     1       0.0065       0.0065     1.47e-06       0.0476       0.0624       0.0381       0.0665       0.0523       0.0479       0.0841        0.066       0.0834     0.000869\n","     55     2      0.00764      0.00759     4.64e-05       0.0508       0.0674       0.0404       0.0716        0.056       0.0511       0.0917       0.0714        0.504      0.00525\n","     55     3      0.00911      0.00908     3.33e-05       0.0563       0.0737       0.0462       0.0764       0.0613       0.0587        0.097       0.0778        0.421      0.00438\n","     55     4      0.00794      0.00792     1.92e-05       0.0517       0.0689       0.0412       0.0728        0.057       0.0527       0.0932       0.0729        0.324      0.00337\n","     55     5      0.00769      0.00769     1.72e-06       0.0514       0.0679       0.0408       0.0725       0.0566       0.0513       0.0924       0.0719       0.0846     0.000881\n","     55     6      0.00806      0.00805     5.17e-06       0.0514       0.0694       0.0401       0.0739        0.057       0.0533       0.0937       0.0735        0.162      0.00168\n","     55     7      0.00879      0.00879     3.65e-06       0.0552       0.0725       0.0443        0.077       0.0606       0.0569       0.0965       0.0767        0.124      0.00129\n","     55     8      0.00995      0.00994      7.2e-06       0.0593       0.0771       0.0497       0.0785       0.0641       0.0626          0.1       0.0813        0.194      0.00202\n","     55     9       0.0132       0.0131     0.000114       0.0694       0.0886       0.0611       0.0859       0.0735       0.0767        0.109       0.0927        0.793      0.00826\n","     55    10       0.0123       0.0123     1.29e-05       0.0679       0.0859       0.0606       0.0825       0.0715       0.0754        0.104       0.0896        0.248      0.00258\n","     55    11       0.0081      0.00805     4.69e-05       0.0535       0.0694       0.0436       0.0732       0.0584       0.0552       0.0915       0.0733        0.505      0.00526\n","     55    12      0.00753       0.0075     3.04e-05       0.0508        0.067       0.0414       0.0695       0.0554       0.0529       0.0888       0.0708        0.404      0.00421\n","     55    13      0.00972      0.00971     5.72e-06        0.058       0.0762       0.0486       0.0767       0.0626       0.0626       0.0979       0.0803        0.157      0.00164\n","     55    14       0.0113       0.0112     5.24e-05       0.0626        0.082       0.0519       0.0839       0.0679       0.0666        0.106       0.0865        0.535      0.00557\n","     55    15      0.00855      0.00855     2.15e-06        0.053       0.0715       0.0421       0.0748       0.0585       0.0567       0.0944       0.0756       0.0979      0.00102\n","     55    16      0.00785      0.00784     2.63e-06       0.0527       0.0685       0.0418       0.0743       0.0581       0.0534       0.0916       0.0725        0.093     0.000968\n","     55    17         0.01      0.00999     6.77e-06       0.0596       0.0773       0.0486       0.0815        0.065        0.061        0.103       0.0817        0.173       0.0018\n","     55    18      0.00971       0.0097     6.37e-06       0.0595       0.0762       0.0526       0.0733       0.0629       0.0664       0.0928       0.0796         0.17      0.00177\n","     55    19      0.00874      0.00874     9.51e-07       0.0547       0.0723       0.0448       0.0745       0.0597       0.0564       0.0965       0.0765       0.0486     0.000507\n","     55    20      0.00785      0.00779     5.66e-05       0.0516       0.0683       0.0409        0.073        0.057       0.0526       0.0919       0.0723        0.549      0.00571\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     55     1      0.00871      0.00871     2.81e-06       0.0545       0.0722       0.0433        0.077       0.0601       0.0561       0.0967       0.0764        0.104      0.00109\n","     55     2      0.00867      0.00867     1.01e-06       0.0536       0.0721       0.0423       0.0763       0.0593       0.0551       0.0974       0.0763       0.0553     0.000576\n","     55     3      0.00806      0.00806     1.43e-06        0.052       0.0694       0.0412       0.0737       0.0575        0.053       0.0941       0.0735       0.0737     0.000768\n","     55     4      0.00847      0.00847      1.3e-06        0.053       0.0712       0.0419       0.0751       0.0585       0.0541       0.0968       0.0754       0.0753     0.000784\n","     55     5      0.00792      0.00791     1.41e-06        0.052       0.0688       0.0419       0.0721        0.057       0.0546       0.0908       0.0727       0.0621     0.000647\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              55  212.463    0.005      0.00901     2.28e-05      0.00903       0.0558       0.0734       0.0459       0.0756       0.0608        0.059        0.096       0.0775        0.283      0.00295\n","! Validation         55  212.463    0.005      0.00836     1.59e-06      0.00837        0.053       0.0708       0.0421       0.0748       0.0585       0.0546       0.0952       0.0749       0.0742     0.000773\n","Wall time: 212.46386486400024\n","! Best model       55    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     56     1      0.00833      0.00828     4.27e-05       0.0533       0.0704       0.0441       0.0716       0.0579       0.0571       0.0914       0.0742        0.479      0.00499\n","     56     2      0.00896      0.00895     8.95e-06        0.055       0.0732       0.0438       0.0774       0.0606       0.0568       0.0981       0.0774        0.215      0.00224\n","     56     3      0.00805      0.00803     2.06e-05       0.0534       0.0693       0.0439       0.0723       0.0581       0.0553       0.0911       0.0732        0.331      0.00345\n","     56     4      0.00808      0.00807     1.07e-05       0.0535       0.0695       0.0428       0.0749       0.0589       0.0537       0.0934       0.0736        0.232      0.00242\n","     56     5      0.00739      0.00739     5.83e-07         0.05       0.0665       0.0394        0.071       0.0552       0.0504       0.0905       0.0704       0.0441      0.00046\n","     56     6      0.00788      0.00788     3.27e-06       0.0524       0.0687       0.0421       0.0731       0.0576       0.0531       0.0923       0.0727        0.129      0.00135\n","     56     7      0.00913      0.00912     9.59e-06       0.0564       0.0739       0.0461       0.0771       0.0616        0.059        0.097        0.078        0.227      0.00236\n","     56     8      0.00855      0.00854     5.96e-06       0.0542       0.0715       0.0441       0.0744       0.0593       0.0556       0.0956       0.0756        0.179      0.00187\n","     56     9      0.00772      0.00772     5.23e-07       0.0511        0.068       0.0406        0.072       0.0563       0.0519       0.0921        0.072        0.043     0.000448\n","     56    10       0.0125       0.0125     9.97e-07       0.0654       0.0864       0.0527       0.0907       0.0717        0.068        0.115       0.0914       0.0652      0.00068\n","     56    11       0.0113       0.0112     6.07e-05       0.0655        0.082       0.0588       0.0789       0.0689       0.0715       0.0997       0.0856        0.573      0.00597\n","     56    12       0.0107       0.0107     4.07e-06       0.0623       0.0799       0.0535       0.0798       0.0667       0.0681       0.0995       0.0838        0.123      0.00128\n","     56    13      0.00839      0.00835      4.2e-05       0.0529       0.0707        0.041       0.0765       0.0588       0.0532       0.0966       0.0749        0.476      0.00496\n","     56    14      0.00802      0.00799     2.75e-05       0.0525       0.0692       0.0408       0.0757       0.0583       0.0521       0.0945       0.0733        0.384        0.004\n","     56    15      0.00743      0.00743     4.38e-06       0.0501       0.0667       0.0394       0.0716       0.0555       0.0502       0.0912       0.0707         0.15      0.00157\n","     56    16      0.00696      0.00694     2.38e-05       0.0481       0.0644       0.0377        0.069       0.0533       0.0476        0.089       0.0683        0.354      0.00369\n","     56    17      0.00904      0.00902     1.17e-05       0.0558       0.0735       0.0443       0.0786       0.0615       0.0578       0.0976       0.0777        0.246      0.00256\n","     56    18      0.00747      0.00747     2.36e-06       0.0508       0.0669       0.0415       0.0693       0.0554       0.0523       0.0891       0.0707        0.101      0.00105\n","     56    19      0.00707      0.00706      3.2e-06       0.0497        0.065       0.0414       0.0664       0.0539       0.0522        0.085       0.0686        0.108      0.00112\n","     56    20      0.00758      0.00758     3.25e-06       0.0505       0.0674       0.0392        0.073       0.0561       0.0509       0.0918       0.0714        0.113      0.00118\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     56     1      0.00864      0.00863     2.69e-06       0.0543       0.0719        0.043       0.0767       0.0599       0.0558       0.0963       0.0761        0.102      0.00106\n","     56     2      0.00861      0.00861      1.1e-06       0.0534       0.0718       0.0421        0.076       0.0591       0.0549       0.0971        0.076       0.0562     0.000586\n","     56     3        0.008        0.008     1.39e-06       0.0518       0.0692        0.041       0.0736       0.0573       0.0527       0.0939       0.0733       0.0697     0.000726\n","     56     4      0.00841       0.0084     1.26e-06       0.0528       0.0709       0.0417       0.0749       0.0583       0.0537       0.0965       0.0751       0.0742     0.000773\n","     56     5      0.00785      0.00785     1.37e-06       0.0517       0.0685       0.0417       0.0719       0.0568       0.0543       0.0905       0.0724       0.0605     0.000631\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              56  216.264    0.005      0.00851     1.43e-05      0.00853       0.0541       0.0714       0.0439       0.0747       0.0593       0.0562       0.0947       0.0754        0.229      0.00238\n","! Validation         56  216.264    0.005       0.0083     1.56e-06       0.0083       0.0528       0.0705       0.0419       0.0746       0.0583       0.0543       0.0949       0.0746       0.0725     0.000755\n","Wall time: 216.26444731100037\n","! Best model       56    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     57     1      0.00818      0.00817        2e-06       0.0536       0.0699       0.0436       0.0736       0.0586       0.0551       0.0927       0.0739       0.0842     0.000877\n","     57     2      0.00766      0.00765     5.36e-06       0.0514       0.0677       0.0418       0.0705       0.0562       0.0533       0.0898       0.0715        0.139      0.00144\n","     57     3      0.00665      0.00665     2.95e-06       0.0477       0.0631       0.0379       0.0672       0.0526       0.0481       0.0855       0.0668         0.12      0.00125\n","     57     4       0.0074      0.00739     3.03e-06       0.0497       0.0665       0.0396         0.07       0.0548       0.0512       0.0897       0.0704        0.104      0.00108\n","     57     5      0.00781       0.0078     4.85e-06       0.0521       0.0683       0.0414       0.0734       0.0574       0.0526       0.0921       0.0723        0.155      0.00162\n","     57     6      0.00935      0.00927     8.59e-05       0.0574       0.0745       0.0451       0.0819       0.0635       0.0565        0.101       0.0789        0.686      0.00714\n","     57     7      0.00876      0.00876     5.82e-06        0.054       0.0724       0.0441        0.074        0.059       0.0574       0.0955       0.0765        0.156      0.00162\n","     57     8      0.00832      0.00828     3.69e-05       0.0529       0.0704       0.0423       0.0741       0.0582       0.0554       0.0935       0.0744        0.448      0.00467\n","     57     9      0.00743      0.00742      6.5e-06       0.0501       0.0667         0.04       0.0703       0.0551        0.052        0.089       0.0705        0.162      0.00168\n","     57    10      0.00756      0.00755     5.24e-06       0.0513       0.0672       0.0417       0.0706       0.0562       0.0531        0.089       0.0711        0.155      0.00162\n","     57    11      0.00731       0.0073     1.05e-05       0.0501       0.0661       0.0399       0.0706       0.0552       0.0512       0.0887       0.0699        0.234      0.00244\n","     57    12      0.00738      0.00738     5.62e-06       0.0492       0.0665       0.0374       0.0728       0.0551       0.0476       0.0933       0.0705        0.172      0.00179\n","     57    13      0.00706      0.00706      6.6e-07       0.0494        0.065       0.0399       0.0684       0.0541       0.0514       0.0859       0.0687       0.0527     0.000549\n","     57    14      0.00764      0.00763      1.5e-05       0.0516       0.0676       0.0408       0.0734       0.0571       0.0512        0.092       0.0716        0.286      0.00297\n","     57    15      0.00801      0.00801     2.82e-06       0.0525       0.0692       0.0436       0.0704        0.057       0.0553       0.0909       0.0731        0.101      0.00105\n","     57    16      0.00818      0.00818     6.53e-06       0.0531         0.07       0.0448       0.0697       0.0572       0.0567       0.0908       0.0738        0.188      0.00196\n","     57    17      0.00948      0.00946     2.43e-05       0.0565       0.0752       0.0445       0.0804       0.0625       0.0579        0.101       0.0796        0.363      0.00378\n","     57    18      0.00811      0.00811     1.71e-06       0.0527       0.0697       0.0415       0.0752       0.0583       0.0531       0.0944       0.0738       0.0791     0.000824\n","     57    19      0.00989      0.00987     2.16e-05       0.0577       0.0769       0.0469       0.0795       0.0632       0.0609        0.102       0.0812        0.345      0.00359\n","     57    20       0.0103       0.0102     3.25e-05       0.0596       0.0783       0.0491       0.0807       0.0649       0.0619        0.103       0.0827        0.422      0.00439\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     57     1      0.00856      0.00856     2.68e-06        0.054       0.0716       0.0428       0.0764       0.0596       0.0555        0.096       0.0757        0.101      0.00105\n","     57     2      0.00854      0.00854     1.19e-06       0.0532       0.0715       0.0419       0.0759       0.0589       0.0545       0.0969       0.0757       0.0604     0.000629\n","     57     3      0.00795      0.00794     1.41e-06       0.0516        0.069       0.0408       0.0733       0.0571       0.0524       0.0937        0.073       0.0689     0.000718\n","     57     4      0.00833      0.00833     1.27e-06       0.0525       0.0706       0.0415       0.0747       0.0581       0.0534       0.0962       0.0748       0.0736     0.000767\n","     57     5      0.00778      0.00778     1.42e-06       0.0515       0.0682       0.0414       0.0717       0.0565        0.054       0.0902       0.0721       0.0615     0.000641\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              57  220.057    0.005      0.00811      1.4e-05      0.00812       0.0526       0.0697       0.0423       0.0733       0.0578       0.0542       0.0932       0.0737        0.222      0.00232\n","! Validation         57  220.057    0.005      0.00823     1.59e-06      0.00823       0.0526       0.0702       0.0417       0.0744        0.058        0.054       0.0946       0.0743       0.0731     0.000762\n","Wall time: 220.05819876999976\n","! Best model       57    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     58     1      0.00785      0.00785     3.68e-06       0.0526       0.0685       0.0413        0.075       0.0582       0.0521       0.0931       0.0726        0.137      0.00142\n","     58     2      0.00659      0.00658      1.3e-05        0.047       0.0627       0.0371        0.067        0.052       0.0474       0.0856       0.0665        0.261      0.00272\n","     58     3      0.00885      0.00881     4.17e-05       0.0544       0.0726       0.0437       0.0759       0.0598       0.0563       0.0973       0.0768        0.471      0.00491\n","     58     4       0.0084      0.00838     1.74e-05        0.054       0.0708       0.0447       0.0727       0.0587       0.0568       0.0927       0.0748        0.302      0.00315\n","     58     5      0.00868      0.00867     1.05e-05       0.0544        0.072       0.0434       0.0765       0.0599       0.0551       0.0974       0.0763        0.221       0.0023\n","     58     6      0.00771      0.00769     1.77e-05       0.0516       0.0679       0.0414        0.072       0.0567       0.0533       0.0902       0.0717        0.305      0.00318\n","     58     7      0.00666      0.00663     2.49e-05       0.0486        0.063       0.0398       0.0663       0.0531       0.0493       0.0839       0.0666        0.366      0.00381\n","     58     8       0.0076       0.0076     2.86e-06       0.0511       0.0674       0.0414       0.0705        0.056       0.0538       0.0887       0.0712         0.11      0.00115\n","     58     9      0.00787      0.00787     7.58e-06        0.052       0.0686       0.0408       0.0742       0.0575       0.0521       0.0933       0.0727        0.199      0.00207\n","     58    10       0.0101       0.0101     3.78e-05       0.0594       0.0776       0.0484       0.0812       0.0648       0.0619        0.102        0.082        0.434      0.00452\n","     58    11      0.00868      0.00867      5.1e-06       0.0548       0.0721       0.0475       0.0694       0.0584       0.0601       0.0914       0.0757        0.158      0.00165\n","     58    12      0.00748      0.00747     6.81e-06       0.0513       0.0669       0.0408       0.0724       0.0566       0.0518       0.0897       0.0708        0.173       0.0018\n","     58    13      0.00837      0.00836     5.47e-06       0.0531       0.0707       0.0409       0.0775       0.0592       0.0529       0.0971        0.075        0.165      0.00172\n","     58    14      0.00716      0.00715     1.83e-05       0.0499       0.0654       0.0399       0.0698       0.0549       0.0508       0.0876       0.0692        0.308      0.00321\n","     58    15      0.00868      0.00867     1.21e-05       0.0543        0.072       0.0444        0.074       0.0592       0.0578       0.0943        0.076        0.256      0.00266\n","     58    16      0.00782      0.00782     3.14e-06       0.0514       0.0684       0.0408       0.0725       0.0566        0.052       0.0929       0.0725        0.117      0.00122\n","     58    17       0.0078      0.00777     3.01e-05       0.0515       0.0682       0.0402        0.074       0.0571       0.0508       0.0937       0.0723        0.405      0.00422\n","     58    18      0.00769      0.00769     5.07e-07        0.051       0.0679       0.0409       0.0713       0.0561       0.0524       0.0913       0.0718       0.0457     0.000476\n","     58    19      0.00701        0.007     9.72e-06       0.0485       0.0647       0.0389       0.0677       0.0533       0.0495       0.0875       0.0685        0.218      0.00227\n","     58    20      0.00752      0.00752      9.8e-07       0.0504       0.0671       0.0395       0.0721       0.0558       0.0512       0.0909       0.0711       0.0648     0.000675\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     58     1       0.0085      0.00849     2.68e-06       0.0538       0.0713       0.0426       0.0762       0.0594       0.0552       0.0957       0.0755        0.102      0.00106\n","     58     2      0.00848      0.00847     1.12e-06        0.053       0.0712       0.0417       0.0757       0.0587       0.0542       0.0966       0.0754       0.0575     0.000599\n","     58     3      0.00789      0.00789     1.41e-06       0.0515       0.0687       0.0406       0.0732       0.0569       0.0521       0.0935       0.0728       0.0702     0.000731\n","     58     4      0.00827      0.00826     1.28e-06       0.0523       0.0703       0.0412       0.0744       0.0578       0.0531       0.0959       0.0745       0.0745     0.000776\n","     58     5      0.00772      0.00771     1.37e-06       0.0513       0.0679       0.0412       0.0714       0.0563       0.0537       0.0899       0.0718       0.0611     0.000637\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              58  223.862    0.005      0.00791     1.35e-05      0.00793       0.0521       0.0688       0.0418       0.0726       0.0572       0.0535       0.0921       0.0728        0.236      0.00246\n","! Validation         58  223.862    0.005      0.00817     1.57e-06      0.00817       0.0524       0.0699       0.0415       0.0742       0.0578       0.0537       0.0944        0.074        0.073     0.000761\n","Wall time: 223.86286062199997\n","! Best model       58    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     59     1      0.00664      0.00664     3.19e-06       0.0472        0.063       0.0378       0.0661       0.0519       0.0481       0.0854       0.0668        0.121      0.00126\n","     59     2      0.00713      0.00713      8.8e-07       0.0491       0.0653        0.038       0.0713       0.0546       0.0481       0.0904       0.0692       0.0576       0.0006\n","     59     3      0.00893      0.00892      3.1e-06        0.056       0.0731       0.0459        0.076        0.061       0.0587       0.0956       0.0771        0.111      0.00116\n","     59     4      0.00872       0.0087     1.77e-05       0.0553       0.0722       0.0467       0.0724       0.0596       0.0595       0.0924        0.076        0.306      0.00319\n","     59     5      0.00832      0.00832     2.51e-06       0.0523       0.0706       0.0404       0.0759       0.0582       0.0529       0.0966       0.0748          0.1      0.00105\n","     59     6      0.00755      0.00754     1.25e-05       0.0515       0.0672       0.0414       0.0716       0.0565       0.0525       0.0896       0.0711        0.259       0.0027\n","     59     7      0.00741       0.0074     1.06e-05       0.0513       0.0666       0.0424       0.0693       0.0558       0.0535        0.087       0.0702        0.241      0.00251\n","     59     8      0.00881       0.0088     1.28e-05       0.0542       0.0726       0.0412       0.0801       0.0607       0.0531        0.101       0.0769        0.264      0.00275\n","     59     9      0.00736      0.00732     4.47e-05       0.0497       0.0662       0.0399       0.0694       0.0547       0.0515       0.0885         0.07        0.496      0.00517\n","     59    10      0.00795      0.00789     5.88e-05       0.0524       0.0687       0.0426       0.0722       0.0574       0.0549       0.0902       0.0725        0.561      0.00584\n","     59    11      0.00667      0.00667     3.71e-06       0.0481       0.0632       0.0385       0.0673       0.0529       0.0487        0.085       0.0669        0.136      0.00142\n","     59    12      0.00867      0.00865     1.94e-05       0.0539        0.072       0.0433       0.0751       0.0592       0.0555       0.0968       0.0762        0.316      0.00329\n","     59    13      0.00779      0.00777     1.85e-05       0.0508       0.0682       0.0404       0.0714       0.0559       0.0514       0.0931       0.0723        0.302      0.00315\n","     59    14      0.00767      0.00766     1.34e-06       0.0515       0.0677       0.0402        0.074       0.0571       0.0512       0.0923       0.0717       0.0738     0.000769\n","     59    15      0.00695      0.00695     1.68e-06       0.0484       0.0645       0.0386       0.0679       0.0533       0.0488       0.0879       0.0683       0.0797      0.00083\n","     59    16      0.00771       0.0077     3.55e-06       0.0514       0.0679       0.0401       0.0738        0.057       0.0521       0.0917       0.0719        0.121      0.00126\n","     59    17      0.00709      0.00708      5.3e-06         0.05       0.0651       0.0409       0.0682       0.0546       0.0513       0.0863       0.0688        0.163       0.0017\n","     59    18      0.00825      0.00824      1.5e-05       0.0536       0.0702       0.0427       0.0755       0.0591       0.0541       0.0945       0.0743        0.268      0.00279\n","     59    19        0.008      0.00798     1.71e-05       0.0521       0.0691       0.0417       0.0729       0.0573       0.0544       0.0917       0.0731        0.292      0.00305\n","     59    20      0.00964      0.00962     1.76e-05       0.0589       0.0759       0.0501       0.0765       0.0633       0.0635        0.096       0.0797        0.309      0.00322\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     59     1      0.00842      0.00842     2.61e-06       0.0535        0.071       0.0423       0.0759       0.0591       0.0548       0.0954       0.0751       0.0993      0.00103\n","     59     2       0.0084       0.0084     1.24e-06       0.0528       0.0709       0.0414       0.0754       0.0584       0.0539       0.0963       0.0751       0.0607     0.000633\n","     59     3      0.00784      0.00784     1.43e-06       0.0513       0.0685       0.0404        0.073       0.0567       0.0518       0.0933       0.0726       0.0698     0.000727\n","     59     4      0.00819      0.00819     1.25e-06       0.0521         0.07        0.041       0.0742       0.0576       0.0528       0.0956       0.0742       0.0729     0.000759\n","     59     5      0.00764      0.00764     1.39e-06       0.0511       0.0676        0.041       0.0712       0.0561       0.0533       0.0896       0.0715       0.0607     0.000633\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              59  227.683    0.005      0.00785     1.35e-05      0.00786       0.0519       0.0685       0.0417       0.0724        0.057       0.0533       0.0917       0.0725        0.229      0.00238\n","! Validation         59  227.683    0.005       0.0081     1.58e-06       0.0081       0.0521       0.0696       0.0412       0.0739       0.0576       0.0533       0.0941       0.0737       0.0727     0.000757\n","Wall time: 227.6836492780003\n","! Best model       59    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     60     1       0.0109       0.0109     1.42e-05       0.0632       0.0808       0.0533        0.083       0.0682       0.0678        0.102       0.0848        0.272      0.00283\n","     60     2       0.0122       0.0122     3.25e-05       0.0658       0.0854       0.0539       0.0894       0.0717        0.068        0.112       0.0902        0.422       0.0044\n","     60     3      0.00982      0.00981     6.81e-06       0.0586       0.0766       0.0476       0.0806       0.0641       0.0602        0.102        0.081        0.184      0.00192\n","     60     4      0.00815      0.00814     6.37e-06       0.0522       0.0698       0.0416       0.0736       0.0576       0.0541       0.0937       0.0739        0.182      0.00189\n","     60     5      0.00722      0.00717     5.25e-05       0.0493       0.0655       0.0408       0.0663       0.0535       0.0521       0.0862       0.0692        0.536      0.00558\n","     60     6      0.00859      0.00858     9.13e-06       0.0557       0.0717       0.0479       0.0714       0.0596       0.0607       0.0896       0.0752        0.206      0.00214\n","     60     7      0.00801      0.00797     3.82e-05       0.0517       0.0691       0.0409       0.0731        0.057       0.0533       0.0929       0.0731        0.458      0.00477\n","     60     8      0.00772      0.00766     5.77e-05       0.0514       0.0677       0.0415       0.0712       0.0563       0.0519       0.0915       0.0717        0.558      0.00581\n","     60     9      0.00901      0.00899     1.49e-05       0.0568       0.0734       0.0489       0.0727       0.0608       0.0617       0.0923        0.077        0.276      0.00287\n","     60    10      0.00804        0.008     4.52e-05       0.0533       0.0692       0.0434       0.0732       0.0583       0.0542       0.0921       0.0731        0.498      0.00519\n","     60    11      0.00859      0.00857     2.28e-05       0.0546       0.0716        0.042       0.0797       0.0609       0.0528       0.0991       0.0759        0.346      0.00361\n","     60    12      0.00701      0.00701     3.45e-06       0.0493       0.0648       0.0387       0.0705       0.0546        0.049       0.0883       0.0686         0.11      0.00115\n","     60    13      0.00764      0.00764      4.7e-06       0.0511       0.0676       0.0402       0.0729       0.0565       0.0511       0.0921       0.0716        0.136      0.00142\n","     60    14       0.0082      0.00816      4.5e-05       0.0536       0.0699       0.0434       0.0739       0.0587       0.0539        0.094        0.074        0.493      0.00513\n","     60    15       0.0112       0.0111     1.03e-05       0.0629       0.0817       0.0541       0.0806       0.0674       0.0683        0.103       0.0858        0.212      0.00221\n","     60    16       0.0122       0.0122     3.36e-06       0.0653       0.0855       0.0536       0.0887       0.0711       0.0693        0.111       0.0902        0.123      0.00128\n","     60    17      0.00999      0.00998     5.22e-06        0.059       0.0773       0.0488       0.0796       0.0642       0.0613        0.102       0.0817        0.163      0.00169\n","     60    18      0.00859      0.00859     1.48e-06       0.0538       0.0717       0.0445       0.0725       0.0585       0.0576       0.0937       0.0757       0.0811     0.000844\n","     60    19      0.00865      0.00864     2.22e-06       0.0537       0.0719       0.0418       0.0776       0.0597       0.0549       0.0975       0.0762          0.1      0.00105\n","     60    20        0.011       0.0109     5.14e-05       0.0626       0.0808        0.055       0.0776       0.0663       0.0704       0.0984       0.0844        0.527      0.00549\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     60     1      0.00836      0.00835     2.75e-06       0.0533       0.0707       0.0421       0.0757       0.0589       0.0545       0.0952       0.0748        0.103      0.00107\n","     60     2      0.00834      0.00833     1.06e-06       0.0526       0.0706       0.0412       0.0752       0.0582       0.0536        0.096       0.0748       0.0552     0.000575\n","     60     3      0.00779      0.00778     1.41e-06       0.0511       0.0683       0.0402       0.0728       0.0565       0.0516       0.0931       0.0723       0.0706     0.000735\n","     60     4      0.00814      0.00813      1.3e-06       0.0519       0.0698       0.0408       0.0741       0.0574       0.0524       0.0954       0.0739       0.0753     0.000784\n","     60     5      0.00758      0.00758     1.32e-06       0.0508       0.0674       0.0408        0.071       0.0559        0.053       0.0894       0.0712       0.0586      0.00061\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              60  231.575    0.005      0.00911     2.14e-05      0.00914       0.0562       0.0739       0.0461       0.0764       0.0612        0.059        0.097        0.078        0.294      0.00306\n","! Validation         60  231.575    0.005      0.00804     1.57e-06      0.00804       0.0519       0.0694        0.041       0.0738       0.0574        0.053       0.0938       0.0734       0.0725     0.000755\n","Wall time: 231.5754550089996\n","! Best model       60    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     61     1       0.0133       0.0133     3.15e-06       0.0701       0.0891       0.0625       0.0853       0.0739       0.0775        0.109       0.0931        0.129      0.00134\n","     61     2       0.0093      0.00929     5.07e-06       0.0568       0.0746       0.0465       0.0775        0.062       0.0596       0.0978       0.0787        0.149      0.00155\n","     61     3      0.00683       0.0068     3.59e-05        0.048       0.0638       0.0375       0.0692       0.0533       0.0478       0.0874       0.0676        0.443      0.00461\n","     61     4      0.00761      0.00761     1.39e-06       0.0514       0.0675       0.0407       0.0728       0.0567       0.0511       0.0919       0.0715       0.0607     0.000633\n","     61     5      0.00796      0.00789      7.2e-05       0.0528       0.0687       0.0446       0.0694        0.057       0.0553       0.0897       0.0725        0.627      0.00653\n","     61     6      0.00675      0.00675     2.64e-07       0.0486       0.0636       0.0402       0.0653       0.0528       0.0508       0.0834       0.0671       0.0342     0.000356\n","     61     7      0.00727      0.00724     2.67e-05       0.0497       0.0658       0.0393       0.0705       0.0549       0.0506       0.0887       0.0697        0.375       0.0039\n","     61     8       0.0077       0.0077      2.7e-06       0.0522       0.0679       0.0426       0.0713       0.0569       0.0541       0.0893       0.0717        0.109      0.00113\n","     61     9      0.00905      0.00904     1.72e-05       0.0567       0.0735       0.0479       0.0742       0.0611       0.0602       0.0948       0.0775        0.284      0.00296\n","     61    10      0.00775      0.00774     1.21e-05       0.0516        0.068       0.0406       0.0735        0.057       0.0514       0.0928       0.0721        0.254      0.00264\n","     61    11      0.00749      0.00747     1.16e-05       0.0502       0.0669        0.041       0.0686       0.0548        0.053       0.0883       0.0707        0.247      0.00257\n","     61    12      0.00978      0.00974     4.39e-05       0.0578       0.0764       0.0465       0.0803       0.0634       0.0599        0.102       0.0807        0.487      0.00508\n","     61    13      0.00976      0.00975     1.25e-05       0.0595       0.0764       0.0507       0.0769       0.0638       0.0631       0.0977       0.0804        0.259       0.0027\n","     61    14      0.00946      0.00944     1.97e-05       0.0569       0.0752        0.047       0.0767       0.0619       0.0599       0.0989       0.0794        0.325      0.00339\n","     61    15      0.00794      0.00794     3.41e-06        0.052       0.0689       0.0399       0.0763       0.0581       0.0514       0.0947        0.073        0.105       0.0011\n","     61    16      0.00886      0.00885     6.91e-06       0.0562       0.0728       0.0481       0.0724       0.0602        0.061        0.092       0.0765        0.188      0.00196\n","     61    17       0.0104       0.0104     3.11e-05       0.0612       0.0788        0.053       0.0775       0.0652       0.0667       0.0987       0.0827        0.404      0.00421\n","     61    18      0.00889      0.00888     9.26e-06       0.0552       0.0729        0.044       0.0777       0.0608       0.0559       0.0985       0.0772        0.215      0.00224\n","     61    19      0.00858      0.00857     5.66e-06       0.0541       0.0716       0.0415       0.0793       0.0604        0.053       0.0988       0.0759        0.153      0.00159\n","     61    20       0.0072      0.00719      1.5e-05       0.0495       0.0656       0.0403       0.0681       0.0542       0.0517       0.0869       0.0693        0.284      0.00296\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     61     1      0.00829      0.00829     2.68e-06       0.0531       0.0704       0.0419       0.0755       0.0587       0.0542       0.0949       0.0745        0.101      0.00106\n","     61     2      0.00827      0.00827      1.1e-06       0.0523       0.0704        0.041        0.075        0.058       0.0533       0.0957       0.0745       0.0562     0.000585\n","     61     3      0.00773      0.00773     1.39e-06       0.0509        0.068         0.04       0.0726       0.0563       0.0513       0.0928       0.0721       0.0695     0.000724\n","     61     4      0.00807      0.00807     1.35e-06       0.0516       0.0695       0.0405       0.0739       0.0572       0.0521       0.0952       0.0737       0.0764     0.000795\n","     61     5      0.00752      0.00752     1.31e-06       0.0506       0.0671       0.0405       0.0708       0.0557       0.0528       0.0891       0.0709        0.059     0.000614\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              61  235.440    0.005      0.00858     1.68e-05      0.00859       0.0545       0.0716       0.0447       0.0741       0.0594       0.0571       0.0942       0.0757        0.257      0.00267\n","! Validation         61  235.440    0.005      0.00798     1.57e-06      0.00798       0.0517       0.0691       0.0408       0.0735       0.0572       0.0528       0.0936       0.0732       0.0725     0.000755\n","Wall time: 235.44109724900045\n","! Best model       61    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     62     1      0.00707      0.00707     1.89e-06       0.0499       0.0651       0.0393       0.0711       0.0552       0.0493       0.0885       0.0689       0.0785     0.000818\n","     62     2      0.00769      0.00766     2.29e-05        0.051       0.0677        0.041       0.0709        0.056        0.053       0.0902       0.0716        0.351      0.00366\n","     62     3      0.00726      0.00724     2.04e-05       0.0495       0.0658       0.0391       0.0705       0.0548       0.0507       0.0886       0.0697        0.324      0.00338\n","     62     4       0.0066       0.0066     3.05e-06       0.0468       0.0628       0.0353       0.0699       0.0526       0.0451       0.0883       0.0667        0.103      0.00108\n","     62     5      0.00843      0.00842     1.19e-05       0.0536        0.071        0.043       0.0749       0.0589        0.055       0.0953       0.0751        0.253      0.00264\n","     62     6      0.00723      0.00723     2.98e-06       0.0496       0.0658       0.0388        0.071       0.0549       0.0492       0.0902       0.0697        0.105      0.00109\n","     62     7      0.00705      0.00704     4.65e-06       0.0502       0.0649       0.0408       0.0691       0.0549       0.0508       0.0865       0.0687         0.15      0.00157\n","     62     8      0.00726      0.00726     7.26e-07       0.0497       0.0659         0.04       0.0692       0.0546       0.0512       0.0883       0.0698        0.049     0.000511\n","     62     9      0.00806      0.00803     2.79e-05       0.0532       0.0693       0.0447       0.0702       0.0574       0.0558       0.0905       0.0731        0.388      0.00404\n","     62    10      0.00982      0.00982     1.79e-06       0.0587       0.0767       0.0485       0.0792       0.0638       0.0621       0.0996       0.0808       0.0861     0.000897\n","     62    11      0.00705        0.007     5.15e-05       0.0487       0.0647       0.0371       0.0718       0.0545       0.0465       0.0908       0.0687         0.53      0.00552\n","     62    12      0.00825      0.00822     3.16e-05       0.0522       0.0701       0.0411       0.0746       0.0578       0.0534       0.0952       0.0743        0.412      0.00429\n","     62    13      0.00813      0.00809     4.02e-05       0.0523       0.0696       0.0432       0.0703       0.0568        0.056       0.0908       0.0734        0.467      0.00487\n","     62    14      0.00782      0.00774     8.23e-05       0.0502       0.0681       0.0391       0.0726       0.0558       0.0499       0.0944       0.0722        0.672        0.007\n","     62    15      0.00785      0.00784     3.14e-06       0.0511       0.0685       0.0401       0.0732       0.0566       0.0527       0.0924       0.0725         0.11      0.00115\n","     62    16      0.00719      0.00717        2e-05       0.0493       0.0655       0.0385        0.071       0.0547       0.0491       0.0897       0.0694        0.329      0.00342\n","     62    17      0.00795      0.00795      1.9e-06        0.052        0.069       0.0425       0.0712       0.0568       0.0546       0.0912       0.0729       0.0914     0.000952\n","     62    18      0.00892      0.00892     4.74e-06       0.0556        0.073        0.047        0.073         0.06       0.0596       0.0944        0.077        0.143      0.00149\n","     62    19      0.00685      0.00684     7.32e-06       0.0487        0.064       0.0391       0.0679       0.0535       0.0491       0.0864       0.0677        0.188      0.00196\n","     62    20      0.00735      0.00735      6.4e-07       0.0512       0.0663       0.0421       0.0694       0.0558       0.0532       0.0869         0.07       0.0469     0.000488\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     62     1      0.00823      0.00822      2.7e-06       0.0529       0.0702       0.0417       0.0752       0.0584       0.0539       0.0946       0.0743        0.101      0.00105\n","     62     2      0.00822      0.00822     1.11e-06       0.0521       0.0701       0.0408       0.0748       0.0578       0.0531       0.0955       0.0743       0.0562     0.000586\n","     62     3      0.00769      0.00769     1.39e-06       0.0507       0.0678       0.0398       0.0725       0.0562       0.0511       0.0926       0.0719       0.0691      0.00072\n","     62     4      0.00801      0.00801      1.3e-06       0.0514       0.0693       0.0403       0.0737        0.057       0.0519       0.0949       0.0734       0.0752     0.000783\n","     62     5      0.00747      0.00747      1.3e-06       0.0504       0.0669       0.0404       0.0706       0.0555       0.0525       0.0889       0.0707       0.0586      0.00061\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              62  239.294    0.005      0.00767     1.71e-05      0.00769       0.0512       0.0678        0.041       0.0715       0.0563       0.0525        0.091       0.0717        0.244      0.00254\n","! Validation         62  239.294    0.005      0.00792     1.56e-06      0.00792       0.0515       0.0689       0.0406       0.0733        0.057       0.0525       0.0933       0.0729       0.0721     0.000751\n","Wall time: 239.29443819399967\n","! Best model       62    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     63     1      0.00846      0.00844     1.15e-05       0.0532       0.0711       0.0427       0.0742       0.0584       0.0558       0.0945       0.0752        0.245      0.00255\n","     63     2       0.0076      0.00757     2.81e-05       0.0513       0.0673       0.0399       0.0741        0.057       0.0498       0.0929       0.0714        0.391      0.00408\n","     63     3      0.00935      0.00934     1.07e-05       0.0558       0.0748       0.0427        0.082       0.0623       0.0553        0.103       0.0793        0.235      0.00245\n","     63     4      0.00825      0.00825     3.51e-06       0.0542       0.0702       0.0464       0.0698       0.0581       0.0592       0.0883       0.0738        0.117      0.00121\n","     63     5      0.00979      0.00978     9.94e-06       0.0592       0.0765       0.0509       0.0756       0.0633       0.0645       0.0962       0.0803        0.216      0.00225\n","     63     6      0.00781       0.0078      5.5e-06       0.0526       0.0683       0.0429       0.0718       0.0574       0.0545       0.0898       0.0722        0.162      0.00169\n","     63     7      0.00766      0.00765     3.87e-06       0.0507       0.0677       0.0401       0.0717       0.0559       0.0525       0.0908       0.0716        0.112      0.00117\n","     63     8       0.0098       0.0098     3.26e-06        0.059       0.0766       0.0498       0.0773       0.0636       0.0627       0.0986       0.0807        0.125       0.0013\n","     63     9       0.0124       0.0124     1.58e-05       0.0681       0.0862       0.0614       0.0814       0.0714       0.0755        0.104       0.0899        0.281      0.00293\n","     63    10      0.00843      0.00842     8.57e-06       0.0543        0.071       0.0445       0.0738       0.0591       0.0559       0.0943       0.0751        0.215      0.00224\n","     63    11      0.00706      0.00706     3.83e-06       0.0487        0.065       0.0384       0.0692       0.0538       0.0494       0.0883       0.0688         0.13      0.00135\n","     63    12      0.00928      0.00928     4.52e-06       0.0572       0.0745       0.0464       0.0788       0.0626       0.0589       0.0986       0.0787        0.146      0.00152\n","     63    13       0.0108       0.0108     2.33e-05       0.0635       0.0802       0.0551       0.0802       0.0676       0.0682          0.1       0.0841        0.355       0.0037\n","     63    14       0.0106       0.0106     1.16e-06       0.0621       0.0797       0.0544       0.0776        0.066       0.0677       0.0995       0.0836       0.0672       0.0007\n","     63    15      0.00766      0.00765     4.67e-06       0.0515       0.0677       0.0427       0.0693        0.056       0.0543       0.0885       0.0714        0.152      0.00158\n","     63    16      0.00753      0.00752     1.35e-05       0.0506       0.0671       0.0397       0.0725       0.0561       0.0507       0.0914       0.0711        0.266      0.00277\n","     63    17       0.0103       0.0102     6.25e-05       0.0604       0.0781       0.0521       0.0771       0.0646       0.0656       0.0984        0.082        0.584      0.00608\n","     63    18       0.0127       0.0127     8.26e-06       0.0681       0.0871       0.0575       0.0891       0.0733       0.0723        0.111       0.0916        0.202      0.00211\n","     63    19       0.0106       0.0106     1.79e-05       0.0606       0.0796       0.0517       0.0782        0.065       0.0655        0.102       0.0838        0.313      0.00326\n","     63    20      0.00684      0.00682     1.75e-05       0.0484       0.0639        0.039        0.067        0.053       0.0486       0.0867       0.0677        0.305      0.00318\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     63     1      0.00817      0.00817     2.64e-06       0.0526       0.0699       0.0415        0.075       0.0582       0.0537       0.0944        0.074          0.1      0.00104\n","     63     2      0.00816      0.00816     1.22e-06       0.0519       0.0699       0.0406       0.0746       0.0576       0.0528       0.0953        0.074       0.0581     0.000605\n","     63     3      0.00764      0.00764     1.41e-06       0.0505       0.0676       0.0396       0.0723        0.056       0.0508       0.0925       0.0717        0.069     0.000719\n","     63     4      0.00796      0.00796      1.3e-06       0.0513        0.069       0.0401       0.0735       0.0568       0.0516       0.0947       0.0731        0.074     0.000771\n","     63     5      0.00741      0.00741     1.29e-06       0.0502       0.0666       0.0402       0.0704       0.0553       0.0522       0.0886       0.0704        0.058     0.000604\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              63  243.092    0.005      0.00913     1.29e-05      0.00914       0.0565       0.0739       0.0469       0.0755       0.0612       0.0598       0.0961        0.078        0.231      0.00241\n","! Validation         63  243.092    0.005      0.00787     1.57e-06      0.00787       0.0513       0.0686       0.0404       0.0732       0.0568       0.0522       0.0931       0.0727       0.0719     0.000749\n","Wall time: 243.09262724499968\n","! Best model       63    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     64     1      0.00853      0.00853     1.36e-06       0.0536       0.0714       0.0432       0.0743       0.0587       0.0562       0.0949       0.0755       0.0701      0.00073\n","     64     2      0.00956       0.0095     6.42e-05       0.0591       0.0754       0.0502        0.077       0.0636       0.0618        0.097       0.0794        0.594      0.00619\n","     64     3      0.00859      0.00859     2.22e-06       0.0542       0.0717       0.0444       0.0738       0.0591        0.057       0.0944       0.0757        0.109      0.00114\n","     64     4      0.00728      0.00728     2.98e-06       0.0499        0.066       0.0395       0.0708       0.0551       0.0496       0.0903       0.0699        0.114      0.00119\n","     64     5      0.00803      0.00802     8.46e-06       0.0519       0.0693       0.0421       0.0715       0.0568       0.0539       0.0927       0.0733        0.207      0.00215\n","     64     6      0.00771       0.0077     8.09e-06       0.0504       0.0679       0.0395       0.0721       0.0558       0.0507       0.0932        0.072        0.198      0.00206\n","     64     7      0.00754      0.00754     1.04e-06       0.0503       0.0672       0.0395        0.072       0.0557       0.0518       0.0904       0.0711       0.0592     0.000616\n","     64     8      0.00771       0.0077      1.2e-05        0.051       0.0679       0.0416       0.0699       0.0557       0.0538       0.0897       0.0717         0.22      0.00229\n","     64     9      0.00713      0.00713      1.9e-06       0.0488       0.0653       0.0395       0.0673       0.0534        0.051       0.0872       0.0691        0.085     0.000885\n","     64    10      0.00809      0.00808     2.44e-06       0.0524       0.0696       0.0414       0.0745        0.058       0.0534       0.0938       0.0736        0.111      0.00116\n","     64    11      0.00657      0.00656     2.64e-06       0.0474       0.0627       0.0382        0.066       0.0521       0.0482       0.0845       0.0663        0.112      0.00117\n","     64    12      0.00808      0.00807     4.75e-06       0.0518       0.0695       0.0393       0.0768       0.0581       0.0498       0.0977       0.0737        0.151      0.00157\n","     64    13      0.00837      0.00837     1.02e-06       0.0526       0.0708       0.0419       0.0738       0.0579       0.0532       0.0968        0.075       0.0725     0.000755\n","     64    14      0.00766      0.00761     5.03e-05       0.0519       0.0675       0.0437       0.0684       0.0561       0.0551       0.0871       0.0711        0.524      0.00546\n","     64    15      0.00728      0.00728     1.13e-06       0.0496        0.066       0.0409        0.067        0.054       0.0524       0.0871       0.0697       0.0713     0.000743\n","     64    16      0.00788      0.00784     3.55e-05       0.0523       0.0685       0.0411       0.0745       0.0578       0.0529       0.0922       0.0725        0.441       0.0046\n","     64    17      0.00791      0.00788     2.62e-05       0.0518       0.0687        0.041       0.0735       0.0572       0.0533       0.0921       0.0727        0.375       0.0039\n","     64    18      0.00773      0.00773     3.21e-06       0.0497        0.068       0.0376        0.074       0.0558       0.0502        0.094       0.0721        0.117      0.00122\n","     64    19      0.00843       0.0084      3.3e-05       0.0532       0.0709       0.0396       0.0805       0.0601       0.0507       0.0996       0.0752         0.41      0.00427\n","     64    20      0.00715      0.00715      8.4e-06       0.0495       0.0654       0.0398       0.0688       0.0543       0.0509       0.0875       0.0692         0.21      0.00219\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     64     1      0.00811       0.0081     2.65e-06       0.0524       0.0696       0.0412       0.0748        0.058       0.0534       0.0941       0.0737       0.0993      0.00103\n","     64     2      0.00811      0.00811     1.19e-06       0.0518       0.0697       0.0404       0.0744       0.0574       0.0526        0.095       0.0738        0.057     0.000594\n","     64     3       0.0076       0.0076     1.41e-06       0.0504       0.0674       0.0395       0.0722       0.0558       0.0506       0.0923       0.0715        0.069     0.000719\n","     64     4       0.0079       0.0079     1.32e-06        0.051       0.0688       0.0399       0.0733       0.0566       0.0513       0.0944       0.0729       0.0758     0.000789\n","     64     5      0.00737      0.00737     1.23e-06       0.0501       0.0664         0.04       0.0702       0.0551        0.052       0.0885       0.0702       0.0559     0.000582\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              64  246.900    0.005      0.00785     1.35e-05      0.00786       0.0516       0.0685       0.0412       0.0723       0.0568       0.0529       0.0922       0.0725        0.213      0.00221\n","! Validation         64  246.900    0.005      0.00782     1.56e-06      0.00782       0.0511       0.0684       0.0402        0.073       0.0566        0.052       0.0929       0.0724       0.0714     0.000744\n","Wall time: 246.90051268499974\n","! Best model       64    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     65     1      0.00775      0.00775     3.63e-07       0.0508       0.0681       0.0396       0.0732       0.0564       0.0515       0.0928       0.0721       0.0326      0.00034\n","     65     2      0.00784      0.00783     4.98e-06       0.0512       0.0685        0.041       0.0716       0.0563       0.0523       0.0927       0.0725        0.161      0.00167\n","     65     3      0.00855      0.00853     1.63e-05       0.0551       0.0715        0.047       0.0712       0.0591       0.0593        0.091       0.0752        0.281      0.00293\n","     65     4      0.00766      0.00766     1.94e-06       0.0511       0.0677       0.0413       0.0706        0.056       0.0518       0.0916       0.0717       0.0898     0.000936\n","     65     5       0.0105       0.0105     7.35e-06       0.0626       0.0793       0.0578       0.0721        0.065       0.0725       0.0913       0.0819        0.197      0.00206\n","     65     6      0.00963      0.00963     6.54e-07       0.0598       0.0759       0.0561       0.0673       0.0617       0.0696       0.0873       0.0784       0.0396     0.000413\n","     65     7      0.00793      0.00793     3.19e-07       0.0517       0.0689       0.0397       0.0759       0.0578         0.05       0.0962       0.0731       0.0402     0.000419\n","     65     8      0.00763      0.00761     2.04e-05       0.0523       0.0675       0.0445       0.0679       0.0562       0.0555       0.0866        0.071        0.334      0.00348\n","     65     9      0.00998      0.00997     5.65e-07       0.0586       0.0773        0.047       0.0817       0.0644       0.0602        0.103       0.0817       0.0482     0.000503\n","     65    10      0.00963      0.00951     0.000125       0.0562       0.0754        0.044       0.0805       0.0622       0.0565        0.103       0.0799        0.829      0.00863\n","     65    11      0.00829      0.00829     3.51e-06        0.055       0.0704       0.0461       0.0726       0.0594       0.0581       0.0902       0.0741        0.117      0.00122\n","     65    12      0.00812      0.00799     0.000132       0.0527       0.0692       0.0427       0.0727       0.0577       0.0542       0.0921       0.0731        0.853      0.00888\n","     65    13      0.00832       0.0083     1.24e-05       0.0546       0.0705       0.0476       0.0685       0.0581       0.0594       0.0886        0.074        0.252      0.00263\n","     65    14      0.00814      0.00809     5.52e-05       0.0532       0.0696       0.0428        0.074       0.0584       0.0552       0.0917       0.0735        0.547      0.00569\n","     65    15      0.00812      0.00806     5.63e-05       0.0529       0.0695       0.0438       0.0709       0.0574       0.0565         0.09       0.0732        0.547      0.00569\n","     65    16      0.00849      0.00848     1.29e-05       0.0559       0.0712       0.0489       0.0699       0.0594       0.0611       0.0881       0.0746        0.252      0.00262\n","     65    17      0.00799       0.0079     8.89e-05       0.0525       0.0688       0.0426       0.0723       0.0574       0.0535        0.092       0.0727        0.697      0.00726\n","     65    18      0.00743      0.00742     5.51e-06       0.0502       0.0667       0.0403       0.0698       0.0551       0.0519       0.0891       0.0705        0.144       0.0015\n","     65    19      0.00818      0.00816     2.75e-05       0.0524       0.0699       0.0405       0.0763       0.0584       0.0514       0.0967       0.0741        0.371      0.00386\n","     65    20      0.00826      0.00825     1.35e-05       0.0534       0.0703       0.0426       0.0748       0.0587       0.0535       0.0953       0.0744        0.257      0.00268\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     65     1      0.00807      0.00806     2.66e-06       0.0522       0.0695       0.0411       0.0746       0.0578       0.0531        0.094       0.0736        0.101      0.00105\n","     65     2      0.00806      0.00806     1.09e-06       0.0516       0.0694       0.0403       0.0743       0.0573       0.0523       0.0948       0.0736       0.0547      0.00057\n","     65     3      0.00756      0.00756     1.33e-06       0.0502       0.0672       0.0393        0.072       0.0557       0.0504       0.0921       0.0713       0.0689     0.000718\n","     65     4      0.00786      0.00785     1.38e-06       0.0509       0.0686       0.0397       0.0732       0.0565       0.0511       0.0943       0.0727       0.0776     0.000809\n","     65     5      0.00731      0.00731     1.22e-06       0.0499       0.0662       0.0398         0.07       0.0549       0.0517       0.0882         0.07       0.0568     0.000592\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              65  250.704    0.005      0.00839     2.93e-05      0.00842       0.0541       0.0709       0.0448       0.0727       0.0587        0.057       0.0926       0.0748        0.304      0.00317\n","! Validation         65  250.704    0.005      0.00777     1.54e-06      0.00777        0.051       0.0682         0.04       0.0728       0.0564       0.0517       0.0927       0.0722       0.0718     0.000748\n","Wall time: 250.70451159099957\n","! Best model       65    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     66     1      0.00826      0.00826     4.04e-06       0.0552       0.0703       0.0472       0.0712       0.0592       0.0595        0.088       0.0738        0.145      0.00151\n","     66     2      0.00693      0.00693     1.19e-06       0.0485       0.0644       0.0388       0.0679       0.0534       0.0488       0.0876       0.0682       0.0705     0.000734\n","     66     3      0.00833       0.0083     3.09e-05       0.0545       0.0705       0.0447        0.074       0.0594       0.0564       0.0925       0.0744         0.41      0.00427\n","     66     4      0.00674      0.00673     1.72e-06       0.0479       0.0635        0.038       0.0678       0.0529       0.0493       0.0851       0.0672       0.0838     0.000873\n","     66     5      0.00758      0.00755     2.98e-05       0.0509       0.0672         0.04       0.0727       0.0564        0.051       0.0913       0.0712        0.395      0.00411\n","     66     6      0.00767      0.00766     3.49e-06       0.0519       0.0677       0.0405       0.0745       0.0575       0.0519       0.0915       0.0717        0.106       0.0011\n","     66     7      0.00795      0.00794     1.43e-05       0.0516       0.0689       0.0402       0.0744       0.0573       0.0513       0.0948       0.0731        0.279       0.0029\n","     66     8      0.00762      0.00761     1.24e-05       0.0517       0.0675       0.0432       0.0688        0.056       0.0556       0.0865       0.0711        0.249      0.00259\n","     66     9      0.00801        0.008     1.19e-05       0.0533       0.0692       0.0454        0.069       0.0572       0.0576        0.088       0.0728        0.255      0.00266\n","     66    10       0.0068       0.0068     4.44e-06       0.0484       0.0638        0.038        0.069       0.0535       0.0481       0.0871       0.0676        0.126      0.00131\n","     66    11      0.00823      0.00815     7.73e-05        0.053       0.0699       0.0414       0.0761       0.0588       0.0522       0.0959        0.074        0.644      0.00671\n","     66    12      0.00744      0.00744     1.38e-06       0.0509       0.0667       0.0409        0.071       0.0559       0.0524       0.0887       0.0705       0.0773     0.000806\n","     66    13      0.00889      0.00883     5.95e-05       0.0564       0.0727       0.0471       0.0752       0.0611       0.0598       0.0933       0.0766        0.569      0.00592\n","     66    14      0.00687      0.00686     1.38e-05       0.0482       0.0641       0.0381       0.0685       0.0533       0.0488       0.0869       0.0678        0.274      0.00286\n","     66    15      0.00921      0.00918     2.99e-05       0.0558       0.0741       0.0458       0.0758       0.0608        0.059       0.0977       0.0783        0.405      0.00422\n","     66    16      0.00685      0.00685     1.94e-06       0.0482        0.064       0.0381       0.0684       0.0532       0.0485       0.0871       0.0678       0.0926     0.000964\n","     66    17       0.0078      0.00779     1.23e-05       0.0516       0.0683        0.041        0.073        0.057       0.0526        0.092       0.0723        0.256      0.00267\n","     66    18      0.00723      0.00722     6.16e-06       0.0475       0.0657       0.0366       0.0691       0.0529       0.0473       0.0921       0.0697        0.177      0.00184\n","     66    19        0.008      0.00796      3.6e-05        0.051        0.069       0.0401       0.0729       0.0565       0.0509       0.0955       0.0732        0.442      0.00461\n","     66    20      0.00774      0.00773     1.32e-05       0.0513        0.068       0.0426       0.0689       0.0557       0.0545       0.0891       0.0718        0.263      0.00274\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     66     1      0.00801      0.00801     2.67e-06        0.052       0.0692       0.0409       0.0744       0.0576       0.0529       0.0937       0.0733       0.0996      0.00104\n","     66     2      0.00801        0.008     1.14e-06       0.0514       0.0692       0.0401       0.0741       0.0571       0.0521       0.0946       0.0733        0.056     0.000583\n","     66     3      0.00752      0.00752     1.38e-06       0.0501       0.0671       0.0392       0.0719       0.0555       0.0502        0.092       0.0711       0.0699     0.000728\n","     66     4       0.0078       0.0078     1.36e-06       0.0507       0.0683       0.0396        0.073       0.0563       0.0508        0.094       0.0724       0.0765     0.000797\n","     66     5      0.00726      0.00726     1.22e-06       0.0497       0.0659       0.0396       0.0698       0.0547       0.0514        0.088       0.0697        0.057     0.000594\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              66  254.500    0.005      0.00769     1.83e-05      0.00771       0.0514       0.0678       0.0414       0.0714       0.0564       0.0529       0.0906       0.0718        0.266      0.00277\n","! Validation         66  254.500    0.005      0.00772     1.55e-06      0.00772       0.0508        0.068       0.0398       0.0726       0.0562       0.0515       0.0925        0.072       0.0718     0.000748\n","Wall time: 254.500721247\n","! Best model       66    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     67     1      0.00779      0.00773      5.8e-05       0.0519        0.068       0.0427       0.0703       0.0565       0.0544       0.0892       0.0718        0.558      0.00582\n","     67     2      0.00728      0.00728     1.55e-06       0.0487        0.066       0.0375       0.0711       0.0543       0.0484       0.0916         0.07       0.0797      0.00083\n","     67     3      0.00713      0.00712     1.34e-05       0.0501       0.0653       0.0412        0.068       0.0546        0.051        0.087        0.069        0.265      0.00276\n","     67     4      0.00781      0.00778     2.21e-05       0.0513       0.0683       0.0395        0.075       0.0572       0.0507       0.0941       0.0724        0.344      0.00358\n","     67     5       0.0093      0.00925     5.11e-05       0.0564       0.0744       0.0451        0.079       0.0621        0.057        0.101       0.0788        0.529      0.00551\n","     67     6      0.00734      0.00733     7.51e-06       0.0496       0.0663       0.0383       0.0722       0.0553       0.0496       0.0908       0.0702          0.2      0.00208\n","     67     7       0.0092      0.00914     6.39e-05       0.0551        0.074       0.0429       0.0796       0.0612       0.0552        0.102       0.0784        0.583      0.00608\n","     67     8       0.0114       0.0114     2.85e-06       0.0639       0.0825       0.0571       0.0773       0.0672       0.0722          0.1       0.0861        0.115       0.0012\n","     67     9       0.0133       0.0132     4.62e-05       0.0708        0.089       0.0655       0.0814       0.0734       0.0809        0.103       0.0921        0.497      0.00517\n","     67    10        0.011       0.0109      6.6e-05        0.063       0.0809       0.0528       0.0833       0.0681       0.0659        0.105       0.0853        0.602      0.00627\n","     67    11      0.00619      0.00611      8.4e-05       0.0456       0.0605       0.0354        0.066       0.0507       0.0443       0.0839       0.0641        0.679      0.00707\n","     67    12      0.00907      0.00898     9.45e-05       0.0557       0.0733        0.044       0.0789       0.0615       0.0559       0.0994       0.0776         0.72       0.0075\n","     67    13       0.0117       0.0117     9.55e-05       0.0646       0.0835       0.0531       0.0874       0.0703       0.0666         0.11       0.0882        0.723      0.00753\n","     67    14      0.00853      0.00853     6.81e-06       0.0563       0.0714       0.0517       0.0655       0.0586        0.065       0.0829       0.0739        0.169      0.00176\n","     67    15      0.00763      0.00761      1.9e-05       0.0512       0.0675       0.0396       0.0743       0.0569       0.0507       0.0924       0.0715        0.309      0.00322\n","     67    16         0.01         0.01     9.32e-07       0.0616       0.0775       0.0572       0.0706       0.0639       0.0708       0.0895       0.0801       0.0654     0.000682\n","     67    17       0.0111       0.0111     2.37e-06       0.0654       0.0816       0.0597       0.0768       0.0682       0.0737       0.0955       0.0846        0.107      0.00111\n","     67    18      0.00898      0.00896     1.97e-05       0.0549       0.0732       0.0423       0.0802       0.0612       0.0546        0.101       0.0776        0.327       0.0034\n","     67    19      0.00783      0.00782     4.04e-06        0.052       0.0684       0.0431       0.0698       0.0565       0.0553       0.0891       0.0722        0.135       0.0014\n","     67    20       0.0105       0.0105     1.69e-05       0.0595       0.0792       0.0454       0.0877       0.0666       0.0573        0.111        0.084        0.298      0.00311\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     67     1      0.00796      0.00796     2.66e-06       0.0519        0.069       0.0407       0.0742       0.0574       0.0526       0.0935       0.0731       0.0995      0.00104\n","     67     2      0.00796      0.00796     1.18e-06       0.0513        0.069       0.0399        0.074       0.0569       0.0519       0.0944       0.0731       0.0572     0.000596\n","     67     3      0.00747      0.00747     1.39e-06       0.0499       0.0669        0.039       0.0717       0.0553         0.05       0.0918       0.0709       0.0689     0.000718\n","     67     4      0.00775      0.00775     1.37e-06       0.0505       0.0681       0.0393       0.0728       0.0561       0.0505       0.0939       0.0722       0.0771     0.000804\n","     67     5      0.00722      0.00721     1.21e-06       0.0495       0.0657       0.0394       0.0697       0.0545       0.0512       0.0878       0.0695       0.0562     0.000585\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              67  258.298    0.005      0.00912     3.38e-05      0.00916       0.0564       0.0739       0.0467       0.0757       0.0612       0.0597       0.0961       0.0779        0.365       0.0038\n","! Validation         67  258.298    0.005      0.00767     1.56e-06      0.00767       0.0506       0.0678       0.0397       0.0725       0.0561       0.0512       0.0923       0.0718       0.0718     0.000748\n","Wall time: 258.2983448690002\n","! Best model       67    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     68     1       0.0106       0.0105     1.84e-05       0.0607       0.0794       0.0495       0.0831       0.0663       0.0627        0.105       0.0839        0.311      0.00324\n","     68     2      0.00867      0.00866     8.31e-06       0.0554        0.072       0.0456        0.075       0.0603       0.0581       0.0938        0.076        0.193      0.00201\n","     68     3      0.00795      0.00792     2.47e-05       0.0509       0.0689       0.0394        0.074       0.0567       0.0507       0.0953        0.073        0.352      0.00366\n","     68     4       0.0135       0.0135     3.49e-05       0.0717       0.0899       0.0681       0.0788       0.0735       0.0838        0.101       0.0924        0.435      0.00453\n","     68     5       0.0153       0.0153     2.59e-06       0.0766       0.0958       0.0673        0.095       0.0812       0.0822        0.118          0.1        0.106       0.0011\n","     68     6       0.0101         0.01     3.21e-05       0.0594       0.0775        0.048       0.0824       0.0652       0.0597        0.104        0.082        0.416      0.00434\n","     68     7      0.00617      0.00615     1.73e-05       0.0461       0.0607       0.0364       0.0655        0.051        0.046       0.0825       0.0643        0.302      0.00314\n","     68     8      0.00953      0.00944     8.96e-05       0.0563       0.0752       0.0453       0.0781       0.0617       0.0571        0.102       0.0796          0.7      0.00729\n","     68     9      0.00941       0.0094      1.7e-05       0.0574        0.075       0.0492       0.0737       0.0615       0.0624       0.0953       0.0788        0.303      0.00315\n","     68    10      0.00729      0.00727      1.8e-05       0.0496        0.066       0.0404        0.068       0.0542       0.0521       0.0873       0.0697        0.305      0.00318\n","     68    11      0.00774      0.00773     1.47e-06       0.0524        0.068       0.0428       0.0716       0.0572       0.0542       0.0896       0.0719       0.0838     0.000873\n","     68    12      0.00839      0.00839     1.38e-06       0.0538       0.0709       0.0442       0.0731       0.0586       0.0566        0.093       0.0748       0.0727     0.000757\n","     68    13      0.00781      0.00781     2.47e-06       0.0522       0.0684        0.041       0.0745       0.0577       0.0519       0.0928       0.0724        0.104      0.00108\n","     68    14      0.00794      0.00794     5.59e-06       0.0531       0.0689       0.0436       0.0722       0.0579       0.0556       0.0898       0.0727        0.148      0.00154\n","     68    15       0.0099      0.00988     1.79e-05       0.0583       0.0769       0.0466       0.0816       0.0641       0.0594        0.103       0.0814        0.313      0.00326\n","     68    16       0.0079       0.0079     3.08e-06       0.0519       0.0687       0.0411       0.0735       0.0573       0.0518       0.0939       0.0728        0.122      0.00127\n","     68    17      0.00732       0.0073     1.86e-05       0.0495       0.0661        0.038       0.0725       0.0552       0.0488       0.0913       0.0701          0.3      0.00313\n","     68    18      0.00769      0.00769     5.83e-07       0.0507       0.0678       0.0403       0.0714       0.0559       0.0517        0.092       0.0718       0.0457     0.000476\n","     68    19      0.00747      0.00746     8.64e-06       0.0507       0.0668       0.0416       0.0687       0.0552       0.0536       0.0875       0.0705        0.175      0.00182\n","     68    20      0.00642      0.00641     1.76e-05       0.0469       0.0619       0.0371       0.0667       0.0519       0.0471       0.0841       0.0656        0.305      0.00318\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     68     1      0.00791      0.00791     2.71e-06       0.0517       0.0688       0.0405       0.0741       0.0573       0.0524       0.0934       0.0729        0.101      0.00105\n","     68     2      0.00792      0.00792     1.17e-06       0.0511       0.0688       0.0398       0.0738       0.0568       0.0517       0.0942        0.073       0.0563     0.000587\n","     68     3      0.00744      0.00744     1.39e-06       0.0498       0.0667       0.0388       0.0716       0.0552       0.0498       0.0917       0.0707       0.0695     0.000724\n","     68     4      0.00771      0.00771     1.38e-06       0.0503       0.0679       0.0391       0.0727       0.0559       0.0503       0.0937        0.072       0.0775     0.000808\n","     68     5      0.00718      0.00718     1.15e-06       0.0493       0.0655       0.0392       0.0696       0.0544        0.051       0.0877       0.0693       0.0552     0.000575\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              68  262.092    0.005      0.00884      1.7e-05      0.00885       0.0552       0.0727       0.0453        0.075       0.0601       0.0581       0.0955       0.0768        0.255      0.00265\n","! Validation         68  262.092    0.005      0.00763     1.56e-06      0.00763       0.0504       0.0676       0.0395       0.0724       0.0559        0.051       0.0922       0.0716       0.0719     0.000749\n","Wall time: 262.0931864260001\n","! Best model       68    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     69     1      0.00689      0.00688     6.16e-06       0.0488       0.0642       0.0396       0.0672       0.0534       0.0508       0.0848       0.0678        0.176      0.00183\n","     69     2      0.00686      0.00686      5.5e-06       0.0478       0.0641       0.0389       0.0657       0.0523       0.0497       0.0858       0.0678        0.167      0.00174\n","     69     3      0.00689      0.00689     4.94e-06        0.048       0.0642       0.0374       0.0693       0.0533       0.0476       0.0885       0.0681        0.158      0.00165\n","     69     4      0.00804      0.00803     8.39e-06       0.0519       0.0693       0.0397       0.0762        0.058       0.0506       0.0964       0.0735         0.21      0.00219\n","     69     5      0.00708      0.00707     4.08e-06       0.0491       0.0651       0.0388       0.0696       0.0542       0.0503       0.0874       0.0688        0.129      0.00134\n","     69     6      0.00808      0.00806     1.62e-05       0.0533       0.0695       0.0436       0.0727       0.0581       0.0563       0.0902       0.0733        0.289      0.00302\n","     69     7        0.008      0.00799     3.19e-06       0.0524       0.0692       0.0415       0.0742       0.0578       0.0533       0.0931       0.0732        0.118      0.00123\n","     69     8      0.00609      0.00607     1.97e-05       0.0457       0.0603        0.036       0.0651       0.0506        0.045       0.0827       0.0639        0.327      0.00341\n","     69     9      0.00748      0.00748     4.32e-07       0.0511       0.0669       0.0405       0.0724       0.0565       0.0508       0.0909       0.0709       0.0385     0.000401\n","     69    10      0.00758      0.00756     1.66e-05        0.051       0.0673       0.0399       0.0732       0.0566       0.0499       0.0927       0.0713          0.3      0.00313\n","     69    11      0.00759      0.00759     5.33e-06       0.0508       0.0674       0.0402       0.0721       0.0561        0.051       0.0918       0.0714        0.154      0.00161\n","     69    12      0.00642      0.00641     1.11e-05       0.0469        0.062       0.0375       0.0656       0.0516       0.0471       0.0841       0.0656        0.243      0.00253\n","     69    13      0.00836      0.00835     4.64e-06       0.0548       0.0707       0.0463       0.0719       0.0591       0.0579        0.091       0.0745        0.149      0.00155\n","     69    14       0.0096      0.00959     7.47e-06       0.0587       0.0758       0.0513       0.0736       0.0624       0.0646       0.0942       0.0794        0.193      0.00201\n","     69    15       0.0073      0.00728     1.88e-05       0.0506        0.066       0.0424       0.0671       0.0547       0.0534       0.0858       0.0696        0.318      0.00331\n","     69    16      0.00725      0.00722     3.16e-05       0.0502       0.0657       0.0411       0.0686       0.0548       0.0519        0.087       0.0695        0.413       0.0043\n","     69    17       0.0112       0.0112     4.36e-05       0.0632       0.0817       0.0523       0.0851       0.0687        0.066        0.106       0.0862        0.485      0.00505\n","     69    18       0.0139       0.0139     2.09e-05       0.0714       0.0911       0.0601       0.0939        0.077       0.0756        0.116       0.0959        0.326       0.0034\n","     69    19      0.00968      0.00968     1.32e-06       0.0583       0.0761       0.0488       0.0772        0.063        0.063       0.0972       0.0801       0.0713     0.000743\n","     69    20      0.00743      0.00743     1.16e-06       0.0501       0.0667       0.0396       0.0712       0.0554       0.0501       0.0912       0.0707       0.0705     0.000734\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     69     1      0.00788      0.00787     2.64e-06       0.0515       0.0686       0.0403       0.0739       0.0571       0.0522       0.0932       0.0727       0.0992      0.00103\n","     69     2      0.00788      0.00788     1.16e-06        0.051       0.0687       0.0396       0.0737       0.0567       0.0515       0.0941       0.0728       0.0548     0.000571\n","     69     3       0.0074       0.0074     1.43e-06       0.0496       0.0666       0.0387       0.0715       0.0551       0.0496       0.0915       0.0705        0.071      0.00074\n","     69     4      0.00767      0.00767     1.47e-06       0.0502       0.0677        0.039       0.0725       0.0558       0.0501       0.0935       0.0718       0.0786     0.000819\n","     69     5      0.00714      0.00713     1.17e-06       0.0492       0.0653       0.0391       0.0694       0.0542       0.0507       0.0875       0.0691       0.0546     0.000569\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              69  265.883    0.005      0.00807     1.16e-05      0.00809       0.0527       0.0695       0.0428       0.0726       0.0577       0.0548       0.0922       0.0735        0.217      0.00226\n","! Validation         69  265.883    0.005      0.00759     1.57e-06      0.00759       0.0503       0.0674       0.0393       0.0722       0.0558       0.0508        0.092       0.0714       0.0716     0.000746\n","Wall time: 265.88386504699974\n","! Best model       69    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     70     1       0.0103       0.0103     3.88e-05        0.062       0.0785       0.0567       0.0726       0.0647       0.0704       0.0925       0.0814        0.459      0.00478\n","     70     2      0.00951      0.00951     2.74e-06       0.0585       0.0754       0.0507       0.0741       0.0624       0.0641       0.0941       0.0791        0.111      0.00115\n","     70     3      0.00706      0.00704     1.94e-05       0.0489       0.0649       0.0408       0.0652        0.053       0.0527       0.0842       0.0684        0.317       0.0033\n","     70     4      0.00875      0.00875      2.1e-06       0.0562       0.0724       0.0489       0.0708       0.0598       0.0616       0.0901       0.0759       0.0918     0.000956\n","     70     5       0.0077       0.0077     1.54e-06       0.0504       0.0679       0.0391       0.0731       0.0561       0.0504       0.0935        0.072       0.0711     0.000741\n","     70     6      0.00722      0.00719     3.41e-05       0.0504       0.0656       0.0415       0.0682       0.0548       0.0521       0.0865       0.0693        0.422       0.0044\n","     70     7      0.00968      0.00968     1.65e-06       0.0575       0.0761       0.0457        0.081       0.0634        0.058        0.103       0.0806        0.091     0.000948\n","     70     8      0.00877      0.00869     8.22e-05       0.0539       0.0721       0.0426       0.0765       0.0595       0.0543       0.0985       0.0764        0.672        0.007\n","     70     9       0.0067       0.0067     7.92e-07       0.0479       0.0633       0.0376       0.0685        0.053       0.0477       0.0864       0.0671        0.057     0.000594\n","     70    10      0.00927      0.00918     9.53e-05       0.0554       0.0741       0.0441       0.0781       0.0611       0.0566          0.1       0.0785        0.724      0.00754\n","     70    11       0.0078      0.00779     7.03e-06       0.0523       0.0683       0.0431       0.0705       0.0568       0.0546       0.0895       0.0721        0.182      0.00189\n","     70    12      0.00631      0.00628      3.2e-05       0.0472       0.0613        0.038       0.0657       0.0518        0.048       0.0816       0.0648        0.413       0.0043\n","     70    13      0.00787      0.00785     2.65e-05       0.0521       0.0685       0.0421       0.0722       0.0572       0.0537       0.0913       0.0725        0.377      0.00393\n","     70    14      0.00806      0.00806     8.66e-07       0.0534       0.0695       0.0445       0.0714       0.0579       0.0563       0.0901       0.0732       0.0551     0.000574\n","     70    15      0.00687      0.00686     1.34e-05       0.0487       0.0641       0.0391       0.0681       0.0536       0.0497       0.0858       0.0678        0.258      0.00269\n","     70    16      0.00889      0.00888     6.07e-06       0.0568       0.0729       0.0505       0.0695         0.06       0.0637       0.0885       0.0761        0.181      0.00189\n","     70    17      0.00857      0.00856     8.83e-07       0.0541       0.0716       0.0439       0.0747       0.0593       0.0562       0.0951       0.0757       0.0598     0.000623\n","     70    18      0.00794      0.00794     2.87e-06       0.0524       0.0689       0.0413       0.0746        0.058       0.0519       0.0942        0.073        0.084     0.000875\n","     70    19       0.0069      0.00689     8.52e-06       0.0488       0.0642        0.039       0.0684       0.0537       0.0492       0.0868        0.068          0.2      0.00209\n","     70    20      0.00771       0.0077     7.72e-06       0.0514       0.0679       0.0398       0.0747       0.0572         0.05        0.094        0.072        0.197      0.00205\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     70     1      0.00783      0.00783     2.58e-06       0.0514       0.0685       0.0402       0.0737        0.057        0.052        0.093       0.0725       0.0972      0.00101\n","     70     2      0.00784      0.00784     1.22e-06       0.0509       0.0685       0.0395       0.0736       0.0565       0.0513       0.0939       0.0726       0.0577     0.000601\n","     70     3      0.00737      0.00737      1.4e-06       0.0495       0.0664       0.0385       0.0714        0.055       0.0494       0.0914       0.0704       0.0694     0.000723\n","     70     4      0.00763      0.00762      1.4e-06         0.05       0.0676       0.0389       0.0724       0.0556       0.0499       0.0933       0.0716       0.0765     0.000797\n","     70     5      0.00709      0.00709     1.19e-06        0.049       0.0651       0.0389       0.0693       0.0541       0.0505       0.0873       0.0689       0.0555     0.000578\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              70  269.688    0.005      0.00808     1.92e-05       0.0081       0.0529       0.0695       0.0434       0.0719       0.0577       0.0554       0.0915       0.0734        0.251      0.00262\n","! Validation         70  269.688    0.005      0.00755     1.56e-06      0.00755       0.0502       0.0672       0.0392       0.0721       0.0556       0.0506       0.0918       0.0712       0.0713     0.000742\n","Wall time: 269.68883392299995\n","! Best model       70    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     71     1       0.0081      0.00809      9.6e-06        0.052       0.0696       0.0405       0.0751       0.0578       0.0527       0.0947       0.0737        0.214      0.00223\n","     71     2      0.00859      0.00859     5.73e-07       0.0553       0.0717       0.0468       0.0724       0.0596       0.0589       0.0921       0.0755       0.0445     0.000464\n","     71     3      0.00751      0.00751     5.01e-07        0.051       0.0671       0.0404       0.0723       0.0563        0.051       0.0911        0.071       0.0479     0.000498\n","     71     4       0.0086      0.00859     7.72e-06        0.055       0.0717       0.0433       0.0782       0.0608       0.0541       0.0979        0.076        0.194      0.00202\n","     71     5      0.00712       0.0071     1.67e-05       0.0485       0.0652       0.0386       0.0681       0.0534         0.05        0.088        0.069          0.3      0.00313\n","     71     6      0.00918      0.00915     3.55e-05       0.0558        0.074       0.0446       0.0781       0.0614       0.0567       0.0999       0.0783        0.437      0.00456\n","     71     7      0.00637      0.00634     2.73e-05       0.0459       0.0616       0.0361       0.0655       0.0508       0.0458       0.0848       0.0653        0.388      0.00404\n","     71     8      0.00741      0.00734     6.49e-05       0.0508       0.0663       0.0407        0.071       0.0559       0.0518       0.0885       0.0701        0.595       0.0062\n","     71     9      0.00742       0.0074     2.39e-05       0.0488       0.0666        0.037       0.0723       0.0546       0.0486       0.0925       0.0706        0.354      0.00368\n","     71    10      0.00672      0.00664     7.67e-05        0.049        0.063       0.0405       0.0661       0.0533       0.0505       0.0826       0.0665        0.645      0.00672\n","     71    11      0.00687      0.00687     5.46e-07       0.0482       0.0641       0.0375       0.0696       0.0536       0.0477       0.0883        0.068        0.051     0.000531\n","     71    12      0.00787      0.00776     0.000104       0.0518       0.0682       0.0416       0.0722       0.0569       0.0534       0.0908       0.0721        0.756      0.00788\n","     71    13      0.00771      0.00767     3.67e-05       0.0502       0.0678       0.0393       0.0721       0.0557       0.0503       0.0933       0.0718        0.446      0.00464\n","     71    14      0.00712      0.00702     9.46e-05       0.0489       0.0648        0.039       0.0687       0.0538       0.0502        0.087       0.0686        0.714      0.00744\n","     71    15      0.00724       0.0072     4.68e-05       0.0495       0.0656       0.0382       0.0722       0.0552        0.048       0.0912       0.0696        0.505      0.00527\n","     71    16      0.00726      0.00725     1.58e-05       0.0488       0.0659       0.0385       0.0696        0.054       0.0488       0.0908       0.0698         0.29      0.00303\n","     71    17       0.0081      0.00807     2.43e-05       0.0538       0.0695       0.0457       0.0702       0.0579       0.0576       0.0887       0.0731        0.358      0.00373\n","     71    18      0.00723      0.00723     3.79e-06       0.0505       0.0658       0.0419       0.0678       0.0548       0.0532       0.0856       0.0694        0.126      0.00131\n","     71    19      0.00703      0.00701     1.22e-05       0.0482       0.0648       0.0378       0.0689       0.0534       0.0483        0.089       0.0687        0.247      0.00257\n","     71    20      0.00659      0.00658     1.41e-05        0.048       0.0628       0.0393       0.0656       0.0524       0.0502       0.0823       0.0663        0.254      0.00264\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     71     1      0.00779      0.00779     2.68e-06       0.0512       0.0683       0.0401       0.0735       0.0568       0.0518       0.0928       0.0723       0.0994      0.00104\n","     71     2       0.0078      0.00779     1.26e-06       0.0507       0.0683       0.0393       0.0735       0.0564       0.0511       0.0937       0.0724       0.0584     0.000608\n","     71     3      0.00734      0.00734     1.45e-06       0.0494       0.0663       0.0384       0.0714       0.0549       0.0492       0.0913       0.0702       0.0705     0.000734\n","     71     4      0.00758      0.00758     1.43e-06       0.0499       0.0674       0.0387       0.0722       0.0555       0.0497       0.0931       0.0714       0.0768       0.0008\n","     71     5      0.00705      0.00705     1.15e-06       0.0489       0.0649       0.0387       0.0691       0.0539       0.0503       0.0871       0.0687       0.0549     0.000572\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              71  273.506    0.005      0.00747     3.08e-05       0.0075       0.0505       0.0669       0.0404       0.0708       0.0556       0.0515       0.0901       0.0708        0.348      0.00363\n","! Validation         71  273.506    0.005      0.00751     1.59e-06      0.00751         0.05        0.067       0.0391       0.0719       0.0555       0.0504       0.0916        0.071        0.072      0.00075\n","Wall time: 273.50709998\n","! Best model       71    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     72     1      0.00672      0.00671     8.42e-06       0.0474       0.0634       0.0374       0.0674       0.0524       0.0485       0.0857       0.0671        0.196      0.00204\n","     72     2      0.00656      0.00656     1.91e-06       0.0467       0.0627       0.0369       0.0662       0.0516       0.0475       0.0853       0.0664       0.0875     0.000911\n","     72     3       0.0067      0.00669     6.25e-06        0.048       0.0633       0.0368       0.0704       0.0536       0.0459       0.0883       0.0671         0.16      0.00167\n","     72     4      0.00734      0.00734     1.73e-06       0.0497       0.0663       0.0394       0.0702       0.0548       0.0513       0.0889       0.0701       0.0902      0.00094\n","     72     5      0.00672      0.00672     3.24e-06       0.0481       0.0634       0.0376       0.0689       0.0533       0.0476       0.0868       0.0672        0.116       0.0012\n","     72     6      0.00834      0.00834     8.72e-07       0.0534       0.0707       0.0418       0.0766       0.0592       0.0534       0.0963       0.0749       0.0633     0.000659\n","     72     7      0.00713      0.00711     1.28e-05       0.0499       0.0653       0.0399       0.0698       0.0548       0.0504       0.0878       0.0691         0.25       0.0026\n","     72     8      0.00686      0.00685     1.15e-05        0.049        0.064       0.0401       0.0667       0.0534       0.0501       0.0853       0.0677         0.22      0.00229\n","     72     9      0.00734      0.00734     5.04e-06       0.0483       0.0663       0.0363       0.0725       0.0544       0.0472       0.0934       0.0703        0.155      0.00161\n","     72    10      0.00758      0.00758      8.9e-06       0.0509       0.0673       0.0402       0.0723       0.0563       0.0517       0.0909       0.0713        0.214      0.00223\n","     72    11      0.00724      0.00724     6.22e-06       0.0491       0.0658       0.0384       0.0706       0.0545        0.049       0.0905       0.0697        0.181      0.00189\n","     72    12      0.00701      0.00698     2.66e-05       0.0477       0.0647        0.037       0.0692       0.0531       0.0469       0.0902       0.0686        0.381      0.00397\n","     72    13      0.00621      0.00621     3.15e-07       0.0465        0.061       0.0367       0.0659       0.0513       0.0462        0.083       0.0646       0.0371     0.000387\n","     72    14      0.00681       0.0068     1.64e-05       0.0478       0.0638       0.0371       0.0693       0.0532       0.0474       0.0878       0.0676        0.297       0.0031\n","     72    15      0.00813      0.00812     9.42e-06       0.0525       0.0697       0.0409       0.0758       0.0583        0.052       0.0958       0.0739        0.223      0.00233\n","     72    16      0.00793       0.0079     3.74e-05        0.052       0.0688        0.042       0.0719        0.057       0.0538       0.0916       0.0727         0.45      0.00469\n","     72    17      0.00722      0.00722     2.53e-06       0.0501       0.0657       0.0396        0.071       0.0553       0.0512       0.0879       0.0695          0.1      0.00104\n","     72    18      0.00638      0.00637     5.22e-06       0.0465       0.0617       0.0363       0.0669       0.0516       0.0456       0.0853       0.0655        0.164      0.00171\n","     72    19      0.00665      0.00664     2.09e-06       0.0478       0.0631       0.0384       0.0666       0.0525       0.0484       0.0851       0.0668       0.0809     0.000842\n","     72    20       0.0073       0.0073     7.62e-07       0.0495       0.0661       0.0386       0.0711       0.0549       0.0499       0.0902         0.07       0.0598     0.000623\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     72     1      0.00774      0.00774     2.63e-06        0.051       0.0681       0.0399       0.0733       0.0566       0.0516       0.0925       0.0721       0.0973      0.00101\n","     72     2      0.00774      0.00774     1.28e-06       0.0506       0.0681       0.0392       0.0733       0.0563       0.0508       0.0935       0.0721       0.0602     0.000627\n","     72     3       0.0073       0.0073     1.44e-06       0.0492       0.0661       0.0382       0.0712       0.0547        0.049       0.0911       0.0701       0.0699     0.000728\n","     72     4      0.00754      0.00754      1.4e-06       0.0498       0.0672       0.0386        0.072       0.0553       0.0495       0.0929       0.0712       0.0755     0.000786\n","     72     5      0.00701      0.00701     1.16e-06       0.0487       0.0648       0.0386        0.069       0.0538       0.0501       0.0869       0.0685       0.0542     0.000565\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              72  277.316    0.005       0.0071     8.39e-06      0.00711        0.049       0.0652       0.0386         0.07       0.0543       0.0493       0.0889       0.0691        0.176      0.00184\n","! Validation         72  277.316    0.005      0.00746     1.58e-06      0.00747       0.0499       0.0668       0.0389       0.0718       0.0553       0.0502       0.0914       0.0708       0.0714     0.000744\n","Wall time: 277.3168823400001\n","! Best model       72    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     73     1      0.00704      0.00704     3.76e-06       0.0485       0.0649       0.0381       0.0692       0.0537       0.0481       0.0895       0.0688        0.129      0.00134\n","     73     2      0.00678      0.00678     1.35e-06       0.0482       0.0637       0.0389       0.0669       0.0529       0.0498       0.0849       0.0674       0.0783     0.000816\n","     73     3      0.00835      0.00834     1.48e-05        0.053       0.0706       0.0417       0.0758       0.0587        0.053       0.0967       0.0749        0.276      0.00288\n","     73     4      0.00877      0.00877     8.25e-06       0.0548       0.0724       0.0438        0.077       0.0604       0.0549       0.0985       0.0767        0.191      0.00199\n","     73     5      0.00765      0.00765     2.24e-06       0.0507       0.0676       0.0413       0.0696       0.0555       0.0538       0.0891       0.0714       0.0975      0.00102\n","     73     6      0.00787      0.00786     7.62e-06       0.0518       0.0686       0.0414       0.0727       0.0571       0.0528       0.0925       0.0726        0.186      0.00193\n","     73     7      0.00924      0.00924     1.08e-06       0.0577       0.0744       0.0473       0.0785       0.0629       0.0593       0.0978       0.0785       0.0617     0.000643\n","     73     8      0.00993      0.00993     3.07e-06       0.0596       0.0771       0.0506       0.0775       0.0641       0.0638       0.0984       0.0811        0.122      0.00127\n","     73     9       0.0086      0.00857     3.59e-05       0.0536       0.0716       0.0429        0.075       0.0589       0.0553       0.0963       0.0758        0.443      0.00461\n","     73    10      0.00662      0.00662     5.77e-07       0.0472        0.063       0.0369       0.0678       0.0523       0.0474        0.086       0.0667       0.0451      0.00047\n","     73    11      0.00911      0.00908     3.16e-05       0.0569       0.0737       0.0457       0.0795       0.0626        0.056          0.1       0.0781        0.416      0.00433\n","     73    12      0.00886      0.00886     1.11e-06        0.057       0.0728       0.0469       0.0772       0.0621       0.0588       0.0948       0.0768       0.0736     0.000767\n","     73    13      0.00697      0.00695     1.69e-05       0.0486       0.0645       0.0403       0.0654       0.0528       0.0513        0.085       0.0681        0.284      0.00296\n","     73    14      0.00741      0.00741      8.2e-07       0.0503       0.0666        0.039       0.0727       0.0559       0.0495       0.0917       0.0706       0.0588     0.000612\n","     73    15      0.00913      0.00906      7.2e-05       0.0568       0.0736       0.0477       0.0749       0.0613       0.0605       0.0946       0.0775         0.63      0.00656\n","     73    16      0.00956      0.00955     1.49e-06       0.0585       0.0756       0.0482       0.0793       0.0637       0.0602       0.0995       0.0799       0.0715     0.000745\n","     73    17      0.00706      0.00706      8.2e-07       0.0486        0.065       0.0387       0.0685       0.0536       0.0493       0.0884       0.0688        0.052     0.000541\n","     73    18      0.00765      0.00763     2.07e-05       0.0514       0.0676       0.0389       0.0762       0.0576       0.0497       0.0936       0.0717        0.329      0.00343\n","     73    19      0.00977      0.00975     1.99e-05       0.0589       0.0764       0.0488        0.079       0.0639       0.0613       0.0999       0.0806        0.325      0.00339\n","     73    20       0.0109       0.0109     3.84e-05       0.0629       0.0807       0.0543       0.0801       0.0672       0.0675        0.102       0.0847        0.455      0.00474\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     73     1       0.0077      0.00769     2.64e-06       0.0509       0.0679       0.0397       0.0732       0.0565       0.0514       0.0924       0.0719       0.0973      0.00101\n","     73     2       0.0077       0.0077     1.22e-06       0.0504       0.0679        0.039       0.0732       0.0561       0.0506       0.0933       0.0719       0.0573     0.000597\n","     73     3      0.00726      0.00726      1.4e-06       0.0491       0.0659       0.0381       0.0711       0.0546       0.0488        0.091       0.0699       0.0697     0.000726\n","     73     4       0.0075      0.00749     1.45e-06       0.0496        0.067       0.0384       0.0719       0.0552       0.0493       0.0927        0.071       0.0777      0.00081\n","     73     5      0.00696      0.00696     1.15e-06       0.0486       0.0646       0.0384       0.0688       0.0536       0.0499       0.0868       0.0683       0.0535     0.000557\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              73  281.146    0.005      0.00835     1.41e-05      0.00837       0.0538       0.0707       0.0436       0.0741       0.0589       0.0554       0.0941       0.0748        0.216      0.00225\n","! Validation         73  281.146    0.005      0.00742     1.57e-06      0.00742       0.0497       0.0666       0.0387       0.0716       0.0552         0.05       0.0913       0.0706       0.0711     0.000741\n","Wall time: 281.1468921180003\n","! Best model       73    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     74     1      0.00741       0.0074     9.07e-06       0.0499       0.0666       0.0393        0.071       0.0552       0.0508       0.0901       0.0705        0.202      0.00211\n","     74     2      0.00726      0.00726     7.16e-06       0.0501       0.0659       0.0387       0.0728       0.0558       0.0491       0.0906       0.0699        0.186      0.00194\n","     74     3      0.00816      0.00816     9.15e-07       0.0528       0.0699       0.0433       0.0717       0.0575       0.0552       0.0925       0.0739        0.059     0.000614\n","     74     4      0.00743      0.00742     1.07e-05         0.05       0.0667       0.0381        0.074        0.056       0.0483       0.0931       0.0707        0.237      0.00247\n","     74     5      0.00749      0.00742     6.25e-05       0.0497       0.0667       0.0383       0.0726       0.0554       0.0495       0.0918       0.0707        0.583      0.00608\n","     74     6       0.0075       0.0075      3.1e-06       0.0499        0.067       0.0389       0.0718       0.0554       0.0501       0.0919        0.071        0.107      0.00111\n","     74     7       0.0085      0.00835     0.000154       0.0542       0.0707        0.044       0.0746       0.0593       0.0552       0.0943       0.0748        0.918      0.00956\n","     74     8      0.00799      0.00796     3.03e-05       0.0516        0.069       0.0422       0.0705       0.0564       0.0552       0.0906       0.0729        0.405      0.00422\n","     74     9      0.00723      0.00722     8.25e-06       0.0499       0.0658       0.0396       0.0706       0.0551       0.0502        0.089       0.0696          0.2      0.00208\n","     74    10      0.00804        0.008     4.54e-05       0.0525       0.0692       0.0428       0.0719       0.0573       0.0554       0.0907        0.073        0.495      0.00515\n","     74    11      0.00777      0.00776     5.02e-06       0.0521       0.0682       0.0434       0.0696       0.0565       0.0546       0.0893       0.0719        0.146      0.00152\n","     74    12       0.0073      0.00727     2.83e-05       0.0498        0.066       0.0393       0.0708       0.0551       0.0499       0.0899       0.0699        0.392      0.00409\n","     74    13      0.00712      0.00711     2.19e-06       0.0496       0.0653       0.0399       0.0689       0.0544       0.0507       0.0873        0.069       0.0953     0.000993\n","     74    14      0.00797      0.00791     6.77e-05       0.0515       0.0688       0.0387       0.0772        0.058       0.0498       0.0961       0.0729        0.609      0.00634\n","     74    15        0.009      0.00899     1.18e-05       0.0563       0.0733       0.0459       0.0771       0.0615       0.0574       0.0977       0.0776        0.248      0.00258\n","     74    16      0.00701      0.00695     5.73e-05       0.0495       0.0645        0.041       0.0664       0.0537       0.0519       0.0842       0.0681         0.56      0.00583\n","     74    17      0.00646      0.00644     1.77e-05       0.0461       0.0621       0.0362       0.0659       0.0511        0.047       0.0845       0.0658        0.305      0.00318\n","     74    18      0.00693      0.00693     1.87e-06       0.0475       0.0644       0.0368       0.0688       0.0528       0.0484       0.0881       0.0682       0.0906     0.000944\n","     74    19      0.00728      0.00726     2.41e-05       0.0497       0.0659       0.0393       0.0706       0.0549       0.0508       0.0887       0.0698        0.351      0.00365\n","     74    20      0.00676      0.00672     4.61e-05       0.0488       0.0634        0.039       0.0684       0.0537       0.0488       0.0855       0.0671        0.495      0.00516\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     74     1      0.00765      0.00765     2.57e-06       0.0507       0.0677       0.0396        0.073       0.0563       0.0512       0.0922       0.0717       0.0957     0.000997\n","     74     2      0.00766      0.00766     1.32e-06       0.0502       0.0677       0.0388        0.073       0.0559       0.0504       0.0931       0.0717       0.0604     0.000629\n","     74     3      0.00722      0.00722     1.43e-06        0.049       0.0657       0.0379        0.071       0.0545       0.0486       0.0908       0.0697        0.069     0.000719\n","     74     4      0.00745      0.00745     1.46e-06       0.0494       0.0668       0.0383       0.0718        0.055        0.049       0.0925       0.0708       0.0769     0.000801\n","     74     5      0.00692      0.00692     1.14e-06       0.0484       0.0644       0.0383       0.0687       0.0535       0.0496       0.0866       0.0681       0.0544     0.000567\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              74  284.980    0.005       0.0075     2.97e-05      0.00753       0.0506        0.067       0.0402       0.0713       0.0557       0.0515       0.0904       0.0709        0.334      0.00348\n","! Validation         74  284.980    0.005      0.00738     1.58e-06      0.00738       0.0496       0.0665       0.0386       0.0715        0.055       0.0498       0.0911       0.0704       0.0713     0.000742\n","Wall time: 284.9805990929999\n","! Best model       74    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     75     1      0.00702      0.00702      5.7e-07       0.0504       0.0648       0.0411        0.069       0.0551       0.0512       0.0858       0.0685       0.0486     0.000507\n","     75     2      0.00718      0.00715     3.47e-05       0.0488       0.0654       0.0394       0.0676       0.0535       0.0507       0.0876       0.0692        0.434      0.00452\n","     75     3      0.00799      0.00799     2.78e-06       0.0509       0.0691       0.0399        0.073       0.0564       0.0526       0.0939       0.0732        0.115      0.00119\n","     75     4      0.00865      0.00864      1.1e-05       0.0559       0.0719       0.0486       0.0705       0.0595        0.061       0.0898       0.0754        0.237      0.00247\n","     75     5      0.00633      0.00633     3.56e-07       0.0472       0.0616       0.0391       0.0634       0.0513        0.049        0.081        0.065       0.0297     0.000309\n","     75     6      0.00789      0.00787     2.24e-05       0.0518       0.0686       0.0427         0.07       0.0564        0.056       0.0886       0.0723        0.331      0.00345\n","     75     7      0.00874      0.00872     1.18e-05       0.0569       0.0723       0.0489       0.0729       0.0609       0.0611       0.0905       0.0758        0.243      0.00253\n","     75     8      0.00912      0.00911     1.81e-05       0.0557       0.0738       0.0454       0.0764       0.0609        0.057       0.0992       0.0781        0.314      0.00327\n","     75     9      0.00733       0.0073      2.9e-05       0.0503       0.0661       0.0392       0.0727       0.0559       0.0493       0.0908       0.0701        0.394      0.00411\n","     75    10      0.00717      0.00715     1.93e-05       0.0491       0.0654       0.0391       0.0691       0.0541       0.0494       0.0892       0.0693        0.321      0.00334\n","     75    11      0.00923      0.00923      1.4e-06        0.056       0.0743       0.0455        0.077       0.0612        0.058       0.0992       0.0786       0.0658     0.000686\n","     75    12      0.00911       0.0091     7.82e-06       0.0568       0.0738        0.048       0.0744       0.0612        0.061       0.0944       0.0777        0.191      0.00199\n","     75    13      0.00713      0.00712      8.6e-06       0.0491       0.0653       0.0399       0.0675       0.0537       0.0513       0.0867        0.069        0.211       0.0022\n","     75    14      0.00799      0.00797     1.54e-05       0.0525       0.0691       0.0411       0.0752       0.0581       0.0524        0.094       0.0732        0.282      0.00294\n","     75    15      0.00914      0.00914     3.01e-07       0.0578        0.074       0.0494       0.0745       0.0619       0.0616        0.094       0.0778       0.0359     0.000374\n","     75    16      0.00914      0.00913     2.62e-06       0.0576       0.0739       0.0493       0.0743       0.0618       0.0608       0.0949       0.0778       0.0998      0.00104\n","     75    17       0.0073      0.00728     1.49e-05         0.05        0.066       0.0393       0.0715       0.0554       0.0495       0.0904         0.07        0.284      0.00296\n","     75    18      0.00695      0.00694     1.71e-05       0.0485       0.0644       0.0386       0.0685       0.0535       0.0497       0.0867       0.0682        0.274      0.00285\n","     75    19      0.00963      0.00961     1.48e-05       0.0568       0.0758       0.0429       0.0846       0.0637       0.0552        0.106       0.0804        0.279       0.0029\n","     75    20      0.00804      0.00804     3.65e-06       0.0526       0.0694       0.0426       0.0727       0.0577       0.0545       0.0921       0.0733        0.121      0.00126\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     75     1      0.00762      0.00762     2.61e-06       0.0506       0.0675       0.0395       0.0729       0.0562        0.051        0.092       0.0715       0.0974      0.00101\n","     75     2      0.00762      0.00762     1.18e-06       0.0501       0.0675       0.0387       0.0729       0.0558       0.0502       0.0929       0.0716       0.0565     0.000589\n","     75     3      0.00719      0.00719     1.42e-06       0.0488       0.0656       0.0378       0.0709       0.0544       0.0484       0.0907       0.0695       0.0698     0.000727\n","     75     4      0.00742      0.00741     1.44e-06       0.0493       0.0666       0.0381       0.0717       0.0549       0.0488       0.0924       0.0706       0.0775     0.000808\n","     75     5      0.00688      0.00688     1.16e-06       0.0483       0.0642       0.0381       0.0686       0.0533       0.0494       0.0864       0.0679       0.0541     0.000564\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              75  288.802    0.005      0.00804     1.18e-05      0.00805       0.0527       0.0694        0.043       0.0722       0.0576       0.0548       0.0919       0.0733        0.216      0.00225\n","! Validation         75  288.802    0.005      0.00734     1.56e-06      0.00735       0.0494       0.0663       0.0384       0.0714       0.0549       0.0496       0.0909       0.0703       0.0711      0.00074\n","Wall time: 288.8026452129998\n","! Best model       75    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     76     1      0.00663      0.00663     1.99e-06       0.0474        0.063       0.0376       0.0669       0.0523       0.0485       0.0848       0.0667       0.0877     0.000913\n","     76     2      0.00704      0.00704     2.44e-06       0.0488       0.0649       0.0378        0.071       0.0544       0.0477       0.0899       0.0688        0.104      0.00108\n","     76     3      0.00863      0.00863     8.03e-07       0.0552       0.0719       0.0462       0.0733       0.0598        0.057       0.0949       0.0759       0.0514     0.000535\n","     76     4       0.0101       0.0101     2.19e-05       0.0598       0.0777       0.0481       0.0832       0.0656       0.0605        0.104       0.0822        0.335      0.00349\n","     76     5      0.00827      0.00826     1.58e-05       0.0536       0.0703       0.0433       0.0744       0.0588       0.0551       0.0936       0.0743        0.288        0.003\n","     76     6      0.00652       0.0065     1.32e-05       0.0469       0.0624       0.0357       0.0695       0.0526       0.0453        0.087       0.0662        0.264      0.00275\n","     76     7      0.00762      0.00761     3.85e-06       0.0509       0.0675         0.04       0.0726       0.0563       0.0508       0.0923       0.0715        0.139      0.00144\n","     76     8      0.00681      0.00681     3.22e-06       0.0479       0.0638       0.0377       0.0682        0.053       0.0482       0.0871       0.0676        0.128      0.00133\n","     76     9      0.00596      0.00595     4.55e-06       0.0449       0.0597        0.033       0.0687       0.0508       0.0419       0.0847       0.0633        0.153      0.00159\n","     76    10       0.0072       0.0072      5.2e-06       0.0493       0.0656       0.0382       0.0716       0.0549       0.0491         0.09       0.0695        0.155      0.00162\n","     76    11      0.00718      0.00717     8.61e-06       0.0493       0.0655       0.0398       0.0683        0.054       0.0511       0.0875       0.0693        0.206      0.00214\n","     76    12      0.00696      0.00696     2.38e-06       0.0494       0.0646       0.0389       0.0704       0.0546       0.0488        0.088       0.0684       0.0992      0.00103\n","     76    13      0.00703      0.00702     1.01e-05       0.0493       0.0648       0.0391       0.0697       0.0544       0.0489       0.0884       0.0687        0.231      0.00241\n","     76    14      0.00743      0.00743     2.24e-06       0.0504       0.0667       0.0407       0.0699       0.0553       0.0514       0.0897       0.0706       0.0961        0.001\n","     76    15      0.00625      0.00624     1.19e-05       0.0449       0.0611       0.0345       0.0656       0.0501       0.0451       0.0845       0.0648        0.245      0.00256\n","     76    16      0.00788      0.00787     2.04e-06        0.051       0.0687       0.0397       0.0737       0.0567        0.051       0.0945       0.0728       0.0695     0.000724\n","     76    17       0.0076      0.00756     4.62e-05       0.0506       0.0672       0.0406       0.0706       0.0556       0.0521       0.0902       0.0712          0.5      0.00521\n","     76    18        0.007      0.00697     2.53e-05       0.0489       0.0646       0.0391       0.0685       0.0538       0.0495       0.0873       0.0684        0.368      0.00383\n","     76    19      0.00723      0.00721     1.34e-05       0.0487       0.0657       0.0386       0.0688       0.0537       0.0505       0.0886       0.0696        0.258      0.00269\n","     76    20      0.00692      0.00691     7.48e-06       0.0486       0.0643        0.039       0.0677       0.0533       0.0496       0.0866       0.0681        0.193      0.00201\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     76     1      0.00758      0.00758     2.63e-06       0.0505       0.0673       0.0393       0.0727        0.056       0.0508       0.0919       0.0713       0.0974      0.00101\n","     76     2      0.00759      0.00759     1.22e-06         0.05       0.0674       0.0386       0.0728       0.0557       0.0501       0.0928       0.0714       0.0569     0.000593\n","     76     3      0.00716      0.00716     1.43e-06       0.0487       0.0654       0.0376       0.0708       0.0542       0.0482       0.0906       0.0694       0.0706     0.000735\n","     76     4      0.00738      0.00737     1.45e-06       0.0492       0.0664        0.038       0.0715       0.0548       0.0486       0.0922       0.0704       0.0774     0.000807\n","     76     5      0.00684      0.00684     1.12e-06       0.0481        0.064        0.038       0.0684       0.0532       0.0492       0.0863       0.0677       0.0525     0.000547\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              76  292.635    0.005       0.0073     1.01e-05      0.00731       0.0498       0.0661       0.0394       0.0706        0.055       0.0503       0.0898         0.07        0.198      0.00207\n","! Validation         76  292.635    0.005      0.00731     1.57e-06      0.00731       0.0493       0.0661       0.0383       0.0713       0.0548       0.0494       0.0908       0.0701        0.071     0.000739\n","Wall time: 292.6360798630003\n","! Best model       76    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     77     1       0.0063       0.0063     1.37e-06       0.0463       0.0614       0.0354       0.0681       0.0517       0.0443        0.086       0.0651       0.0707     0.000736\n","     77     2      0.00776      0.00776     3.39e-06        0.051       0.0681       0.0401       0.0729       0.0565       0.0511       0.0934       0.0722        0.125       0.0013\n","     77     3      0.00645      0.00644     5.37e-06       0.0468       0.0621       0.0384       0.0637       0.0511       0.0486       0.0827       0.0656        0.154       0.0016\n","     77     4      0.00615      0.00615     7.65e-07        0.045       0.0607       0.0349       0.0653       0.0501       0.0443       0.0844       0.0643       0.0629     0.000655\n","     77     5      0.00755      0.00753        2e-05         0.05       0.0671       0.0403       0.0693       0.0548       0.0516       0.0905        0.071        0.323      0.00337\n","     77     6      0.00751      0.00749     1.22e-05       0.0502        0.067        0.038       0.0744       0.0562       0.0488       0.0932        0.071        0.254      0.00265\n","     77     7      0.00633      0.00632     1.06e-05       0.0461       0.0615       0.0358       0.0667       0.0513       0.0456       0.0847       0.0652        0.236      0.00246\n","     77     8      0.00695      0.00694     3.27e-06       0.0489       0.0645        0.038       0.0705       0.0543       0.0485       0.0882       0.0683        0.132      0.00138\n","     77     9      0.00783      0.00782     5.55e-06       0.0521       0.0684        0.041       0.0742       0.0576       0.0518       0.0932       0.0725        0.161      0.00168\n","     77    10      0.00828      0.00827      5.5e-06       0.0527       0.0704       0.0404       0.0773       0.0589       0.0512        0.098       0.0746        0.167      0.00174\n","     77    11      0.00766      0.00765     4.13e-06       0.0512       0.0677       0.0418         0.07       0.0559       0.0538       0.0891       0.0715        0.136      0.00142\n","     77    12      0.00705      0.00705     4.81e-06       0.0496       0.0649       0.0394         0.07       0.0547       0.0498       0.0877       0.0688        0.145      0.00151\n","     77    13      0.00631      0.00629     2.05e-05       0.0461       0.0613        0.036       0.0664       0.0512       0.0468       0.0831        0.065        0.319      0.00332\n","     77    14      0.00626      0.00625     1.37e-05       0.0465       0.0611        0.037       0.0656       0.0513        0.047       0.0825       0.0647        0.248      0.00259\n","     77    15      0.00747      0.00746     9.72e-06       0.0502       0.0668       0.0404       0.0697       0.0551       0.0522       0.0892       0.0707        0.208      0.00216\n","     77    16      0.00728      0.00728     2.11e-06       0.0507        0.066       0.0419       0.0684       0.0551       0.0527       0.0867       0.0697       0.0912      0.00095\n","     77    17      0.00642      0.00642     6.55e-06       0.0479        0.062       0.0383        0.067       0.0527       0.0481       0.0831       0.0656        0.174      0.00181\n","     77    18      0.00613      0.00613     1.61e-06       0.0461       0.0606       0.0375       0.0632       0.0504       0.0467       0.0815       0.0641       0.0887     0.000924\n","     77    19      0.00807      0.00806     3.28e-06       0.0528       0.0695       0.0431       0.0722       0.0577       0.0554       0.0914       0.0734        0.118      0.00123\n","     77    20       0.0072       0.0072     3.12e-06       0.0491       0.0656       0.0365       0.0745       0.0555       0.0462        0.093       0.0696        0.127      0.00132\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     77     1      0.00754      0.00753     2.61e-06       0.0503       0.0671       0.0392       0.0725       0.0559       0.0506       0.0916       0.0711       0.0977      0.00102\n","     77     2      0.00754      0.00754     1.23e-06       0.0499       0.0672       0.0384       0.0727       0.0556       0.0498       0.0926       0.0712       0.0565     0.000589\n","     77     3      0.00713      0.00712      1.4e-06       0.0486       0.0653       0.0375       0.0707       0.0541        0.048       0.0905       0.0692       0.0694     0.000723\n","     77     4      0.00734      0.00734     1.47e-06        0.049       0.0663       0.0379       0.0714       0.0546       0.0485       0.0921       0.0703       0.0778     0.000811\n","     77     5      0.00681       0.0068     1.11e-06        0.048       0.0638       0.0379       0.0683       0.0531        0.049       0.0861       0.0676       0.0527     0.000549\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              77  296.459    0.005      0.00704     6.88e-06      0.00705        0.049       0.0649       0.0387       0.0695       0.0541       0.0493       0.0882       0.0688        0.167      0.00174\n","! Validation         77  296.459    0.005      0.00727     1.56e-06      0.00727       0.0492       0.0659       0.0382       0.0711       0.0546       0.0492       0.0906       0.0699       0.0708     0.000738\n","Wall time: 296.4601167769997\n","! Best model       77    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     78     1      0.00657      0.00656     5.27e-06       0.0474       0.0627       0.0378       0.0667       0.0523       0.0475       0.0852       0.0664        0.123      0.00128\n","     78     2       0.0083      0.00829     1.41e-05       0.0541       0.0704       0.0436       0.0753       0.0594       0.0541        0.095       0.0746        0.273      0.00285\n","     78     3      0.00854      0.00853     1.37e-05       0.0546       0.0714        0.044       0.0759       0.0599       0.0557       0.0954       0.0756        0.275      0.00286\n","     78     4      0.00743      0.00743     3.64e-07        0.051       0.0667       0.0424       0.0683       0.0554        0.054       0.0867       0.0703       0.0391     0.000407\n","     78     5      0.00683      0.00682     9.58e-06       0.0479       0.0639       0.0374       0.0691       0.0532       0.0477       0.0878       0.0677        0.224      0.00233\n","     78     6      0.00856      0.00856     5.43e-07       0.0552       0.0716       0.0475       0.0708       0.0591       0.0595       0.0911       0.0753       0.0373     0.000389\n","     78     7      0.00764      0.00764     2.32e-06       0.0517       0.0676       0.0417       0.0716       0.0567       0.0532       0.0898       0.0715        0.107      0.00112\n","     78     8      0.00803      0.00803     5.25e-07       0.0524       0.0693       0.0423       0.0724       0.0574       0.0543       0.0923       0.0733       0.0457     0.000476\n","     78     9      0.00683      0.00683     1.41e-06       0.0487       0.0639       0.0403       0.0656       0.0529       0.0512       0.0838       0.0675       0.0535     0.000557\n","     78    10      0.00668      0.00668     7.55e-07       0.0478       0.0632       0.0373       0.0686        0.053       0.0473       0.0866        0.067       0.0586      0.00061\n","     78    11      0.00616      0.00616     3.64e-06       0.0448       0.0607       0.0341       0.0661       0.0501       0.0428       0.0859       0.0644        0.132      0.00138\n","     78    12      0.00773      0.00772     2.52e-06       0.0507        0.068       0.0389       0.0742       0.0566         0.05       0.0942       0.0721       0.0865     0.000901\n","     78    13      0.00804        0.008     3.89e-05       0.0522       0.0692       0.0421       0.0724       0.0573       0.0544       0.0919       0.0731        0.457      0.00476\n","     78    14      0.00716      0.00715     9.93e-06       0.0494       0.0654       0.0397       0.0689       0.0543       0.0502       0.0884       0.0693        0.225      0.00234\n","     78    15      0.00699      0.00691     7.41e-05       0.0482       0.0643       0.0369       0.0706       0.0538       0.0463       0.0901       0.0682        0.637      0.00664\n","     78    16      0.00887      0.00885     1.17e-05       0.0552       0.0728       0.0484        0.069       0.0587       0.0621       0.0904       0.0763        0.227      0.00237\n","     78    17      0.00751      0.00748     3.78e-05       0.0507       0.0669       0.0406        0.071       0.0558       0.0524        0.089       0.0707        0.451       0.0047\n","     78    18      0.00663      0.00661     1.51e-05       0.0479       0.0629       0.0385       0.0666       0.0526       0.0484       0.0848       0.0666        0.278       0.0029\n","     78    19      0.00718      0.00714     4.02e-05       0.0509       0.0654       0.0417       0.0693       0.0555       0.0526       0.0854        0.069        0.466      0.00485\n","     78    20       0.0092      0.00916     4.72e-05       0.0556        0.074       0.0435       0.0798       0.0616       0.0551        0.102       0.0785        0.503      0.00523\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     78     1      0.00749      0.00749     2.65e-06       0.0502        0.067       0.0391       0.0724       0.0557       0.0504       0.0915       0.0709       0.0973      0.00101\n","     78     2      0.00749      0.00749     1.22e-06       0.0497        0.067       0.0383       0.0726       0.0554       0.0496       0.0924        0.071       0.0563     0.000587\n","     78     3      0.00709      0.00709     1.44e-06       0.0484       0.0651       0.0373       0.0706        0.054       0.0478       0.0903       0.0691       0.0707     0.000736\n","     78     4       0.0073       0.0073     1.49e-06       0.0489       0.0661       0.0377       0.0712       0.0545       0.0483       0.0919       0.0701       0.0784     0.000817\n","     78     5      0.00677      0.00677     1.06e-06       0.0479       0.0637       0.0377       0.0682       0.0529       0.0488       0.0859       0.0674       0.0521     0.000543\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              78  300.281    0.005      0.00753     1.65e-05      0.00754       0.0508       0.0671       0.0409       0.0706       0.0558       0.0521       0.0899        0.071        0.235      0.00245\n","! Validation         78  300.281    0.005      0.00723     1.57e-06      0.00723        0.049       0.0658        0.038        0.071       0.0545        0.049       0.0904       0.0697        0.071     0.000739\n","Wall time: 300.28205313499984\n","! Best model       78    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     79     1      0.00878      0.00871     6.52e-05       0.0543       0.0722       0.0447       0.0737       0.0592       0.0565       0.0962       0.0764        0.598      0.00623\n","     79     2      0.00735      0.00733     1.88e-05       0.0498       0.0662       0.0389       0.0717       0.0553       0.0492       0.0913       0.0702        0.319      0.00332\n","     79     3      0.00768      0.00764     4.24e-05       0.0503       0.0676       0.0381       0.0748       0.0564       0.0489       0.0945       0.0717        0.476      0.00496\n","     79     4      0.00821      0.00821      2.5e-06       0.0543       0.0701       0.0455        0.072       0.0587       0.0572       0.0906       0.0739       0.0965      0.00101\n","     79     5      0.00746      0.00744     2.12e-05       0.0517       0.0667       0.0427       0.0696       0.0562       0.0538        0.087       0.0704        0.328      0.00342\n","     79     6      0.00751       0.0075     9.82e-06       0.0504        0.067       0.0393       0.0725       0.0559       0.0495       0.0925        0.071        0.227      0.00237\n","     79     7      0.00709      0.00705     3.66e-05       0.0495        0.065       0.0407        0.067       0.0538       0.0526       0.0845       0.0685        0.438      0.00456\n","     79     8      0.00719      0.00716     3.25e-05       0.0484       0.0654       0.0373       0.0706        0.054       0.0477       0.0911       0.0694        0.417      0.00434\n","     79     9      0.00734      0.00729     5.22e-05       0.0488       0.0661       0.0372       0.0718       0.0545       0.0487       0.0913         0.07        0.534      0.00556\n","     79    10      0.00673      0.00667     6.26e-05       0.0476       0.0632       0.0371       0.0686       0.0529        0.047        0.087        0.067        0.584      0.00608\n","     79    11      0.00767      0.00763     3.74e-05       0.0508       0.0676       0.0397       0.0732       0.0564       0.0511       0.0921       0.0716        0.448      0.00467\n","     79    12      0.00855      0.00852     2.34e-05       0.0561       0.0714         0.05       0.0684       0.0592       0.0621       0.0872       0.0746        0.356      0.00371\n","     79    13      0.00774      0.00774     1.44e-06       0.0531       0.0681       0.0467       0.0661       0.0564       0.0582       0.0843       0.0713       0.0648     0.000675\n","     79    14       0.0068      0.00678     2.06e-05        0.048       0.0637       0.0367       0.0707       0.0537       0.0467       0.0883       0.0675        0.329      0.00343\n","     79    15      0.00853      0.00853     4.49e-07       0.0552       0.0714       0.0468        0.072       0.0594       0.0589       0.0915       0.0752       0.0412     0.000429\n","     79    16      0.00801      0.00797     4.12e-05       0.0518       0.0691       0.0419       0.0716       0.0567       0.0528       0.0935       0.0731        0.474      0.00494\n","     79    17      0.00683      0.00683     1.08e-06       0.0472       0.0639       0.0362       0.0692       0.0527       0.0471       0.0885       0.0678       0.0754     0.000785\n","     79    18      0.00841      0.00835      6.4e-05       0.0544       0.0707       0.0436        0.076       0.0598       0.0553       0.0942       0.0747        0.589      0.00613\n","     79    19      0.00913      0.00913     2.36e-06       0.0569       0.0739       0.0475       0.0758       0.0616       0.0592       0.0969        0.078        0.102      0.00106\n","     79    20      0.00718      0.00718     6.19e-06       0.0501       0.0655       0.0411       0.0681       0.0546       0.0519       0.0866       0.0693        0.169      0.00176\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     79     1      0.00745      0.00745     2.57e-06         0.05       0.0668       0.0389       0.0722       0.0556       0.0502       0.0913       0.0708       0.0954     0.000994\n","     79     2      0.00745      0.00745     1.21e-06       0.0496       0.0668       0.0382       0.0724       0.0553       0.0494       0.0922       0.0708       0.0569     0.000593\n","     79     3      0.00706      0.00706     1.42e-06       0.0483        0.065       0.0372       0.0705       0.0539       0.0476       0.0902       0.0689       0.0705     0.000734\n","     79     4      0.00727      0.00727     1.53e-06       0.0488       0.0659       0.0376       0.0712       0.0544       0.0481       0.0918       0.0699       0.0783     0.000816\n","     79     5      0.00673      0.00673     1.07e-06       0.0477       0.0635       0.0376       0.0681       0.0528       0.0486       0.0858       0.0672        0.052     0.000541\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              79  304.112    0.005      0.00768     2.71e-05      0.00771       0.0514       0.0678       0.0416       0.0712       0.0564       0.0529       0.0905       0.0717        0.333      0.00347\n","! Validation         79  304.112    0.005      0.00719     1.56e-06      0.00719       0.0489       0.0656       0.0379       0.0709       0.0544       0.0488       0.0903       0.0695       0.0706     0.000736\n","Wall time: 304.1125928270003\n","! Best model       79    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     80     1      0.00652       0.0065     2.01e-05       0.0471       0.0624       0.0368       0.0676       0.0522       0.0463       0.0859       0.0661        0.328      0.00341\n","     80     2      0.00777      0.00776      8.8e-06       0.0517       0.0681       0.0404       0.0741       0.0573       0.0512       0.0932       0.0722        0.196      0.00204\n","     80     3      0.00846      0.00844      1.9e-05       0.0543       0.0711        0.045       0.0729        0.059       0.0568       0.0932        0.075        0.312      0.00326\n","     80     4      0.00672      0.00672     2.98e-07       0.0486       0.0634       0.0392       0.0674       0.0533       0.0495       0.0846       0.0671        0.033     0.000344\n","     80     5      0.00668      0.00668     2.22e-06       0.0479       0.0632       0.0384       0.0669       0.0527       0.0482       0.0856       0.0669        0.104      0.00109\n","     80     6      0.00577      0.00577     2.13e-06       0.0438       0.0588       0.0336       0.0642       0.0489       0.0434       0.0812       0.0623        0.103      0.00107\n","     80     7      0.00646      0.00646     2.41e-06       0.0473       0.0622       0.0371       0.0677       0.0524       0.0468        0.085       0.0659        0.105       0.0011\n","     80     8      0.00664      0.00664     9.83e-07       0.0467        0.063       0.0361       0.0679        0.052       0.0473       0.0863       0.0668       0.0676     0.000704\n","     80     9      0.00669      0.00669      1.4e-06       0.0472       0.0633       0.0355       0.0706        0.053        0.046       0.0882       0.0671       0.0561     0.000584\n","     80    10      0.00662      0.00662     1.21e-06       0.0472       0.0629       0.0359       0.0699       0.0529       0.0455        0.088       0.0667       0.0797      0.00083\n","     80    11      0.00619      0.00619     4.13e-06       0.0451       0.0609       0.0335       0.0681       0.0508        0.042       0.0871       0.0645        0.138      0.00144\n","     80    12      0.00711      0.00711     3.53e-06       0.0485       0.0652       0.0375       0.0705        0.054       0.0486       0.0897       0.0691        0.114      0.00119\n","     80    13      0.00643      0.00642      3.8e-06       0.0466        0.062       0.0367       0.0664       0.0516       0.0474       0.0839       0.0657        0.109      0.00114\n","     80    14      0.00745      0.00745     3.59e-07       0.0481       0.0668       0.0361       0.0721       0.0541        0.048       0.0936       0.0708       0.0352     0.000366\n","     80    15      0.00778      0.00778     2.04e-06       0.0508       0.0682        0.038       0.0763       0.0572       0.0495       0.0952       0.0724       0.0898     0.000936\n","     80    16      0.00705      0.00704     6.38e-06       0.0496       0.0649       0.0388       0.0712        0.055       0.0486        0.089       0.0688        0.166      0.00173\n","     80    17      0.00639      0.00638     1.09e-05       0.0472       0.0618        0.039       0.0634       0.0512       0.0499       0.0805       0.0652        0.238      0.00248\n","     80    18      0.00754      0.00753     7.75e-06       0.0517       0.0671       0.0436       0.0677       0.0557       0.0556       0.0857       0.0706        0.195      0.00203\n","     80    19      0.00781       0.0078     4.36e-06       0.0498       0.0683       0.0365       0.0764       0.0564       0.0467       0.0982       0.0725        0.152      0.00158\n","     80    20      0.00887      0.00883     4.06e-05        0.055       0.0727       0.0433       0.0784       0.0609        0.055        0.099        0.077        0.468      0.00487\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     80     1      0.00741      0.00741     2.62e-06       0.0499       0.0666       0.0388        0.072       0.0554         0.05       0.0911       0.0706       0.0971      0.00101\n","     80     2      0.00742      0.00742     1.18e-06       0.0495       0.0666        0.038       0.0723       0.0552       0.0492        0.092       0.0706       0.0556     0.000579\n","     80     3      0.00703      0.00703     1.42e-06       0.0482       0.0649       0.0371       0.0704       0.0538       0.0475       0.0901       0.0688       0.0712     0.000742\n","     80     4      0.00723      0.00723     1.54e-06       0.0486       0.0658       0.0375        0.071       0.0542       0.0479       0.0916       0.0698       0.0791     0.000824\n","     80     5       0.0067       0.0067     1.06e-06       0.0476       0.0633       0.0374        0.068       0.0527       0.0484       0.0857       0.0671        0.051     0.000531\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              80  307.945    0.005      0.00704     7.11e-06      0.00705       0.0487       0.0649       0.0381         0.07        0.054       0.0488       0.0888       0.0688        0.154      0.00161\n","! Validation         80  307.945    0.005      0.00716     1.57e-06      0.00716       0.0488       0.0654       0.0378       0.0707       0.0543       0.0486       0.0901       0.0694       0.0708     0.000737\n","Wall time: 307.94579280699963\n","! Best model       80    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     81     1      0.00722      0.00722     6.29e-07       0.0503       0.0657       0.0407       0.0695       0.0551       0.0517       0.0873       0.0695        0.057     0.000594\n","     81     2      0.00644      0.00644     3.88e-06       0.0476       0.0621       0.0377       0.0674       0.0526       0.0474       0.0841       0.0657        0.131      0.00137\n","     81     3      0.00631       0.0063     2.04e-06       0.0468       0.0614        0.038       0.0644       0.0512       0.0481       0.0818        0.065       0.0787      0.00082\n","     81     4      0.00631      0.00631      3.8e-06       0.0474       0.0614       0.0393       0.0636       0.0515       0.0494       0.0802       0.0648        0.122      0.00127\n","     81     5      0.00669      0.00668     4.74e-06       0.0474       0.0632       0.0356       0.0711       0.0533       0.0448       0.0894       0.0671        0.146      0.00152\n","     81     6      0.00699      0.00699     4.06e-07       0.0497       0.0647       0.0405       0.0681       0.0543       0.0503       0.0866       0.0684       0.0469     0.000488\n","     81     7      0.00729      0.00728     1.08e-05       0.0492        0.066       0.0389       0.0699       0.0544       0.0502       0.0896       0.0699         0.24       0.0025\n","     81     8      0.00634      0.00633     8.84e-06       0.0466       0.0615       0.0354        0.069       0.0522       0.0452       0.0853       0.0653        0.216      0.00225\n","     81     9      0.00588      0.00586     1.54e-05       0.0445       0.0592       0.0351       0.0633       0.0492       0.0444       0.0811       0.0628        0.287      0.00299\n","     81    10      0.00803      0.00803     3.65e-06       0.0515       0.0693       0.0403       0.0739       0.0571       0.0524       0.0945       0.0734         0.14      0.00146\n","     81    11      0.00837      0.00837     1.62e-06       0.0535       0.0708       0.0435       0.0736       0.0585       0.0562       0.0933       0.0748       0.0807      0.00084\n","     81    12      0.00666      0.00665     1.69e-06       0.0473       0.0631       0.0377       0.0665       0.0521       0.0478       0.0859       0.0669       0.0742     0.000773\n","     81    13      0.00668      0.00668     3.16e-06       0.0481       0.0632       0.0396       0.0651       0.0523       0.0498       0.0838       0.0668        0.117      0.00122\n","     81    14      0.00732       0.0073     1.81e-05       0.0512       0.0661       0.0412       0.0711       0.0561       0.0512       0.0888         0.07         0.31      0.00323\n","     81    15      0.00871      0.00871     1.19e-06       0.0549       0.0722       0.0428       0.0791       0.0609       0.0544       0.0985       0.0765        0.074     0.000771\n","     81    16      0.00932      0.00925     7.63e-05       0.0571       0.0744       0.0461        0.079       0.0626       0.0575          0.1       0.0787        0.643       0.0067\n","     81    17      0.00835      0.00834     6.02e-06       0.0543       0.0707       0.0442       0.0745       0.0593        0.056       0.0933       0.0746        0.171      0.00178\n","     81    18      0.00715      0.00712     2.67e-05       0.0492       0.0653       0.0392       0.0693       0.0542       0.0499       0.0883       0.0691        0.378      0.00393\n","     81    19      0.00811      0.00811     2.47e-06       0.0528       0.0697       0.0426       0.0732       0.0579       0.0544       0.0929       0.0737        0.102      0.00106\n","     81    20      0.00875      0.00872     2.22e-05       0.0558       0.0723        0.046       0.0754       0.0607        0.058       0.0945       0.0763        0.337      0.00351\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     81     1      0.00738      0.00738     2.59e-06       0.0497       0.0664       0.0387       0.0719       0.0553       0.0498        0.091       0.0704       0.0957     0.000997\n","     81     2      0.00737      0.00737     1.16e-06       0.0493       0.0664       0.0379       0.0722        0.055        0.049       0.0918       0.0704       0.0534     0.000556\n","     81     3        0.007        0.007     1.37e-06       0.0481       0.0647        0.037       0.0703       0.0536       0.0473       0.0899       0.0686       0.0704     0.000733\n","     81     4      0.00719      0.00719      1.5e-06       0.0485       0.0656       0.0373       0.0709       0.0541       0.0477       0.0915       0.0696       0.0786     0.000819\n","     81     5      0.00667      0.00667        1e-06       0.0475       0.0632       0.0373       0.0678       0.0526       0.0482       0.0856       0.0669       0.0498     0.000519\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              81  311.765    0.005      0.00733     1.07e-05      0.00735       0.0503       0.0663       0.0402       0.0704       0.0553       0.0511       0.0891       0.0701        0.188      0.00195\n","! Validation         81  311.765    0.005      0.00712     1.53e-06      0.00712       0.0486       0.0653       0.0376       0.0706       0.0541       0.0484         0.09       0.0692       0.0696     0.000725\n","Wall time: 311.76570675200037\n","! Best model       81    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     82     1      0.00938      0.00936        2e-05       0.0578       0.0749       0.0483       0.0768       0.0626       0.0606       0.0973       0.0789        0.328      0.00342\n","     82     2      0.00878      0.00871     7.34e-05        0.053       0.0722       0.0413       0.0766       0.0589       0.0535       0.0996       0.0765        0.636      0.00662\n","     82     3      0.00619      0.00619     2.95e-06       0.0461       0.0609       0.0353       0.0675       0.0514       0.0447       0.0843       0.0645        0.105      0.00109\n","     82     4      0.00697      0.00695      2.3e-05       0.0485       0.0645       0.0378       0.0699       0.0539       0.0482       0.0885       0.0683         0.35      0.00364\n","     82     5      0.00712      0.00712     7.99e-07       0.0492       0.0653       0.0403       0.0669       0.0536       0.0523       0.0856       0.0689       0.0623     0.000649\n","     82     6      0.00661      0.00661     1.41e-06       0.0477       0.0629       0.0373       0.0686       0.0529        0.047       0.0864       0.0667       0.0766     0.000798\n","     82     7      0.00629      0.00629     8.74e-07       0.0455       0.0614       0.0355       0.0657       0.0506       0.0462       0.0838        0.065         0.06     0.000625\n","     82     8      0.00698      0.00698     2.84e-06       0.0484       0.0646       0.0387       0.0677       0.0532         0.05       0.0868       0.0684       0.0912      0.00095\n","     82     9      0.00798      0.00798     3.63e-07       0.0508       0.0691       0.0397        0.073       0.0563       0.0507       0.0958       0.0733         0.04     0.000417\n","     82    10      0.00646      0.00644     2.35e-05       0.0465       0.0621       0.0351       0.0692       0.0522       0.0446       0.0871       0.0658        0.349      0.00364\n","     82    11      0.00648      0.00648     1.98e-06       0.0464       0.0623       0.0343       0.0706       0.0524       0.0434       0.0887        0.066       0.0949     0.000989\n","     82    12      0.00788      0.00782     5.33e-05       0.0512       0.0684       0.0397       0.0742        0.057       0.0506       0.0945       0.0725        0.534      0.00556\n","     82    13      0.00683      0.00683     2.89e-06       0.0495       0.0639       0.0412       0.0661       0.0537       0.0519       0.0829       0.0674        0.121      0.00126\n","     82    14      0.00678      0.00676     2.21e-05       0.0487       0.0636       0.0379       0.0701        0.054       0.0471       0.0877       0.0674        0.343      0.00357\n","     82    15      0.00608      0.00608     2.66e-07       0.0452       0.0603        0.035       0.0656       0.0503       0.0447       0.0832       0.0639       0.0285     0.000297\n","     82    16      0.00678      0.00677     1.33e-05       0.0486       0.0636        0.039       0.0678       0.0534       0.0496        0.085       0.0673        0.259       0.0027\n","     82    17      0.00622      0.00621     7.48e-06       0.0463        0.061       0.0367       0.0654       0.0511       0.0471        0.082       0.0645        0.174      0.00181\n","     82    18      0.00655      0.00654     1.26e-05       0.0479       0.0626       0.0388        0.066       0.0524       0.0486       0.0838       0.0662        0.262      0.00273\n","     82    19      0.00802      0.00801     1.43e-05        0.052       0.0692        0.042       0.0722       0.0571       0.0534       0.0932       0.0733        0.252      0.00263\n","     82    20      0.00872       0.0087     1.35e-05       0.0543       0.0722        0.042       0.0791       0.0605       0.0527          0.1       0.0765        0.271      0.00282\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     82     1      0.00734      0.00734     2.51e-06       0.0496       0.0663       0.0385       0.0718       0.0552       0.0497       0.0908       0.0702       0.0948     0.000988\n","     82     2      0.00734      0.00734     1.17e-06       0.0492       0.0663       0.0378       0.0721       0.0549       0.0489       0.0917       0.0703       0.0551     0.000574\n","     82     3      0.00697      0.00697     1.38e-06        0.048       0.0646       0.0368       0.0702       0.0535       0.0471       0.0898       0.0685       0.0693     0.000722\n","     82     4      0.00716      0.00716     1.55e-06       0.0484       0.0654       0.0372       0.0708        0.054       0.0475       0.0913       0.0694       0.0794     0.000827\n","     82     5      0.00664      0.00664     9.95e-07       0.0473        0.063       0.0372       0.0677       0.0524        0.048       0.0854       0.0667         0.05     0.000521\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              82  315.590    0.005      0.00714     1.45e-05      0.00716       0.0492       0.0654       0.0388         0.07       0.0544       0.0495        0.089       0.0693        0.222      0.00231\n","! Validation         82  315.590    0.005      0.00709     1.52e-06      0.00709       0.0485       0.0651       0.0375       0.0705        0.054       0.0482       0.0898        0.069       0.0697     0.000726\n","Wall time: 315.59084583000003\n","! Best model       82    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     83     1        0.007        0.007     4.01e-07       0.0479       0.0647       0.0376       0.0684        0.053       0.0479       0.0893       0.0686       0.0439     0.000458\n","     83     2      0.00695      0.00694     8.93e-06       0.0491       0.0645        0.038       0.0714       0.0547       0.0473       0.0894       0.0684        0.215      0.00224\n","     83     3      0.00829      0.00826     3.35e-05       0.0552       0.0703       0.0465       0.0726       0.0595       0.0582       0.0898        0.074        0.425      0.00443\n","     83     4       0.0102       0.0102     4.61e-06       0.0626       0.0781       0.0585       0.0708       0.0647       0.0725       0.0882       0.0803        0.134      0.00139\n","     83     5        0.008        0.008      1.2e-06       0.0534       0.0692       0.0462        0.068       0.0571       0.0584       0.0869       0.0726       0.0613     0.000639\n","     83     6      0.00795      0.00789     6.07e-05       0.0518       0.0687       0.0405       0.0745       0.0575        0.052       0.0936       0.0728        0.578      0.00602\n","     83     7      0.00698      0.00698     5.19e-06       0.0487       0.0646       0.0376        0.071       0.0543       0.0482       0.0888       0.0685        0.152      0.00158\n","     83     8      0.00801      0.00799     1.48e-05       0.0523       0.0692       0.0403       0.0764       0.0583       0.0503       0.0964       0.0734        0.282      0.00294\n","     83     9      0.00788      0.00788     5.39e-06       0.0518       0.0687        0.042       0.0714       0.0567       0.0534       0.0918       0.0726        0.149      0.00156\n","     83    10      0.00759      0.00758     5.77e-06       0.0511       0.0674       0.0413       0.0707        0.056       0.0519       0.0907       0.0713        0.173      0.00181\n","     83    11      0.00705      0.00703     2.68e-05       0.0496       0.0649       0.0392       0.0704       0.0548       0.0486       0.0888       0.0687        0.378      0.00394\n","     83    12      0.00976      0.00976     1.31e-06       0.0593       0.0764       0.0514       0.0751       0.0632       0.0639       0.0967       0.0803        0.077     0.000802\n","     83    13       0.0103       0.0103     1.49e-06       0.0608       0.0786       0.0515       0.0795       0.0655       0.0647        0.101       0.0828       0.0709     0.000739\n","     83    14      0.00871      0.00867     3.41e-05       0.0538        0.072       0.0417        0.078       0.0598       0.0544       0.0983       0.0763         0.43      0.00448\n","     83    15      0.00696      0.00695     9.68e-06       0.0481       0.0645       0.0379       0.0685       0.0532       0.0478       0.0889       0.0683         0.23       0.0024\n","     83    16      0.00674      0.00674     1.52e-06       0.0485       0.0635       0.0388       0.0679       0.0533       0.0484       0.0861       0.0673       0.0807      0.00084\n","     83    17      0.00804      0.00804     1.09e-06        0.053       0.0694        0.043        0.073        0.058       0.0548       0.0919       0.0733       0.0604     0.000629\n","     83    18       0.0071      0.00706     4.84e-05       0.0508        0.065       0.0442       0.0642       0.0542        0.055       0.0813       0.0682         0.51      0.00531\n","     83    19      0.00651      0.00649     1.84e-05       0.0468       0.0623       0.0362        0.068       0.0521       0.0466       0.0855        0.066         0.31      0.00323\n","     83    20      0.00788      0.00785      3.6e-05       0.0526       0.0685        0.042       0.0737       0.0578       0.0536       0.0913       0.0725        0.443      0.00461\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     83     1      0.00731      0.00731     2.55e-06       0.0495       0.0661       0.0384       0.0717       0.0551       0.0495       0.0907       0.0701       0.0945     0.000985\n","     83     2      0.00731      0.00731     1.21e-06       0.0491       0.0661       0.0377       0.0719       0.0548       0.0487       0.0915       0.0701        0.056     0.000583\n","     83     3      0.00694      0.00694      1.4e-06       0.0478       0.0644       0.0367       0.0701       0.0534        0.047       0.0897       0.0683       0.0702     0.000731\n","     83     4      0.00713      0.00713     1.56e-06       0.0483       0.0653        0.037       0.0707       0.0539       0.0473       0.0912       0.0693        0.079     0.000823\n","     83     5      0.00661       0.0066     1.03e-06       0.0472       0.0629        0.037       0.0676       0.0523       0.0478       0.0853       0.0666       0.0519      0.00054\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              83  319.416    0.005      0.00788      1.6e-05       0.0079       0.0524       0.0687       0.0427       0.0717       0.0572       0.0543       0.0908       0.0726         0.24       0.0025\n","! Validation         83  319.416    0.005      0.00706     1.55e-06      0.00706       0.0484        0.065       0.0374       0.0704       0.0539       0.0481       0.0897       0.0689       0.0703     0.000732\n","Wall time: 319.41658944200026\n","! Best model       83    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     84     1      0.00744      0.00742     2.55e-05       0.0502       0.0666       0.0407       0.0693        0.055       0.0517       0.0892       0.0705        0.368      0.00384\n","     84     2      0.00705      0.00705     1.54e-06       0.0482       0.0649       0.0375       0.0696       0.0535       0.0482       0.0895       0.0688       0.0779     0.000812\n","     84     3      0.00681      0.00679     2.46e-05       0.0486       0.0637       0.0397       0.0662        0.053       0.0503       0.0845       0.0674        0.363      0.00378\n","     84     4      0.00716      0.00716     2.52e-07       0.0496       0.0655       0.0392       0.0702       0.0547       0.0498       0.0889       0.0694       0.0303     0.000315\n","     84     5      0.00854      0.00849     5.71e-05       0.0527       0.0713       0.0395       0.0791       0.0593       0.0512          0.1       0.0756        0.548      0.00571\n","     84     6      0.00757      0.00755     1.86e-05       0.0522       0.0672        0.044       0.0685       0.0563       0.0555        0.086       0.0708        0.315      0.00328\n","     84     7      0.00706      0.00702     3.88e-05       0.0486       0.0648       0.0378       0.0702        0.054       0.0482       0.0892       0.0687        0.455      0.00474\n","     84     8       0.0061      0.00609     7.98e-06       0.0448       0.0604       0.0346       0.0651       0.0498       0.0444       0.0836        0.064        0.187      0.00195\n","     84     9      0.00694       0.0069     3.53e-05       0.0495       0.0643       0.0419       0.0646       0.0533       0.0521       0.0834       0.0678        0.439      0.00457\n","     84    10      0.00739      0.00739     1.68e-06       0.0504       0.0665       0.0394       0.0724       0.0559       0.0499        0.091       0.0705       0.0822     0.000857\n","     84    11      0.00724      0.00718     6.01e-05       0.0496       0.0656       0.0388       0.0711        0.055       0.0492       0.0898       0.0695        0.575      0.00599\n","     84    12       0.0057      0.00569     4.48e-06       0.0445       0.0584       0.0354       0.0626        0.049       0.0447       0.0789       0.0618        0.134       0.0014\n","     84    13      0.00812      0.00807      4.5e-05       0.0525       0.0695       0.0414       0.0749       0.0581       0.0519       0.0954       0.0737        0.484      0.00505\n","     84    14      0.00854      0.00854     5.57e-06       0.0561       0.0715       0.0471        0.074       0.0606       0.0586        0.092       0.0753        0.161      0.00168\n","     84    15      0.00737      0.00737     2.46e-06       0.0516       0.0664       0.0428       0.0693       0.0561       0.0539       0.0861         0.07        0.106       0.0011\n","     84    16      0.00654      0.00651     3.08e-05       0.0467       0.0624       0.0361        0.068       0.0521       0.0467       0.0856       0.0661        0.404       0.0042\n","     84    17      0.00592      0.00592     2.79e-06       0.0448       0.0595       0.0347        0.065       0.0499       0.0435       0.0827       0.0631        0.114      0.00119\n","     84    18      0.00741      0.00738     2.85e-05       0.0498       0.0665        0.039       0.0713       0.0552       0.0501       0.0907       0.0704        0.385      0.00401\n","     84    19       0.0074      0.00738     1.28e-05       0.0495       0.0665       0.0399       0.0686       0.0542        0.051       0.0898       0.0704        0.261      0.00272\n","     84    20      0.00719      0.00719     3.51e-06        0.049       0.0656       0.0384       0.0703       0.0544       0.0492       0.0898       0.0695        0.133      0.00139\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     84     1      0.00728      0.00728     2.55e-06       0.0494        0.066       0.0383       0.0716       0.0549       0.0493       0.0905       0.0699        0.094      0.00098\n","     84     2      0.00727      0.00727     1.15e-06        0.049        0.066       0.0375       0.0718       0.0547       0.0485       0.0913       0.0699        0.055     0.000573\n","     84     3      0.00691      0.00691     1.33e-06       0.0477       0.0643       0.0366         0.07       0.0533       0.0468       0.0896       0.0682       0.0688     0.000717\n","     84     4       0.0071       0.0071     1.55e-06       0.0482       0.0652       0.0369       0.0706       0.0538       0.0472       0.0911       0.0691       0.0798     0.000831\n","     84     5      0.00657      0.00657     9.92e-07       0.0471       0.0627       0.0369       0.0675       0.0522       0.0477       0.0852       0.0664        0.049     0.000511\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              84  323.250    0.005      0.00715     2.04e-05      0.00717       0.0494       0.0654       0.0394       0.0695       0.0545       0.0501       0.0884       0.0693        0.281      0.00293\n","! Validation         84  323.250    0.005      0.00703     1.51e-06      0.00703       0.0483       0.0648       0.0372       0.0703       0.0538       0.0479       0.0896       0.0687       0.0693     0.000722\n","Wall time: 323.2505584290002\n","! Best model       84    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     85     1      0.00636      0.00635     1.07e-06       0.0462       0.0617       0.0356       0.0675       0.0515        0.046       0.0847       0.0654       0.0707     0.000736\n","     85     2      0.00691      0.00689     1.61e-05       0.0481       0.0642       0.0381       0.0683       0.0532       0.0489       0.0872        0.068        0.279      0.00291\n","     85     3      0.00841       0.0084     1.59e-05       0.0533       0.0709       0.0423       0.0754       0.0588       0.0543       0.0958       0.0751        0.286      0.00298\n","     85     4      0.00663      0.00656     6.81e-05       0.0473       0.0627       0.0369       0.0679       0.0524       0.0469       0.0859       0.0664        0.611      0.00637\n","     85     5      0.00581      0.00581     3.84e-06       0.0438        0.059       0.0336       0.0641       0.0488       0.0424       0.0827       0.0625        0.139      0.00145\n","     85     6      0.00663      0.00662     1.29e-05       0.0468       0.0629        0.036       0.0682       0.0521       0.0452       0.0882       0.0667        0.262      0.00273\n","     85     7      0.00723      0.00722     5.14e-06       0.0494       0.0658       0.0376       0.0729       0.0553       0.0471       0.0924       0.0697        0.166      0.00173\n","     85     8      0.00652      0.00652     2.27e-06       0.0469       0.0625       0.0364       0.0677       0.0521       0.0464        0.086       0.0662        0.103      0.00107\n","     85     9      0.00592      0.00591     1.32e-05       0.0445       0.0595       0.0351       0.0634       0.0492       0.0455       0.0805        0.063        0.266      0.00277\n","     85    10      0.00635      0.00633     1.49e-05       0.0469       0.0616       0.0371       0.0663       0.0517       0.0473        0.083       0.0652        0.267      0.00278\n","     85    11      0.00729      0.00728     1.39e-05       0.0499        0.066         0.04       0.0696       0.0548       0.0506       0.0891       0.0699        0.273      0.00284\n","     85    12      0.00601        0.006     1.21e-05       0.0454       0.0599       0.0354       0.0653       0.0504       0.0444       0.0826       0.0635        0.243      0.00253\n","     85    13      0.00677      0.00677     6.46e-07       0.0471       0.0636       0.0367        0.068       0.0523       0.0473       0.0876       0.0675       0.0434     0.000452\n","     85    14      0.00676      0.00674     1.24e-05       0.0473       0.0635       0.0366       0.0687       0.0526       0.0478       0.0868       0.0673        0.254      0.00265\n","     85    15      0.00669      0.00667     1.41e-05       0.0482       0.0632       0.0386       0.0675        0.053       0.0485       0.0853       0.0669        0.262      0.00273\n","     85    16      0.00645      0.00643     2.09e-05       0.0469        0.062       0.0364       0.0678       0.0521        0.046       0.0855       0.0658        0.336       0.0035\n","     85    17      0.00684      0.00683     1.53e-05       0.0481       0.0639       0.0379       0.0686       0.0533       0.0482       0.0872       0.0677        0.285      0.00297\n","     85    18      0.00705      0.00704     6.04e-07       0.0487       0.0649       0.0373       0.0717       0.0545       0.0477         0.09       0.0688       0.0488     0.000509\n","     85    19      0.00666      0.00663      2.7e-05       0.0476        0.063       0.0372       0.0684       0.0528        0.048       0.0855       0.0667        0.379      0.00394\n","     85    20      0.00709      0.00706     2.88e-05       0.0491        0.065       0.0374       0.0725       0.0549       0.0477       0.0901       0.0689        0.387      0.00403\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     85     1      0.00724      0.00724     2.46e-06       0.0493       0.0658       0.0382       0.0715       0.0548       0.0492       0.0904       0.0698       0.0928     0.000966\n","     85     2      0.00724      0.00724     1.15e-06       0.0488       0.0658       0.0374       0.0717       0.0546       0.0484       0.0912       0.0698        0.054     0.000563\n","     85     3      0.00688      0.00688      1.4e-06       0.0476       0.0642       0.0365         0.07       0.0532       0.0466       0.0894        0.068       0.0703     0.000732\n","     85     4      0.00707      0.00706     1.54e-06        0.048        0.065       0.0368       0.0705       0.0536        0.047       0.0909        0.069       0.0791     0.000824\n","     85     5      0.00655      0.00655        1e-06        0.047       0.0626       0.0368       0.0674       0.0521       0.0475       0.0851       0.0663       0.0497     0.000518\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              85  327.074    0.005       0.0067      1.5e-05      0.00672       0.0476       0.0633       0.0371       0.0685       0.0528       0.0474       0.0869       0.0671        0.248      0.00258\n","! Validation         85  327.074    0.005      0.00699     1.51e-06        0.007       0.0481       0.0647       0.0371       0.0702       0.0537       0.0477       0.0894       0.0686       0.0692     0.000721\n","Wall time: 327.07449007000014\n","! Best model       85    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     86     1      0.00612      0.00608     3.49e-05       0.0454       0.0603       0.0348       0.0668       0.0508       0.0436       0.0844        0.064        0.436      0.00455\n","     86     2      0.00653      0.00646     6.92e-05       0.0461       0.0622       0.0346        0.069       0.0518        0.044       0.0879        0.066        0.614       0.0064\n","     86     3      0.00747      0.00746     1.66e-05       0.0501       0.0668       0.0398       0.0706       0.0552       0.0513       0.0901       0.0707          0.3      0.00313\n","     86     4      0.00699      0.00694     4.82e-05       0.0488       0.0645       0.0389       0.0685       0.0537       0.0493       0.0872       0.0683         0.51      0.00531\n","     86     5       0.0071       0.0071     1.88e-06       0.0495       0.0652       0.0385       0.0715        0.055       0.0488       0.0894       0.0691        0.077     0.000802\n","     86     6      0.00738      0.00734     3.67e-05       0.0488       0.0663       0.0374       0.0716       0.0545       0.0482       0.0924       0.0703        0.446      0.00465\n","     86     7      0.00677      0.00676     7.74e-06       0.0476       0.0636       0.0375        0.068       0.0527       0.0485       0.0862       0.0674        0.193      0.00201\n","     86     8      0.00567      0.00564     3.06e-05       0.0439       0.0581        0.033       0.0656       0.0493       0.0413        0.082       0.0616        0.404      0.00421\n","     86     9      0.00662      0.00659     2.82e-05       0.0478       0.0628       0.0379       0.0675       0.0527        0.048        0.085       0.0665        0.385      0.00401\n","     86    10      0.00637      0.00632     5.75e-05       0.0462       0.0615        0.036       0.0667       0.0513       0.0462       0.0841       0.0651        0.562      0.00585\n","     86    11        0.006      0.00599      4.5e-06       0.0458       0.0599       0.0359       0.0656       0.0507       0.0452       0.0817       0.0635        0.142      0.00148\n","     86    12      0.00668      0.00665     2.69e-05       0.0479       0.0631       0.0385       0.0665       0.0525       0.0491       0.0844       0.0667        0.381      0.00397\n","     86    13      0.00633      0.00629     3.37e-05       0.0463       0.0614       0.0364       0.0659       0.0512        0.046       0.0841        0.065        0.429      0.00447\n","     86    14      0.00652      0.00652     1.41e-06       0.0475       0.0625       0.0385       0.0655        0.052        0.048       0.0842       0.0661       0.0721     0.000751\n","     86    15      0.00687      0.00685     2.44e-05        0.048        0.064       0.0365        0.071       0.0537       0.0469       0.0888       0.0679        0.363      0.00378\n","     86    16      0.00769      0.00769     1.07e-06       0.0513       0.0679       0.0399       0.0741        0.057       0.0509       0.0929       0.0719       0.0703     0.000732\n","     86    17      0.00823      0.00822     9.52e-06       0.0528       0.0701       0.0416       0.0752       0.0584       0.0538       0.0947       0.0742        0.218      0.00227\n","     86    18      0.00841      0.00841      1.9e-06       0.0549       0.0709       0.0489       0.0668       0.0579       0.0611       0.0874       0.0742       0.0922      0.00096\n","     86    19      0.00777      0.00777     8.86e-07       0.0516       0.0682       0.0418       0.0711       0.0564       0.0541         0.09        0.072       0.0533     0.000555\n","     86    20      0.00764      0.00762     2.61e-05       0.0508       0.0675       0.0402       0.0721       0.0562       0.0502        0.093       0.0716        0.363      0.00378\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     86     1      0.00721      0.00721     2.56e-06       0.0491       0.0657        0.038       0.0713       0.0547        0.049       0.0902       0.0696       0.0944     0.000984\n","     86     2      0.00721      0.00721     1.11e-06       0.0487       0.0657       0.0373       0.0716       0.0544       0.0482       0.0911       0.0696       0.0519      0.00054\n","     86     3      0.00685      0.00685     1.36e-06       0.0475        0.064       0.0363       0.0699       0.0531       0.0465       0.0894       0.0679       0.0704     0.000733\n","     86     4      0.00703      0.00703     1.53e-06       0.0479       0.0649       0.0367       0.0704       0.0535       0.0468       0.0908       0.0688       0.0786     0.000819\n","     86     5      0.00652      0.00652     9.51e-07       0.0469       0.0625       0.0367       0.0673        0.052       0.0473        0.085       0.0662       0.0482     0.000503\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              86  330.899    0.005      0.00694     2.31e-05      0.00696       0.0485       0.0644       0.0383        0.069       0.0536       0.0489       0.0876       0.0682        0.306      0.00318\n","! Validation         86  330.899    0.005      0.00696      1.5e-06      0.00697        0.048       0.0646        0.037       0.0701       0.0535       0.0476       0.0893       0.0684       0.0687     0.000716\n","Wall time: 330.89928179599974\n","! Best model       86    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     87     1      0.00604      0.00604     6.53e-07       0.0455       0.0601       0.0366       0.0634         0.05       0.0469       0.0803       0.0636       0.0551     0.000574\n","     87     2      0.00769      0.00769     5.52e-06         0.05       0.0678        0.038       0.0739       0.0559       0.0481       0.0958        0.072         0.17      0.00177\n","     87     3      0.00742      0.00741     1.41e-05       0.0498       0.0666       0.0399       0.0698       0.0548       0.0513       0.0897       0.0705        0.278       0.0029\n","     87     4      0.00907      0.00907     3.58e-06       0.0571       0.0737       0.0511       0.0693       0.0602        0.065       0.0885       0.0768        0.131      0.00137\n","     87     5      0.00731      0.00726     5.16e-05        0.051       0.0659       0.0423       0.0684       0.0554       0.0525       0.0867       0.0696         0.53      0.00553\n","     87     6      0.00672      0.00672     4.65e-06       0.0482       0.0634       0.0377       0.0692       0.0535       0.0469       0.0875       0.0672        0.146      0.00152\n","     87     7      0.00676      0.00667     8.28e-05       0.0483       0.0632       0.0374       0.0701       0.0538       0.0466       0.0874        0.067        0.667      0.00694\n","     87     8      0.00744      0.00742      1.8e-05       0.0513       0.0667       0.0402       0.0735       0.0568       0.0513       0.0898       0.0706        0.301      0.00314\n","     87     9      0.00756      0.00752     4.05e-05       0.0503       0.0671       0.0385       0.0738       0.0562       0.0489       0.0934       0.0711        0.473      0.00493\n","     87    10      0.00725      0.00724      1.1e-05       0.0498       0.0658       0.0393       0.0708        0.055         0.05       0.0894       0.0697        0.236      0.00245\n","     87    11      0.00792      0.00792      1.6e-06       0.0526       0.0689       0.0447       0.0685       0.0566       0.0567       0.0883       0.0725       0.0846     0.000881\n","     87    12      0.00741       0.0074     1.56e-05       0.0503       0.0665       0.0387       0.0735       0.0561        0.049       0.0921       0.0705        0.293      0.00305\n","     87    13      0.00882      0.00881     9.75e-06       0.0542       0.0726         0.04       0.0827       0.0613       0.0507        0.103        0.077        0.226      0.00236\n","     87    14      0.00703        0.007     2.69e-05       0.0482       0.0647       0.0385       0.0676        0.053       0.0495       0.0876       0.0686        0.375       0.0039\n","     87    15      0.00739      0.00737     1.58e-05        0.051       0.0664       0.0408       0.0713       0.0561       0.0509       0.0898       0.0703        0.271      0.00282\n","     87    16      0.00603        0.006     3.08e-05       0.0449       0.0599        0.034       0.0665       0.0503       0.0432       0.0839       0.0635        0.407      0.00424\n","     87    17      0.00679      0.00678      1.4e-05       0.0482       0.0637       0.0393       0.0659       0.0526       0.0511       0.0833       0.0672         0.27      0.00281\n","     87    18      0.00749      0.00749     3.02e-06         0.05        0.067       0.0395       0.0711       0.0553       0.0518       0.0899       0.0709          0.1      0.00104\n","     87    19      0.00654       0.0065     4.38e-05       0.0463       0.0624       0.0361       0.0666       0.0513       0.0471       0.0851       0.0661        0.482      0.00502\n","     87    20      0.00623      0.00623     1.17e-06       0.0458       0.0611       0.0358       0.0658       0.0508       0.0459       0.0836       0.0647       0.0664     0.000692\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     87     1      0.00717      0.00717     2.51e-06        0.049       0.0655       0.0379       0.0712       0.0546       0.0488       0.0901       0.0694       0.0932      0.00097\n","     87     2      0.00718      0.00717      1.2e-06       0.0486       0.0655       0.0372       0.0715       0.0543        0.048       0.0909       0.0695       0.0552     0.000575\n","     87     3      0.00683      0.00683     1.41e-06       0.0474       0.0639       0.0362       0.0698        0.053       0.0463       0.0893       0.0678       0.0714     0.000744\n","     87     4        0.007        0.007     1.57e-06       0.0478       0.0647       0.0366       0.0702       0.0534       0.0466       0.0907       0.0686       0.0791     0.000824\n","     87     5      0.00648      0.00648     9.62e-07       0.0467       0.0623       0.0365       0.0672       0.0518       0.0471       0.0848        0.066       0.0482     0.000503\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              87  334.728    0.005      0.00723     1.97e-05      0.00725       0.0496       0.0658       0.0394       0.0701       0.0547       0.0504       0.0889       0.0696        0.278       0.0029\n","! Validation         87  334.728    0.005      0.00693     1.53e-06      0.00693       0.0479       0.0644       0.0369         0.07       0.0534       0.0474       0.0892       0.0683       0.0694     0.000723\n","Wall time: 334.72854385699975\n","! Best model       87    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     88     1      0.00728      0.00724     3.95e-05       0.0496       0.0658       0.0389        0.071        0.055       0.0499       0.0896       0.0697        0.466      0.00486\n","     88     2      0.00762      0.00762     1.26e-06       0.0511       0.0675        0.041       0.0713       0.0561       0.0515       0.0916       0.0715       0.0705     0.000734\n","     88     3       0.0064      0.00639     8.95e-06       0.0474       0.0618       0.0381       0.0659        0.052       0.0485       0.0823       0.0654        0.213      0.00222\n","     88     4      0.00578      0.00577     1.28e-05        0.044       0.0588       0.0346       0.0628       0.0487       0.0441       0.0804       0.0623        0.263      0.00274\n","     88     5      0.00811      0.00811     4.13e-06       0.0537       0.0697       0.0452       0.0706       0.0579       0.0575       0.0891       0.0733        0.131      0.00136\n","     88     6      0.00816      0.00811     4.43e-05       0.0532       0.0697        0.044       0.0715       0.0578       0.0559       0.0912       0.0736        0.491      0.00512\n","     88     7      0.00663      0.00662     2.67e-06       0.0469        0.063       0.0348       0.0712        0.053       0.0437       0.0899       0.0668        0.107      0.00111\n","     88     8      0.00615      0.00613     2.75e-05       0.0456       0.0605       0.0367       0.0634         0.05       0.0464       0.0818       0.0641        0.385      0.00401\n","     88     9      0.00813      0.00813      1.3e-06       0.0527       0.0697       0.0407       0.0768       0.0587       0.0517       0.0962       0.0739       0.0682      0.00071\n","     88    10      0.00857      0.00857     1.24e-06       0.0554       0.0716       0.0439       0.0784       0.0612       0.0538        0.098       0.0759       0.0777      0.00081\n","     88    11      0.00849      0.00847     1.32e-05       0.0556       0.0712       0.0476       0.0717       0.0597       0.0595       0.0902       0.0749        0.246      0.00257\n","     88    12      0.00655      0.00655     1.83e-06       0.0471       0.0626       0.0388       0.0637       0.0513       0.0501       0.0821       0.0661       0.0846     0.000881\n","     88    13      0.00681       0.0068     8.47e-06       0.0476       0.0638       0.0358       0.0713       0.0535       0.0458       0.0895       0.0677        0.215      0.00224\n","     88    14      0.00701      0.00701     1.56e-06       0.0491       0.0648       0.0382       0.0711       0.0546       0.0483        0.089       0.0687        0.076     0.000791\n","     88    15      0.00923      0.00923     7.69e-07       0.0569       0.0743       0.0467       0.0772       0.0619       0.0583       0.0989       0.0786       0.0482     0.000503\n","     88    16      0.00967      0.00964     2.82e-05       0.0571        0.076       0.0462       0.0788       0.0625       0.0596        0.101       0.0803        0.384        0.004\n","     88    17      0.00702      0.00697     4.74e-05       0.0499       0.0646       0.0422       0.0653       0.0538       0.0529       0.0832       0.0681        0.508      0.00529\n","     88    18      0.00604      0.00604      6.1e-06       0.0454       0.0601       0.0351       0.0658       0.0505       0.0444       0.0831       0.0637        0.176      0.00184\n","     88    19      0.00683      0.00681     2.55e-05        0.048       0.0638       0.0388       0.0665       0.0527       0.0503       0.0847       0.0675        0.373      0.00388\n","     88    20      0.00863      0.00863     1.98e-06       0.0558       0.0719       0.0475       0.0725         0.06       0.0597       0.0914       0.0756       0.0873     0.000909\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     88     1      0.00714      0.00714     2.47e-06       0.0489       0.0654       0.0378       0.0711       0.0544       0.0486       0.0899       0.0693       0.0921     0.000959\n","     88     2      0.00714      0.00714     1.13e-06       0.0485       0.0654        0.037       0.0714       0.0542       0.0479       0.0908       0.0693       0.0545     0.000568\n","     88     3      0.00679      0.00679     1.33e-06       0.0473       0.0638        0.036       0.0697       0.0529       0.0461       0.0891       0.0676       0.0685     0.000713\n","     88     4      0.00697      0.00697     1.56e-06       0.0477       0.0646       0.0364       0.0702       0.0533       0.0464       0.0906       0.0685       0.0791     0.000824\n","     88     5      0.00645      0.00645      9.5e-07       0.0466       0.0621       0.0364       0.0671       0.0517       0.0469       0.0847       0.0658       0.0478     0.000497\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              88  338.556    0.005      0.00744     1.39e-05      0.00746       0.0506       0.0667       0.0407       0.0703       0.0555       0.0519       0.0894       0.0706        0.224      0.00233\n","! Validation         88  338.556    0.005       0.0069     1.49e-06       0.0069       0.0478       0.0643       0.0367       0.0699       0.0533       0.0472       0.0891       0.0681       0.0684     0.000712\n","Wall time: 338.5564166109998\n","! Best model       88    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     89     1      0.00879      0.00879     8.88e-07       0.0561       0.0725       0.0477       0.0729       0.0603       0.0592       0.0936       0.0764       0.0613     0.000639\n","     89     2      0.00654      0.00654     4.05e-06       0.0469       0.0626       0.0369       0.0667       0.0518       0.0466        0.086       0.0663        0.146      0.00152\n","     89     3       0.0068       0.0068     8.45e-07       0.0479       0.0638       0.0377       0.0683        0.053       0.0483       0.0869       0.0676       0.0479     0.000498\n","     89     4      0.00537      0.00536     2.74e-06        0.043       0.0567       0.0336       0.0617       0.0477       0.0421        0.078       0.0601        0.102      0.00106\n","     89     5      0.00615      0.00614     4.94e-06       0.0461       0.0606       0.0359       0.0664       0.0511       0.0452       0.0833       0.0643        0.148      0.00155\n","     89     6      0.00642      0.00642     3.53e-06       0.0461        0.062       0.0353       0.0676       0.0514       0.0452       0.0863       0.0657        0.131      0.00137\n","     89     7      0.00692      0.00691     1.01e-05       0.0488       0.0643       0.0383         0.07       0.0541       0.0483       0.0879       0.0681         0.21      0.00219\n","     89     8      0.00758      0.00758     8.99e-07       0.0499       0.0673       0.0402       0.0693       0.0547       0.0519       0.0907       0.0713       0.0615     0.000641\n","     89     9      0.00656      0.00656      1.7e-06        0.048       0.0626       0.0392       0.0655       0.0524       0.0496       0.0827       0.0662       0.0875     0.000911\n","     89    10      0.00732      0.00731     1.86e-05       0.0497       0.0661       0.0392       0.0706       0.0549       0.0498       0.0903       0.0701         0.32      0.00333\n","     89    11      0.00686      0.00686     1.28e-06       0.0483       0.0641       0.0368       0.0713        0.054       0.0468       0.0891       0.0679       0.0775     0.000808\n","     89    12       0.0082      0.00818     2.64e-05       0.0526         0.07       0.0425       0.0727       0.0576       0.0537       0.0944       0.0741        0.376      0.00391\n","     89    13      0.00715      0.00715     5.89e-07       0.0505       0.0654        0.042       0.0673       0.0547       0.0526       0.0855        0.069       0.0473     0.000492\n","     89    14        0.007        0.007      2.6e-06       0.0483       0.0647       0.0362       0.0725       0.0544       0.0466       0.0907       0.0686        0.101      0.00105\n","     89    15      0.00674      0.00672      1.8e-05       0.0479       0.0634       0.0379       0.0678       0.0529       0.0489       0.0853       0.0671        0.303      0.00315\n","     89    16      0.00718      0.00717     3.28e-06       0.0494       0.0655       0.0398       0.0687       0.0542       0.0511       0.0876       0.0693        0.123      0.00128\n","     89    17      0.00713       0.0071     2.41e-05        0.048       0.0652       0.0351        0.074       0.0545       0.0445       0.0937       0.0691        0.362      0.00377\n","     89    18      0.00669      0.00669      6.8e-07       0.0476       0.0633       0.0365       0.0696       0.0531       0.0461       0.0881       0.0671       0.0518     0.000539\n","     89    19      0.00597      0.00596     4.21e-06       0.0445       0.0597        0.034       0.0655       0.0497       0.0436       0.0831       0.0634        0.116       0.0012\n","     89    20      0.00637      0.00637     5.23e-06       0.0463       0.0617       0.0358       0.0673       0.0516        0.046       0.0849       0.0654        0.152      0.00158\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     89     1      0.00711      0.00711     2.42e-06       0.0488       0.0652       0.0377        0.071       0.0543       0.0485       0.0898       0.0691       0.0908     0.000946\n","     89     2      0.00711      0.00711     1.11e-06       0.0484       0.0652       0.0369       0.0713       0.0541       0.0477       0.0906       0.0692       0.0532     0.000554\n","     89     3      0.00677      0.00677     1.39e-06       0.0472       0.0636       0.0359       0.0697       0.0528        0.046        0.089       0.0675       0.0703     0.000732\n","     89     4      0.00694      0.00694     1.57e-06       0.0476       0.0644       0.0363       0.0701       0.0532       0.0463       0.0904       0.0684       0.0791     0.000824\n","     89     5      0.00642      0.00642     9.56e-07       0.0465        0.062       0.0363        0.067       0.0516       0.0468       0.0846       0.0657       0.0479     0.000498\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              89  342.393    0.005      0.00688     6.74e-06      0.00689       0.0483       0.0642        0.038       0.0688       0.0534       0.0485       0.0875        0.068        0.151      0.00158\n","! Validation         89  342.393    0.005      0.00687     1.49e-06      0.00687       0.0477       0.0641       0.0366       0.0698       0.0532       0.0471       0.0889        0.068       0.0683     0.000711\n","Wall time: 342.39408124799957\n","! Best model       89    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     90     1      0.00706      0.00705     6.99e-06       0.0477        0.065       0.0363       0.0705       0.0534       0.0475       0.0903       0.0689         0.19      0.00198\n","     90     2      0.00669      0.00669     1.12e-06       0.0483       0.0633       0.0374       0.0702       0.0538       0.0472       0.0869       0.0671       0.0674     0.000702\n","     90     3       0.0072       0.0072     4.65e-06       0.0491       0.0656       0.0387       0.0699       0.0543        0.049       0.0901       0.0696         0.11      0.00114\n","     90     4      0.00706      0.00706     2.05e-06       0.0487        0.065         0.04       0.0662       0.0531       0.0511       0.0863       0.0687        0.104      0.00108\n","     90     5       0.0066      0.00659     1.27e-05       0.0459       0.0628       0.0355       0.0668       0.0511       0.0462       0.0869       0.0666        0.262      0.00273\n","     90     6      0.00604      0.00604     1.38e-06       0.0457       0.0601       0.0346       0.0679       0.0513       0.0436       0.0839       0.0638       0.0615     0.000641\n","     90     7      0.00629      0.00628     1.75e-06       0.0462       0.0613       0.0364       0.0659       0.0512       0.0463       0.0836        0.065       0.0947     0.000987\n","     90     8      0.00655      0.00655     5.25e-07       0.0477       0.0626       0.0367       0.0696       0.0532       0.0461       0.0866       0.0664       0.0512     0.000533\n","     90     9      0.00603      0.00603     2.08e-06       0.0453       0.0601       0.0348       0.0664       0.0506        0.044       0.0834       0.0637        0.101      0.00105\n","     90    10      0.00628      0.00628     4.36e-06       0.0458       0.0613       0.0352       0.0671       0.0512       0.0448       0.0852        0.065        0.143      0.00149\n","     90    11      0.00754      0.00754     4.04e-06       0.0498       0.0672       0.0378       0.0738       0.0558       0.0484        0.094       0.0712        0.142      0.00148\n","     90    12      0.00751      0.00747     3.85e-05        0.051       0.0669       0.0419       0.0693       0.0556       0.0522       0.0892       0.0707        0.456      0.00475\n","     90    13      0.00748      0.00747     2.52e-06       0.0511       0.0669       0.0422       0.0689       0.0556       0.0535       0.0878       0.0706       0.0787      0.00082\n","     90    14      0.00677      0.00676     6.65e-06       0.0478       0.0636       0.0366         0.07       0.0533       0.0464       0.0885       0.0674        0.183      0.00191\n","     90    15       0.0069      0.00688     1.88e-05       0.0475       0.0642       0.0371       0.0682       0.0526       0.0482       0.0878        0.068         0.32      0.00333\n","     90    16      0.00574      0.00573     1.02e-06       0.0448       0.0586       0.0354       0.0634       0.0494        0.044       0.0802       0.0621       0.0619     0.000645\n","     90    17      0.00711      0.00706      4.9e-05        0.049        0.065       0.0389       0.0691        0.054       0.0501       0.0875       0.0688        0.513      0.00534\n","     90    18      0.00606      0.00606     1.08e-06       0.0445       0.0602       0.0338       0.0659       0.0498       0.0431       0.0847       0.0639       0.0668     0.000696\n","     90    19      0.00695      0.00691      3.6e-05       0.0491       0.0643       0.0391       0.0692       0.0542        0.049       0.0872       0.0681        0.445      0.00463\n","     90    20      0.00605      0.00605     4.77e-06       0.0447       0.0602       0.0336       0.0668       0.0502       0.0436        0.084       0.0638        0.155      0.00161\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     90     1      0.00708      0.00708     2.46e-06       0.0487       0.0651       0.0376       0.0709       0.0542       0.0483       0.0896        0.069       0.0916     0.000954\n","     90     2      0.00708      0.00707     1.09e-06       0.0483       0.0651       0.0368       0.0712        0.054       0.0475       0.0905        0.069        0.053     0.000552\n","     90     3      0.00674      0.00674     1.34e-06       0.0471       0.0635       0.0358       0.0696       0.0527       0.0458       0.0889       0.0674       0.0706     0.000735\n","     90     4      0.00691      0.00691     1.54e-06       0.0475       0.0643       0.0362       0.0699       0.0531       0.0461       0.0903       0.0682       0.0785     0.000818\n","     90     5       0.0064       0.0064     9.56e-07       0.0464       0.0619       0.0361       0.0669       0.0515       0.0466       0.0845       0.0656       0.0475     0.000494\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              90  346.214    0.005      0.00669        1e-05       0.0067       0.0475       0.0633       0.0371       0.0682       0.0527       0.0473       0.0868        0.067         0.18      0.00188\n","! Validation         90  346.214    0.005      0.00684     1.48e-06      0.00684       0.0476        0.064       0.0365       0.0697       0.0531       0.0469       0.0888       0.0678       0.0682     0.000711\n","Wall time: 346.2144573839996\n","! Best model       90    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     91     1      0.00793       0.0079     3.22e-05       0.0524       0.0688        0.044       0.0693       0.0567       0.0564       0.0884       0.0724        0.409      0.00426\n","     91     2      0.00643      0.00643     3.33e-06       0.0464        0.062       0.0356       0.0681       0.0518       0.0446        0.087       0.0658        0.122      0.00127\n","     91     3      0.00932      0.00928     3.86e-05        0.057       0.0745       0.0444       0.0823       0.0633       0.0553        0.103        0.079        0.452      0.00471\n","     91     4       0.0088      0.00879     1.22e-05        0.056       0.0725       0.0467       0.0747       0.0607       0.0592       0.0936       0.0764        0.249       0.0026\n","     91     5      0.00752      0.00752     2.37e-06       0.0525       0.0671       0.0463       0.0648       0.0556       0.0574       0.0831       0.0703       0.0924     0.000962\n","     91     6      0.00619      0.00619      1.1e-06       0.0457       0.0609       0.0353       0.0666       0.0509       0.0451       0.0839       0.0645       0.0762     0.000793\n","     91     7      0.00626      0.00624     1.48e-05       0.0462       0.0611       0.0366       0.0654        0.051       0.0464       0.0831       0.0647        0.267      0.00279\n","     91     8      0.00617      0.00617     6.35e-07       0.0455       0.0608       0.0335       0.0695       0.0515       0.0426       0.0863       0.0645       0.0381     0.000397\n","     91     9      0.00652      0.00651        1e-05       0.0468       0.0624        0.037       0.0663       0.0517       0.0469       0.0854       0.0661         0.23      0.00239\n","     91    10      0.00646      0.00645     5.23e-06       0.0465       0.0621       0.0357       0.0681       0.0519       0.0449       0.0869       0.0659        0.169      0.00176\n","     91    11      0.00759      0.00759     7.38e-06       0.0498       0.0674       0.0371       0.0752       0.0561       0.0479       0.0951       0.0715        0.199      0.00207\n","     91    12      0.00641      0.00641     2.99e-06       0.0465       0.0619       0.0368       0.0659       0.0513        0.046       0.0853       0.0656        0.112      0.00117\n","     91    13      0.00765      0.00765     2.35e-06        0.052       0.0677       0.0444       0.0673       0.0558        0.056       0.0864       0.0712        0.109      0.00113\n","     91    14      0.00623      0.00622     6.96e-06       0.0464        0.061        0.037       0.0652       0.0511       0.0473       0.0818       0.0646        0.186      0.00194\n","     91    15      0.00606      0.00606     1.29e-06        0.045       0.0602       0.0341       0.0667       0.0504       0.0432       0.0845       0.0639       0.0713     0.000743\n","     91    16       0.0068      0.00677      2.4e-05       0.0478       0.0637        0.038       0.0673       0.0527        0.048       0.0869       0.0675        0.359      0.00374\n","     91    17      0.00779      0.00777     1.62e-05       0.0504       0.0682       0.0387       0.0737       0.0562        0.051       0.0936       0.0723        0.296      0.00308\n","     91    18      0.00638      0.00637     1.51e-05       0.0444       0.0617       0.0335       0.0662       0.0498       0.0435       0.0875       0.0655        0.286      0.00298\n","     91    19      0.00674      0.00673     1.01e-05       0.0476       0.0635       0.0357       0.0714       0.0535       0.0449       0.0897       0.0673        0.221       0.0023\n","     91    20       0.0086      0.00854     5.69e-05       0.0552       0.0715       0.0457       0.0744         0.06       0.0576       0.0933       0.0754        0.557      0.00581\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     91     1      0.00705      0.00705     2.53e-06       0.0486        0.065       0.0374       0.0708       0.0541       0.0482       0.0895       0.0689       0.0931     0.000969\n","     91     2      0.00706      0.00706      1.1e-06       0.0482        0.065       0.0367       0.0711       0.0539       0.0474       0.0904       0.0689       0.0517     0.000538\n","     91     3      0.00672      0.00672     1.35e-06        0.047       0.0634       0.0357       0.0695       0.0526       0.0457       0.0888       0.0672       0.0706     0.000735\n","     91     4      0.00688      0.00688     1.51e-06       0.0473       0.0642       0.0361       0.0698        0.053        0.046       0.0901       0.0681       0.0793     0.000826\n","     91     5      0.00637      0.00637      9.3e-07       0.0463       0.0617        0.036       0.0668       0.0514       0.0464       0.0844       0.0654       0.0465     0.000484\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              91  350.059    0.005      0.00708     1.32e-05      0.00709        0.049       0.0651       0.0388       0.0694       0.0541       0.0495       0.0884       0.0689        0.225      0.00234\n","! Validation         91  350.059    0.005      0.00681     1.48e-06      0.00682       0.0475       0.0639       0.0364       0.0696        0.053       0.0468       0.0887       0.0677       0.0682     0.000711\n","Wall time: 350.0597456209998\n","! Best model       91    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     92     1      0.00935      0.00935        4e-06       0.0587       0.0748       0.0529       0.0702       0.0616       0.0652        0.091       0.0781        0.134       0.0014\n","     92     2      0.00671      0.00666     4.61e-05       0.0478       0.0631       0.0387        0.066       0.0523       0.0485       0.0852       0.0668        0.501      0.00522\n","     92     3      0.00622      0.00621      1.1e-05       0.0466        0.061        0.038       0.0638       0.0509       0.0475       0.0815       0.0645         0.23      0.00239\n","     92     4      0.00851      0.00847     4.19e-05       0.0549       0.0712       0.0454        0.074       0.0597       0.0573       0.0929       0.0751        0.478      0.00498\n","     92     5      0.00964      0.00949     0.000144       0.0579       0.0754       0.0469       0.0799       0.0634       0.0588        0.101       0.0797         0.89      0.00927\n","     92     6      0.00868      0.00862     5.93e-05       0.0544       0.0718       0.0438       0.0755       0.0596       0.0547       0.0975       0.0761         0.57      0.00594\n","     92     7      0.00644      0.00634     9.98e-05       0.0466       0.0616       0.0373       0.0652       0.0513       0.0475       0.0828       0.0652        0.739      0.00769\n","     92     8      0.00691       0.0069     2.85e-06       0.0482       0.0643       0.0376       0.0694       0.0535       0.0471       0.0893       0.0682        0.122      0.00127\n","     92     9      0.00649      0.00647     1.76e-05       0.0479       0.0622       0.0401       0.0636       0.0518       0.0506       0.0805       0.0656        0.307       0.0032\n","     92    10      0.00748      0.00746     1.99e-05       0.0509       0.0668       0.0434        0.066       0.0547       0.0549       0.0858       0.0703         0.33      0.00344\n","     92    11      0.00629      0.00628     1.04e-05       0.0458       0.0613       0.0365       0.0642       0.0504       0.0461       0.0838        0.065         0.23      0.00239\n","     92    12      0.00657      0.00656     7.85e-06       0.0482       0.0627       0.0388        0.067       0.0529       0.0491       0.0834       0.0663        0.182       0.0019\n","     92    13       0.0069       0.0069     6.07e-07       0.0477       0.0643       0.0371       0.0688        0.053       0.0483       0.0879       0.0681       0.0502     0.000523\n","     92    14      0.00724      0.00723      3.8e-06       0.0496       0.0658       0.0389        0.071        0.055       0.0494       0.0901       0.0697        0.141      0.00147\n","     92    15      0.00587      0.00587     1.11e-06       0.0446       0.0593       0.0337       0.0664       0.0501       0.0428        0.083       0.0629       0.0633     0.000659\n","     92    16      0.00742      0.00742     1.43e-06       0.0508       0.0667       0.0393       0.0737       0.0565       0.0492       0.0921       0.0707       0.0699     0.000728\n","     92    17      0.00683      0.00682     1.66e-05       0.0478       0.0639       0.0355       0.0725        0.054       0.0453       0.0902       0.0677        0.298       0.0031\n","     92    18      0.00781      0.00781     3.02e-06       0.0527       0.0684       0.0432       0.0716       0.0574       0.0548       0.0896       0.0722        0.109      0.00113\n","     92    19      0.00669      0.00667     1.57e-05       0.0475       0.0632       0.0371       0.0683       0.0527       0.0467       0.0873        0.067        0.284      0.00296\n","     92    20      0.00708      0.00708     4.87e-06       0.0498       0.0651       0.0397       0.0702       0.0549       0.0497       0.0881       0.0689        0.155      0.00162\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     92     1      0.00703      0.00703     2.47e-06       0.0485       0.0649       0.0373       0.0707        0.054       0.0481       0.0894       0.0688       0.0917     0.000955\n","     92     2      0.00703      0.00703     1.09e-06       0.0481       0.0648       0.0366        0.071       0.0538       0.0473       0.0903       0.0688       0.0521     0.000542\n","     92     3      0.00669      0.00669     1.34e-06       0.0469       0.0633       0.0356       0.0695       0.0525       0.0455       0.0887       0.0671       0.0701      0.00073\n","     92     4      0.00685      0.00685     1.58e-06       0.0473        0.064        0.036       0.0698       0.0529       0.0458         0.09       0.0679       0.0808     0.000841\n","     92     5      0.00634      0.00634      9.3e-07       0.0462       0.0616       0.0359       0.0667       0.0513       0.0463       0.0843       0.0653       0.0472     0.000491\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              92  353.891    0.005      0.00723     2.56e-05      0.00726       0.0499       0.0658       0.0402       0.0694       0.0548       0.0509       0.0883       0.0696        0.294      0.00306\n","! Validation         92  353.891    0.005      0.00679     1.48e-06      0.00679       0.0474       0.0637       0.0363       0.0695       0.0529       0.0466       0.0886       0.0676       0.0684     0.000712\n","Wall time: 353.89143848799995\n","! Best model       92    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     93     1      0.00661       0.0066     1.17e-05       0.0469       0.0628       0.0361       0.0684       0.0523       0.0464       0.0869       0.0666        0.241      0.00251\n","     93     2      0.00794      0.00794      7.6e-07        0.055       0.0689       0.0498       0.0654       0.0576        0.061       0.0825       0.0718       0.0512     0.000533\n","     93     3      0.00818      0.00817      5.2e-06       0.0536       0.0699       0.0445       0.0718       0.0581        0.057       0.0904       0.0737        0.167      0.00174\n","     93     4      0.00754      0.00754     3.72e-06       0.0504       0.0672       0.0389       0.0732       0.0561       0.0489       0.0936       0.0712        0.141      0.00146\n","     93     5      0.00645      0.00642      2.8e-05       0.0459        0.062        0.034       0.0697       0.0519       0.0436       0.0879       0.0658        0.386      0.00402\n","     93     6      0.00696      0.00693     3.22e-05       0.0482       0.0644       0.0367        0.071       0.0539       0.0465       0.0901       0.0683        0.418      0.00435\n","     93     7      0.00754      0.00744      9.4e-05       0.0509       0.0667       0.0421       0.0685       0.0553       0.0538        0.087       0.0704        0.718      0.00748\n","     93     8      0.00716      0.00714     2.01e-05       0.0487       0.0654       0.0396       0.0669       0.0533       0.0504       0.0879       0.0692        0.323      0.00337\n","     93     9      0.00676      0.00673     2.71e-05       0.0478       0.0635       0.0363        0.071       0.0536       0.0461       0.0886       0.0673        0.373      0.00388\n","     93    10      0.00718      0.00714     4.81e-05       0.0491       0.0654       0.0407       0.0658       0.0533       0.0527       0.0852        0.069        0.511      0.00532\n","     93    11      0.00746      0.00746     1.42e-06       0.0506       0.0668       0.0394       0.0729       0.0562       0.0498       0.0918       0.0708       0.0666     0.000694\n","     93    12      0.00747      0.00733     0.000134       0.0499       0.0662       0.0386       0.0724       0.0555        0.048       0.0925       0.0703        0.855      0.00891\n","     93    13      0.00574      0.00574     7.13e-06       0.0443       0.0586       0.0344       0.0641       0.0493       0.0438       0.0804       0.0621        0.189      0.00197\n","     93    14      0.00761      0.00755     6.16e-05       0.0505       0.0672       0.0394       0.0728       0.0561       0.0504       0.0921       0.0712        0.581      0.00605\n","     93    15      0.00726      0.00726     2.75e-06       0.0491       0.0659       0.0385       0.0705       0.0545       0.0493       0.0904       0.0698        0.117      0.00122\n","     93    16      0.00685      0.00684     8.37e-06       0.0486        0.064         0.04       0.0657       0.0528       0.0506       0.0847       0.0676        0.207      0.00215\n","     93    17      0.00544      0.00544     5.25e-06        0.043        0.057       0.0344       0.0602       0.0473       0.0443       0.0764       0.0603        0.127      0.00132\n","     93    18      0.00741      0.00741     5.32e-06       0.0515       0.0666       0.0411       0.0723       0.0567       0.0512       0.0897       0.0705        0.157      0.00164\n","     93    19       0.0104       0.0103     1.99e-05       0.0597       0.0787       0.0484       0.0823       0.0654       0.0615        0.105       0.0832        0.327       0.0034\n","     93    20      0.00948      0.00937     0.000112        0.058       0.0749       0.0481       0.0779        0.063         0.06        0.098        0.079        0.784      0.00817\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     93     1        0.007        0.007     2.45e-06       0.0484       0.0647       0.0372       0.0706       0.0539       0.0479       0.0893       0.0686       0.0942     0.000982\n","     93     2        0.007        0.007     9.55e-07        0.048       0.0647       0.0365        0.071       0.0537       0.0471       0.0902       0.0686       0.0465     0.000484\n","     93     3      0.00667      0.00667     1.37e-06       0.0468       0.0632       0.0355       0.0694       0.0524       0.0454       0.0886        0.067       0.0719     0.000749\n","     93     4      0.00683      0.00682     1.62e-06       0.0471       0.0639       0.0359       0.0697       0.0528       0.0457       0.0899       0.0678       0.0822     0.000857\n","     93     5      0.00633      0.00632     8.96e-07       0.0461       0.0615       0.0358       0.0666       0.0512       0.0461       0.0843       0.0652       0.0464     0.000483\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              93  357.714    0.005      0.00734     3.14e-05      0.00737       0.0501       0.0663       0.0401       0.0701       0.0551       0.0511       0.0892       0.0702        0.337      0.00351\n","! Validation         93  357.714    0.005      0.00676     1.46e-06      0.00676       0.0473       0.0636       0.0362       0.0695       0.0528       0.0465       0.0885       0.0675       0.0682     0.000711\n","Wall time: 357.71472245099994\n","! Best model       93    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     94     1      0.00701      0.00698     2.25e-05       0.0488       0.0646       0.0389       0.0685       0.0537       0.0496       0.0872       0.0684        0.345      0.00359\n","     94     2      0.00692      0.00688     4.16e-05       0.0478       0.0642       0.0364       0.0706       0.0535       0.0471        0.089        0.068        0.477      0.00497\n","     94     3      0.00743      0.00738     4.36e-05        0.051       0.0665        0.041       0.0709       0.0559       0.0512       0.0895       0.0704        0.488      0.00508\n","     94     4      0.00824      0.00824     2.48e-06       0.0556       0.0702       0.0482       0.0705       0.0593       0.0591       0.0883       0.0737       0.0865     0.000901\n","     94     5      0.00853      0.00847      5.9e-05       0.0548       0.0712       0.0449       0.0746       0.0598       0.0564        0.094       0.0752        0.565      0.00588\n","     94     6      0.00649      0.00649     4.58e-07        0.046       0.0623       0.0359        0.066        0.051        0.046       0.0861       0.0661       0.0445     0.000464\n","     94     7      0.00658      0.00657     1.32e-05       0.0471       0.0627       0.0365       0.0683       0.0524       0.0473       0.0855       0.0664        0.261      0.00272\n","     94     8      0.00833      0.00832     5.33e-06       0.0539       0.0706       0.0426       0.0764       0.0595       0.0529       0.0967       0.0748        0.164      0.00171\n","     94     9      0.00884      0.00883     1.24e-05       0.0558       0.0727       0.0462       0.0749       0.0606       0.0582       0.0953       0.0768        0.245      0.00256\n","     94    10      0.00694      0.00694     7.12e-06       0.0485       0.0644       0.0394       0.0668       0.0531       0.0508       0.0854       0.0681        0.195      0.00203\n","     94    11      0.00647      0.00647     1.22e-06        0.047       0.0622       0.0354         0.07       0.0527       0.0447       0.0873        0.066       0.0537     0.000559\n","     94    12      0.00758      0.00757     1.71e-05       0.0512       0.0673       0.0434       0.0667       0.0551       0.0554       0.0863       0.0709        0.283      0.00295\n","     94    13      0.00766      0.00765     7.57e-06       0.0518       0.0677       0.0417       0.0721       0.0569       0.0527       0.0904       0.0716         0.17      0.00177\n","     94    14       0.0057      0.00569      7.3e-06       0.0433       0.0584        0.033        0.064       0.0485       0.0419       0.0819       0.0619        0.185      0.00193\n","     94    15      0.00669      0.00668     9.89e-07       0.0484       0.0633       0.0394       0.0665       0.0529       0.0495       0.0842       0.0669       0.0686     0.000714\n","     94    16      0.00795      0.00795        8e-07       0.0521        0.069       0.0423       0.0717        0.057       0.0538       0.0922        0.073       0.0547      0.00057\n","     94    17      0.00754      0.00751     2.92e-05       0.0494        0.067       0.0374       0.0734       0.0554       0.0483       0.0938       0.0711        0.398      0.00415\n","     94    18      0.00615      0.00614     1.24e-06       0.0456       0.0606       0.0348       0.0672        0.051       0.0438       0.0849       0.0643       0.0631     0.000657\n","     94    19      0.00587      0.00586     9.88e-06       0.0451       0.0592        0.036       0.0633       0.0497       0.0451       0.0803       0.0627        0.229      0.00238\n","     94    20      0.00618      0.00618     1.39e-06       0.0456       0.0608       0.0355       0.0659       0.0507       0.0455       0.0834       0.0644       0.0797      0.00083\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     94     1      0.00698      0.00698     2.42e-06       0.0483       0.0646       0.0371       0.0705       0.0538       0.0478       0.0892       0.0685        0.091     0.000948\n","     94     2      0.00698      0.00698     1.06e-06       0.0479       0.0646       0.0364       0.0709       0.0536        0.047       0.0901       0.0685       0.0507     0.000528\n","     94     3      0.00665      0.00665     1.34e-06       0.0467       0.0631       0.0354       0.0693       0.0524       0.0453       0.0885       0.0669       0.0696     0.000725\n","     94     4       0.0068       0.0068     1.56e-06        0.047       0.0638       0.0358       0.0696       0.0527       0.0456       0.0898       0.0677       0.0794     0.000827\n","     94     5       0.0063       0.0063     9.23e-07        0.046       0.0614       0.0357       0.0665       0.0511        0.046       0.0842       0.0651       0.0457     0.000476\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              94  361.539    0.005      0.00714     1.42e-05      0.00715       0.0494       0.0654       0.0394       0.0694       0.0544       0.0502       0.0882       0.0692        0.223      0.00232\n","! Validation         94  361.539    0.005      0.00674     1.46e-06      0.00674       0.0472       0.0635       0.0361       0.0694       0.0527       0.0463       0.0884       0.0674       0.0673     0.000701\n","Wall time: 361.5393526079997\n","! Best model       94    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     95     1       0.0064       0.0064     7.46e-07       0.0464       0.0619       0.0357       0.0678       0.0517       0.0456       0.0857       0.0656        0.052     0.000541\n","     95     2       0.0067      0.00668     1.17e-05       0.0487       0.0633       0.0392       0.0676       0.0534       0.0491       0.0848       0.0669        0.218      0.00227\n","     95     3      0.00605      0.00605     4.23e-07       0.0458       0.0602       0.0372       0.0628         0.05       0.0467       0.0806       0.0636       0.0451      0.00047\n","     95     4      0.00691      0.00689     2.17e-05       0.0475       0.0642       0.0363       0.0699       0.0531       0.0473       0.0888       0.0681        0.346       0.0036\n","     95     5      0.00608      0.00604      3.4e-05       0.0451       0.0601       0.0348       0.0657       0.0503       0.0439       0.0836       0.0638         0.43      0.00448\n","     95     6      0.00739      0.00739      1.4e-06        0.049       0.0665       0.0363       0.0745       0.0554       0.0468       0.0943       0.0705       0.0746     0.000777\n","     95     7      0.00721      0.00705     0.000165       0.0498        0.065       0.0408       0.0677       0.0543       0.0516       0.0857       0.0686        0.952      0.00992\n","     95     8      0.00657      0.00656     6.19e-06       0.0477       0.0627       0.0373       0.0685       0.0529       0.0473       0.0855       0.0664        0.182       0.0019\n","     95     9      0.00672      0.00662     9.21e-05       0.0467        0.063       0.0352       0.0696       0.0524       0.0445        0.089       0.0668        0.712      0.00742\n","     95    10      0.00832       0.0083     2.12e-05       0.0546       0.0705       0.0481       0.0678       0.0579       0.0618       0.0853       0.0735        0.318      0.00331\n","     95    11      0.00795      0.00794     8.74e-06       0.0528       0.0689        0.042       0.0744       0.0582       0.0529        0.093        0.073        0.217      0.00226\n","     95    12      0.00583       0.0058     2.99e-05       0.0443       0.0589       0.0334       0.0662       0.0498       0.0421        0.083       0.0625        0.395      0.00412\n","     95    13      0.00613      0.00611     2.04e-05       0.0452       0.0605       0.0365       0.0628       0.0496       0.0468       0.0812        0.064        0.329      0.00343\n","     95    14      0.00782      0.00776     5.77e-05       0.0505       0.0681       0.0378       0.0758       0.0568       0.0488       0.0958       0.0723        0.562      0.00585\n","     95    15      0.00695       0.0069     5.53e-05       0.0487       0.0643        0.038       0.0699        0.054       0.0485       0.0877       0.0681        0.551      0.00574\n","     95    16      0.00765      0.00756     8.69e-05       0.0507       0.0673         0.04       0.0721       0.0561       0.0513       0.0912       0.0713         0.69      0.00719\n","     95    17      0.00608      0.00607     7.45e-06       0.0447       0.0603       0.0345       0.0653       0.0499       0.0445       0.0832       0.0639        0.198      0.00207\n","     95    18      0.00704      0.00695     8.62e-05        0.049       0.0645       0.0397       0.0678       0.0537       0.0505        0.086       0.0682        0.685      0.00714\n","     95    19      0.00706      0.00704     2.54e-05       0.0473       0.0649       0.0361       0.0696       0.0528       0.0466       0.0911       0.0688         0.37      0.00385\n","     95    20      0.00626       0.0062     5.92e-05       0.0457       0.0609       0.0353       0.0666       0.0509       0.0452        0.084       0.0646        0.568      0.00591\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     95     1      0.00696      0.00695     2.47e-06       0.0482       0.0645       0.0371       0.0704       0.0537       0.0477       0.0891       0.0684       0.0923     0.000961\n","     95     2      0.00696      0.00695     9.82e-07       0.0478       0.0645       0.0363       0.0708       0.0536       0.0469       0.0899       0.0684       0.0485     0.000506\n","     95     3      0.00662      0.00662     1.31e-06       0.0466        0.063       0.0353       0.0693       0.0523       0.0451       0.0884       0.0668       0.0694     0.000723\n","     95     4      0.00678      0.00678     1.55e-06       0.0469       0.0637       0.0357       0.0695       0.0526       0.0454       0.0897       0.0676       0.0803     0.000836\n","     95     5      0.00628      0.00628     9.13e-07       0.0459       0.0613       0.0356       0.0665        0.051       0.0458       0.0841        0.065       0.0463     0.000482\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              95  365.363    0.005      0.00682     3.96e-05      0.00686        0.048       0.0639       0.0377       0.0686       0.0532       0.0483       0.0871       0.0677        0.395      0.00411\n","! Validation         95  365.363    0.005      0.00672     1.45e-06      0.00672       0.0471       0.0634        0.036       0.0693       0.0526       0.0462       0.0883       0.0672       0.0674     0.000702\n","Wall time: 365.36389149299976\n","! Best model       95    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     96     1      0.00695      0.00688     7.15e-05       0.0489       0.0642       0.0386       0.0696       0.0541        0.048        0.088        0.068        0.625      0.00651\n","     96     2       0.0087      0.00865     4.46e-05       0.0536        0.072       0.0432       0.0744       0.0588       0.0547       0.0977       0.0762        0.493      0.00514\n","     96     3      0.00839      0.00839     3.62e-06       0.0552       0.0708       0.0454       0.0748       0.0601       0.0567       0.0928       0.0748        0.139      0.00145\n","     96     4      0.00668      0.00668     5.71e-07       0.0473       0.0632       0.0373       0.0673       0.0523       0.0474       0.0866        0.067       0.0488     0.000509\n","     96     5      0.00642      0.00642     1.72e-06       0.0457        0.062       0.0354       0.0661       0.0508        0.046       0.0854       0.0657       0.0869     0.000905\n","     96     6      0.00746      0.00746     6.95e-06       0.0512       0.0668        0.042       0.0695       0.0558       0.0542       0.0867       0.0704        0.186      0.00194\n","     96     7       0.0082      0.00818     2.17e-05       0.0525         0.07        0.042       0.0734       0.0577       0.0538       0.0944       0.0741        0.344      0.00359\n","     96     8      0.00762      0.00761     1.66e-06       0.0507       0.0675       0.0415       0.0692       0.0553       0.0525       0.0903       0.0714       0.0896     0.000934\n","     96     9      0.00662      0.00662     2.12e-06       0.0474       0.0629       0.0386        0.065       0.0518       0.0494       0.0837       0.0665       0.0975      0.00102\n","     96    10      0.00628      0.00628     3.44e-06        0.045       0.0613       0.0333       0.0682       0.0508       0.0428       0.0872        0.065        0.127      0.00132\n","     96    11      0.00683      0.00683     5.89e-06       0.0483       0.0639       0.0382       0.0684       0.0533       0.0489       0.0864       0.0677        0.166      0.00173\n","     96    12      0.00544      0.00543     5.55e-06        0.043        0.057       0.0334       0.0622       0.0478       0.0422       0.0787       0.0605        0.151      0.00157\n","     96    13      0.00599      0.00598     9.66e-06       0.0451       0.0598       0.0353       0.0648       0.0501       0.0441       0.0828       0.0634        0.209      0.00218\n","     96    14      0.00587      0.00586     1.53e-05       0.0435       0.0592       0.0321       0.0663       0.0492       0.0407       0.0849       0.0628        0.289      0.00301\n","     96    15      0.00606      0.00605      9.8e-06       0.0452       0.0602       0.0342       0.0672       0.0507       0.0433       0.0844       0.0638        0.218      0.00227\n","     96    16      0.00526      0.00521     4.95e-05       0.0418       0.0559        0.033       0.0594       0.0462        0.042       0.0764       0.0592        0.521      0.00542\n","     96    17      0.00639      0.00639     1.55e-06       0.0466       0.0618       0.0371       0.0657       0.0514       0.0477       0.0832       0.0654        0.083     0.000865\n","     96    18      0.00582      0.00575     6.27e-05       0.0444       0.0587       0.0339       0.0654       0.0496       0.0427       0.0817       0.0622        0.586       0.0061\n","     96    19      0.00573      0.00572     1.21e-05       0.0439       0.0585       0.0337       0.0644        0.049       0.0437       0.0803        0.062        0.254      0.00264\n","     96    20       0.0064      0.00636      3.9e-05       0.0463       0.0617       0.0346       0.0697       0.0522       0.0436       0.0873       0.0655        0.455      0.00474\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     96     1      0.00693      0.00693     2.51e-06       0.0481       0.0644        0.037       0.0703       0.0536       0.0476        0.089       0.0683       0.0938     0.000977\n","     96     2      0.00693      0.00693        1e-06       0.0477       0.0644       0.0362       0.0707       0.0535       0.0468       0.0898       0.0683       0.0479     0.000499\n","     96     3       0.0066       0.0066     1.39e-06       0.0465       0.0629       0.0352       0.0692       0.0522        0.045       0.0883       0.0667       0.0723     0.000753\n","     96     4      0.00676      0.00675     1.55e-06       0.0469       0.0636       0.0356       0.0694       0.0525       0.0453       0.0896       0.0674       0.0803     0.000836\n","     96     5      0.00626      0.00626     8.83e-07       0.0458       0.0612       0.0355       0.0664        0.051       0.0457        0.084       0.0649       0.0465     0.000484\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              96  369.193    0.005      0.00664     1.85e-05      0.00666       0.0473        0.063       0.0371       0.0676       0.0523       0.0475       0.0861       0.0668        0.258      0.00269\n","! Validation         96  369.193    0.005      0.00669     1.47e-06       0.0067        0.047       0.0633       0.0359       0.0692       0.0526       0.0461       0.0882       0.0671       0.0681      0.00071\n","Wall time: 369.1932551359996\n","! Best model       96    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     97     1      0.00652       0.0065     1.69e-05       0.0463       0.0624       0.0354       0.0682       0.0518       0.0454       0.0869       0.0661          0.3      0.00313\n","     97     2       0.0075      0.00747     2.31e-05       0.0509       0.0669       0.0415       0.0696       0.0556       0.0527       0.0887       0.0707        0.345      0.00359\n","     97     3      0.00628      0.00627     5.49e-06       0.0464       0.0613       0.0372       0.0649       0.0511       0.0471       0.0826       0.0648        0.169      0.00176\n","     97     4      0.00636      0.00635     1.17e-05       0.0449       0.0617       0.0339       0.0668       0.0504       0.0434       0.0874       0.0654        0.251      0.00262\n","     97     5       0.0065      0.00647     3.19e-05       0.0466       0.0622       0.0365        0.067       0.0517       0.0473       0.0846       0.0659        0.413       0.0043\n","     97     6      0.00587      0.00585      1.9e-05        0.045       0.0592       0.0351        0.065         0.05       0.0438       0.0817       0.0627        0.313      0.00326\n","     97     7      0.00649      0.00639     9.44e-05        0.045       0.0619       0.0342       0.0666       0.0504       0.0443       0.0869       0.0656        0.717      0.00747\n","     97     8      0.00609      0.00608     1.17e-05       0.0453       0.0603       0.0354       0.0652       0.0503       0.0453       0.0825       0.0639        0.251      0.00261\n","     97     9      0.00645      0.00633     0.000123       0.0461       0.0616       0.0366       0.0653       0.0509       0.0466       0.0838       0.0652        0.821      0.00855\n","     97    10      0.00687      0.00686     2.23e-06       0.0479       0.0641       0.0357       0.0723        0.054        0.046         0.09        0.068        0.091     0.000948\n","     97    11      0.00775      0.00772      3.6e-05       0.0522        0.068       0.0431       0.0705       0.0568       0.0549       0.0885       0.0717        0.437      0.00455\n","     97    12      0.00757      0.00749     7.82e-05        0.052        0.067       0.0437       0.0687       0.0562        0.055       0.0861       0.0705        0.654      0.00681\n","     97    13      0.00708      0.00707     1.46e-05       0.0483        0.065       0.0388       0.0673        0.053       0.0506        0.087       0.0688        0.278       0.0029\n","     97    14      0.00552      0.00551      1.3e-05       0.0426       0.0574        0.033       0.0617       0.0474       0.0419       0.0799       0.0609        0.263      0.00274\n","     97    15      0.00745      0.00744     1.14e-06       0.0484       0.0668       0.0362       0.0727       0.0544       0.0475       0.0941       0.0708       0.0617     0.000643\n","     97    16       0.0078       0.0078     7.84e-06       0.0525       0.0683       0.0416       0.0742       0.0579       0.0516       0.0931       0.0724        0.202       0.0021\n","     97    17      0.00846      0.00845     8.21e-06       0.0547       0.0711       0.0463       0.0714       0.0589       0.0584       0.0914       0.0749        0.193      0.00201\n","     97    18      0.00746      0.00745     1.55e-05       0.0498       0.0668       0.0387       0.0722       0.0554       0.0501       0.0914       0.0707        0.291      0.00303\n","     97    19      0.00597      0.00597     1.36e-06       0.0456       0.0598       0.0365       0.0638       0.0501       0.0463       0.0802       0.0632        0.082     0.000854\n","     97    20      0.00631      0.00631     2.84e-06       0.0461       0.0614       0.0367       0.0649       0.0508       0.0466       0.0835       0.0651        0.116       0.0012\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     97     1      0.00691      0.00691     2.46e-06        0.048       0.0643       0.0369       0.0702       0.0535       0.0474       0.0889       0.0682       0.0927     0.000965\n","     97     2      0.00691       0.0069     1.01e-06       0.0476       0.0643       0.0361       0.0707       0.0534       0.0466       0.0897       0.0682       0.0488     0.000509\n","     97     3      0.00658      0.00658     1.33e-06       0.0465       0.0628       0.0351       0.0692       0.0521       0.0449       0.0882       0.0666       0.0708     0.000738\n","     97     4      0.00674      0.00673     1.61e-06       0.0468       0.0635       0.0355       0.0694       0.0524       0.0452       0.0895       0.0673       0.0814     0.000848\n","     97     5      0.00624      0.00624     8.88e-07       0.0457       0.0611       0.0354       0.0663       0.0509       0.0456       0.0839       0.0648       0.0455     0.000474\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              97  373.028    0.005      0.00679     2.59e-05      0.00681       0.0478       0.0637       0.0378       0.0679       0.0529       0.0484       0.0866       0.0675        0.312      0.00325\n","! Validation         97  373.028    0.005      0.00667     1.46e-06      0.00667       0.0469       0.0632       0.0358       0.0692       0.0525        0.046       0.0881        0.067       0.0679     0.000707\n","Wall time: 373.02870110299955\n","! Best model       97    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     98     1        0.007      0.00697     2.89e-05        0.049       0.0646       0.0384       0.0701       0.0542       0.0484       0.0885       0.0684        0.391      0.00407\n","     98     2        0.007      0.00699     4.11e-06       0.0492       0.0647       0.0379       0.0719       0.0549        0.048       0.0891       0.0686        0.113      0.00118\n","     98     3      0.00582      0.00581     1.46e-05       0.0432        0.059       0.0319       0.0657       0.0488       0.0402       0.0848       0.0625         0.28      0.00291\n","     98     4      0.00726      0.00726     4.86e-07       0.0483       0.0659       0.0356       0.0736       0.0546       0.0461       0.0937       0.0699       0.0445     0.000464\n","     98     5      0.00698      0.00696     2.37e-05       0.0475       0.0645        0.036       0.0705       0.0532       0.0461       0.0908       0.0684        0.361      0.00376\n","     98     6      0.00578      0.00578     3.65e-06       0.0442       0.0588       0.0327       0.0671       0.0499       0.0415       0.0833       0.0624        0.123      0.00128\n","     98     7      0.00624      0.00623     1.65e-05       0.0455        0.061       0.0348       0.0668       0.0508       0.0448       0.0847       0.0647        0.299      0.00311\n","     98     8      0.00577      0.00577     2.91e-06       0.0443       0.0588       0.0349        0.063        0.049       0.0445         0.08       0.0622        0.113      0.00118\n","     98     9      0.00686      0.00686     2.82e-06       0.0484       0.0641       0.0386       0.0682       0.0534       0.0487       0.0869       0.0678        0.112      0.00117\n","     98    10      0.00717      0.00713     3.73e-05       0.0491       0.0653       0.0387         0.07       0.0543       0.0499       0.0885       0.0692         0.45      0.00469\n","     98    11      0.00718      0.00717     6.84e-06       0.0491       0.0655       0.0375       0.0724        0.055       0.0481       0.0909       0.0695        0.188      0.00196\n","     98    12      0.00634      0.00634     6.17e-06       0.0467       0.0616       0.0376       0.0649       0.0512       0.0473       0.0831       0.0652        0.179      0.00187\n","     98    13      0.00581      0.00581     4.08e-06       0.0445        0.059       0.0338       0.0658       0.0498       0.0428       0.0823       0.0625        0.136      0.00142\n","     98    14       0.0062       0.0062     5.17e-06       0.0452       0.0609       0.0353        0.065       0.0501       0.0445       0.0847       0.0646        0.159      0.00166\n","     98    15      0.00583      0.00583     9.29e-07        0.044        0.059        0.034       0.0639        0.049       0.0429       0.0823       0.0626       0.0682      0.00071\n","     98    16      0.00496      0.00496     1.55e-06       0.0416       0.0545       0.0332       0.0584       0.0458       0.0418       0.0736       0.0577       0.0762     0.000793\n","     98    17      0.00721      0.00721        3e-07       0.0484       0.0657       0.0369       0.0713       0.0541       0.0477       0.0916       0.0697       0.0348     0.000362\n","     98    18      0.00642       0.0064     1.93e-05       0.0471       0.0619       0.0365       0.0682       0.0523       0.0454       0.0858       0.0656        0.325      0.00338\n","     98    19       0.0063       0.0063     1.15e-06       0.0463       0.0614       0.0367       0.0654        0.051       0.0474       0.0826        0.065       0.0674     0.000702\n","     98    20      0.00573      0.00572     1.01e-05       0.0434       0.0585       0.0334       0.0633       0.0484       0.0438       0.0802        0.062        0.234      0.00243\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     98     1      0.00688      0.00688     2.42e-06       0.0479       0.0642       0.0368       0.0701       0.0535       0.0473       0.0888        0.068       0.0904     0.000942\n","     98     2      0.00688      0.00688     1.05e-06       0.0476       0.0642        0.036       0.0706       0.0533       0.0465       0.0896        0.068        0.051     0.000531\n","     98     3      0.00656      0.00656      1.4e-06       0.0464       0.0627        0.035       0.0691        0.052       0.0448       0.0881       0.0664       0.0718     0.000748\n","     98     4      0.00671      0.00671     1.55e-06       0.0467       0.0634       0.0354       0.0693       0.0523        0.045       0.0894       0.0672       0.0787      0.00082\n","     98     5      0.00622      0.00622     8.62e-07       0.0456        0.061       0.0353       0.0663       0.0508       0.0455       0.0839       0.0647       0.0455     0.000474\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              98  376.846    0.005      0.00638     9.53e-06      0.00639       0.0462       0.0618       0.0357       0.0673       0.0515       0.0456       0.0855       0.0655        0.188      0.00195\n","! Validation         98  376.846    0.005      0.00665     1.46e-06      0.00665       0.0468       0.0631       0.0357       0.0691       0.0524       0.0458        0.088       0.0669       0.0675     0.000703\n","Wall time: 376.8465094459998\n","! Best model       98    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     99     1      0.00634      0.00633     1.32e-05        0.046       0.0615       0.0356       0.0669       0.0512       0.0453       0.0852       0.0653        0.259       0.0027\n","     99     2      0.00599      0.00598     1.08e-05       0.0442       0.0598       0.0349       0.0628       0.0489       0.0454       0.0813       0.0633        0.217      0.00226\n","     99     3      0.00641       0.0064      5.5e-06        0.046       0.0619       0.0342       0.0697        0.052       0.0442        0.087       0.0656        0.167      0.00174\n","     99     4      0.00864      0.00859      4.8e-05       0.0546       0.0717       0.0449       0.0741       0.0595       0.0568       0.0948       0.0758        0.513      0.00534\n","     99     5      0.00912      0.00911     2.68e-06       0.0566       0.0739       0.0459       0.0779       0.0619       0.0574       0.0988       0.0781        0.105      0.00109\n","     99     6      0.00862      0.00862     6.42e-07       0.0564       0.0718       0.0462       0.0769       0.0615       0.0563       0.0955       0.0759       0.0465     0.000484\n","     99     7      0.00806      0.00803     3.14e-05       0.0535       0.0693       0.0444       0.0718       0.0581       0.0569       0.0891        0.073        0.413       0.0043\n","     99     8      0.00605      0.00605      2.7e-06       0.0445       0.0602       0.0349       0.0636       0.0493       0.0442       0.0834       0.0638        0.114      0.00118\n","     99     9       0.0066       0.0066     2.54e-06       0.0468       0.0629       0.0354       0.0696       0.0525       0.0452       0.0881       0.0667       0.0998      0.00104\n","     99    10        0.007      0.00699     6.28e-06       0.0484       0.0647       0.0392       0.0669        0.053       0.0508        0.086       0.0684        0.183       0.0019\n","     99    11      0.00712       0.0071     1.73e-05       0.0498       0.0652       0.0421       0.0651       0.0536       0.0537       0.0836       0.0686        0.299      0.00312\n","     99    12      0.00568      0.00568     4.26e-06       0.0435       0.0583       0.0338       0.0628       0.0483       0.0429       0.0807       0.0618        0.145      0.00151\n","     99    13      0.00585      0.00585     9.65e-07        0.044       0.0592       0.0324       0.0672       0.0498       0.0413       0.0842       0.0628       0.0574     0.000598\n","     99    14      0.00694      0.00694     1.02e-06       0.0467       0.0645       0.0354       0.0692       0.0523       0.0467         0.09       0.0684       0.0645     0.000671\n","     99    15       0.0064      0.00638     2.01e-05       0.0462       0.0618       0.0355       0.0675       0.0515       0.0449       0.0862       0.0656        0.323      0.00336\n","     99    16      0.00619      0.00618     6.56e-06       0.0452       0.0608       0.0332       0.0694       0.0513       0.0422       0.0868       0.0645        0.188      0.00196\n","     99    17      0.00657      0.00654     2.56e-05       0.0477       0.0626       0.0384       0.0664       0.0524       0.0482       0.0842       0.0662        0.371      0.00386\n","     99    18      0.00755      0.00755      8.8e-07       0.0526       0.0672       0.0443       0.0693       0.0568       0.0553       0.0862       0.0708       0.0518     0.000539\n","     99    19      0.00822      0.00821     2.52e-06        0.053       0.0701       0.0427       0.0736       0.0581        0.055       0.0933       0.0741       0.0957     0.000997\n","     99    20      0.00629      0.00628     2.39e-06       0.0456       0.0613       0.0348       0.0672        0.051       0.0445       0.0856        0.065        0.101      0.00105\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     99     1      0.00686      0.00685     2.42e-06       0.0478        0.064       0.0367         0.07       0.0534       0.0472       0.0886       0.0679       0.0915     0.000953\n","     99     2      0.00686      0.00686     1.01e-06       0.0475       0.0641       0.0359       0.0706       0.0532       0.0464       0.0895       0.0679       0.0485     0.000506\n","     99     3      0.00654      0.00654     1.36e-06       0.0463       0.0626       0.0349       0.0691        0.052       0.0446       0.0881       0.0664       0.0713     0.000743\n","     99     4      0.00669      0.00669     1.59e-06       0.0466       0.0633       0.0353       0.0693       0.0523       0.0449       0.0893       0.0671       0.0806     0.000839\n","     99     5      0.00619      0.00619     8.56e-07       0.0455       0.0609       0.0352       0.0662       0.0507       0.0453       0.0837       0.0645       0.0464     0.000483\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              99  380.688    0.005      0.00697     1.03e-05      0.00698       0.0486       0.0646       0.0384       0.0689       0.0536       0.0492       0.0876       0.0684        0.191      0.00199\n","! Validation         99  380.688    0.005      0.00663     1.45e-06      0.00663       0.0467        0.063       0.0356        0.069       0.0523       0.0457       0.0879       0.0668       0.0677     0.000705\n","Wall time: 380.6890034930002\n","! Best model       99    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    100     1      0.00575      0.00575     2.84e-06       0.0442       0.0587       0.0339       0.0647       0.0493        0.042       0.0824       0.0622       0.0926     0.000964\n","    100     2      0.00585      0.00584     5.58e-06       0.0445       0.0591       0.0345       0.0645       0.0495       0.0444       0.0809       0.0627        0.161      0.00168\n","    100     3      0.00614      0.00613     6.34e-06        0.045       0.0606        0.034       0.0668       0.0504       0.0442       0.0843       0.0642        0.174      0.00181\n","    100     4      0.00541      0.00539     1.35e-05       0.0431       0.0568       0.0341        0.061       0.0476       0.0427       0.0777       0.0602        0.265      0.00276\n","    100     5      0.00612      0.00611     6.78e-06       0.0445       0.0605       0.0334       0.0666         0.05       0.0422        0.086       0.0641        0.186      0.00193\n","    100     6      0.00636      0.00636     5.62e-07       0.0444       0.0617       0.0333       0.0666         0.05        0.043       0.0879       0.0654       0.0436     0.000454\n","    100     7      0.00645      0.00645     1.11e-06       0.0466       0.0621       0.0369       0.0659       0.0514       0.0476        0.084       0.0658       0.0635     0.000661\n","    100     8      0.00663      0.00662      1.2e-05       0.0483       0.0629       0.0403       0.0643       0.0523       0.0512       0.0815       0.0664        0.246      0.00256\n","    100     9      0.00644      0.00644     1.33e-06       0.0475       0.0621       0.0384       0.0657        0.052       0.0481       0.0832       0.0657       0.0781     0.000814\n","    100    10      0.00577      0.00577     1.45e-06       0.0442       0.0588       0.0333       0.0661       0.0497       0.0423       0.0824       0.0623       0.0727     0.000757\n","    100    11      0.00703      0.00702     2.85e-06       0.0488       0.0648       0.0394       0.0676       0.0535       0.0501       0.0872       0.0686        0.108      0.00112\n","    100    12      0.00658      0.00657     1.03e-06        0.047       0.0627       0.0363       0.0685       0.0524       0.0455       0.0876       0.0665       0.0643     0.000669\n","    100    13      0.00745      0.00743     2.51e-05       0.0494       0.0667       0.0372       0.0738       0.0555       0.0477       0.0938       0.0707        0.353      0.00368\n","    100    14      0.00614      0.00614     7.18e-07       0.0455       0.0606       0.0343       0.0679       0.0511       0.0434       0.0852       0.0643       0.0541     0.000564\n","    100    15      0.00553      0.00552     1.52e-05       0.0437       0.0575       0.0341       0.0628       0.0484       0.0435       0.0783       0.0609        0.279       0.0029\n","    100    16      0.00693      0.00693      1.7e-06       0.0483       0.0644       0.0383       0.0682       0.0533       0.0498       0.0865       0.0682       0.0912      0.00095\n","    100    17      0.00652      0.00651     9.14e-06       0.0486       0.0624       0.0388        0.068       0.0534       0.0485       0.0836        0.066        0.217      0.00226\n","    100    18      0.00654      0.00654     2.06e-06       0.0468       0.0626       0.0351       0.0703       0.0527        0.045       0.0877       0.0663        0.073     0.000761\n","    100    19      0.00617      0.00616      5.7e-06       0.0449       0.0607        0.035       0.0647       0.0499       0.0446       0.0842       0.0644        0.173       0.0018\n","    100    20      0.00662      0.00661     3.65e-06       0.0464       0.0629       0.0338       0.0716       0.0527       0.0437       0.0898       0.0667        0.128      0.00134\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    100     1      0.00683      0.00683     2.43e-06       0.0477       0.0639       0.0366       0.0699       0.0533        0.047       0.0885       0.0678       0.0919     0.000957\n","    100     2      0.00683      0.00683     9.91e-07       0.0474       0.0639       0.0358       0.0705       0.0532       0.0462       0.0894       0.0678       0.0497     0.000518\n","    100     3      0.00652      0.00652     1.37e-06       0.0462       0.0625       0.0348        0.069       0.0519       0.0445        0.088       0.0663       0.0709     0.000739\n","    100     4      0.00666      0.00666     1.58e-06       0.0465       0.0631       0.0352       0.0692       0.0522       0.0448       0.0892        0.067       0.0803     0.000836\n","    100     5      0.00617      0.00617     8.65e-07       0.0454       0.0608       0.0351       0.0661       0.0506       0.0452       0.0836       0.0644       0.0454     0.000473\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train             100  384.519    0.005      0.00631     5.93e-06      0.00632       0.0461       0.0615       0.0357       0.0668       0.0512       0.0456       0.0848       0.0652        0.146      0.00152\n","! Validation        100  384.519    0.005       0.0066     1.45e-06       0.0066       0.0466       0.0629       0.0355       0.0689       0.0522       0.0456       0.0878       0.0667       0.0676     0.000705\n","Wall time: 384.5202458570002\n","! Best model      100    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    101     1      0.00603      0.00603      4.3e-07       0.0451       0.0601       0.0343       0.0666       0.0505       0.0431       0.0844       0.0637       0.0369     0.000385\n","    101     2      0.00635      0.00634     7.62e-06       0.0456       0.0616       0.0332       0.0703       0.0517       0.0424       0.0882       0.0653        0.189      0.00197\n","    101     3      0.00515      0.00514     2.77e-06       0.0422       0.0555       0.0319       0.0626       0.0473         0.04       0.0777       0.0588       0.0762     0.000793\n","    101     4      0.00612      0.00611     9.79e-06       0.0457       0.0605       0.0345       0.0683       0.0514       0.0436       0.0846       0.0641        0.227      0.00236\n","    101     5      0.00725      0.00725     9.94e-07       0.0505       0.0659       0.0427       0.0661       0.0544       0.0537       0.0851       0.0694       0.0676     0.000704\n","    101     6      0.00793      0.00793     2.13e-06       0.0531       0.0689       0.0451       0.0689        0.057       0.0568       0.0882       0.0725        0.098      0.00102\n","    101     7      0.00667      0.00665     2.51e-05       0.0476       0.0631       0.0367       0.0694       0.0531        0.046       0.0878       0.0669        0.367      0.00383\n","    101     8       0.0059       0.0059     6.32e-07       0.0436       0.0594       0.0334       0.0641       0.0487        0.043        0.083        0.063       0.0574     0.000598\n","    101     9      0.00676      0.00673     3.22e-05       0.0483       0.0635        0.038        0.069       0.0535        0.048       0.0864       0.0672        0.418      0.00435\n","    101    10      0.00729      0.00728     5.23e-06       0.0501        0.066       0.0413       0.0677       0.0545       0.0529       0.0864       0.0697        0.165      0.00172\n","    101    11      0.00568      0.00568     1.44e-06       0.0441       0.0583        0.034       0.0644       0.0492       0.0431       0.0805       0.0618       0.0812     0.000846\n","    101    12      0.00625      0.00625      1.1e-06       0.0457       0.0612       0.0342       0.0686       0.0514        0.043       0.0868       0.0649       0.0668     0.000696\n","    101    13      0.00578      0.00577     9.99e-06       0.0442       0.0588       0.0344       0.0638       0.0491       0.0443       0.0803       0.0623        0.232      0.00241\n","    101    14      0.00732      0.00731     6.08e-07       0.0492       0.0662       0.0377       0.0722       0.0549        0.049       0.0913       0.0701       0.0387     0.000403\n","    101    15      0.00592      0.00591     1.29e-05       0.0453       0.0595       0.0342       0.0676       0.0509       0.0426       0.0835       0.0631        0.256      0.00267\n","    101    16      0.00608      0.00608        1e-06       0.0452       0.0603        0.035       0.0654       0.0502       0.0452       0.0826       0.0639       0.0666     0.000694\n","    101    17      0.00592      0.00592     3.83e-06       0.0439       0.0595       0.0328       0.0663       0.0495       0.0419       0.0844       0.0631        0.131      0.00137\n","    101    18      0.00645      0.00645     9.17e-07       0.0462       0.0621       0.0343       0.0702       0.0522       0.0441       0.0877       0.0659       0.0641     0.000667\n","    101    19      0.00609      0.00609        3e-06       0.0448       0.0604       0.0356       0.0631       0.0494       0.0456       0.0823        0.064       0.0959     0.000999\n","    101    20      0.00717      0.00715     1.69e-05        0.049       0.0654        0.039        0.069        0.054       0.0496        0.089       0.0693        0.289      0.00301\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    101     1       0.0068       0.0068     2.41e-06       0.0476       0.0638       0.0365       0.0698       0.0532       0.0469       0.0884       0.0677       0.0916     0.000954\n","    101     2      0.00681      0.00681     1.03e-06       0.0473       0.0638       0.0357       0.0704       0.0531       0.0461       0.0893       0.0677        0.048       0.0005\n","    101     3       0.0065       0.0065     1.27e-06       0.0461       0.0624       0.0347       0.0689       0.0518       0.0444       0.0879       0.0662       0.0694     0.000723\n","    101     4      0.00664      0.00664     1.58e-06       0.0464        0.063       0.0351       0.0691       0.0521       0.0446       0.0891       0.0669       0.0805     0.000838\n","    101     5      0.00615      0.00615     8.61e-07       0.0454       0.0607        0.035        0.066       0.0505        0.045       0.0836       0.0643       0.0455     0.000474\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train             101  388.358    0.005       0.0064     6.93e-06      0.00641       0.0465       0.0619       0.0361       0.0672       0.0516       0.0461       0.0851       0.0656        0.151      0.00158\n","! Validation        101  388.358    0.005      0.00658     1.43e-06      0.00658       0.0466       0.0628       0.0354       0.0689       0.0521       0.0454       0.0877       0.0666        0.067     0.000698\n","Wall time: 388.35864692500036\n","! Best model      101    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    102     1      0.00582      0.00581     2.37e-06       0.0443        0.059       0.0331       0.0669         0.05       0.0423       0.0828       0.0626        0.101      0.00105\n","    102     2      0.00694      0.00693      9.5e-06        0.049       0.0644       0.0396       0.0678       0.0537       0.0508       0.0853       0.0681        0.228      0.00238\n","    102     3      0.00761      0.00761     2.11e-06       0.0513       0.0675       0.0412       0.0714       0.0563       0.0521       0.0908       0.0714       0.0988      0.00103\n","    102     4      0.00801        0.008     1.91e-06       0.0524       0.0692       0.0413       0.0748        0.058       0.0526        0.094       0.0733        0.077     0.000802\n","    102     5      0.00773      0.00773     7.95e-06       0.0522        0.068       0.0415       0.0735       0.0575       0.0518       0.0922        0.072        0.199      0.00207\n","    102     6      0.00646      0.00645     1.01e-05       0.0472       0.0621       0.0393       0.0629       0.0511       0.0497       0.0815       0.0656        0.219      0.00228\n","    102     7      0.00671      0.00671     4.47e-06       0.0483       0.0634       0.0386       0.0678       0.0532       0.0491        0.085       0.0671        0.149      0.00155\n","    102     8      0.00587      0.00585     2.12e-05       0.0445       0.0591       0.0349       0.0639       0.0494       0.0438       0.0816       0.0627        0.335      0.00349\n","    102     9      0.00834      0.00834     1.73e-06       0.0546       0.0707       0.0453       0.0732       0.0592       0.0568       0.0923       0.0746       0.0746     0.000777\n","    102    10      0.00848      0.00847     8.22e-06       0.0545       0.0712       0.0428       0.0781       0.0604       0.0538        0.097       0.0754        0.206      0.00215\n","    102    11      0.00785      0.00783      2.5e-05       0.0523       0.0684        0.042       0.0731       0.0575       0.0535       0.0913       0.0724        0.363      0.00378\n","    102    12      0.00637      0.00633     4.17e-05        0.047       0.0616       0.0387       0.0635       0.0511       0.0485       0.0817       0.0651        0.474      0.00494\n","    102    13      0.00572      0.00569     2.44e-05       0.0443       0.0584       0.0354       0.0621       0.0487       0.0449       0.0787       0.0618        0.363      0.00378\n","    102    14      0.00665      0.00659     5.81e-05       0.0475       0.0628       0.0364       0.0696        0.053       0.0466       0.0866       0.0666        0.558      0.00581\n","    102    15      0.00742      0.00737     4.58e-05       0.0513       0.0664       0.0422       0.0694       0.0558       0.0524        0.088       0.0702        0.497      0.00518\n","    102    16       0.0079       0.0079     9.45e-07       0.0518       0.0688       0.0406       0.0742       0.0574       0.0513       0.0945       0.0729       0.0592     0.000616\n","    102    17      0.00653      0.00635     0.000181       0.0467       0.0617       0.0377       0.0645       0.0511       0.0476       0.0829       0.0653            1       0.0104\n","    102    18      0.00594      0.00593     1.41e-05       0.0436       0.0596       0.0331       0.0647       0.0489       0.0422       0.0841       0.0632        0.269       0.0028\n","    102    19      0.00757      0.00746     0.000108         0.05       0.0668       0.0383       0.0734       0.0558       0.0486       0.0931       0.0709        0.772      0.00804\n","    102    20       0.0092       0.0092     2.19e-06       0.0575       0.0742       0.0498        0.073       0.0614       0.0633       0.0922       0.0778       0.0977      0.00102\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    102     1      0.00678      0.00678     2.38e-06       0.0475       0.0637       0.0364       0.0697       0.0531       0.0468       0.0883       0.0675       0.0893      0.00093\n","    102     2      0.00678      0.00678     1.02e-06       0.0472       0.0637       0.0356       0.0703        0.053        0.046       0.0891       0.0675        0.049     0.000511\n","    102     3      0.00648      0.00648     1.32e-06        0.046       0.0623       0.0346       0.0688       0.0517       0.0442       0.0878        0.066       0.0699     0.000728\n","    102     4      0.00662      0.00662     1.56e-06       0.0463       0.0629        0.035        0.069        0.052       0.0445        0.089       0.0668       0.0796     0.000829\n","    102     5      0.00612      0.00612     8.76e-07       0.0452       0.0605       0.0349        0.066       0.0504       0.0449       0.0835       0.0642       0.0461      0.00048\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train             102  392.200    0.005      0.00713     2.86e-05      0.00716       0.0495       0.0653       0.0396       0.0694       0.0545       0.0503       0.0879       0.0691        0.307       0.0032\n","! Validation        102  392.200    0.005      0.00655     1.43e-06      0.00656       0.0465       0.0626       0.0353       0.0688        0.052       0.0453       0.0876       0.0664       0.0668     0.000696\n","Wall time: 392.2009978599999\n","! Best model      102    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    103     1      0.00817      0.00816     5.02e-06       0.0546       0.0699        0.047       0.0699       0.0584       0.0583       0.0886       0.0735         0.14      0.00146\n","    103     2      0.00655      0.00655     3.72e-06       0.0468       0.0626       0.0348       0.0708       0.0528       0.0441       0.0887       0.0664        0.134       0.0014\n","    103     3      0.00691      0.00689     2.44e-05       0.0481       0.0642       0.0367       0.0707       0.0537       0.0468       0.0894       0.0681        0.361      0.00376\n","    103     4      0.00642      0.00641     4.91e-06       0.0465       0.0619        0.036       0.0674       0.0517       0.0457       0.0856       0.0657        0.139      0.00145\n","    103     5      0.00543      0.00536     6.85e-05       0.0435       0.0567       0.0347       0.0612       0.0479       0.0434       0.0766         0.06        0.607      0.00633\n","    103     6      0.00592      0.00592     5.63e-06       0.0443       0.0595       0.0335       0.0659       0.0497       0.0427       0.0835       0.0631        0.163      0.00169\n","    103     7      0.00753      0.00747     5.38e-05       0.0504       0.0669        0.038       0.0753       0.0566       0.0479        0.094       0.0709        0.542      0.00565\n","    103     8      0.00775      0.00775     2.99e-07       0.0519       0.0681       0.0413        0.073       0.0571       0.0522        0.092       0.0721       0.0316      0.00033\n","    103     9      0.00814      0.00812     1.79e-05       0.0551       0.0697       0.0506        0.064       0.0573       0.0622       0.0827       0.0724        0.313      0.00326\n","    103    10      0.00692      0.00691     8.61e-06       0.0482       0.0643        0.039       0.0667       0.0528       0.0501       0.0859        0.068        0.216      0.00225\n","    103    11       0.0068      0.00679     1.03e-05       0.0484       0.0637       0.0364       0.0723       0.0543       0.0456       0.0896       0.0676        0.229      0.00239\n","    103    12      0.00606      0.00605     2.78e-06       0.0458       0.0602       0.0369       0.0635       0.0502       0.0475       0.0797       0.0636        0.108      0.00113\n","    103    13      0.00811      0.00809     2.16e-05        0.052       0.0696        0.039       0.0782       0.0586       0.0489       0.0987       0.0738        0.337      0.00351\n","    103    14       0.0067       0.0067     7.92e-06       0.0468       0.0633       0.0363       0.0679       0.0521       0.0469       0.0874       0.0671        0.192        0.002\n","    103    15      0.00626      0.00626     7.35e-06       0.0471       0.0612       0.0381       0.0651       0.0516       0.0471       0.0825       0.0648        0.188      0.00196\n","    103    16      0.00687      0.00687      4.2e-06       0.0486       0.0641       0.0376       0.0706       0.0541       0.0468       0.0891        0.068        0.146      0.00152\n","    103    17       0.0086       0.0086     4.83e-07       0.0574       0.0717        0.052       0.0682       0.0601       0.0636       0.0858       0.0747       0.0434     0.000452\n","    103    18      0.00919      0.00918     4.53e-06       0.0587       0.0741        0.053       0.0701       0.0615       0.0654        0.089       0.0772        0.143      0.00149\n","    103    19      0.00954      0.00952     1.14e-05       0.0573       0.0755       0.0442       0.0835       0.0639        0.056        0.104         0.08         0.24       0.0025\n","    103    20       0.0073      0.00729     1.11e-05       0.0501        0.066       0.0389       0.0725       0.0557       0.0482       0.0918         0.07        0.245      0.00256\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    103     1      0.00676      0.00675     2.32e-06       0.0474       0.0636       0.0363       0.0696        0.053       0.0466       0.0882       0.0674       0.0896     0.000933\n","    103     2      0.00676      0.00675     9.36e-07       0.0471       0.0636       0.0356       0.0702       0.0529       0.0459        0.089       0.0674       0.0471      0.00049\n","    103     3      0.00646      0.00646     1.36e-06       0.0459       0.0622       0.0345       0.0687       0.0516       0.0441       0.0877       0.0659        0.071      0.00074\n","    103     4       0.0066       0.0066     1.56e-06       0.0462       0.0628       0.0349        0.069       0.0519       0.0444       0.0889       0.0666       0.0803     0.000836\n","    103     5      0.00611      0.00611     8.08e-07       0.0452       0.0605       0.0348       0.0659       0.0504       0.0448       0.0834       0.0641       0.0446     0.000465\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train             103  396.041    0.005      0.00724     1.37e-05      0.00726       0.0501       0.0658       0.0402       0.0698        0.055       0.0509       0.0885       0.0697        0.226      0.00235\n","! Validation        103  396.041    0.005      0.00653      1.4e-06      0.00654       0.0464       0.0625       0.0352       0.0687        0.052       0.0452       0.0875       0.0663       0.0665     0.000693\n","Wall time: 396.0414959760001\n","! Best model      103    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    104     1      0.00525      0.00525     2.99e-06       0.0423        0.056       0.0315        0.064       0.0478       0.0398       0.0791       0.0594        0.118      0.00123\n","    104     2      0.00726      0.00724     2.03e-05       0.0485       0.0658       0.0357       0.0742       0.0549       0.0459       0.0938       0.0698         0.33      0.00344\n","    104     3      0.00773      0.00772     1.03e-05       0.0537        0.068       0.0475       0.0662       0.0568       0.0593       0.0826        0.071        0.223      0.00232\n","    104     4      0.00919      0.00919     2.17e-06       0.0574       0.0742       0.0497       0.0727       0.0612       0.0632       0.0923       0.0777       0.0752     0.000783\n","    104     5      0.00781      0.00779     1.65e-05       0.0516       0.0683       0.0417       0.0714       0.0566       0.0528       0.0918       0.0723        0.287      0.00299\n","    104     6      0.00558      0.00556     1.88e-05       0.0429       0.0577       0.0323        0.064       0.0482       0.0408       0.0816       0.0612        0.317       0.0033\n","    104     7      0.00563      0.00562     7.68e-06       0.0433        0.058       0.0336       0.0626       0.0481       0.0423       0.0807       0.0615        0.204      0.00212\n","    104     8      0.00678      0.00676      1.9e-05       0.0479       0.0636       0.0384        0.067       0.0527       0.0481       0.0867       0.0674        0.322      0.00335\n","    104     9      0.00732      0.00732      2.7e-07       0.0507       0.0662       0.0419       0.0681        0.055       0.0529       0.0868       0.0699       0.0354     0.000368\n","    104    10      0.00636      0.00636     4.36e-06       0.0475       0.0617       0.0377       0.0669       0.0523       0.0469       0.0838       0.0653        0.143      0.00149\n","    104    11      0.00675      0.00674     6.09e-06       0.0486       0.0635        0.038       0.0697       0.0539       0.0488       0.0857       0.0673        0.161      0.00167\n","    104    12       0.0073       0.0073     3.51e-06       0.0513       0.0661       0.0442       0.0655       0.0549       0.0555       0.0833       0.0694        0.136      0.00142\n","    104    13      0.00742      0.00742     1.78e-06       0.0501       0.0666       0.0392       0.0718       0.0555       0.0499       0.0913       0.0706        0.076     0.000791\n","    104    14      0.00666      0.00663     3.43e-05       0.0469        0.063       0.0352       0.0701       0.0527       0.0447       0.0889       0.0668        0.431      0.00449\n","    104    15      0.00603      0.00603     3.69e-06       0.0441       0.0601       0.0347        0.063       0.0489       0.0451       0.0822       0.0636         0.11      0.00114\n","    104    16      0.00607      0.00605      2.2e-05       0.0455       0.0602       0.0362       0.0641       0.0501        0.046       0.0815       0.0637        0.336       0.0035\n","    104    17      0.00772      0.00772     1.12e-06       0.0526        0.068       0.0435       0.0708       0.0571       0.0547       0.0887       0.0717       0.0633     0.000659\n","    104    18      0.00724      0.00724     2.38e-07       0.0513       0.0658       0.0429        0.068       0.0555       0.0534       0.0854       0.0694       0.0287     0.000299\n","    104    19      0.00716      0.00716     1.28e-06        0.048       0.0654        0.035       0.0742       0.0546       0.0444       0.0943       0.0694       0.0666     0.000694\n","    104    20      0.00778      0.00777     8.65e-07       0.0528       0.0682       0.0459       0.0667       0.0563       0.0574       0.0858       0.0716       0.0633     0.000659\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    104     1      0.00674      0.00673     2.36e-06       0.0473       0.0635       0.0362       0.0696       0.0529       0.0465       0.0881       0.0673       0.0908     0.000946\n","    104     2      0.00674      0.00674     9.54e-07        0.047       0.0635       0.0355       0.0702       0.0528       0.0458       0.0889       0.0673        0.046     0.000479\n","    104     3      0.00644      0.00644     1.34e-06       0.0459       0.0621       0.0345       0.0687       0.0516       0.0441       0.0877       0.0659        0.071      0.00074\n","    104     4      0.00658      0.00658     1.61e-06       0.0462       0.0628       0.0348       0.0689       0.0518       0.0443       0.0888       0.0666       0.0819     0.000853\n","    104     5      0.00609      0.00609     8.38e-07       0.0451       0.0604       0.0347       0.0659       0.0503       0.0446       0.0834        0.064       0.0458     0.000477\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train             104  399.876    0.005      0.00694     8.86e-06      0.00695       0.0488       0.0645       0.0392       0.0681       0.0536         0.05       0.0864       0.0682        0.176      0.00184\n","! Validation        104  399.876    0.005      0.00652     1.42e-06      0.00652       0.0463       0.0625       0.0351       0.0686       0.0519       0.0451       0.0874       0.0662       0.0671     0.000699\n","Wall time: 399.87655146399993\n","! Best model      104    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    105     1      0.00718      0.00718     1.25e-06         0.05       0.0655       0.0417       0.0667       0.0542       0.0533       0.0849       0.0691        0.077     0.000802\n","    105     2      0.00697      0.00696     1.28e-05       0.0475       0.0645       0.0344       0.0738       0.0541       0.0445       0.0924       0.0684        0.265      0.00276\n","    105     3      0.00696      0.00696      3.8e-07       0.0485       0.0646       0.0386       0.0683       0.0534       0.0499       0.0868       0.0683        0.033     0.000344\n","    105     4       0.0063      0.00627     2.86e-05        0.047       0.0613       0.0379       0.0652       0.0516        0.047       0.0827       0.0648        0.394      0.00411\n","    105     5      0.00617      0.00617     1.52e-06       0.0458       0.0608       0.0359       0.0654       0.0507       0.0464       0.0823       0.0644       0.0777      0.00081\n","    105     6      0.00607      0.00606     1.51e-05       0.0449       0.0602       0.0344       0.0657       0.0501       0.0431       0.0846       0.0639        0.283      0.00295\n","    105     7      0.00634      0.00634     7.07e-07       0.0461       0.0616        0.035       0.0685       0.0517       0.0444       0.0863       0.0653       0.0516     0.000537\n","    105     8        0.007        0.007     4.14e-06       0.0496       0.0647       0.0413       0.0661       0.0537        0.052       0.0845       0.0683        0.131      0.00137\n","    105     9      0.00608      0.00607     2.71e-06       0.0443       0.0603        0.032       0.0689       0.0505        0.041       0.0869       0.0639        0.118      0.00123\n","    105    10      0.00681       0.0068     7.88e-06       0.0502       0.0638       0.0421       0.0663       0.0542       0.0526       0.0817       0.0672        0.202      0.00211\n","    105    11      0.00832       0.0083     1.94e-05       0.0547       0.0705       0.0458       0.0726       0.0592       0.0577       0.0908       0.0742        0.317       0.0033\n","    105    12      0.00849      0.00847     2.45e-05       0.0545       0.0712       0.0435       0.0765         0.06       0.0537       0.0971       0.0754        0.364      0.00379\n","    105    13      0.00645      0.00639     5.17e-05       0.0458       0.0619       0.0357       0.0658       0.0508       0.0457       0.0855       0.0656        0.528       0.0055\n","    105    14      0.00654      0.00648     6.08e-05       0.0461       0.0623       0.0343       0.0697        0.052       0.0434       0.0887       0.0661        0.574      0.00598\n","    105    15      0.00771      0.00765     5.08e-05        0.051       0.0677       0.0419       0.0692       0.0556       0.0539       0.0891       0.0715        0.524      0.00546\n","    105    16      0.00828      0.00828      3.5e-07       0.0553       0.0704       0.0499        0.066       0.0579       0.0619       0.0848       0.0734       0.0311     0.000323\n","    105    17      0.00778      0.00778     2.87e-06       0.0512       0.0682       0.0398       0.0741       0.0569       0.0503       0.0944       0.0723        0.118      0.00123\n","    105    18      0.00668      0.00667     1.02e-06       0.0475       0.0632       0.0375       0.0677       0.0526       0.0468       0.0872        0.067       0.0666     0.000694\n","    105    19      0.00578      0.00575     2.68e-05       0.0437       0.0587        0.034       0.0631       0.0486       0.0434        0.081       0.0622        0.378      0.00393\n","    105    20      0.00799      0.00796     2.71e-05       0.0522        0.069       0.0401       0.0764       0.0582       0.0508       0.0957       0.0732        0.373      0.00388\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    105     1      0.00672      0.00672      2.3e-06       0.0473       0.0634       0.0362       0.0695       0.0528       0.0464        0.088       0.0672       0.0889     0.000926\n","    105     2      0.00672      0.00672     1.02e-06        0.047       0.0634       0.0354       0.0701       0.0528       0.0457       0.0889       0.0673       0.0488     0.000509\n","    105     3      0.00643      0.00643     1.36e-06       0.0458        0.062       0.0344       0.0687       0.0516        0.044       0.0876       0.0658       0.0706     0.000735\n","    105     4      0.00657      0.00657     1.58e-06       0.0461       0.0627       0.0347       0.0689       0.0518       0.0442       0.0888       0.0665       0.0804     0.000837\n","    105     5      0.00607      0.00607     8.45e-07        0.045       0.0603       0.0346       0.0658       0.0502       0.0445       0.0833       0.0639        0.045     0.000469\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train             105  403.730    0.005      0.00698      1.7e-05      0.00699       0.0488       0.0646       0.0388       0.0688       0.0538       0.0494       0.0875       0.0684        0.245      0.00255\n","! Validation        105  403.730    0.005       0.0065     1.42e-06       0.0065       0.0462       0.0624       0.0351       0.0686       0.0518        0.045       0.0873       0.0662       0.0667     0.000695\n","Wall time: 403.73115984000015\n","! Best model      105    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    106     1      0.00682      0.00669     0.000135       0.0479       0.0633       0.0381       0.0677       0.0529       0.0482       0.0859        0.067        0.858      0.00894\n","    106     2       0.0073      0.00725     5.01e-05       0.0513       0.0659       0.0442       0.0656       0.0549        0.055       0.0835       0.0692        0.524      0.00546\n","    106     3      0.00627      0.00624     2.66e-05       0.0462       0.0611       0.0353       0.0679       0.0516       0.0457       0.0839       0.0648         0.37      0.00386\n","    106     4       0.0065      0.00635     0.000153       0.0462       0.0616       0.0373       0.0642       0.0507       0.0472       0.0834       0.0653        0.918      0.00956\n","    106     5      0.00607      0.00607     4.88e-07       0.0445       0.0603       0.0337       0.0661       0.0499       0.0433       0.0845       0.0639       0.0434     0.000452\n","    106     6      0.00717      0.00704     0.000123       0.0482       0.0649       0.0364       0.0717        0.054       0.0473       0.0904       0.0689        0.821      0.00856\n","    106     7      0.00715      0.00715     3.12e-06       0.0495       0.0654       0.0398       0.0688       0.0543       0.0501       0.0884       0.0692         0.11      0.00114\n","    106     8      0.00639      0.00638     1.24e-05       0.0473       0.0618       0.0384       0.0651       0.0517       0.0482       0.0825       0.0654        0.252      0.00262\n","    106     9      0.00552      0.00552     5.97e-07        0.043       0.0575       0.0325       0.0639       0.0482       0.0405       0.0814       0.0609        0.052     0.000541\n","    106    10      0.00648      0.00647     5.73e-06       0.0472       0.0623       0.0384       0.0647       0.0516       0.0484       0.0833       0.0659        0.156      0.00163\n","    106    11      0.00502      0.00502     3.52e-07       0.0409       0.0548       0.0309        0.061        0.046       0.0394       0.0768       0.0581       0.0344     0.000358\n","    106    12      0.00711      0.00708     3.19e-05       0.0499       0.0651       0.0402       0.0691       0.0547       0.0509       0.0868       0.0688        0.418      0.00436\n","    106    13      0.00655      0.00655     2.39e-06       0.0474       0.0626       0.0396        0.063       0.0513       0.0499       0.0823       0.0661        0.114      0.00119\n","    106    14      0.00771      0.00764      7.4e-05       0.0511       0.0676       0.0399       0.0735       0.0567       0.0501       0.0933       0.0717        0.637      0.00664\n","    106    15      0.00643      0.00642     3.92e-06       0.0461        0.062       0.0342       0.0699       0.0521       0.0433       0.0883       0.0658        0.138      0.00143\n","    106    16      0.00655      0.00653      2.7e-05       0.0469       0.0625       0.0368       0.0673        0.052        0.047       0.0855       0.0662        0.382      0.00398\n","    106    17      0.00648      0.00648      7.3e-06       0.0452       0.0623       0.0327       0.0702       0.0514       0.0418       0.0902        0.066        0.198      0.00207\n","    106    18      0.00638      0.00637     9.34e-06       0.0459       0.0618       0.0356       0.0665        0.051       0.0463       0.0847       0.0655        0.197      0.00205\n","    106    19      0.00584      0.00583     9.59e-07       0.0448       0.0591        0.035       0.0642       0.0496       0.0443        0.081       0.0626        0.067     0.000698\n","    106    20      0.00664      0.00664     7.53e-07       0.0474       0.0631        0.036         0.07        0.053       0.0456       0.0882       0.0669       0.0512     0.000533\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    106     1       0.0067       0.0067     2.27e-06       0.0472       0.0633       0.0361       0.0694       0.0528       0.0464       0.0879       0.0671       0.0895     0.000932\n","    106     2       0.0067       0.0067     9.73e-07       0.0469       0.0633       0.0354       0.0701       0.0527       0.0456       0.0888       0.0672       0.0469     0.000488\n","    106     3      0.00642      0.00642     1.37e-06       0.0458        0.062       0.0344       0.0687       0.0515       0.0439       0.0876       0.0657       0.0721     0.000751\n","    106     4      0.00655      0.00655     1.58e-06       0.0461       0.0626       0.0347       0.0688       0.0518       0.0441       0.0887       0.0664       0.0805     0.000838\n","    106     5      0.00606      0.00606     7.73e-07       0.0449       0.0602       0.0345       0.0657       0.0501       0.0444       0.0832       0.0638       0.0436     0.000454\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train             106  407.592    0.005      0.00649     3.34e-05      0.00652       0.0468       0.0623       0.0368        0.067       0.0519       0.0468       0.0853        0.066        0.317       0.0033\n","! Validation        106  407.592    0.005      0.00649     1.39e-06      0.00649       0.0462       0.0623        0.035       0.0686       0.0518       0.0449       0.0873       0.0661       0.0665     0.000693\n","Wall time: 407.59293088100003\n","! Best model      106    0.006\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    107     1      0.00574      0.00574     4.83e-06       0.0441       0.0586       0.0346       0.0633       0.0489       0.0438       0.0804       0.0621        0.152      0.00158\n","    107     2       0.0058       0.0058     3.92e-06        0.045       0.0589       0.0358       0.0634       0.0496       0.0452       0.0795       0.0624        0.132      0.00137\n","    107     3      0.00648      0.00646     1.41e-05       0.0458       0.0622       0.0354       0.0668       0.0511       0.0461       0.0858       0.0659        0.271      0.00283\n","    107     4      0.00623      0.00622     3.32e-06       0.0451        0.061       0.0346       0.0662       0.0504       0.0443       0.0852       0.0647        0.119      0.00123\n","    107     5      0.00598      0.00597     9.94e-06       0.0433       0.0598       0.0323       0.0652       0.0488       0.0417        0.085       0.0634        0.226      0.00235\n","    107     6      0.00616      0.00615      1.7e-05       0.0453       0.0607       0.0347       0.0666       0.0507       0.0442       0.0844       0.0643        0.299      0.00311\n","    107     7      0.00618      0.00617     3.13e-06        0.046       0.0608       0.0351       0.0677       0.0514       0.0441       0.0848       0.0645        0.125       0.0013\n","    107     8      0.00695      0.00689      5.6e-05       0.0483       0.0642       0.0378       0.0692       0.0535       0.0486       0.0875        0.068        0.553      0.00576\n","    107     9      0.00591      0.00591     3.92e-06       0.0445       0.0595       0.0333        0.067       0.0501       0.0425       0.0836       0.0631        0.115      0.00119\n","    107    10      0.00609      0.00606     2.85e-05       0.0451       0.0602       0.0335       0.0683       0.0509       0.0426       0.0851       0.0639        0.389      0.00405\n","    107    11      0.00608      0.00605     3.22e-05       0.0455       0.0602       0.0353       0.0659       0.0506       0.0454        0.082       0.0637        0.418      0.00436\n","    107    12      0.00637      0.00636     2.44e-06       0.0459       0.0617       0.0347       0.0683       0.0515       0.0444       0.0865       0.0655        0.104      0.00109\n","    107    13      0.00611      0.00607     4.24e-05       0.0447       0.0603       0.0336       0.0669       0.0503       0.0423       0.0856       0.0639        0.477      0.00497\n","    107    14      0.00578      0.00574     4.16e-05       0.0442       0.0586       0.0334       0.0657       0.0495       0.0428       0.0815       0.0622        0.477      0.00497\n","    107    15      0.00585      0.00583     1.36e-05       0.0446       0.0591       0.0355       0.0627       0.0491       0.0449       0.0803       0.0626        0.269       0.0028\n","    107    16      0.00618      0.00617     5.66e-06       0.0451       0.0608       0.0342        0.067       0.0506       0.0443       0.0846       0.0644        0.168      0.00175\n","    107    17      0.00589      0.00587      1.6e-05       0.0442       0.0593       0.0345       0.0635        0.049       0.0436       0.0821       0.0628        0.296      0.00308\n","    107    18      0.00591      0.00591     2.94e-06       0.0438       0.0595       0.0332        0.065       0.0491       0.0426       0.0836       0.0631        0.104      0.00109\n","    107    19      0.00694      0.00691     2.74e-05       0.0485       0.0643       0.0385       0.0686       0.0535        0.049       0.0872       0.0681        0.387      0.00403\n","    107    20      0.00711      0.00711     6.12e-07       0.0506       0.0652       0.0411       0.0695       0.0553       0.0515       0.0864       0.0689       0.0443     0.000462\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    107     1      0.00668      0.00668     2.24e-06       0.0471       0.0632        0.036       0.0694       0.0527       0.0462       0.0878        0.067       0.0874      0.00091\n","    107     2      0.00668      0.00668     9.66e-07       0.0468       0.0632       0.0353         0.07       0.0526       0.0454       0.0887       0.0671       0.0467     0.000486\n","    107     3       0.0064       0.0064     1.33e-06       0.0457       0.0619       0.0343       0.0686       0.0514       0.0438       0.0875       0.0656       0.0703     0.000732\n","    107     4      0.00653      0.00653     1.57e-06        0.046       0.0625       0.0346       0.0687       0.0517        0.044       0.0886       0.0663       0.0794     0.000827\n","    107     5      0.00604      0.00604     8.26e-07       0.0449       0.0601       0.0344       0.0657       0.0501       0.0443       0.0832       0.0637       0.0447     0.000466\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train             107  411.432    0.005      0.00617     1.65e-05      0.00619       0.0455       0.0608       0.0351       0.0663       0.0507       0.0448       0.0841       0.0644        0.256      0.00267\n","! Validation        107  411.432    0.005      0.00647     1.39e-06      0.00647       0.0461       0.0622       0.0349       0.0685       0.0517       0.0448       0.0872        0.066       0.0657     0.000684\n","Wall time: 411.4331861259998\n","! Best model      107    0.006\n","! Stop training: Early stopping: validation_loss has not reduced for 50 epochs\n","Wall time: 411.4605003460001\n","Cumulative wall time: 411.4605003460001\n"]}],"source":["!rm -rf ./results\n","!nequip-train /content/nequip/configs/my-full-example.yaml"]},{"cell_type":"markdown","metadata":{"id":"kJitSZgLYNNF"},"source":["### Deploy the model"]},{"cell_type":"markdown","metadata":{"id":"Lo_kIpYV00as"},"source":["<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/deploy.png?raw=true\" width=\"60%\">"]},{"cell_type":"markdown","metadata":{"id":"7VoeGtlA02KQ"},"source":["We now convert the model to a potential file. This makes it independent of NequIP and we can use it any downstream application, such as LAMMPS. "]},{"cell_type":"code","execution_count":72,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":118,"status":"ok","timestamp":1657104930638,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"eLlagzVhrVGz","outputId":"751194c1-818d-40b1-ba5a-a96abd07ad54"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 5.3M\n","-rw------- 1 root root 227K Jul  6 10:48 config.yaml\n","-rw-r--r-- 1 root root  489 Jul  6 10:48 metrics_initialization.csv\n","-rw-r--r-- 1 root root 517K Jul  6 10:55 metrics_batch_train.csv\n","-rw-r--r-- 1 root root 131K Jul  6 10:55 metrics_batch_val.csv\n","-rw-r--r-- 1 root root  45K Jul  6 10:55 metrics_epoch.csv\n","-rw------- 1 root root 657K Jul  6 10:55 best_model.pth\n","-rw-r--r-- 1 root root 594K Jul  6 10:55 log\n","-rw------- 1 root root 2.5M Jul  6 10:55 trainer.pth\n","-rw------- 1 root root 657K Jul  6 10:55 last_model.pth\n"]}],"source":["! ls -lrth results/water/example-run-water"]},{"cell_type":"code","execution_count":73,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2822,"status":"ok","timestamp":1657104933457,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"PJmFAbBzez3P","outputId":"41b229c9-6cb6-40b9-d17e-c3963e136876"},"outputs":[{"name":"stdout","output_type":"stream","text":["usage: nequip-deploy [-h] [--verbose VERBOSE] {info,build} ...\n","\n","Create and view information about deployed NequIP potentials.\n","\n","optional arguments:\n","  -h, --help         show this help message and exit\n","  --verbose VERBOSE  log level\n","\n","commands:\n","  {info,build}\n","    info             Get information from a deployed model file\n","    build            Build a deployment model\n"]}],"source":["! nequip-deploy -h"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5460,"status":"ok","timestamp":1657104938914,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"Y3NJJgtDIDNc","outputId":"7fd5a777-c683-4238-8c4b-99ac6d7833a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:root:Loading best_model from training session...\n","INFO:root:Compiled & optimized model.\n"]}],"source":["!nequip-deploy build --train-dir /content/results/water/example-run-water water-deploy.pth"]},{"cell_type":"markdown","metadata":{"id":"UXpcE3oP0LyD"},"source":["## Evaluate Test Error on all remaining frames"]},{"cell_type":"markdown","metadata":{"id":"4wRKKCZ2PRl3"},"source":["Before running inference, we'd like to know how well the model is doing on a hold-out test set. We run the nequip-evaluate command to compute the test error on all data that we didn't use for training or validation. "]},{"cell_type":"code","execution_count":75,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78391,"status":"ok","timestamp":1657105017292,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"mB54WSrN0PaS","outputId":"b68379b3-f3d7-49c0-ba2a-fd7b4663c4bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","WARNING: please note that models running on CUDA are usually nondeterministc and that this manifests in the final test errors; for a _more_ deterministic result, please use `--device cpu`\n","Loading model... \n","loaded model from training session\n","Loading original dataset...\n","Loaded dataset specified in config.yaml.\n","Using origial training dataset (10000 frames) minus training (100 frames) and validation frames (50 frames), yielding a test set size of 9850 frames.\n","Starting...\n","  0% 0/9850 [00:00<?, ?it/s]\n","\u001b[A\n","  1% 50/9850 [00:00<01:45, 93.30it/s]\n","  1% 100/9850 [00:01<02:28, 65.86it/s]\n","  2% 150/9850 [00:02<03:02, 53.02it/s]\n","  2% 200/9850 [00:03<03:17, 48.91it/s]\n","  3% 250/9850 [00:04<02:28, 64.57it/s]\n","  3% 300/9850 [00:04<01:59, 79.93it/s]\n","  4% 350/9850 [00:04<01:41, 93.72it/s]\n","  4% 400/9850 [00:05<01:29, 106.00it/s]\n","  5% 450/9850 [00:05<01:20, 116.35it/s]\n","  5% 500/9850 [00:05<01:15, 124.28it/s]\n","  6% 550/9850 [00:06<01:11, 129.70it/s]\n","  6% 600/9850 [00:06<01:09, 133.95it/s]\n","  7% 650/9850 [00:06<01:07, 136.99it/s]\n","  7% 700/9850 [00:07<01:05, 139.44it/s]\n","  8% 750/9850 [00:07<01:04, 141.27it/s]\n","  8% 800/9850 [00:07<01:03, 142.71it/s]\n","  9% 850/9850 [00:08<01:02, 144.11it/s]\n","  9% 900/9850 [00:08<01:01, 145.52it/s]\n"," 10% 950/9850 [00:08<01:00, 146.47it/s]\n"," 10% 1000/9850 [00:09<01:00, 146.95it/s]\n"," 11% 1050/9850 [00:09<00:59, 147.16it/s]\n"," 11% 1100/9850 [00:09<00:59, 146.85it/s]\n"," 12% 1150/9850 [00:10<00:59, 147.12it/s]\n"," 12% 1200/9850 [00:10<00:58, 147.99it/s]\n"," 13% 1250/9850 [00:10<00:57, 148.42it/s]\n"," 13% 1300/9850 [00:11<00:57, 149.41it/s]\n"," 14% 1350/9850 [00:11<00:56, 149.69it/s]\n"," 14% 1400/9850 [00:11<00:56, 149.16it/s]\n"," 15% 1450/9850 [00:12<00:56, 148.63it/s]\n"," 15% 1500/9850 [00:12<00:56, 148.84it/s]\n"," 16% 1550/9850 [00:12<00:55, 148.94it/s]\n"," 16% 1600/9850 [00:13<00:55, 149.20it/s]\n"," 17% 1650/9850 [00:13<00:54, 149.34it/s]\n"," 17% 1700/9850 [00:13<00:54, 149.27it/s]\n"," 18% 1750/9850 [00:14<00:54, 148.70it/s]\n"," 18% 1800/9850 [00:14<00:54, 148.25it/s]\n"," 19% 1850/9850 [00:14<00:53, 148.33it/s]\n"," 19% 1900/9850 [00:15<00:53, 148.50it/s]\n"," 20% 1950/9850 [00:15<00:53, 148.66it/s]\n"," 20% 2000/9850 [00:15<00:52, 148.76it/s]\n"," 21% 2050/9850 [00:16<00:52, 148.15it/s]\n"," 21% 2100/9850 [00:16<00:52, 148.39it/s]\n"," 22% 2150/9850 [00:16<00:51, 148.18it/s]\n"," 22% 2200/9850 [00:17<00:51, 148.15it/s]\n"," 23% 2250/9850 [00:17<00:51, 148.59it/s]\n"," 23% 2300/9850 [00:17<00:50, 148.93it/s]\n"," 24% 2350/9850 [00:18<00:50, 148.69it/s]\n"," 24% 2400/9850 [00:18<00:49, 149.38it/s]\n"," 25% 2450/9850 [00:18<00:49, 149.08it/s]\n"," 25% 2500/9850 [00:19<00:49, 149.12it/s]\n"," 26% 2550/9850 [00:19<00:49, 148.58it/s]\n"," 26% 2600/9850 [00:19<00:49, 147.72it/s]\n"," 27% 2650/9850 [00:20<00:48, 147.74it/s]\n"," 27% 2700/9850 [00:20<00:48, 148.02it/s]\n"," 28% 2750/9850 [00:20<00:47, 148.03it/s]\n"," 28% 2800/9850 [00:21<00:47, 148.76it/s]\n"," 29% 2850/9850 [00:21<00:47, 148.52it/s]\n"," 29% 2900/9850 [00:21<00:46, 148.41it/s]\n"," 30% 2950/9850 [00:22<00:46, 148.57it/s]\n"," 30% 3000/9850 [00:22<00:45, 149.02it/s]\n"," 31% 3050/9850 [00:22<00:45, 149.14it/s]\n"," 31% 3100/9850 [00:23<00:45, 149.98it/s]\n"," 32% 3150/9850 [00:23<00:44, 150.49it/s]\n"," 32% 3200/9850 [00:23<00:44, 150.06it/s]\n"," 33% 3250/9850 [00:24<00:43, 150.44it/s]\n"," 34% 3300/9850 [00:24<00:43, 149.74it/s]\n"," 34% 3350/9850 [00:24<00:43, 149.65it/s]\n"," 35% 3400/9850 [00:25<00:43, 149.65it/s]\n"," 35% 3450/9850 [00:25<00:42, 149.46it/s]\n"," 36% 3500/9850 [00:25<00:42, 149.26it/s]\n"," 36% 3550/9850 [00:26<00:42, 149.20it/s]\n"," 37% 3600/9850 [00:26<00:42, 148.58it/s]\n"," 37% 3650/9850 [00:27<00:41, 148.08it/s]\n"," 38% 3700/9850 [00:27<00:41, 147.85it/s]\n"," 38% 3750/9850 [00:27<00:41, 147.23it/s]\n"," 39% 3800/9850 [00:28<00:41, 146.69it/s]\n"," 39% 3850/9850 [00:28<00:40, 147.03it/s]\n"," 40% 3900/9850 [00:28<00:40, 147.02it/s]\n"," 40% 3950/9850 [00:29<00:40, 146.90it/s]\n"," 41% 4000/9850 [00:29<00:39, 146.62it/s]\n"," 41% 4050/9850 [00:29<00:39, 146.38it/s]\n"," 42% 4100/9850 [00:30<00:39, 146.48it/s]\n"," 42% 4150/9850 [00:30<00:38, 146.38it/s]\n"," 43% 4200/9850 [00:30<00:38, 146.34it/s]\n"," 43% 4250/9850 [00:31<00:38, 146.15it/s]\n"," 44% 4300/9850 [00:31<00:38, 145.98it/s]\n"," 44% 4350/9850 [00:31<00:37, 145.83it/s]\n"," 45% 4400/9850 [00:32<00:37, 145.95it/s]\n"," 45% 4450/9850 [00:32<00:36, 146.18it/s]\n"," 46% 4500/9850 [00:32<00:36, 146.62it/s]\n"," 46% 4550/9850 [00:33<00:36, 146.34it/s]\n"," 47% 4600/9850 [00:33<00:35, 146.17it/s]\n"," 47% 4650/9850 [00:33<00:35, 145.84it/s]\n"," 48% 4700/9850 [00:34<00:35, 146.11it/s]\n"," 48% 4750/9850 [00:34<00:34, 146.47it/s]\n"," 49% 4800/9850 [00:34<00:34, 145.90it/s]\n"," 49% 4850/9850 [00:35<00:34, 145.78it/s]\n"," 50% 4900/9850 [00:35<00:33, 145.81it/s]\n"," 50% 4950/9850 [00:35<00:33, 145.53it/s]\n"," 51% 5000/9850 [00:36<00:33, 145.49it/s]\n"," 51% 5050/9850 [00:36<00:32, 145.46it/s]\n"," 52% 5100/9850 [00:36<00:32, 145.16it/s]\n"," 52% 5150/9850 [00:37<00:32, 145.14it/s]\n"," 53% 5200/9850 [00:37<00:32, 145.12it/s]\n"," 53% 5250/9850 [00:37<00:31, 145.38it/s]\n"," 54% 5300/9850 [00:38<00:31, 144.98it/s]\n"," 54% 5350/9850 [00:38<00:31, 144.92it/s]\n"," 55% 5400/9850 [00:39<00:30, 144.53it/s]\n"," 55% 5450/9850 [00:39<00:30, 144.12it/s]\n"," 56% 5500/9850 [00:39<00:30, 143.98it/s]\n"," 56% 5550/9850 [00:40<00:29, 143.64it/s]\n"," 57% 5600/9850 [00:40<00:29, 143.87it/s]\n"," 57% 5650/9850 [00:40<00:29, 144.13it/s]\n"," 58% 5700/9850 [00:41<00:28, 143.60it/s]\n"," 58% 5750/9850 [00:41<00:28, 143.42it/s]\n"," 59% 5800/9850 [00:41<00:28, 143.43it/s]\n"," 59% 5850/9850 [00:42<00:27, 143.58it/s]\n"," 60% 5900/9850 [00:42<00:27, 144.42it/s]\n"," 60% 5950/9850 [00:42<00:26, 144.45it/s]\n"," 61% 6000/9850 [00:43<00:26, 144.34it/s]\n"," 61% 6050/9850 [00:43<00:26, 144.55it/s]\n"," 62% 6100/9850 [00:43<00:25, 144.56it/s]\n"," 62% 6150/9850 [00:44<00:25, 144.56it/s]\n"," 63% 6200/9850 [00:44<00:25, 144.58it/s]\n"," 63% 6250/9850 [00:44<00:24, 144.22it/s]\n"," 64% 6300/9850 [00:45<00:24, 144.41it/s]\n"," 64% 6350/9850 [00:45<00:24, 144.36it/s]\n"," 65% 6400/9850 [00:45<00:23, 144.65it/s]\n"," 65% 6450/9850 [00:46<00:23, 144.19it/s]\n"," 66% 6500/9850 [00:46<00:23, 144.57it/s]\n"," 66% 6550/9850 [00:46<00:22, 144.47it/s]\n"," 67% 6600/9850 [00:47<00:22, 144.40it/s]\n"," 68% 6650/9850 [00:47<00:22, 144.36it/s]\n"," 68% 6700/9850 [00:48<00:21, 144.52it/s]\n"," 69% 6750/9850 [00:48<00:21, 144.89it/s]\n"," 69% 6800/9850 [00:48<00:21, 144.64it/s]\n"," 70% 6850/9850 [00:49<00:20, 144.83it/s]\n"," 70% 6900/9850 [00:49<00:20, 145.05it/s]\n"," 71% 6950/9850 [00:49<00:19, 145.29it/s]\n"," 71% 7000/9850 [00:50<00:19, 145.45it/s]\n"," 72% 7050/9850 [00:50<00:19, 145.29it/s]\n"," 72% 7100/9850 [00:50<00:18, 145.19it/s]\n"," 73% 7150/9850 [00:51<00:18, 144.69it/s]\n"," 73% 7200/9850 [00:51<00:18, 144.67it/s]\n"," 74% 7250/9850 [00:51<00:17, 144.73it/s]\n"," 74% 7300/9850 [00:52<00:17, 145.22it/s]\n"," 75% 7350/9850 [00:52<00:17, 145.08it/s]\n"," 75% 7400/9850 [00:52<00:16, 145.14it/s]\n"," 76% 7450/9850 [00:53<00:16, 145.37it/s]\n"," 76% 7500/9850 [00:53<00:16, 145.09it/s]\n"," 77% 7550/9850 [00:53<00:15, 145.05it/s]\n"," 77% 7600/9850 [00:54<00:15, 145.37it/s]\n"," 78% 7650/9850 [00:54<00:15, 145.23it/s]\n"," 78% 7700/9850 [00:54<00:14, 145.23it/s]\n"," 79% 7750/9850 [00:55<00:14, 145.19it/s]\n"," 79% 7800/9850 [00:55<00:14, 145.03it/s]\n"," 80% 7850/9850 [00:55<00:13, 145.22it/s]\n"," 80% 7900/9850 [00:56<00:13, 144.91it/s]\n"," 81% 7950/9850 [00:56<00:13, 144.96it/s]\n"," 81% 8000/9850 [00:56<00:12, 144.70it/s]\n"," 82% 8050/9850 [00:57<00:12, 144.73it/s]\n"," 82% 8100/9850 [00:57<00:12, 144.92it/s]\n"," 83% 8150/9850 [00:58<00:11, 145.07it/s]\n"," 83% 8200/9850 [00:58<00:11, 145.46it/s]\n"," 84% 8250/9850 [00:58<00:10, 145.83it/s]\n"," 84% 8300/9850 [00:59<00:10, 146.08it/s]\n"," 85% 8350/9850 [00:59<00:10, 146.32it/s]\n"," 85% 8400/9850 [00:59<00:09, 146.39it/s]\n"," 86% 8450/9850 [01:00<00:09, 145.96it/s]\n"," 86% 8500/9850 [01:00<00:09, 145.46it/s]\n"," 87% 8550/9850 [01:00<00:08, 145.62it/s]\n"," 87% 8600/9850 [01:01<00:08, 145.64it/s]\n"," 88% 8650/9850 [01:01<00:08, 146.16it/s]\n"," 88% 8700/9850 [01:01<00:07, 146.66it/s]\n"," 89% 8750/9850 [01:02<00:07, 146.71it/s]\n"," 89% 8800/9850 [01:02<00:07, 146.53it/s]\n"," 90% 8850/9850 [01:02<00:06, 145.97it/s]\n"," 90% 8900/9850 [01:03<00:06, 145.83it/s]\n"," 91% 8950/9850 [01:03<00:06, 145.53it/s]\n"," 91% 9000/9850 [01:03<00:05, 145.49it/s]\n"," 92% 9050/9850 [01:04<00:05, 145.33it/s]\n"," 92% 9100/9850 [01:04<00:05, 145.30it/s]\n"," 93% 9150/9850 [01:04<00:04, 145.16it/s]\n"," 93% 9200/9850 [01:05<00:04, 145.28it/s]\n"," 94% 9250/9850 [01:05<00:04, 145.42it/s]\n"," 94% 9300/9850 [01:05<00:03, 146.08it/s]\n"," 95% 9350/9850 [01:06<00:03, 146.41it/s]\n"," 95% 9400/9850 [01:06<00:03, 146.36it/s]\n"," 96% 9450/9850 [01:06<00:02, 145.88it/s]\n"," 96% 9500/9850 [01:07<00:02, 145.03it/s]\n"," 97% 9550/9850 [01:07<00:02, 144.85it/s]\n"," 97% 9600/9850 [01:07<00:01, 145.07it/s]\n"," 98% 9650/9850 [01:08<00:01, 144.91it/s]\n"," 98% 9700/9850 [01:08<00:01, 144.64it/s]\n"," 99% 9750/9850 [01:09<00:00, 144.24it/s]\n"," 99% 9800/9850 [01:09<00:00, 143.80it/s]\n","100% 9850/9850 [01:09<00:00, 141.29it/s]\n","\n","\n","--- Final result: ---\n","               f_mae =  0.046357           \n","              f_rmse =  0.062583           \n","             H_f_mae =  0.035020           \n","             O_f_mae =  0.069030           \n","         psavg_f_mae =  0.052025           \n","            H_f_rmse =  0.045198           \n","            O_f_rmse =  0.087545           \n","        psavg_f_rmse =  0.066372           \n","               e_mae =  0.059501           \n","             e/N_mae =  0.000620           \n","               f_mae =  0.046357           \n","              f_rmse =  0.062583           \n","             H_f_mae =  0.035020           \n","             O_f_mae =  0.069030           \n","         psavg_f_mae =  0.052025           \n","            H_f_rmse =  0.045198           \n","            O_f_rmse =  0.087545           \n","        psavg_f_rmse =  0.066372           \n","               e_mae =  0.059501           \n","             e/N_mae =  0.000620           \n"]}],"source":["!nequip-evaluate --train-dir results/water/example-run-water --batch-size 50"]},{"cell_type":"markdown","metadata":{"id":"HQHrMMnsPaJO"},"source":["Again, energy errors of < 1meV/atom (converted from kcal/mol to eV), and force errors of ~45 meV/A 🎉"]},{"cell_type":"markdown","metadata":{"id":"H4r5FBXaum9n"},"source":["# LAMMPS"]},{"cell_type":"markdown","metadata":{"id":"0qIYIYyr1B4O"},"source":["We are now in a position to run MD with our potential."]},{"cell_type":"markdown","metadata":{"id":"UirNBTlJ1BNZ"},"source":["<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/run.png?raw=true\" width=\"60%\">"]},{"cell_type":"markdown","metadata":{"id":"JQs0ijPhvAGb"},"source":["Set up a simple LAMMPS input file\n","\n","CAUTION: the reference data here are in eV for the energies and eV/A for the forces. The NequIP model will therefore also be predicting outputs in these units. We are therefore using `units metal` in LAMMPS (see [docs](https://docs.lammps.org/units.html)). Time units are also in`ps`)."]},{"cell_type":"code","execution_count":108,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":393,"status":"ok","timestamp":1657110413597,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"W090KfMsd2Do","outputId":"eb326e38-498b-4f69-a8c3-332ef230e10d"},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘lammps_run’: File exists\n"]}],"source":["lammps_input_md = \"\"\"\n","units           metal\n","boundary        p p p\n","atom_style      atomic\n","thermo 1\n","newton off\n","read_data structure.data\n","\n","neighbor        1.0 bin\n","neigh_modify    every 10 delay 0 check no\n","\n","pair_style\tnequip\n","pair_coeff\t* * ../water-deploy.pth H O\n","mass            1 1.00794\n","mass            2 15.9994\n","\n","velocity        all create 300.0 23456789\n","\n","fix             1 all nvt temp 300.0 300.0 $(100.0*dt)\n","thermo_style    custom step pe ke etotal temp press vol\n","#thermo          100\n","\n","#dump             1 all custom 100 water.dump id type x y z\n","dump              1 all xyz 10 water.xyz type xu yu zu\n","dump_modify       1 element H O\n","# dump            2 all custom 1 dump_frc.lamppstrj id type element fx fy fz\n","# dump_modify     2 element O H\n","# dump            3 all custom 1 dump.lammpstrj id type element x y z\n","# dump_modify     3 element O H\n","\n","run             10000\n","\"\"\"\n","\n","!mkdir lammps_run\n","with open(\"lammps_run/water_md.in\", \"w\") as f:\n","    f.write(lammps_input_md)"]},{"cell_type":"code","execution_count":95,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1657109743508,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"4tAHO8ODrpwG","outputId":"a2700ab1-6908-4034-fa00-2ea3c5d1df4b"},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘lammps_run’: File exists\n"]}],"source":["lammps_input_minimize = \"\"\"\n","units\treal\n","atom_style atomic\n","newton off\n","thermo 1\n","read_data structure.data\n","\n","pair_style\tnequip\n","pair_coeff\t* * ../water-deploy.pth H O\n","mass            1 1.00794\n","mass            2 15.9994\n","\n","neighbor 1.0 bin\n","neigh_modify delay 5 every 1\n","\n","minimize 0.0 1.0e-8 10000 1000000\n","write_dump all custom output.dump id type x y z fx fy fz\n","\"\"\"\n","!mkdir lammps_run\n","with open(\"lammps_run/water_minimize.in\", \"w\") as f:\n","    f.write(lammps_input_minimize)"]},{"cell_type":"code","execution_count":109,"metadata":{"executionInfo":{"elapsed":505,"status":"ok","timestamp":1657110434674,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"Mu4kbiXOt1pI"},"outputs":[],"source":["! cp /content/water-deploy.pth /content/lammps_run/."]},{"cell_type":"markdown","metadata":{"id":"AvWZCw1zvjRc"},"source":["Here's starting configuration for Toluene at CCSD(T) accuracy. We will strongly perturb the inital positions by sampling from a uniform distribution $\\mathcal{U}([0, 0.5])$"]},{"cell_type":"code","execution_count":111,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1657110437876,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"eXeHb2ZPvIbU"},"outputs":[],"source":["wat_pos_frc_trj = read('/content/nequip/data/AIMD_data/wat_pos_frc-10k.extxyz')\n","write(\"/content/lammps_run/structure.data\", wat_pos_frc_trj,format='lammps-data')\n"]},{"cell_type":"markdown","metadata":{"id":"QDuyueY11YBF"},"source":["### Run the LAMMPS command: "]},{"cell_type":"code","execution_count":112,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1657110439838,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"RG1LE98LukSO","outputId":"7268e399-99a3-4eba-dcee-02fcd20483de"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","units           metal\n","boundary        p p p\n","atom_style      atomic\n","thermo 1\n","newton off\n","read_data structure.data\n","\n","neighbor        1.0 bin\n","neigh_modify    every 10 delay 0 check no\n","\n","pair_style\tnequip\n","pair_coeff\t* * ../water-deploy.pth H O\n","mass            1 1.00794\n","mass            2 15.9994\n","\n","velocity        all create 330.0 23456789\n","\n","fix             1 all nvt temp 330.0 330.0 $(100.0*dt)\n","thermo_style    custom step pe ke etotal temp press vol\n","#thermo          100\n","\n","#dump             1 all custom 100 water.dump id type x y z\n","dump              1 all xyz 10 water.xyz type xu yu zu\n","dump_modify       1 element H O\n","# dump            2 all custom 1 dump_frc.lamppstrj id type element fx fy fz\n","# dump_modify     2 element O H\n","# dump            3 all custom 1 dump.lammpstrj id type element x y z\n","# dump_modify     3 element O H\n","\n","run             10000\n","/content/lammps_run/structure.data (written by ASE) \n","\n","96 \t atoms \n","2  atom types\n","0.0      9.8499999999999996  xlo xhi\n","0.0      9.8499999999999996  ylo yhi\n","0.0      9.8499999999999996  zlo zhi\n","\n","\n","Atoms \n","\n","     1   1      42.886169670000001   -0.055681660000000001      38.329161120000002\n","     2   1      34.202588720000001              -0.6185484      37.365568080000003\n","     3   1      30.080392589999999     -2.0124176500000002      36.480796079999998\n","     4   1      28.705791179999999     -2.6880392799999999      36.602098329999997\n","     5   1             36.24794267    -0.51634849000000005      34.492359610000001\n","     6   1      37.696472460000003   -0.041087279999999997      35.014073510000003\n","     7   1      27.760669979999999      7.4854206000000003      33.927691950000003\n","     8   1      28.816099959999999              6.49857774             34.21636084\n","     9   1      37.157637200000003      9.0188280800000005      31.926581209999998\n","    10   1      38.606381650000003      9.5820079600000003      32.343597279999997\n","    11   1      34.303195930000001      2.2195014400000002      45.988045190000001\n","    12   1      33.244413940000001              1.30253325      46.469842720000003\n","    13   1      38.728617479999997     -5.0541897699999998      26.074396839999999\n","    14   1      38.348392150000002     -6.2832846900000003      26.986725310000001\n","    15   1      32.864252090000001              3.20606327               30.897116\n","    16   1      31.290408809999999      3.0871834699999998             30.62739775\n","    17   1      33.751986969999997             -3.13832624      39.672760769999996\n","    18   1      34.664297990000001             -3.66438591      38.646602719999997\n","    19   1      42.717321439999999      5.1246883700000003      32.588340119999998\n","    20   1             41.56274552      5.5893544000000004      33.417490280000003\n","    21   1      32.428380089999997      9.1182520999999994             30.54776786\n","    22   1              32.6432407             10.77068308             30.48427787\n","    23   1      31.484867080000001      4.6777144699999997      37.395719499999998\n","    24   1      32.317188299999998     -6.2287496200000003      36.467186439999999\n","    25   1      26.662134099999999      3.1708123800000001      35.682014649999999\n","    26   1             26.52713675              1.60390403      35.488348299999998\n","    27   1      32.023823659999998      16.918208029999999      31.688356989999999\n","    28   1      31.400657970000001      7.0315610800000004      30.239455469999999\n","    29   1             33.52642531     -3.5594808100000002             34.26368308\n","    30   1      34.640485550000001             -3.26538336      35.497148240000001\n","    31   1      40.056437510000002    -0.30543864999999998      29.831207429999999\n","    32   1      39.478446419999997             -1.09483146      38.310114040000002\n","    33   1      39.704076149999999      1.9584631400000001      33.390237599999999\n","    34   1      38.333857080000001              2.69671781             42.92619457\n","    35   1      40.182045549999998     -7.2199289499999999      27.658039049999999\n","    36   1      39.320443179999998     -8.4564252700000004             28.13196589\n","    37   1             36.38769637      8.8117085900000003      38.354536240000002\n","    38   1      36.320563759999999      9.0063075799999996      36.752600139999998\n","    39   1      29.999158309999999     -5.5637817500000004      33.929505059999997\n","    40   1      30.772854550000002     -5.0385870199999996      35.199806719999998\n","    41   1      40.059251779999997      6.3305279499999996      28.257946189999998\n","    42   1      40.239836089999997      5.1745923999999999             29.29629568\n","    43   1             26.33209111      2.4393638599999998      33.565386850000003\n","    44   1      26.960697119999999      1.2711078899999999      32.592388440000001\n","    45   1      34.837269769999999    -0.47227084000000003      30.382436200000001\n","    46   1      35.396881370000003             -1.92684834      30.308183759999999\n","    47   1      32.121760719999997    -0.73334299000000003      36.510438299999997\n","    48   1      32.218084349999998      7.8454304099999996      35.667196779999998\n","    49   1             36.37809987     -4.3048878799999999              36.4539793\n","    50   1      35.811927560000001             -3.00139282      27.034893759999999\n","    51   1      29.645249140000001      1.0652123600000001      35.714365399999998\n","    52   1      30.379465499999998   -0.066814650000000003      34.988246869999998\n","    53   1             34.21493366             -1.65591205      33.887643709999999\n","    54   1      34.784243549999999     -1.0252141100000001      32.503483260000003\n","    55   1      40.464995450000004              1.14678254      31.307350320000001\n","    56   1      41.326246990000001     0.65508034999999998      32.455588290000001\n","    57   1             29.02108599      3.5038194900000001             39.90877029\n","    58   1      29.494542689999999      3.7276637300000002      41.376613800000001\n","    59   1      34.135966400000001     -6.7533422300000003      32.356841029999998\n","    60   1      34.954657009999998     -5.7704242399999996             31.45710669\n","    61   1      33.253235689999997      1.5268048299999999      44.056217189999998\n","    62   1      33.793166939999999     0.50146325999999997      43.059759010000001\n","    63   1      36.820540960000002      2.6214681999999998      40.683400659999997\n","    64   1      37.555270669999999              1.56498329             39.76489351\n","    65   2      43.209908720000001   -0.062845650000000003             47.25931559\n","    66   2      29.394058350000002     -2.3133019500000001      37.140788360000002\n","    67   2      36.741570840000001   -0.083871000000000001      35.259178339999998\n","    68   2             27.94247769      6.7622961500000001      34.564838440000003\n","    69   2      37.681265600000003      9.4216399800000001      32.647864349999999\n","    70   2      33.317129080000001      2.0951401700000001      45.872226509999997\n","    71   2      37.995135589999997      4.3611431200000004             26.55718199\n","    72   2      32.182467090000003      2.6611503399999998      30.457724819999999\n","    73   2      34.653801209999997     -3.4374573100000001      39.588924570000003\n","    74   2      42.292983370000002      5.9471069500000002      32.846099549999998\n","    75   2      32.960469009999997      9.9050313200000009      30.158730670000001\n","    76   2             31.42818866     -5.8338304000000001      36.673874359999999\n","    77   2             26.05637308      2.4973869199999998      35.348687040000002\n","    78   2      32.033492709999997      17.325228939999999      30.811601379999999\n","    79   2      33.825218239999998     -2.9520949600000002      35.022046070000002\n","    80   2      39.456998159999998    -0.30727594000000003      38.934782900000002\n","    81   2             29.48467089      2.8692561099999998      43.006186839999998\n","    82   2      39.286418410000003             -7.62061031      27.627114779999999\n","    83   2      35.879750280000003      8.6515870800000005      37.522173430000002\n","    84   2      30.358254389999999     -4.7607656800000004      34.335564570000003\n","    85   2      40.709895690000003      5.8331250199999998      28.755837589999999\n","    86   2      26.717908300000001      2.2415138300000002      32.657729799999998\n","    87   2      35.658925619999998    -0.99689035000000004      30.574953059999999\n","    88   2      31.585160200000001             -1.31218042      35.901110940000002\n","    89   2      35.548938669999998     -3.9056138900000001      26.821449000000001\n","    90   2      29.565661609999999     0.46817945999999999      34.967071199999999\n","    91   2             34.76151282    -0.95696802000000003      33.489136729999998\n","    92   2      40.485340669999999     0.40236209000000001             31.94254162\n","    93   2      29.672828979999998      4.0134825300000001               40.450578\n","    94   2      34.127228619999997     -5.8796882899999998      31.892542299999999\n","    95   2      33.116888420000002      1.2338084899999999      43.112771199999997\n","    96   2      37.199699350000003      2.5049007099999998      39.791712660000002\n"]}],"source":["! cat /content/lammps_run/water_md.in\n","! cat /content/lammps_run/structure.data"]},{"cell_type":"code","execution_count":113,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4217,"status":"ok","timestamp":1657110445171,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"gurLjNK5upvq","outputId":"d0b57593-5576-4468-c755-6a096fa7a5ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["LAMMPS (29 Sep 2021 - Update 2)\n","OMP_NUM_THREADS environment is not set. Defaulting to 1 thread. (src/comm.cpp:98)\n","  using 1 OpenMP thread(s) per MPI task\n","Reading data file ...\n","  orthogonal box = (0.0000000 0.0000000 0.0000000) to (9.8500000 9.8500000 9.8500000)\n","  1 by 1 by 1 MPI processor grid\n","  reading atoms ...\n","  96 atoms\n","  read_data CPU = 0.001 seconds\n","NEQUIP is using device cuda\n","NequIP Coeff: type 1 is element H\n","NequIP Coeff: type 2 is element O\n","Loading model from ../water-deploy.pth\n","Freezing TorchScript model...\n","ERROR: Illegal dump xyz command (src/dump_xyz.cpp:34)\n","Last command: dump              1 all xyz 10 water.xyz type xu yu zu\n","[045abf234b49:05650] *** Process received signal ***\n","[045abf234b49:05650] Signal: Segmentation fault (11)\n","[045abf234b49:05650] Signal code: Address not mapped (1)\n","[045abf234b49:05650] Failing at address: 0x7fa9e87ba20d\n","[045abf234b49:05650] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fa9eb25f980]\n","[045abf234b49:05650] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7fa9eae9e775]\n","[045abf234b49:05650] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7fa9eb709e44]\n","[045abf234b49:05650] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7fa9eae9f605]\n","[045abf234b49:05650] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7fa9eb707cb3]\n","[045abf234b49:05650] *** End of error message ***\n"]}],"source":["!cd /content/lammps_run/ && ../lammps/build/lmp -in water_md.in"]},{"cell_type":"code","execution_count":114,"metadata":{"executionInfo":{"elapsed":432,"status":"ok","timestamp":1657110686315,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"YGEqNBu2fmKQ"},"outputs":[],"source":["from ase.visualize import view"]},{"cell_type":"code","execution_count":115,"metadata":{"executionInfo":{"elapsed":799,"status":"ok","timestamp":1657110688903,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"QYxvyvyK2ctM"},"outputs":[],"source":["wat_traj = read(\"/content/lammps_run/water.xyz\",index=':')"]},{"cell_type":"code","execution_count":131,"metadata":{"executionInfo":{"elapsed":1036,"status":"ok","timestamp":1657111817025,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"1G_nFd7dgWhc"},"outputs":[],"source":["for i in range(len(wat_traj)):\n","  wat_traj[i].cell = cell_vec_abc\n","  wat_traj[i].pbc = np.array([True,True,True])"]},{"cell_type":"code","execution_count":116,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1657110690555,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"cZLtkZ7bdszJ"},"outputs":[],"source":["from google.colab import output\n","output.enable_custom_widget_manager()"]},{"cell_type":"code","execution_count":134,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":517,"referenced_widgets":["54e94a62b6c54594b67347a6f6eb55b2","65d157c9924541c2abc455f11541747d","52e102f75a5f471380a5a0d8d2c34f54","d113599d5cca4936af70db4ff0c3fca0","bf752ff3324b4fd592dae7a63d3d6599","7054019b7bcd4b39a1e51f51960d8149","125ff5f187034b47b8ed3d17d3864e16","3ac88f99d5fa473385dc757157db239f","f1f23bc1808b40ee925d68a4187a15f9","312b80dfddc8496d8470b147128ccf3e","68a923337da14eb1a36fe8f8aa248609","afe357d7757f4fffb21156b76b400043","df255ff80b7f430191a8460f1c480336","73d502faa8c349f496319324e50566e5","b89da8795cb9418c832ad13cd753297e","51268fc518cf41beb5d01dccf0b8e2ec","8bd49818a6584e779a8455d87b2b7b52","efef54626a934b2d8742253c29a45521","1ee832207fac4478b3a9f945d056fa32","45925e6d1dea48688630579594b20f2f","101119d7d3114659a999cc8e37a445ca","0a0d3710980c4f5cbd242f6870d0494f","4074d2aab25d4ffab7ce34423b7c6ea0","479767bf13b94c3aa692a00e76f0404a","f6c6fa02de244a8c9b0228b8f4b32947","acba6420aae643178711ab72d5985b10","e03365508d5c4c50a4d82e1e93bde143","3be2522c2e5f4cc4b6773c3af1a0281f","4cbfff0a3e644192a6d1d6f9f4d099d9"]},"executionInfo":{"elapsed":448,"status":"ok","timestamp":1657111838490,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"-P5s2IbvfocL","outputId":"e1da50e5-b793-431b-9803-a7d675446d36"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"54e94a62b6c54594b67347a6f6eb55b2","version_major":2,"version_minor":0},"text/plain":["HBox(children=(NGLWidget(max_frame=1000), VBox(children=(Dropdown(description='Show', options=('All', 'O', 'H'…"]},"metadata":{"application/vnd.jupyter.widget-view+json":{"colab":{"custom_widget_manager":{"url":"https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/a8874ba6619b6106/manager.min.js"}}}},"output_type":"display_data"}],"source":["view(wat_traj, viewer='ngl')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"my-short-nequip-tutorial.ipynb","provenance":[{"file_id":"1_r348f6oIyKxH4FnpKeD8g4QjwDhP8mT","timestamp":1656689687220}]},"gpuClass":"standard","interpreter":{"hash":"c9be9acec9edbd902b751bf46a8fbd7b71bbc5f0438c72d3ebaee4bffeb5e5e4"},"kernelspec":{"display_name":"Python 3.7.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0a0d3710980c4f5cbd242f6870d0494f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"101119d7d3114659a999cc8e37a445ca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"125ff5f187034b47b8ed3d17d3864e16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FloatSliderView","continuous_update":true,"description":"Ball size","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_73d502faa8c349f496319324e50566e5","max":1.5,"min":0,"orientation":"horizontal","readout":true,"readout_format":".2f","step":0.01,"style":"IPY_MODEL_b89da8795cb9418c832ad13cd753297e","value":1.5}},"1ee832207fac4478b3a9f945d056fa32":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"","disabled":false,"icon":"compress","layout":"IPY_MODEL_3be2522c2e5f4cc4b6773c3af1a0281f","style":"IPY_MODEL_4cbfff0a3e644192a6d1d6f9f4d099d9","tooltip":""}},"312b80dfddc8496d8470b147128ccf3e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ac88f99d5fa473385dc757157db239f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"IntSliderView","continuous_update":true,"description":"","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_51268fc518cf41beb5d01dccf0b8e2ec","max":1000,"min":0,"orientation":"horizontal","readout":true,"readout_format":"d","step":1,"style":"IPY_MODEL_8bd49818a6584e779a8455d87b2b7b52","value":0}},"3be2522c2e5f4cc4b6773c3af1a0281f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"34px"}},"4074d2aab25d4ffab7ce34423b7c6ea0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45925e6d1dea48688630579594b20f2f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f6c6fa02de244a8c9b0228b8f4b32947","IPY_MODEL_acba6420aae643178711ab72d5985b10"],"layout":"IPY_MODEL_e03365508d5c4c50a4d82e1e93bde143"}},"479767bf13b94c3aa692a00e76f0404a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"4cbfff0a3e644192a6d1d6f9f4d099d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"51268fc518cf41beb5d01dccf0b8e2ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52e102f75a5f471380a5a0d8d2c34f54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_bf752ff3324b4fd592dae7a63d3d6599","IPY_MODEL_7054019b7bcd4b39a1e51f51960d8149","IPY_MODEL_125ff5f187034b47b8ed3d17d3864e16","IPY_MODEL_3ac88f99d5fa473385dc757157db239f"],"layout":"IPY_MODEL_f1f23bc1808b40ee925d68a4187a15f9"}},"54e94a62b6c54594b67347a6f6eb55b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_65d157c9924541c2abc455f11541747d","IPY_MODEL_52e102f75a5f471380a5a0d8d2c34f54"],"layout":"IPY_MODEL_d113599d5cca4936af70db4ff0c3fca0"}},"65d157c9924541c2abc455f11541747d":{"model_module":"nglview-js-widgets","model_module_version":"3.0.1","model_name":"NGLModel","state":{"_camera_orientation":[2.780363612984834,-2.040170880195216,31.598951299951107,0,7.34212147696704,30.897577097788616,1.3488602600203148,0,-30.801773856298144,7.180799019831384,3.173845465636968,0,-0.12300000339746475,-4.013000011444092,-1.0509999990463257,1],"_camera_str":"orthographic","_dom_classes":[],"_gui_theme":null,"_ibtn_fullscreen":"IPY_MODEL_1ee832207fac4478b3a9f945d056fa32","_igui":null,"_iplayer":"IPY_MODEL_45925e6d1dea48688630579594b20f2f","_model_module":"nglview-js-widgets","_model_module_version":"3.0.1","_model_name":"NGLModel","_ngl_color_dict":{},"_ngl_coordinate_resource":{},"_ngl_full_stage_parameters":{"ambientColor":14540253,"ambientIntensity":0.2,"backgroundColor":"white","cameraEyeSep":0.3,"cameraFov":40,"cameraType":"orthographic","clipDist":0,"clipFar":100,"clipNear":0,"fogFar":100,"fogNear":50,"hoverTimeout":0,"impostor":true,"lightColor":14540253,"lightIntensity":1,"mousePreset":"default","panSpeed":1,"quality":"medium","rotateSpeed":2,"sampleLevel":0,"tooltip":true,"workerDefault":true,"zoomSpeed":1.2},"_ngl_msg_archive":[{"args":[{"binary":false,"data":"CRYST1    9.850    9.850    9.850  90.00  90.00  90.00 P 1\nMODEL     1\nATOM      1    H MOL     1       3.486   9.794   8.779  1.00  0.00           H  \nATOM      2    H MOL     1       4.653   9.231   7.816  1.00  0.00           H  \nATOM      3    H MOL     1       0.530   7.838   6.931  1.00  0.00           H  \nATOM      4    H MOL     1       9.006   7.162   7.052  1.00  0.00           H  \nATOM      5    H MOL     1       6.698   9.334   4.942  1.00  0.00           H  \nATOM      6    H MOL     1       8.146   9.809   5.464  1.00  0.00           H  \nATOM      7    H MOL     1       8.061   7.485   4.378  1.00  0.00           H  \nATOM      8    H MOL     1       9.116   6.499   4.666  1.00  0.00           H  \nATOM      9    H MOL     1       7.608   9.019   2.377  1.00  0.00           H  \nATOM     10    H MOL     1       9.056   9.582   2.794  1.00  0.00           H  \nATOM     11    H MOL     1       4.753   2.220   6.588  1.00  0.00           H  \nATOM     12    H MOL     1       3.694   1.303   7.070  1.00  0.00           H  \nATOM     13    H MOL     1       9.179   4.796   6.374  1.00  0.00           H  \nATOM     14    H MOL     1       8.798   3.567   7.287  1.00  0.00           H  \nATOM     15    H MOL     1       3.314   3.206   1.347  1.00  0.00           H  \nATOM     16    H MOL     1       1.740   3.087   1.077  1.00  0.00           H  \nATOM     17    H MOL     1       4.202   6.712   0.273  1.00  0.00           H  \nATOM     18    H MOL     1       5.114   6.186   9.097  1.00  0.00           H  \nATOM     19    H MOL     1       3.317   5.125   3.038  1.00  0.00           H  \nATOM     20    H MOL     1       2.163   5.589   3.867  1.00  0.00           H  \nATOM     21    H MOL     1       2.878   9.118   0.998  1.00  0.00           H  \nATOM     22    H MOL     1       3.093   0.921   0.934  1.00  0.00           H  \nATOM     23    H MOL     1       1.935   4.678   7.846  1.00  0.00           H  \nATOM     24    H MOL     1       2.767   3.621   6.917  1.00  0.00           H  \nATOM     25    H MOL     1       6.962   3.171   6.132  1.00  0.00           H  \nATOM     26    H MOL     1       6.827   1.604   5.938  1.00  0.00           H  \nATOM     27    H MOL     1       2.474   7.068   2.138  1.00  0.00           H  \nATOM     28    H MOL     1       1.851   7.032   0.689  1.00  0.00           H  \nATOM     29    H MOL     1       3.976   6.291   4.714  1.00  0.00           H  \nATOM     30    H MOL     1       5.090   6.585   5.947  1.00  0.00           H  \nATOM     31    H MOL     1       0.656   9.545   0.281  1.00  0.00           H  \nATOM     32    H MOL     1       0.078   8.755   8.760  1.00  0.00           H  \nATOM     33    H MOL     1       0.304   1.958   3.840  1.00  0.00           H  \nATOM     34    H MOL     1       8.784   2.697   3.526  1.00  0.00           H  \nATOM     35    H MOL     1       0.782   2.630   7.958  1.00  0.00           H  \nATOM     36    H MOL     1       9.770   1.394   8.432  1.00  0.00           H  \nATOM     37    H MOL     1       6.838   8.812   8.805  1.00  0.00           H  \nATOM     38    H MOL     1       6.771   9.006   7.203  1.00  0.00           H  \nATOM     39    H MOL     1       0.449   4.286   4.380  1.00  0.00           H  \nATOM     40    H MOL     1       1.223   4.811   5.650  1.00  0.00           H  \nATOM     41    H MOL     1       0.659   6.331   8.558  1.00  0.00           H  \nATOM     42    H MOL     1       0.840   5.175   9.596  1.00  0.00           H  \nATOM     43    H MOL     1       6.632   2.439   4.015  1.00  0.00           H  \nATOM     44    H MOL     1       7.261   1.271   3.042  1.00  0.00           H  \nATOM     45    H MOL     1       5.287   9.378   0.832  1.00  0.00           H  \nATOM     46    H MOL     1       5.847   7.923   0.758  1.00  0.00           H  \nATOM     47    H MOL     1       2.572   9.117   6.960  1.00  0.00           H  \nATOM     48    H MOL     1       2.668   7.845   6.117  1.00  0.00           H  \nATOM     49    H MOL     1       6.828   5.545   6.904  1.00  0.00           H  \nATOM     50    H MOL     1       6.262   6.849   7.335  1.00  0.00           H  \nATOM     51    H MOL     1       0.095   1.065   6.164  1.00  0.00           H  \nATOM     52    H MOL     1       0.829   9.783   5.438  1.00  0.00           H  \nATOM     53    H MOL     1       4.665   8.194   4.338  1.00  0.00           H  \nATOM     54    H MOL     1       5.234   8.825   2.953  1.00  0.00           H  \nATOM     55    H MOL     1       1.065   1.147   1.757  1.00  0.00           H  \nATOM     56    H MOL     1       1.926   0.655   2.906  1.00  0.00           H  \nATOM     57    H MOL     1       9.321   3.504   0.509  1.00  0.00           H  \nATOM     58    H MOL     1       9.795   3.728   1.977  1.00  0.00           H  \nATOM     59    H MOL     1       4.586   3.097   2.807  1.00  0.00           H  \nATOM     60    H MOL     1       5.405   4.080   1.907  1.00  0.00           H  \nATOM     61    H MOL     1       3.703   1.527   4.656  1.00  0.00           H  \nATOM     62    H MOL     1       4.243   0.501   3.660  1.00  0.00           H  \nATOM     63    H MOL     1       7.271   2.621   1.283  1.00  0.00           H  \nATOM     64    H MOL     1       8.005   1.565   0.365  1.00  0.00           H  \nATOM     65    O MOL     1       3.810   9.787   7.859  1.00  0.00           O  \nATOM     66    O MOL     1       9.694   7.537   7.591  1.00  0.00           O  \nATOM     67    O MOL     1       7.192   9.766   5.709  1.00  0.00           O  \nATOM     68    O MOL     1       8.242   6.762   5.015  1.00  0.00           O  \nATOM     69    O MOL     1       8.131   9.422   3.098  1.00  0.00           O  \nATOM     70    O MOL     1       3.767   2.095   6.472  1.00  0.00           O  \nATOM     71    O MOL     1       8.445   4.361   6.857  1.00  0.00           O  \nATOM     72    O MOL     1       2.632   2.661   0.908  1.00  0.00           O  \nATOM     73    O MOL     1       5.104   6.413   0.189  1.00  0.00           O  \nATOM     74    O MOL     1       2.893   5.947   3.296  1.00  0.00           O  \nATOM     75    O MOL     1       3.410   0.055   0.609  1.00  0.00           O  \nATOM     76    O MOL     1       1.878   4.016   7.124  1.00  0.00           O  \nATOM     77    O MOL     1       6.356   2.497   5.799  1.00  0.00           O  \nATOM     78    O MOL     1       2.483   7.475   1.262  1.00  0.00           O  \nATOM     79    O MOL     1       4.275   6.898   5.472  1.00  0.00           O  \nATOM     80    O MOL     1       0.057   9.543   9.385  1.00  0.00           O  \nATOM     81    O MOL     1       9.785   2.869   3.606  1.00  0.00           O  \nATOM     82    O MOL     1       9.736   2.229   7.927  1.00  0.00           O  \nATOM     83    O MOL     1       6.330   8.652   7.972  1.00  0.00           O  \nATOM     84    O MOL     1       0.808   5.089   4.786  1.00  0.00           O  \nATOM     85    O MOL     1       1.310   5.833   9.056  1.00  0.00           O  \nATOM     86    O MOL     1       7.018   2.242   3.108  1.00  0.00           O  \nATOM     87    O MOL     1       6.109   8.853   1.025  1.00  0.00           O  \nATOM     88    O MOL     1       2.035   8.538   6.351  1.00  0.00           O  \nATOM     89    O MOL     1       5.999   5.944   7.121  1.00  0.00           O  \nATOM     90    O MOL     1       0.016   0.468   5.417  1.00  0.00           O  \nATOM     91    O MOL     1       5.212   8.893   3.939  1.00  0.00           O  \nATOM     92    O MOL     1       1.085   0.402   2.393  1.00  0.00           O  \nATOM     93    O MOL     1       0.123   4.013   1.051  1.00  0.00           O  \nATOM     94    O MOL     1       4.577   3.970   2.343  1.00  0.00           O  \nATOM     95    O MOL     1       3.567   1.234   3.713  1.00  0.00           O  \nATOM     96    O MOL     1       7.650   2.505   0.392  1.00  0.00           O  \nENDMDL\n","type":"blob"}],"kwargs":{"defaultRepresentation":false,"ext":"pdb","name":"nglview.adaptor.ASETrajectory"},"methodName":"loadFile","reconstruc_color_scheme":false,"target":"Stage","type":"call_method"},{"args":["500px","500px"],"kwargs":{},"methodName":"setSize","reconstruc_color_scheme":false,"target":"Widget","type":"call_method"},{"args":["unitcell"],"component_index":0,"kwargs":{"sele":"all"},"methodName":"addRepresentation","reconstruc_color_scheme":false,"target":"compList","type":"call_method"},{"args":["spacefill"],"component_index":0,"kwargs":{"sele":"all"},"methodName":"addRepresentation","reconstruc_color_scheme":false,"target":"compList","type":"call_method"},{"args":[],"kwargs":{"cameraType":"orthographic"},"methodName":"setParameters","reconstruc_color_scheme":false,"target":"Stage","type":"call_method"},{"args":[{"clipDist":0}],"kwargs":{},"methodName":"setParameters","reconstruc_color_scheme":false,"target":"Widget","type":"call_method"},{"args":["spacefill",0],"kwargs":{"colorScale":"rainbow","colorScheme":"element","radiusScale":0.5,"radiusType":"covalent"},"methodName":"updateRepresentationsByName","reconstruc_color_scheme":false,"target":"Widget","type":"call_method"},{"args":["spacefill",0],"kwargs":{"colorScale":"rainbow","colorScheme":"element","radiusScale":0.5,"radiusType":"covalent"},"methodName":"updateRepresentationsByName","reconstruc_color_scheme":false,"target":"Widget","type":"call_method"},{"args":["spacefill",0],"kwargs":{"colorScale":"rainbow","colorScheme":"element","radiusScale":0.49999999999999994,"radiusType":"covalent"},"methodName":"updateRepresentationsByName","reconstruc_color_scheme":false,"target":"Widget","type":"call_method"},{"args":["spacefill",0],"kwargs":{"colorScale":"rainbow","colorScheme":"element","radiusScale":0.49999999999999994,"radiusType":"covalent"},"methodName":"updateRepresentationsByName","reconstruc_color_scheme":false,"target":"Widget","type":"call_method"},{"args":["spacefill",0],"kwargs":{"colorScale":"rainbow","colorScheme":"element","radiusScale":0.49999999999999994,"radiusType":"covalent"},"methodName":"updateRepresentationsByName","reconstruc_color_scheme":false,"target":"Widget","type":"call_method"},{"args":["spacefill",0],"kwargs":{"colorScale":"rainbow","colorScheme":"element","radiusScale":0.51,"radiusType":"covalent"},"methodName":"updateRepresentationsByName","reconstruc_color_scheme":false,"target":"Widget","type":"call_method"},{"args":["spacefill",0],"kwargs":{"colorScale":"rainbow","colorScheme":"element","radiusScale":0.51,"radiusType":"covalent"},"methodName":"updateRepresentationsByName","reconstruc_color_scheme":false,"target":"Widget","type":"call_method"},{"args":["spacefill",0],"kwargs":{"colorScale":"rainbow","colorScheme":"element","radiusScale":0.51,"radiusType":"covalent"},"methodName":"updateRepresentationsByName","reconstruc_color_scheme":false,"target":"Widget","type":"call_method"},{"args":["spacefill",0],"kwargs":{"colorScale":"rainbow","colorScheme":"element","radiusScale":1.5,"radiusType":"covalent"},"methodName":"updateRepresentationsByName","reconstruc_color_scheme":false,"target":"Widget","type":"call_method"},{"args":["spacefill",0],"kwargs":{"colorScale":"rainbow","colorScheme":"element","radiusScale":1.5,"radiusType":"covalent"},"methodName":"updateRepresentationsByName","reconstruc_color_scheme":false,"target":"Widget","type":"call_method"}],"_ngl_original_stage_parameters":{"ambientColor":14540253,"ambientIntensity":0.2,"backgroundColor":"white","cameraEyeSep":0.3,"cameraFov":40,"cameraType":"perspective","clipDist":10,"clipFar":100,"clipNear":0,"fogFar":100,"fogNear":50,"hoverTimeout":0,"impostor":true,"lightColor":14540253,"lightIntensity":1,"mousePreset":"default","panSpeed":1,"quality":"medium","rotateSpeed":2,"sampleLevel":0,"tooltip":true,"workerDefault":true,"zoomSpeed":1.2},"_ngl_repr_dict":{"0":{"0":{"params":{"clipCenter":{"x":0,"y":0,"z":0},"clipNear":0,"clipRadius":0,"colorMode":"hcl","colorReverse":false,"colorScale":"","colorScheme":"element","colorValue":"orange","defaultAssembly":"","depthWrite":true,"diffuse":16777215,"diffuseInterior":false,"disableImpostor":false,"disablePicking":false,"flatShaded":false,"interiorColor":2236962,"interiorDarkening":0,"lazy":false,"matrix":{"elements":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1]},"metalness":0,"opacity":1,"quality":"medium","radialSegments":10,"radiusData":{},"radiusScale":1,"radiusSize":0.04924999783811635,"radiusType":"vdw","roughness":0.4,"sele":"all","side":"double","sphereDetail":1,"useInteriorColor":true,"visible":true,"wireframe":false},"type":"unitcell"},"1":{"params":{"clipCenter":{"x":0,"y":0,"z":0},"clipNear":0,"clipRadius":0,"colorMode":"hcl","colorReverse":false,"colorScale":"","colorScheme":"element","colorValue":"orange","defaultAssembly":"","depthWrite":true,"diffuse":16777215,"diffuseInterior":false,"disableImpostor":false,"disablePicking":false,"flatShaded":false,"interiorColor":2236962,"interiorDarkening":0,"lazy":false,"matrix":{"elements":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1]},"metalness":0,"opacity":1,"quality":"medium","radialSegments":10,"radiusData":{},"radiusScale":1,"radiusSize":0.04924999783811635,"radiusType":"vdw","roughness":0.4,"sele":"all","side":"double","sphereDetail":1,"useInteriorColor":true,"visible":true,"wireframe":false},"type":"unitcell"},"2":{"params":{"assembly":"default","clipCenter":{"x":0,"y":0,"z":0},"clipNear":0,"clipRadius":0,"colorMode":"hcl","colorReverse":false,"colorScale":"rainbow","colorScheme":"element","colorValue":9474192,"defaultAssembly":"","depthWrite":true,"diffuse":16777215,"diffuseInterior":false,"disableImpostor":false,"disablePicking":false,"flatShaded":false,"interiorColor":2236962,"interiorDarkening":0,"lazy":false,"matrix":{"elements":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1]},"metalness":0,"opacity":1,"quality":"medium","radiusData":{},"radiusScale":1.5,"radiusSize":1,"radiusType":"covalent","roughness":0.4,"sele":"all","side":"double","sphereDetail":1,"useInteriorColor":true,"visible":true,"wireframe":false},"type":"spacefill"},"3":{"params":{"assembly":"default","clipCenter":{"x":0,"y":0,"z":0},"clipNear":0,"clipRadius":0,"colorMode":"hcl","colorReverse":false,"colorScale":"rainbow","colorScheme":"element","colorValue":9474192,"defaultAssembly":"","depthWrite":true,"diffuse":16777215,"diffuseInterior":false,"disableImpostor":false,"disablePicking":false,"flatShaded":false,"interiorColor":2236962,"interiorDarkening":0,"lazy":false,"matrix":{"elements":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1]},"metalness":0,"opacity":1,"quality":"medium","radiusData":{},"radiusScale":1.5,"radiusSize":1,"radiusType":"covalent","roughness":0.4,"sele":"all","side":"double","sphereDetail":1,"useInteriorColor":true,"visible":true,"wireframe":false},"type":"spacefill"}},"1":{}},"_ngl_serialize":false,"_ngl_version":"2.0.0-dev.36","_ngl_view_id":["61DFAA6F-50CD-4C57-9A6E-0D0EF73F6E75"],"_player_dict":{},"_scene_position":{},"_scene_rotation":{},"_synced_model_ids":[],"_synced_repr_model_ids":[],"_view_count":null,"_view_height":"","_view_module":"nglview-js-widgets","_view_module_version":"3.0.1","_view_name":"NGLView","_view_width":"","background":"white","frame":665,"gui_style":null,"layout":"IPY_MODEL_efef54626a934b2d8742253c29a45521","max_frame":1000,"n_components":2,"picked":{"atom1":{"altloc":"","atomname":"O","bfactor":0,"chainname":"A","covalent":0.66,"element":"O","hetero":0,"index":92,"modelIndex":0,"name":"[MOL]1:A.O","residueIndex":0,"resname":"MOL","resno":1,"serial":93,"vdw":1.52,"x":0.12300000339746475,"y":4.013000011444092,"z":1.0509999990463257},"component":0}}},"68a923337da14eb1a36fe8f8aa248609":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7054019b7bcd4b39a1e51f51960d8149":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DropdownModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DropdownModel","_options_labels":[" ","picking","random","uniform","atomindex","residueindex","chainindex","modelindex","sstruc","element","resname","bfactor","hydrophobicity","value","volume","occupancy"],"_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"DropdownView","description":"Color scheme","description_tooltip":null,"disabled":false,"index":9,"layout":"IPY_MODEL_afe357d7757f4fffb21156b76b400043","style":"IPY_MODEL_df255ff80b7f430191a8460f1c480336"}},"73d502faa8c349f496319324e50566e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bd49818a6584e779a8455d87b2b7b52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"acba6420aae643178711ab72d5985b10":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"IntSliderView","continuous_update":true,"description":"","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_4074d2aab25d4ffab7ce34423b7c6ea0","max":1000,"min":0,"orientation":"horizontal","readout":true,"readout_format":"d","step":1,"style":"IPY_MODEL_479767bf13b94c3aa692a00e76f0404a","value":665}},"afe357d7757f4fffb21156b76b400043":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b89da8795cb9418c832ad13cd753297e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"bf752ff3324b4fd592dae7a63d3d6599":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DropdownModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DropdownModel","_options_labels":["All","O","H"],"_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"DropdownView","description":"Show","description_tooltip":null,"disabled":false,"index":0,"layout":"IPY_MODEL_312b80dfddc8496d8470b147128ccf3e","style":"IPY_MODEL_68a923337da14eb1a36fe8f8aa248609"}},"d113599d5cca4936af70db4ff0c3fca0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df255ff80b7f430191a8460f1c480336":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e03365508d5c4c50a4d82e1e93bde143":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efef54626a934b2d8742253c29a45521":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1f23bc1808b40ee925d68a4187a15f9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6c6fa02de244a8c9b0228b8f4b32947":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"PlayModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PlayModel","_playing":false,"_repeat":false,"_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PlayView","description":"","description_tooltip":null,"disabled":false,"interval":100,"layout":"IPY_MODEL_101119d7d3114659a999cc8e37a445ca","max":1000,"min":0,"show_repeat":true,"step":1,"style":"IPY_MODEL_0a0d3710980c4f5cbd242f6870d0494f","value":665}}}}},"nbformat":4,"nbformat_minor":0}
