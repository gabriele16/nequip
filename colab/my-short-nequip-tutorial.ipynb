{"cells":[{"cell_type":"markdown","metadata":{"id":"CFpAi8g9XmUU"},"source":["# Molecular Dynamics with NequIP \n","\n","### Authors: Simon Batzner, Albert Musaelian, Lixin Sun, Anders Johansson, Boris Kozinsky"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22855,"status":"ok","timestamp":1657024454749,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"ZOLIFOJZaeZ5","outputId":"59f8eab1-d3d4-4396-9c78-8170725da8d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import warnings\n","import os\n","\n","USE_COLAB = True\n","if USE_COLAB == True:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    work_dir = '/content/drive/MyDrive/Colab Notebooks/nequip/'\n","    data_dir = '/content/'\n","else:\n","    work_dir = '/Users/gabrieletocci/Google Drive/My Drive/Colab Notebooks/nequip/'\n","    data_dir = '/Users/gabrieletocci/Documents/projects/MD_DFT/'\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PHCh2aC7WfKj","executionInfo":{"status":"ok","timestamp":1657024837794,"user_tz":-120,"elapsed":2253,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}},"outputId":"e8883d66-44f1-4767-efda-eb062dab704c"},"outputs":[{"output_type":"stream","name":"stdout","text":["._AIMD_data\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n","AIMD_data/\n","AIMD_data/._WATER-frc-10k-1.xyz\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n","AIMD_data/WATER-frc-10k-1.xyz\n","AIMD_data/._celldata.dat\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n","AIMD_data/celldata.dat\n","AIMD_data/._WATER-pos-10k-1.xyz\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n","AIMD_data/WATER-pos-10k-1.xyz\n"]}],"source":["# Uncomment if running on google colab\n","! tar -xzvf  /content/drive/MyDrive/Colab\\ Notebooks/MD_DFT/AIMD_data.tar.gz\n"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":21498,"status":"ok","timestamp":1657025027396,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"J51CA0Bod1Jv"},"outputs":[],"source":["%%capture\n","import numpy as np\n","import pandas as pd\n","# install wandb\n","!pip install wandb\n","# install nequip\n","#uncomment if running on google colab\n","!git clone --depth 1 \"https://github.com/gabriele16/nequip.git\"\n","!pip install nequip/\n","# fix colab imports\n","import site\n","site.main()\n","# set to allow anonymous WandB\n","import os\n","os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n","import numpy as np\n","import torch \n","from ase.io import read, write\n","np.random.seed(0)\n","torch.manual_seed(0)"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":352,"status":"ok","timestamp":1657025041296,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"jDJ9Re0arOb0"},"outputs":[],"source":["def MD_reader_xyz(f, data_dir, no_skip = 0):\n","  filename = os.path.join(data_dir, f)\n","  fo = open(filename, 'r')\n","  natoms_str = fo.read().rsplit(' i = ')[0]\n","  natoms = int(natoms_str.split('\\n')[0])\n","  fo.close()  \n","  fo = open(filename, 'r')\n","  samples = fo.read().split(natoms_str)[1:]\n","  steps = []\n","  xyz = []\n","  temperatures = []\n","  energies = []\n","  for sample in samples[::no_skip]:\n","     entries = sample.split('\\n')[:-1]\n","     energies.append(float(entries[0].split(\"=\")[-1]))\n","     temp = np.array([list(map(float, lv.split()[1:])) for lv in entries[1:]])\n","     xyz.append(temp[:,:])\n","  return natoms_str, np.array(xyz), np.array(energies)     "]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1657025042616,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"DziQT_zjMKS4"},"outputs":[],"source":["def MD_writer_xyz(positions,forces,cell_vec_abc,energies,\n","                  data_dir,f):\n","\n","  filename = os.path.join(data_dir, f)\n","  fo = open(filename, 'w')\n","\n","  for it, frame in enumerate(positions):\n","    natoms = len(frame)\n","    fo.write(\"{:5d}\\n\".format(natoms))\n","    fo.write('Lattice=\"{:.5f} 0.0 0.0 0.0 {:.5f} 0.0 0.0 0.0 {:.5f}\" \\\n","    Properties=\"species:S:1:pos:R:3:forces:R:3\" \\\n","    energy={:.10f} pbc=\"T T T\"\\n'.format(cell_vec_abc[0],cell_vec_abc[1],cell_vec_abc[2],energies[it])    \n","    )\n","    if it%100 == 0.0:\n","      print(it)\n","    force_frame = forces[it]\n","\n","    fo.write(\"\".join(\"{:8s} {:.8f} {:16.8f} {:16.8f}\\\n","     {:16.8f} {:16.8f} {:16.8f}\\n\".format(frame[iat].symbol,\n","                                          frame[iat].position[0],frame[iat].position[1],frame[iat].position[2],\n","                                          force_frame[iat].position[0],force_frame[iat].position[1],force_frame[iat].position[2])\n","                                          for iat in range(len(frame))))"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":354,"status":"ok","timestamp":1657025044775,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"Z2U5i3IZqLBI","outputId":"82274e4e-034d-4096-9872-193ee331cff2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([9.85, 9.85, 9.85])"]},"metadata":{},"execution_count":30}],"source":["def read_cell(f,data_dir):\n","  filename = os.path.join(data_dir,f)\n","  fo = open(filename,'r')\n","  cell_list_abc = fo.read().split('\\n')[:-1]\n","  cell_vec_abc = np.array([list(map(float, lv.split())) for lv in cell_list_abc]).squeeze()\n","  return(cell_vec_abc)\n","\n","cell_vec_abc = read_cell('celldata.dat',data_dir + '/AIMD_data')\n","cell_vec_abc"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":8750,"status":"ok","timestamp":1657025054888,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"X9fcRMYnu5-V"},"outputs":[],"source":["wat_traj = read(data_dir +'AIMD_data/WATER-pos-10k-1.xyz',index=':')\n","wat_frc = read(data_dir + 'AIMD_data/WATER-frc-10k-1.xyz', index=':')"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":2137,"status":"ok","timestamp":1657025063069,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"jT7B4ryYu82t"},"outputs":[],"source":["natoms, positions, energies = MD_reader_xyz('WATER-pos-10k-1.xyz', data_dir + '/AIMD_data/', no_skip=1)"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24317,"status":"ok","timestamp":1657025092082,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"xgmmJAU5l5F2","outputId":"b8ec4f39-7b04-4531-82a0-52d9204b974a"},"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","100\n","200\n","300\n","400\n","500\n","600\n","700\n","800\n","900\n","1000\n","1100\n","1200\n","1300\n","1400\n","1500\n","1600\n","1700\n","1800\n","1900\n","2000\n","2100\n","2200\n","2300\n","2400\n","2500\n","2600\n","2700\n","2800\n","2900\n","3000\n","3100\n","3200\n","3300\n","3400\n","3500\n","3600\n","3700\n","3800\n","3900\n","4000\n","4100\n","4200\n","4300\n","4400\n","4500\n","4600\n","4700\n","4800\n","4900\n","5000\n","5100\n","5200\n","5300\n","5400\n","5500\n","5600\n","5700\n","5800\n","5900\n","6000\n","6100\n","6200\n","6300\n","6400\n","6500\n","6600\n","6700\n","6800\n","6900\n","7000\n","7100\n","7200\n","7300\n","7400\n","7500\n","7600\n","7700\n","7800\n","7900\n","8000\n","8100\n","8200\n","8300\n","8400\n","8500\n","8600\n","8700\n","8800\n","8900\n","9000\n","9100\n","9200\n","9300\n","9400\n","9500\n","9600\n","9700\n","9800\n","9900\n"]}],"source":["MD_writer_xyz(wat_traj, wat_frc, cell_vec_abc, energies, data_dir + '/AIMD_data/', 'wat_pos_frc-10k.extxyz')\n"]},{"cell_type":"markdown","metadata":{"id":"ZPqnt-SAXyvL"},"source":["### Turn on GPU\n","\n","Make sure Runtime --> Change runtime type is set to GPU"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":193347,"status":"ok","timestamp":1657025285423,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"u0j3j9gVp0C7"},"outputs":[],"source":["%%capture\n","# compile lammps\n","!wget \"https://github.com/lammps/lammps/archive/stable.zip\"\n","!unzip -q stable.zip\n","!rm stable.zip\n","!mv lammps-stable lammps\n","!wget \"https://github.com/mir-group/pair_nequip/archive/main.zip\"\n","!unzip -q main.zip\n","!rm main.zip\n","!mv pair_nequip-main pair_nequip\n","!cd pair_nequip && ./patch_lammps.sh ../lammps\n","!pip install mkl mkl-include\n","!cd lammps && mkdir -p build && cd build && cmake ../cmake -DCMAKE_PREFIX_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` && make -j4"]},{"cell_type":"markdown","metadata":{"id":"-HDmxkn3z8_m"},"source":["## 3 Steps: \n","* Train: using a data set, train the neural network 🧠 \n","* Deploy: convert the Python-based model into a stand-alone potential file for fast execution ⚡\n","* Run: run Molecular Dynamics, Monte Carlo, Structural Minimization, ...  with it in LAMMPS 🏃"]},{"cell_type":"markdown","metadata":{"id":"6OD71eeDz7dA"},"source":["<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/all.png?raw=true\" width=\"60%\">"]},{"cell_type":"markdown","metadata":{"id":"62aEgq6QYFIn"},"source":["### Train a model"]},{"cell_type":"markdown","metadata":{"id":"ELdBzH_8z4_2"},"source":["<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/train.png?raw=true\" width=\"60%\">"]},{"cell_type":"markdown","metadata":{"id":"qX0QKkAauSZO"},"source":["This tutorial is set up to use `wandb` in anonymous mode; when you use NequIP yourself you will be presented with a login prompt."]},{"cell_type":"markdown","metadata":{"id":"1KuOIippfVfd"},"source":["Here, we will train a NequIP potential on the following system\n","\n","* Toluene\n","* sampled at T=500K from AIMD\n","* at CCSD(T) accuracy (gold standard of quantum chemistry)\n","* Using 100 training configurations\n","* The units of the reference data are in kcal/mol and A. If you're more familiar with eV, remember 1 kcal/mol is chemical accuracy and is approximately 43 meV"]},{"cell_type":"markdown","metadata":{"id":"2q_GyQfC0npt"},"source":["Start a training run: this will print output to our console, but it is usually more convenient to view the results in a web interface called Weights and Biases. Click the link next to the rocket emoji to watch the run in the WandB interface 🚀 \n","\n","In WandB, watch the followingkeys:\n","\n","* Plot 1: validation_all_f_mae, training_all_f_mae\n","* Plot 2: validation_e/N_mae, training_e/N_mae\n","\n","These are the validation/training error in all force components and the validation/training error in the potential energy, normalized by the number of atoms, respectively. "]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":360,"status":"ok","timestamp":1657025521602,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"fFCszShRk2RP","outputId":"4b7866a5-587e-44ab-9159-b3eff3de30db"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'root': 'results/water', 'run_name': 'example-run-water', 'seed': 123, 'dataset_seed': 456, 'append': True, 'default_dtype': 'float32', 'allow_tf32': False, 'r_max': 4.0, 'num_layers': 4, 'l_max': 1, 'parity': True, 'num_features': 32, 'nonlinearity_type': 'gate', 'resnet': False, 'nonlinearity_scalars': {'e': 'silu', 'o': 'tanh'}, 'nonlinearity_gates': {'e': 'silu', 'o': 'tanh'}, 'num_basis': 8, 'BesselBasis_trainable': True, 'PolynomialCutoff_p': 6, 'invariant_layers': 2, 'invariant_neurons': 64, 'avg_num_neighbors': 'auto', 'use_sc': True, 'dataset': 'ase', 'dataset_file_name': './AIMD_data/wat_pos_frc-10k.extxyz', 'ase_args': {'format': 'extxyz'}, 'include_keys': ['user_label'], 'key_mapping': {'user_label': 'label0'}, 'verbose': 'info', 'log_batch_freq': 1, 'log_epoch_freq': 1, 'save_checkpoint_freq': -1, 'save_ema_checkpoint_freq': -1, 'n_train': 100, 'n_val': 50, 'learning_rate': 0.005, 'batch_size': 5, 'validation_batch_size': 10, 'max_epochs': 100000, 'train_val_split': 'random', 'shuffle': True, 'metrics_key': 'validation_loss', 'use_ema': True, 'ema_decay': 0.99, 'ema_use_num_updates': True, 'report_init_validation': True, 'early_stopping_patiences': {'validation_loss': 50}, 'early_stopping_delta': {'validation_loss': 0.005}, 'early_stopping_cumulative_delta': False, 'early_stopping_lower_bounds': {'LR': 1e-05}, 'early_stopping_upper_bounds': {'cumulative_wall': 1e+100}, 'loss_coeffs': {'forces': 1, 'total_energy': [1, 'PerAtomMSELoss']}, 'metrics_components': [['forces', 'mae'], ['forces', 'rmse'], ['forces', 'mae', {'PerSpecies': True, 'report_per_component': False}], ['forces', 'rmse', {'PerSpecies': True, 'report_per_component': False}], ['total_energy', 'mae'], ['total_energy', 'mae', {'PerAtom': True}]], 'optimizer_name': 'Adam', 'optimizer_amsgrad': False, 'optimizer_betas': (0.9, 0.999), 'optimizer_eps': 1e-08, 'optimizer_weight_decay': 0, 'max_gradient_norm': None, 'lr_scheduler_name': 'ReduceLROnPlateau', 'lr_scheduler_patience': 100, 'lr_scheduler_factor': 0.5, 'per_species_rescale_scales_trainable': False, 'per_species_rescale_shifts_trainable': False, 'per_species_rescale_shifts': 'dataset_per_atom_total_energy_mean', 'per_species_rescale_scales': 'dataset_forces_rms', 'global_rescale_shift': None, 'global_rescale_scale': 'dataset_forces_rms', 'global_rescale_shift_trainable': False, 'global_rescale_scale_trainable': False}"]},"metadata":{},"execution_count":39}],"source":["from nequip.utils import Config\n","config = Config.from_file(work_dir + 'configs/my-full-example.yaml')\n","config"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1657025521989,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"twCcVwsM5KS3","outputId":"55eec5da-e031-4cf9-e254-fa4b34311158"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Colab Notebooks/nequip/'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":40}],"source":["work_dir"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1657025522531,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"},"user_tz":-120},"id":"t60v4vTK9Lrt","outputId":"23481c15-cb36-451c-e7d2-17b9e36c9cf0"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["! pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ukSnt_QD5avu","outputId":"c4b5b140-57be-4bd0-a12a-8c032ba18d2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["'/content/drive/MyDrive/Colab Notebooks/nequip/configs/my-full-example.yaml'\n","Torch device: cuda\n","Processing dataset...\n","Loaded data: Batch(atomic_numbers=[960000, 1], batch=[960000], cell=[10000, 3, 3], edge_cell_shift=[25556838, 3], edge_index=[2, 25556838], forces=[960000, 3], pbc=[10000, 3], pos=[960000, 3], ptr=[10001], total_energy=[10000, 1])\n","Cached processed data to disk\n","Done!\n","Successfully loaded the data set of type ASEDataset(10000)...\n","Replace string dataset_forces_rms to 0.7736424803733826\n","Replace string dataset_per_atom_total_energy_mean to -5.736269474029541\n","Atomic outputs are scaled by: [H, O: 0.773642], shifted by [H, O: -5.736269].\n","Replace string dataset_forces_rms to 0.7736424803733826\n","Initially outputs are globally scaled by: 0.7736424803733826, total_energy are globally shifted by None.\n","Successfully built the network...\n","Number of weights: 154200\n","! Starting training ...\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      0     1          1.1         1.09       0.0137        0.606        0.807        0.515        0.787        0.651        0.688            1        0.846         8.69       0.0905\n","      0     2         1.02         1.01       0.0136        0.577        0.776        0.491        0.749         0.62        0.662        0.965        0.813         8.67       0.0904\n","      0     3        0.993        0.979       0.0136        0.574        0.765        0.494        0.734        0.614        0.659        0.943        0.801         8.66       0.0902\n","      0     4        0.973         0.96       0.0136        0.564        0.758         0.48        0.733        0.606        0.653        0.933        0.793         8.66       0.0902\n","      0     5         0.97        0.956       0.0136        0.571        0.756        0.491        0.732        0.612         0.65        0.934        0.792         8.67       0.0903\n","\n","\n","  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Initial Validation          0    3.894    0.005        0.998       0.0136         1.01        0.578        0.773        0.494        0.747        0.621        0.663        0.956        0.809         8.67       0.0903\n","Wall time: 3.8946681170000375\n","! Best model        0    1.012\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      1     1        0.886        0.873       0.0136         0.55        0.723        0.481        0.687        0.584        0.633        0.875        0.754         8.67       0.0904\n","      1     2         1.02         1.01      0.00602        0.576        0.778        0.498        0.733        0.616        0.664        0.968        0.816         5.76         0.06\n","      1     3         1.05         1.05      0.00202        0.586        0.792        0.499        0.759        0.629        0.675        0.986         0.83         3.34       0.0348\n","      1     4         1.01         1.01     0.000398        0.573        0.777        0.492        0.735        0.613        0.672        0.954        0.813         1.48       0.0154\n","      1     5        0.962        0.962      1.1e-05        0.576        0.759        0.499         0.73        0.615        0.655        0.933        0.794        0.245      0.00255\n","      1     6        0.956        0.956     1.51e-05        0.573        0.756        0.491        0.737        0.614        0.647        0.938        0.792        0.281      0.00293\n","      1     7        0.904        0.904     1.25e-05        0.553        0.736        0.472        0.715        0.594        0.632        0.908         0.77         0.26      0.00271\n","      1     8         0.89         0.89     1.98e-06         0.53         0.73        0.457        0.677        0.567        0.626        0.903        0.764        0.092     0.000959\n","      1     9        0.826        0.826     0.000134        0.533        0.703        0.457        0.685        0.571        0.597        0.878        0.737         0.85      0.00886\n","      1    10        0.893        0.892     0.000483        0.558        0.731        0.474        0.726          0.6        0.627        0.903        0.765         1.63        0.017\n","      1    11        0.769        0.767      0.00151        0.505        0.678        0.435        0.646         0.54        0.581        0.838         0.71         2.89       0.0301\n","      1    12        0.654        0.651      0.00336        0.474        0.624        0.403        0.615        0.509        0.525        0.785        0.655          4.3       0.0448\n","      1    13        0.596         0.59      0.00568        0.458        0.594        0.394        0.586         0.49        0.509        0.735        0.622          5.6       0.0583\n","      1    14         0.59        0.581      0.00889        0.445         0.59        0.381        0.574        0.477          0.5        0.737        0.618            7       0.0729\n","      1    15        0.528        0.516       0.0123         0.41        0.556        0.343        0.544        0.443        0.463        0.705        0.584         8.24       0.0858\n","      1    16         0.46        0.445       0.0149        0.394        0.516        0.331         0.52        0.425        0.434         0.65        0.542         9.06       0.0943\n","      1    17        0.346        0.333       0.0135        0.345        0.446        0.303        0.427        0.365        0.388        0.545        0.466         8.63       0.0898\n","      1    18        0.335        0.325      0.00958         0.34        0.441        0.302        0.415        0.359        0.386        0.535         0.46         7.26       0.0756\n","      1    19        0.295        0.292      0.00273         0.33        0.418        0.286        0.416        0.351         0.36        0.515        0.438         3.86       0.0402\n","      1    20        0.273        0.272     0.000258        0.313        0.404        0.279        0.381         0.33        0.355        0.487        0.421         1.05       0.0109\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      1     1        0.298        0.298     0.000459        0.326        0.422        0.281        0.415        0.348        0.357        0.529        0.443         1.54       0.0161\n","      1     2        0.272        0.272     0.000465        0.312        0.403        0.272        0.391        0.332        0.348        0.497        0.422         1.55       0.0161\n","      1     3        0.253        0.253     0.000478        0.303        0.389        0.271        0.368         0.32        0.343        0.467        0.405          1.6       0.0166\n","      1     4        0.264        0.263     0.000554        0.311        0.397        0.274        0.385        0.329        0.343        0.486        0.415         1.71       0.0178\n","      1     5        0.265        0.264     0.000584        0.308        0.398        0.266        0.392        0.329        0.338        0.496        0.417         1.77       0.0185\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               1   12.753    0.005        0.707      0.00477        0.712        0.481        0.651        0.414        0.615        0.515        0.557        0.805        0.681         4.02       0.0419\n","! Validation          1   12.753    0.005         0.27     0.000508         0.27        0.312        0.402        0.273         0.39        0.332        0.346        0.495        0.421         1.63        0.017\n","Wall time: 12.75334339899996\n","! Best model        1    0.270\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      2     1        0.255         0.25      0.00547          0.3        0.387        0.271         0.36        0.315        0.345        0.459        0.402         5.46       0.0569\n","      2     2        0.258        0.247       0.0112        0.295        0.384        0.258        0.369        0.313        0.341        0.459          0.4         7.84       0.0817\n","      2     3        0.225        0.213        0.012        0.277        0.357        0.244        0.343        0.293        0.312        0.434        0.373         8.13       0.0847\n","      2     4        0.195         0.19      0.00487        0.262        0.337         0.23        0.324        0.277        0.296        0.407        0.352         5.12       0.0533\n","      2     5        0.175        0.174     0.000491        0.253        0.323        0.222        0.315        0.269        0.283        0.392        0.337         1.62       0.0169\n","      2     6        0.197        0.197      2.6e-05         0.26        0.343        0.222        0.337        0.279        0.289        0.432         0.36        0.347      0.00361\n","      2     7         0.18         0.18      0.00013        0.257        0.328        0.225        0.323        0.274         0.28        0.409        0.344        0.744      0.00775\n","      2     8        0.151        0.151     8.17e-05        0.232          0.3          0.2        0.296        0.248        0.254        0.377        0.315        0.533      0.00555\n","      2     9        0.168        0.168     0.000249        0.245        0.317        0.208        0.317        0.263        0.268        0.397        0.333         1.12       0.0116\n","      2    10        0.167        0.167      0.00013        0.245        0.316        0.213        0.309        0.261        0.268        0.394        0.331        0.741      0.00771\n","      2    11        0.167        0.167     0.000116        0.245        0.316        0.211        0.312        0.262        0.269        0.393        0.331        0.711      0.00741\n","      2    12        0.149        0.149     0.000119         0.23        0.299        0.195        0.299        0.247        0.249         0.38        0.314        0.767      0.00799\n","      2    13        0.152        0.151     0.000303        0.236        0.301        0.204        0.301        0.252        0.255        0.377        0.316         1.27       0.0132\n","      2    14        0.154        0.153      0.00159        0.233        0.302        0.199        0.301         0.25        0.253        0.383        0.318         2.96       0.0308\n","      2    15        0.139        0.135      0.00372        0.223        0.285        0.198        0.272        0.235        0.249        0.345        0.297         4.53       0.0472\n","      2    16        0.163        0.159      0.00405        0.243        0.308         0.22        0.287        0.254        0.276        0.365         0.32         4.73       0.0492\n","      2    17         0.13        0.128      0.00151        0.213        0.277        0.182        0.274        0.228        0.233         0.35        0.291         2.88         0.03\n","      2    18        0.122        0.122     1.27e-05        0.211         0.27         0.18        0.273        0.227        0.228        0.339        0.284        0.214      0.00223\n","      2    19        0.142        0.142     0.000662        0.224        0.291        0.196        0.279        0.238         0.25         0.36        0.305         1.89       0.0197\n","      2    20         0.13        0.129      0.00105        0.215        0.278        0.182        0.279        0.231        0.233        0.351        0.292         2.38       0.0248\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      2     1        0.131        0.131     2.36e-05        0.214         0.28        0.182        0.277         0.23        0.231        0.358        0.294        0.279      0.00291\n","      2     2        0.133        0.132     2.27e-05        0.216        0.282        0.186        0.277        0.232        0.238        0.352        0.295        0.303      0.00316\n","      2     3        0.114        0.114     1.86e-05        0.203        0.261        0.178        0.254        0.216        0.225         0.32        0.273        0.259      0.00269\n","      2     4        0.121         0.12      4.5e-05        0.206        0.269        0.179        0.259        0.219        0.228        0.335        0.282        0.449      0.00467\n","      2     5        0.128        0.127     3.14e-05        0.211        0.276        0.178        0.276        0.227        0.228        0.354        0.291        0.376      0.00392\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               2   15.757    0.005        0.169      0.00239        0.171        0.245        0.318        0.213        0.309        0.261        0.273        0.392        0.332          2.7       0.0281\n","! Validation          2   15.757    0.005        0.125     2.83e-05        0.125         0.21        0.273        0.181        0.269        0.225         0.23        0.344        0.287        0.333      0.00347\n","Wall time: 15.75764650400015\n","! Best model        2    0.125\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      3     1        0.119        0.118      0.00046        0.202        0.266        0.168        0.269        0.219        0.218        0.341         0.28         1.55       0.0161\n","      3     2        0.125        0.125     0.000319        0.205        0.273        0.172        0.273        0.222        0.219        0.357        0.288         1.29       0.0134\n","      3     3        0.118        0.118     0.000248        0.202        0.265        0.171        0.263        0.217        0.223        0.334        0.279         1.14       0.0119\n","      3     4         0.11         0.11      4.7e-05        0.202        0.257        0.173        0.258        0.216        0.218        0.321        0.269        0.431      0.00449\n","      3     5        0.116        0.116      7.2e-06        0.205        0.263        0.181        0.253        0.217        0.231        0.318        0.275        0.159      0.00166\n","      3     6        0.108        0.108     6.54e-05        0.194        0.255         0.17        0.243        0.206        0.221        0.311        0.266         0.56      0.00583\n","      3     7         0.11         0.11     0.000284        0.196        0.257        0.172        0.245        0.209        0.223        0.314        0.268         1.22       0.0127\n","      3     8        0.108        0.108     0.000111        0.194        0.254        0.171         0.24        0.205        0.222        0.309        0.265        0.759      0.00791\n","      3     9        0.102        0.102      2.8e-05        0.191        0.247        0.162        0.249        0.205        0.208         0.31        0.259          0.3      0.00313\n","      3    10        0.108        0.108     0.000286        0.196        0.254        0.172        0.245        0.209        0.219        0.312        0.266         1.25        0.013\n","      3    11        0.118        0.118     0.000222        0.203        0.266        0.174        0.261        0.217        0.224        0.334        0.279          1.1       0.0115\n","      3    12        0.118        0.118     6.86e-06        0.202        0.265        0.171        0.263        0.217        0.224        0.333        0.278        0.154       0.0016\n","      3    13         0.11        0.109     0.000331        0.196        0.256        0.168        0.251         0.21        0.216         0.32        0.268         1.29       0.0134\n","      3    14       0.0942       0.0938      0.00045        0.183        0.237        0.161        0.227        0.194        0.206        0.289        0.247         1.51       0.0157\n","      3    15       0.0989       0.0987      0.00025        0.187        0.243        0.162        0.235        0.199        0.209        0.299        0.254          1.1       0.0115\n","      3    16       0.0956       0.0956     3.48e-05        0.185        0.239        0.159        0.236        0.197        0.206        0.295         0.25        0.383      0.00399\n","      3    17        0.099       0.0989     5.52e-05        0.192        0.243        0.167        0.241        0.204        0.209        0.301        0.255        0.442       0.0046\n","      3    18       0.0997       0.0996     4.11e-05         0.19        0.244        0.164        0.243        0.204        0.207        0.305        0.256        0.359      0.00374\n","      3    19       0.0975       0.0974      6.7e-05        0.185        0.241        0.159        0.239        0.199        0.209        0.296        0.253        0.505      0.00526\n","      3    20       0.0836       0.0835     3.09e-05        0.171        0.224        0.147        0.219        0.183         0.19        0.279        0.235        0.369      0.00384\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      3     1        0.101        0.101     4.16e-05        0.187        0.245         0.16         0.24          0.2        0.204        0.312        0.258        0.357      0.00372\n","      3     2        0.104        0.104     2.55e-05        0.191         0.25        0.165        0.244        0.204        0.215        0.308        0.262        0.314      0.00327\n","      3     3       0.0864       0.0864     2.02e-05        0.176        0.227        0.155        0.217        0.186        0.199        0.275        0.237        0.236      0.00246\n","      3     4       0.0953       0.0953     4.43e-05        0.182        0.239        0.158         0.23        0.194        0.204        0.297         0.25        0.403       0.0042\n","      3     5       0.0992       0.0992     4.25e-05        0.186        0.244        0.158        0.241          0.2        0.203        0.309        0.256        0.422      0.00439\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               3   18.757    0.005        0.107     0.000167        0.107        0.194        0.253        0.167        0.248        0.208        0.215        0.315        0.265        0.793      0.00826\n","! Validation          3   18.757    0.005       0.0972     3.48e-05       0.0972        0.184        0.241        0.159        0.234        0.197        0.205          0.3        0.253        0.346      0.00361\n","Wall time: 18.757418142999995\n","! Best model        3    0.097\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      4     1        0.084       0.0839     2.99e-05        0.172        0.224        0.154         0.21        0.182        0.199        0.267        0.233        0.319      0.00332\n","      4     2       0.0771        0.077     7.58e-05        0.166        0.215        0.142        0.214        0.178        0.181        0.269        0.225        0.532      0.00554\n","      4     3       0.0824       0.0821     0.000311         0.17        0.222        0.146        0.218        0.182        0.189        0.275        0.232         1.28       0.0134\n","      4     4       0.0829       0.0827     0.000256         0.17        0.222        0.147        0.217        0.182        0.187         0.28        0.234         1.14       0.0119\n","      4     5        0.086       0.0858     0.000251        0.178        0.227        0.152        0.228         0.19        0.192        0.283        0.238         1.11       0.0116\n","      4     6       0.0816       0.0816     6.61e-06        0.172        0.221         0.15        0.216        0.183        0.191        0.271        0.231        0.148      0.00154\n","      4     7       0.0699       0.0699     3.21e-05        0.155        0.205        0.135        0.197        0.166        0.177         0.25        0.214        0.403       0.0042\n","      4     8       0.0691       0.0691     1.04e-05        0.158        0.203        0.137          0.2        0.168        0.176         0.25        0.213        0.223      0.00232\n","      4     9       0.0675       0.0675     2.93e-05        0.156        0.201        0.135          0.2        0.167        0.171         0.25        0.211         0.32      0.00333\n","      4    10       0.0842       0.0842     6.42e-05        0.174        0.224         0.15        0.222        0.186        0.195        0.275        0.235        0.558      0.00581\n","      4    11       0.0689       0.0689     1.89e-05        0.158        0.203        0.138        0.199        0.169        0.175        0.249        0.212        0.279      0.00291\n","      4    12       0.0663       0.0662     6.88e-05        0.154        0.199        0.134        0.193        0.164         0.17        0.247        0.208        0.575      0.00599\n","      4    13       0.0673       0.0672     8.56e-05        0.155        0.201        0.135        0.193        0.164        0.174        0.246         0.21         0.59      0.00614\n","      4    14       0.0727       0.0726     3.07e-05        0.158        0.209        0.136        0.203         0.17        0.176        0.261        0.219        0.298      0.00311\n","      4    15       0.0638       0.0638     1.45e-05         0.15        0.195        0.132        0.187         0.16        0.169         0.24        0.204        0.265      0.00276\n","      4    16       0.0822       0.0822     2.73e-05         0.17        0.222        0.147        0.217        0.182        0.192        0.272        0.232        0.334      0.00348\n","      4    17       0.0604       0.0604     1.74e-05        0.148         0.19        0.127        0.188        0.158        0.164        0.234        0.199        0.238      0.00247\n","      4    18       0.0721        0.072     4.82e-05         0.16        0.208        0.136        0.208        0.172        0.175        0.261        0.218        0.413       0.0043\n","      4    19       0.0618       0.0617     3.85e-05        0.151        0.192        0.129        0.196        0.162        0.165        0.238        0.201         0.43      0.00448\n","      4    20       0.0727       0.0726     5.97e-05        0.161        0.208        0.141        0.201        0.171        0.184         0.25        0.217        0.549      0.00572\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      4     1       0.0744       0.0744     3.87e-05         0.16        0.211        0.138        0.205        0.172        0.178        0.265        0.221         0.36      0.00375\n","      4     2       0.0769       0.0768     1.25e-05        0.164        0.214        0.143        0.205        0.174        0.189        0.259        0.224        0.222      0.00231\n","      4     3       0.0626       0.0626     1.48e-05         0.15        0.193        0.133        0.184        0.159        0.171        0.232        0.202        0.205      0.00213\n","      4     4       0.0685       0.0685     4.12e-05        0.156        0.202        0.135        0.197        0.166        0.173        0.251        0.212        0.404       0.0042\n","      4     5       0.0685       0.0685     3.48e-05        0.155        0.202        0.132          0.2        0.166        0.172        0.253        0.212        0.392      0.00408\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               4   21.766    0.005       0.0736     7.38e-05       0.0736        0.162         0.21         0.14        0.205        0.173         0.18        0.259         0.22          0.5      0.00521\n","! Validation          4   21.766    0.005       0.0701     2.84e-05       0.0702        0.157        0.205        0.136        0.198        0.167        0.177        0.252        0.214        0.316       0.0033\n","Wall time: 21.76635743099996\n","! Best model        4    0.070\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      5     1       0.0655       0.0654     0.000119        0.155        0.198        0.139        0.189        0.164        0.175        0.237        0.206        0.794      0.00827\n","      5     2       0.0583       0.0583     1.78e-05        0.145        0.187        0.124        0.188        0.156        0.159        0.233        0.196        0.252      0.00263\n","      5     3       0.0639       0.0637     0.000202         0.15        0.195        0.134        0.184        0.159         0.17        0.237        0.204         1.04       0.0109\n","      5     4       0.0642       0.0642     1.25e-05        0.149        0.196         0.13        0.188        0.159        0.169        0.242        0.205        0.233      0.00243\n","      5     5       0.0671       0.0671     1.33e-05        0.155          0.2        0.135        0.194        0.164        0.175        0.243        0.209        0.196      0.00204\n","      5     6       0.0534       0.0534     3.09e-05        0.139        0.179        0.124        0.168        0.146         0.16        0.212        0.186        0.343      0.00357\n","      5     7       0.0555        0.055     0.000486        0.141        0.181        0.121         0.18         0.15        0.155        0.225         0.19         1.59       0.0165\n","      5     8       0.0517       0.0513     0.000453        0.136        0.175        0.118        0.171        0.145        0.151        0.216        0.183         1.54        0.016\n","      5     9        0.054       0.0539      2.3e-05        0.139         0.18        0.119        0.179        0.149        0.153        0.223        0.188        0.331      0.00345\n","      5    10       0.0508       0.0505     0.000272        0.134        0.174        0.121         0.16        0.141        0.154        0.208        0.181         1.13       0.0118\n","      5    11       0.0536       0.0533     0.000313         0.14        0.179        0.122        0.176        0.149        0.154         0.22        0.187         1.29       0.0135\n","      5    12       0.0651       0.0651     1.63e-05        0.149        0.197        0.133        0.182        0.158        0.178        0.232        0.205        0.235      0.00244\n","      5    13       0.0526       0.0526     1.06e-05        0.137        0.177         0.12        0.172        0.146        0.155        0.215        0.185         0.17      0.00177\n","      5    14       0.0591        0.059      1.8e-05        0.146        0.188        0.133        0.171        0.152        0.171        0.218        0.194        0.278      0.00289\n","      5    15       0.0572        0.057     0.000136         0.14        0.185        0.121        0.176        0.149         0.16        0.226        0.193        0.803      0.00837\n","      5    16       0.0547       0.0545     0.000152        0.137        0.181        0.122        0.168        0.145        0.159        0.217        0.188        0.876      0.00913\n","      5    17       0.0502       0.0502     6.94e-05        0.134        0.173        0.117         0.17        0.143         0.15        0.212        0.181        0.566      0.00589\n","      5    18       0.0504       0.0501     0.000266        0.134        0.173        0.118        0.167        0.142        0.151        0.211        0.181         1.19       0.0124\n","      5    19       0.0491       0.0491     2.48e-05        0.134        0.171        0.118        0.166        0.142        0.151        0.206        0.179        0.312      0.00325\n","      5    20       0.0495       0.0494     6.99e-05        0.133        0.172        0.118        0.164        0.141        0.154        0.204        0.179          0.5      0.00521\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      5     1        0.059        0.059     6.18e-05        0.143        0.188        0.125        0.181        0.153        0.161        0.232        0.197        0.552      0.00575\n","      5     2       0.0604       0.0604     1.79e-05        0.146         0.19         0.13        0.178        0.154         0.17        0.225        0.197        0.265      0.00276\n","      5     3       0.0489       0.0489     2.84e-05        0.133        0.171        0.119        0.159        0.139        0.154        0.201        0.178         0.36      0.00375\n","      5     4       0.0531        0.053     5.29e-05        0.137        0.178         0.12        0.173        0.146        0.154        0.218        0.186        0.464      0.00483\n","      5     5       0.0514       0.0513      4.5e-05        0.134        0.175        0.116        0.171        0.143        0.152        0.214        0.183        0.437      0.00455\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               5   24.777    0.005       0.0562     0.000135       0.0563        0.141        0.183        0.124        0.176         0.15         0.16        0.222        0.191        0.684      0.00712\n","! Validation          5   24.777    0.005       0.0545     4.12e-05       0.0546        0.139        0.181        0.122        0.172        0.147        0.158        0.218        0.188        0.415      0.00433\n","Wall time: 24.77746947100013\n","! Best model        5    0.055\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      6     1       0.0485       0.0484     4.46e-05        0.132         0.17        0.116        0.164         0.14        0.152        0.202        0.177        0.398      0.00414\n","      6     2        0.046        0.046      3.4e-05        0.128        0.166        0.114        0.156        0.135        0.148        0.197        0.172        0.404      0.00421\n","      6     3       0.0466       0.0466     4.06e-06         0.13        0.167        0.116        0.158        0.137        0.147        0.201        0.174        0.133      0.00139\n","      6     4       0.0482       0.0481     0.000104        0.134         0.17        0.116         0.17        0.143        0.146        0.209        0.177        0.738      0.00769\n","      6     5       0.0491        0.049     7.95e-05        0.128        0.171        0.113        0.159        0.136        0.151        0.206        0.178        0.514      0.00535\n","      6     6       0.0514       0.0513     2.45e-05        0.136        0.175        0.117        0.175        0.146        0.149        0.219        0.184        0.322      0.00335\n","      6     7       0.0451        0.045      5.6e-05        0.127        0.164        0.112        0.156        0.134        0.147        0.194        0.171        0.426      0.00443\n","      6     8       0.0405       0.0404     3.53e-05        0.122        0.156        0.109        0.148        0.128        0.138        0.186        0.162        0.378      0.00394\n","      6     9       0.0448       0.0448      4.2e-05        0.128        0.164        0.116        0.152        0.134        0.151        0.187        0.169         0.43      0.00448\n","      6    10       0.0496       0.0496     6.76e-05        0.133        0.172        0.116        0.165        0.141        0.154        0.204        0.179        0.608      0.00633\n","      6    11       0.0434       0.0434     9.88e-06        0.125        0.161        0.114        0.147         0.13        0.147        0.186        0.167        0.173      0.00181\n","      6    12       0.0449       0.0449     5.86e-05        0.126        0.164        0.109        0.158        0.134        0.141        0.202        0.171        0.503      0.00524\n","      6    13       0.0514       0.0514     6.36e-05        0.132        0.175        0.114        0.167        0.141        0.149        0.219        0.184        0.542      0.00564\n","      6    14       0.0349       0.0349     1.04e-05        0.112        0.145        0.101        0.135        0.118         0.13         0.17         0.15        0.212      0.00221\n","      6    15       0.0471        0.047     6.13e-05         0.13        0.168        0.114        0.161        0.137        0.149        0.199        0.174        0.516      0.00538\n","      6    16       0.0492       0.0491     6.02e-05        0.132        0.171         0.12        0.157        0.138        0.155          0.2        0.178        0.527      0.00549\n","      6    17        0.046       0.0459     0.000151        0.128        0.166        0.114        0.155        0.135        0.147        0.199        0.173        0.762      0.00793\n","      6    18       0.0453       0.0453     1.78e-05        0.127        0.165        0.114        0.152        0.133        0.148        0.194        0.171        0.277      0.00288\n","      6    19        0.039       0.0389     9.44e-05        0.119        0.153        0.105        0.145        0.125        0.135        0.183        0.159        0.703      0.00732\n","      6    20       0.0477       0.0477     2.33e-05         0.13        0.169        0.116        0.159        0.137        0.149        0.202        0.176        0.279       0.0029\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      6     1       0.0503       0.0503     4.53e-05        0.133        0.173        0.117        0.166        0.141        0.151        0.211        0.181        0.463      0.00482\n","      6     2       0.0507       0.0507     1.22e-05        0.134        0.174         0.12        0.162        0.141        0.157        0.205        0.181        0.214      0.00223\n","      6     3       0.0417       0.0417      1.8e-05        0.123        0.158        0.111        0.146        0.129        0.143        0.185        0.164        0.268      0.00279\n","      6     4       0.0449       0.0448     5.12e-05        0.127        0.164         0.11         0.16        0.135        0.142          0.2        0.171        0.431      0.00449\n","      6     5       0.0432       0.0432      4.2e-05        0.123        0.161        0.107        0.154        0.131        0.141        0.194        0.168        0.435      0.00453\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               6   27.801    0.005       0.0459     5.21e-05       0.0459        0.128        0.166        0.113        0.157        0.135        0.147        0.198        0.173        0.442      0.00461\n","! Validation          6   27.801    0.005       0.0461     3.38e-05       0.0462        0.128        0.166        0.113        0.158        0.135        0.147        0.199        0.173        0.362      0.00377\n","Wall time: 27.8014433620001\n","! Best model        6    0.046\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      7     1       0.0417       0.0416     8.75e-05        0.121        0.158        0.106         0.15        0.128        0.137        0.193        0.165        0.648      0.00675\n","      7     2        0.043       0.0429      7.8e-05        0.124         0.16        0.112        0.149        0.131        0.147        0.185        0.166        0.578      0.00602\n","      7     3       0.0414       0.0414     2.21e-05        0.123        0.157         0.11        0.149        0.129         0.14        0.187        0.164        0.269       0.0028\n","      7     4       0.0445       0.0444     0.000114        0.127        0.163        0.112        0.157        0.135        0.145        0.195         0.17        0.681       0.0071\n","      7     5       0.0401       0.0401     3.74e-05        0.121        0.155        0.107        0.148        0.128        0.137        0.185        0.161        0.426      0.00444\n","      7     6       0.0383       0.0383        4e-05        0.117        0.151        0.106         0.14        0.123        0.138        0.174        0.156        0.344      0.00359\n","      7     7       0.0433       0.0433      7.5e-05        0.124        0.161         0.11        0.153        0.131        0.143        0.192        0.167        0.627      0.00653\n","      7     8       0.0333       0.0333     1.85e-05        0.109        0.141       0.0961        0.134        0.115        0.124        0.171        0.147         0.25      0.00261\n","      7     9       0.0426       0.0425     4.46e-05        0.123         0.16        0.108        0.155        0.131         0.14        0.192        0.166        0.393       0.0041\n","      7    10        0.041        0.041     1.88e-05         0.12        0.157        0.104         0.15        0.127        0.137         0.19        0.163        0.232      0.00242\n","      7    11       0.0412       0.0412     1.45e-05        0.122        0.157        0.108        0.149        0.129        0.139        0.188        0.163        0.227      0.00236\n","      7    12       0.0443       0.0443     1.23e-05        0.123        0.163        0.108        0.154        0.131        0.142        0.198         0.17        0.186      0.00194\n","      7    13       0.0363       0.0363     3.07e-05        0.114        0.147        0.101         0.14         0.12        0.131        0.176        0.153        0.358      0.00373\n","      7    14       0.0348       0.0347     3.45e-05        0.111        0.144        0.099        0.136        0.118        0.128        0.172         0.15        0.344      0.00359\n","      7    15        0.042        0.042     1.84e-05        0.123        0.158        0.109         0.15        0.129        0.142        0.188        0.165        0.209      0.00218\n","      7    16       0.0341       0.0339     0.000177        0.113        0.143       0.0988        0.141         0.12        0.125        0.172        0.149        0.977       0.0102\n","      7    17        0.042       0.0418     0.000147        0.119        0.158        0.106        0.146        0.126         0.14        0.189        0.165        0.859      0.00895\n","      7    18       0.0343       0.0342     5.64e-05         0.11        0.143       0.0996        0.131        0.115        0.128        0.169        0.149        0.451       0.0047\n","      7    19       0.0409       0.0409     3.89e-05         0.12        0.156        0.109        0.141        0.125        0.142        0.182        0.162         0.43      0.00448\n","      7    20       0.0335       0.0332     0.000303        0.108        0.141       0.0924        0.138        0.115        0.119        0.177        0.148         1.25        0.013\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      7     1       0.0447       0.0446     5.02e-05        0.125        0.163         0.11        0.156        0.133        0.143        0.198        0.171        0.496      0.00517\n","      7     2       0.0443       0.0442     1.49e-05        0.126        0.163        0.112        0.153        0.132        0.146        0.192        0.169        0.256      0.00266\n","      7     3        0.037        0.037     2.02e-05        0.116        0.149        0.104        0.139        0.122        0.134        0.175        0.154        0.285      0.00297\n","      7     4       0.0394       0.0393     5.58e-05        0.119        0.153        0.103        0.149        0.126        0.134        0.187         0.16        0.453      0.00472\n","      7     5       0.0384       0.0384     4.67e-05        0.116        0.152        0.102        0.144        0.123        0.134        0.182        0.158        0.461       0.0048\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               7   30.826    0.005       0.0396     6.85e-05       0.0396        0.119        0.154        0.105        0.146        0.125        0.136        0.184         0.16        0.487      0.00507\n","! Validation          7   30.826    0.005       0.0407     3.75e-05       0.0408         0.12        0.156        0.106        0.148        0.127        0.138        0.187        0.163         0.39      0.00406\n","Wall time: 30.827097513000126\n","! Best model        7    0.041\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      8     1       0.0356        0.035     0.000553        0.113        0.145       0.0998        0.138        0.119        0.128        0.174        0.151         1.74       0.0181\n","      8     2       0.0374       0.0373     8.11e-05        0.114        0.149        0.101        0.141        0.121        0.132        0.179        0.156        0.594      0.00619\n","      8     3       0.0362       0.0356     0.000597        0.113        0.146        0.103        0.135        0.119        0.132         0.17        0.151         1.81       0.0189\n","      8     4        0.041       0.0409     0.000153        0.119        0.156        0.103         0.15        0.127        0.133        0.195        0.164        0.919      0.00957\n","      8     5       0.0347       0.0347     4.65e-05        0.111        0.144       0.0973        0.139        0.118        0.125        0.176        0.151        0.451       0.0047\n","      8     6       0.0391        0.039     3.88e-05        0.117        0.153        0.103        0.146        0.125        0.132        0.188         0.16        0.441       0.0046\n","      8     7       0.0339       0.0337     0.000279        0.109        0.142       0.0965        0.135        0.116        0.124        0.172        0.148         1.21       0.0127\n","      8     8       0.0417       0.0414     0.000293        0.121        0.157        0.107        0.149        0.128        0.141        0.186        0.163         1.24       0.0129\n","      8     9       0.0341        0.034     0.000112        0.109        0.143       0.0963        0.136        0.116        0.125        0.173        0.149        0.722      0.00752\n","      8    10       0.0372       0.0368     0.000376        0.115        0.148        0.103         0.14        0.121        0.134        0.174        0.154         1.43       0.0149\n","      8    11       0.0399       0.0396     0.000237        0.119        0.154        0.105        0.147        0.126        0.135        0.186        0.161         1.03       0.0107\n","      8    12       0.0314       0.0314     3.06e-05        0.106        0.137       0.0957        0.126        0.111        0.124        0.159        0.142         0.37      0.00386\n","      8    13       0.0398       0.0398     4.29e-05        0.119        0.154        0.107        0.143        0.125        0.139        0.182         0.16        0.454      0.00473\n","      8    14       0.0368       0.0362     0.000603        0.114        0.147        0.104        0.133        0.119        0.137        0.166        0.151         1.81       0.0188\n","      8    15       0.0311       0.0307     0.000426        0.107        0.136       0.0946        0.131        0.113        0.121        0.161        0.141         1.46       0.0152\n","      8    16        0.039       0.0389     9.77e-05        0.118        0.153        0.106        0.143        0.124        0.136        0.181        0.159        0.627      0.00653\n","      8    17       0.0361       0.0359     0.000152        0.113        0.147       0.0985        0.142         0.12        0.127         0.18        0.153        0.808      0.00842\n","      8    18       0.0324       0.0324      1.9e-05        0.107        0.139        0.097        0.128        0.113        0.125        0.163        0.144         0.24       0.0025\n","      8    19       0.0314       0.0313     7.03e-05        0.108        0.137       0.0956        0.132        0.114        0.121        0.164        0.142        0.505      0.00526\n","      8    20       0.0298       0.0298      2.6e-05        0.103        0.133       0.0903        0.128        0.109        0.119        0.159        0.139        0.355      0.00369\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      8     1       0.0403       0.0403     3.77e-05        0.119        0.155        0.105        0.148        0.127        0.136        0.188        0.162        0.384        0.004\n","      8     2       0.0392       0.0391     1.33e-05        0.119        0.153        0.105        0.145        0.125        0.137        0.182        0.159        0.206      0.00215\n","      8     3       0.0334       0.0333     1.77e-05         0.11        0.141       0.0986        0.133        0.116        0.126        0.167        0.147        0.244      0.00254\n","      8     4       0.0354       0.0353     5.34e-05        0.112        0.145       0.0978        0.141        0.119        0.127        0.176        0.152        0.462      0.00482\n","      8     5       0.0345       0.0345     4.39e-05         0.11        0.144        0.097        0.137        0.117        0.127        0.172         0.15        0.424      0.00442\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               8   33.839    0.005       0.0357     0.000212       0.0359        0.113        0.146          0.1        0.138        0.119         0.13        0.175        0.152        0.911      0.00949\n","! Validation          8   33.839    0.005       0.0365     3.32e-05       0.0366        0.114        0.148        0.101        0.141        0.121        0.131        0.177        0.154        0.344      0.00358\n","Wall time: 33.839615600000116\n","! Best model        8    0.037\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      9     1       0.0317       0.0317     4.22e-05        0.107        0.138       0.0935        0.134        0.114        0.118         0.17        0.144        0.427      0.00445\n","      9     2       0.0273       0.0273      2.3e-05        0.101        0.128       0.0922        0.118        0.105        0.116        0.149        0.132        0.283      0.00295\n","      9     3       0.0313       0.0311     0.000243        0.105        0.136       0.0921        0.131        0.112         0.12        0.164        0.142         1.14       0.0118\n","      9     4       0.0344       0.0343     8.95e-05        0.112        0.143       0.0958        0.143         0.12        0.121        0.179         0.15        0.613      0.00638\n","      9     5        0.038       0.0379     7.33e-05        0.116        0.151        0.105        0.138        0.122        0.137        0.175        0.156        0.574      0.00598\n","      9     6        0.028       0.0279     2.52e-05          0.1        0.129       0.0895        0.122        0.106        0.114        0.156        0.135        0.297      0.00309\n","      9     7       0.0353       0.0353      8.8e-06        0.113        0.145        0.101        0.135        0.118        0.129        0.174        0.151        0.207      0.00215\n","      9     8       0.0424       0.0424     4.55e-05        0.121        0.159         0.11        0.142        0.126        0.146        0.183        0.164        0.417      0.00434\n","      9     9       0.0345       0.0345     2.41e-05        0.111        0.144       0.0993        0.136        0.117        0.128        0.171        0.149        0.302      0.00315\n","      9    10       0.0368       0.0367     0.000139        0.113        0.148        0.104         0.13        0.117        0.136        0.169        0.153        0.727      0.00757\n","      9    11       0.0369       0.0369     1.49e-05        0.114        0.149       0.0998        0.143        0.122        0.128        0.183        0.155        0.259       0.0027\n","      9    12       0.0309       0.0308     0.000111        0.106        0.136       0.0934        0.132        0.113         0.12        0.163        0.141        0.712      0.00742\n","      9    13       0.0376       0.0376     7.71e-05        0.116         0.15        0.102        0.144        0.123        0.133        0.179        0.156        0.596      0.00621\n","      9    14       0.0304       0.0304     5.69e-05        0.106        0.135       0.0947        0.129        0.112        0.119        0.162         0.14        0.408      0.00425\n","      9    15       0.0282        0.028     0.000237          0.1        0.129       0.0905         0.12        0.105        0.116        0.152        0.134          1.1       0.0114\n","      9    16       0.0305       0.0305      5.4e-05        0.105        0.135       0.0929         0.13        0.111        0.119        0.162        0.141        0.459      0.00478\n","      9    17       0.0285       0.0284     0.000108          0.1         0.13       0.0871        0.126        0.106        0.112        0.161        0.137        0.723      0.00754\n","      9    18       0.0332       0.0331     7.74e-05        0.109        0.141       0.0971        0.132        0.115        0.126        0.167        0.146        0.633       0.0066\n","      9    19       0.0286       0.0286     2.18e-05       0.0987        0.131       0.0856        0.125        0.105        0.113        0.161        0.137        0.303      0.00315\n","      9    20       0.0366       0.0366     2.61e-05        0.115        0.148        0.103        0.139        0.121        0.133        0.174        0.153        0.289      0.00301\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      9     1       0.0365       0.0364     3.66e-05        0.113        0.148       0.0991        0.142         0.12        0.129        0.179        0.154        0.344      0.00358\n","      9     2       0.0349       0.0349     1.67e-05        0.112        0.145       0.0991        0.138        0.119        0.128        0.173         0.15        0.224      0.00233\n","      9     3       0.0302       0.0302     2.13e-05        0.104        0.134       0.0931        0.127         0.11        0.119        0.161         0.14        0.281      0.00292\n","      9     4        0.032       0.0319     5.61e-05        0.107        0.138       0.0929        0.134        0.113        0.121        0.168        0.144        0.475      0.00495\n","      9     5       0.0313       0.0313     4.67e-05        0.105        0.137       0.0923        0.131        0.112         0.12        0.165        0.143        0.426      0.00444\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               9   36.847    0.005        0.033     7.49e-05       0.0331        0.108        0.141       0.0964        0.133        0.114        0.125        0.168        0.146        0.523      0.00545\n","! Validation          9   36.847    0.005       0.0329     3.55e-05        0.033        0.108         0.14       0.0953        0.134        0.115        0.124        0.169        0.146         0.35      0.00364\n","Wall time: 36.84798588800004\n","! Best model        9    0.033\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     10     1       0.0289       0.0282     0.000613          0.1         0.13       0.0899        0.121        0.105        0.116        0.154        0.135         1.82        0.019\n","     10     2       0.0294        0.029     0.000366        0.101        0.132        0.089        0.125        0.107        0.116        0.158        0.137         1.41       0.0146\n","     10     3       0.0266       0.0265     4.64e-05       0.0986        0.126        0.088         0.12        0.104        0.112         0.15        0.131        0.431      0.00449\n","     10     4       0.0303       0.0297     0.000525        0.101        0.133       0.0864        0.129        0.108        0.113        0.167         0.14         1.68       0.0175\n","     10     5       0.0305       0.0296     0.000894        0.104        0.133       0.0902        0.131         0.11        0.117        0.161        0.139         2.21        0.023\n","     10     6       0.0303       0.0302     0.000105        0.102        0.134        0.091        0.125        0.108        0.119        0.161         0.14        0.664      0.00692\n","     10     7       0.0339       0.0336     0.000225        0.108        0.142       0.0956        0.134        0.115        0.124        0.172        0.148         1.03       0.0108\n","     10     8       0.0305       0.0302     0.000273        0.105        0.134       0.0925        0.131        0.112        0.117        0.164         0.14         1.17       0.0122\n","     10     9        0.028        0.028     1.22e-05          0.1        0.129       0.0866        0.128        0.107        0.113        0.158        0.135        0.241      0.00251\n","     10    10       0.0255       0.0254     0.000184       0.0948        0.123       0.0849        0.115       0.0998        0.109        0.147        0.128        0.858      0.00894\n","     10    11       0.0278       0.0277     7.97e-05          0.1        0.129       0.0878        0.125        0.106        0.113        0.157        0.135        0.621      0.00646\n","     10    12       0.0309       0.0309     2.74e-05        0.104        0.136       0.0915         0.13        0.111         0.12        0.163        0.142        0.275      0.00286\n","     10    13       0.0266       0.0266     4.33e-05        0.098        0.126        0.087         0.12        0.103        0.112         0.15        0.131        0.463      0.00483\n","     10    14       0.0271       0.0268     0.000243        0.098        0.127       0.0857        0.123        0.104         0.11        0.155        0.133         1.09       0.0114\n","     10    15       0.0249       0.0247     0.000245       0.0935        0.122       0.0806        0.119          0.1        0.103        0.152        0.128          1.1       0.0115\n","     10    16       0.0272       0.0272        2e-05       0.0975        0.128       0.0879        0.117        0.102        0.115        0.149        0.132         0.27      0.00281\n","     10    17       0.0253       0.0251       0.0002        0.095        0.123       0.0837        0.118        0.101        0.109        0.146        0.128        0.961         0.01\n","     10    18       0.0306       0.0304     0.000249        0.104        0.135        0.094        0.123        0.108        0.124        0.154        0.139         1.09       0.0113\n","     10    19        0.034       0.0338     0.000248         0.11        0.142       0.0958        0.139        0.117        0.124        0.173        0.148         1.16       0.0121\n","     10    20       0.0258       0.0257     9.33e-05       0.0958        0.124       0.0842        0.119        0.102        0.108        0.151         0.13        0.653      0.00681\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     10     1       0.0328       0.0328     3.62e-05        0.107         0.14        0.094        0.134        0.114        0.122        0.171        0.146        0.359      0.00374\n","     10     2       0.0313       0.0313     1.66e-05        0.106        0.137       0.0933        0.131        0.112        0.121        0.164        0.142        0.224      0.00233\n","     10     3       0.0272       0.0272     1.95e-05       0.0991        0.128       0.0878        0.122        0.105        0.112        0.153        0.133        0.271      0.00283\n","     10     4       0.0287       0.0287     5.25e-05        0.101        0.131       0.0881        0.126        0.107        0.115        0.159        0.137        0.459      0.00478\n","     10     5       0.0283       0.0283     4.42e-05          0.1         0.13       0.0874        0.126        0.107        0.114        0.157        0.136        0.409      0.00426\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              10   39.878    0.005       0.0285     0.000235       0.0287        0.101        0.131       0.0886        0.124        0.107        0.115        0.157        0.136         0.96         0.01\n","! Validation         10   39.878    0.005       0.0296     3.38e-05       0.0297        0.103        0.133       0.0901        0.128        0.109        0.117        0.161        0.139        0.344      0.00359\n","Wall time: 39.87870888900011\n","! Best model       10    0.030\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     11     1       0.0273       0.0271     0.000151       0.0992        0.127       0.0856        0.126        0.106        0.111        0.155        0.133         0.91      0.00948\n","     11     2       0.0245       0.0243     0.000228       0.0931        0.121       0.0823        0.115       0.0984        0.107        0.145        0.126         1.09       0.0113\n","     11     3       0.0268       0.0268     1.89e-05        0.098        0.127       0.0851        0.124        0.104        0.109        0.156        0.133        0.274      0.00286\n","     11     4       0.0261       0.0261     5.92e-05       0.0966        0.125       0.0864        0.117        0.102        0.112        0.147         0.13        0.459      0.00478\n","     11     5       0.0239       0.0236     0.000328       0.0899        0.119       0.0801        0.109       0.0948        0.106        0.142        0.124         1.34        0.014\n","     11     6       0.0263        0.026      0.00022       0.0952        0.125       0.0825         0.12        0.101        0.108        0.153         0.13         1.04       0.0109\n","     11     7       0.0244       0.0244     1.17e-05       0.0942        0.121       0.0806        0.121        0.101        0.102        0.151        0.127        0.213      0.00222\n","     11     8       0.0257       0.0255     0.000233       0.0965        0.124       0.0855        0.118        0.102        0.109        0.149        0.129         1.06       0.0111\n","     11     9       0.0256       0.0254     0.000207       0.0966        0.123        0.086        0.118        0.102         0.11        0.145        0.128        0.906      0.00944\n","     11    10       0.0261        0.026       0.0001       0.0948        0.125       0.0808        0.123        0.102        0.107        0.155        0.131         0.66      0.00687\n","     11    11       0.0261       0.0259      0.00021       0.0966        0.125       0.0839        0.122        0.103        0.106        0.155        0.131        0.981       0.0102\n","     11    12       0.0327       0.0325     0.000197        0.106        0.139       0.0935        0.131        0.112        0.123        0.167        0.145         1.03       0.0107\n","     11    13       0.0243       0.0242     3.86e-05       0.0934         0.12       0.0833        0.114       0.0984        0.107        0.144        0.125        0.415      0.00432\n","     11    14       0.0216       0.0211     0.000488        0.087        0.112       0.0774        0.106       0.0918       0.0989        0.136        0.117         1.63        0.017\n","     11    15       0.0285       0.0281     0.000396        0.102         0.13       0.0933        0.118        0.106        0.118         0.15        0.134         1.45       0.0151\n","     11    16       0.0299       0.0296     0.000303        0.103        0.133        0.089         0.13        0.109        0.114        0.165        0.139         1.28       0.0133\n","     11    17        0.027       0.0263     0.000651        0.096        0.125       0.0857        0.117        0.101        0.112        0.149         0.13         1.87       0.0195\n","     11    18       0.0231        0.023     9.95e-05       0.0914        0.117       0.0793        0.115       0.0974          0.1        0.146        0.123        0.654      0.00682\n","     11    19       0.0248       0.0248     5.11e-05        0.095        0.122        0.084        0.117        0.101        0.108        0.146        0.127        0.487      0.00507\n","     11    20       0.0279       0.0276     0.000345       0.0977        0.129       0.0855        0.122        0.104        0.113        0.155        0.134         1.36       0.0142\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     11     1       0.0296       0.0296      3.5e-05        0.102        0.133        0.089        0.128        0.108        0.116        0.162        0.139        0.322      0.00335\n","     11     2       0.0282       0.0282     2.07e-05          0.1         0.13       0.0877        0.125        0.106        0.114        0.157        0.135        0.255      0.00266\n","     11     3       0.0246       0.0245     2.38e-05        0.094        0.121       0.0827        0.117       0.0997        0.106        0.147        0.126        0.313      0.00326\n","     11     4       0.0259       0.0258     5.51e-05       0.0956        0.124       0.0834         0.12        0.102        0.109        0.151         0.13        0.487      0.00507\n","     11     5       0.0256       0.0255     4.77e-05       0.0954        0.124       0.0828        0.121        0.102        0.108        0.151        0.129        0.413       0.0043\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              11   42.910    0.005       0.0259     0.000217       0.0261       0.0961        0.125       0.0845        0.119        0.102        0.109        0.151         0.13        0.956      0.00995\n","! Validation         11   42.910    0.005       0.0267     3.65e-05       0.0268       0.0974        0.126       0.0851        0.122        0.104         0.11        0.154        0.132        0.358      0.00373\n","Wall time: 42.91115726400017\n","! Best model       11    0.027\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     12     1       0.0215       0.0212     0.000294       0.0871        0.113       0.0743        0.113       0.0935       0.0946        0.142        0.118         1.26       0.0131\n","     12     2       0.0227       0.0227     5.32e-05       0.0899        0.116       0.0787        0.112       0.0954        0.101        0.143        0.122        0.518      0.00539\n","     12     3       0.0237       0.0234     0.000267       0.0921        0.118       0.0824        0.112        0.097        0.106         0.14        0.123         1.13       0.0118\n","     12     4       0.0235       0.0233     0.000129       0.0894        0.118       0.0775        0.113       0.0953        0.103        0.143        0.123        0.706      0.00736\n","     12     5       0.0229       0.0228     3.67e-05         0.09        0.117       0.0782        0.113       0.0959        0.101        0.144        0.122        0.427      0.00445\n","     12     6       0.0228       0.0227     0.000118        0.089        0.117       0.0772        0.113       0.0949        0.101        0.143        0.122        0.752      0.00783\n","     12     7       0.0224       0.0224     7.36e-05       0.0895        0.116       0.0779        0.113       0.0953       0.0999        0.142        0.121         0.55      0.00573\n","     12     8       0.0246       0.0246     4.34e-05       0.0931        0.121       0.0801        0.119       0.0996        0.104        0.151        0.127        0.421      0.00438\n","     12     9        0.023       0.0229     4.57e-05       0.0922        0.117       0.0821        0.112       0.0972        0.104        0.139        0.122        0.439      0.00458\n","     12    10       0.0239       0.0238     5.45e-05       0.0902        0.119        0.078        0.115       0.0964        0.104        0.145        0.125        0.477      0.00497\n","     12    11       0.0253       0.0251     0.000162       0.0956        0.123       0.0861        0.115          0.1         0.11        0.144        0.127         0.88      0.00916\n","     12    12       0.0304       0.0304     3.77e-05        0.105        0.135       0.0917        0.131        0.111        0.117        0.165        0.141        0.433      0.00451\n","     12    13       0.0255       0.0254     3.54e-05       0.0957        0.123        0.081        0.125        0.103        0.104        0.155        0.129        0.372      0.00388\n","     12    14       0.0242       0.0242     3.47e-05       0.0919         0.12       0.0766        0.123       0.0996       0.0994        0.154        0.127        0.403       0.0042\n","     12    15       0.0239       0.0238     8.47e-05       0.0927        0.119       0.0821        0.114        0.098        0.105        0.144        0.125        0.615       0.0064\n","     12    16       0.0223       0.0223     1.32e-05       0.0904        0.116       0.0824        0.106       0.0944        0.106        0.133        0.119        0.223      0.00232\n","     12    17       0.0226       0.0224     0.000184       0.0893        0.116       0.0772        0.114       0.0954       0.0989        0.144        0.121        0.989       0.0103\n","     12    18       0.0279       0.0274     0.000409        0.101        0.128       0.0939        0.114        0.104         0.12        0.143        0.132         1.47       0.0154\n","     12    19       0.0276       0.0276     3.49e-05       0.0977        0.129       0.0867         0.12        0.103        0.114        0.154        0.134         0.38      0.00396\n","     12    20       0.0211       0.0203     0.000782       0.0846         0.11       0.0722        0.109       0.0907       0.0939        0.137        0.115         2.04       0.0212\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     12     1       0.0269       0.0268     3.61e-05       0.0968        0.127       0.0842        0.122        0.103        0.109        0.156        0.133        0.314      0.00327\n","     12     2       0.0256       0.0256     2.47e-05        0.095        0.124       0.0825         0.12        0.101        0.108        0.151        0.129        0.284      0.00296\n","     12     3       0.0223       0.0223     2.86e-05       0.0894        0.115        0.078        0.112       0.0951          0.1        0.141        0.121        0.346       0.0036\n","     12     4       0.0236       0.0235     5.89e-05       0.0912        0.119       0.0793        0.115       0.0972        0.103        0.144        0.124        0.505      0.00526\n","     12     5       0.0233       0.0233     5.24e-05       0.0911        0.118       0.0787        0.116       0.0973        0.102        0.145        0.123        0.435      0.00453\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              12   45.947    0.005       0.0239     0.000145       0.0241       0.0923         0.12       0.0808        0.115        0.098        0.104        0.145        0.125        0.724      0.00754\n","! Validation         12   45.947    0.005       0.0243     4.01e-05       0.0243       0.0927        0.121       0.0806        0.117       0.0988        0.105        0.147        0.126        0.377      0.00392\n","Wall time: 45.94793806400003\n","! Best model       12    0.024\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     13     1       0.0221       0.0217     0.000443        0.088        0.114       0.0771         0.11       0.0934       0.0994        0.138        0.119          1.5       0.0156\n","     13     2       0.0251       0.0251     1.48e-05       0.0952        0.123       0.0852        0.115          0.1         0.11        0.145        0.127         0.21      0.00219\n","     13     3        0.024       0.0233     0.000652       0.0904        0.118       0.0775        0.116       0.0969          0.1        0.148        0.124         1.86       0.0194\n","     13     4       0.0212       0.0205     0.000663       0.0858        0.111       0.0734         0.11        0.092       0.0945        0.138        0.116         1.89       0.0197\n","     13     5       0.0316       0.0316     5.97e-05        0.104        0.137       0.0884        0.136        0.112        0.115        0.174        0.144        0.479      0.00498\n","     13     6       0.0253       0.0249     0.000327       0.0946        0.122       0.0867         0.11       0.0985        0.113        0.139        0.126         1.26       0.0131\n","     13     7       0.0212       0.0209     0.000295       0.0852        0.112       0.0742        0.107       0.0907       0.0974        0.136        0.117         1.18       0.0123\n","     13     8       0.0211       0.0211     1.33e-05       0.0872        0.112       0.0775        0.107       0.0921       0.0984        0.136        0.117        0.196      0.00205\n","     13     9       0.0209       0.0204     0.000484       0.0836         0.11       0.0714        0.108       0.0896       0.0947        0.137        0.116         1.62       0.0169\n","     13    10       0.0205       0.0203     0.000187       0.0852         0.11       0.0755        0.105       0.0901       0.0976        0.132        0.115        0.974       0.0101\n","     13    11       0.0169       0.0168     4.34e-05        0.078          0.1       0.0684       0.0973       0.0828       0.0867        0.123        0.105        0.387      0.00403\n","     13    12       0.0207       0.0204     0.000299       0.0853        0.111       0.0718        0.112       0.0921       0.0927         0.14        0.116         1.23       0.0128\n","     13    13       0.0207       0.0201      0.00059        0.085         0.11       0.0746        0.106       0.0902       0.0968        0.132        0.114         1.72       0.0179\n","     13    14       0.0244       0.0244     1.42e-05        0.094        0.121       0.0809         0.12          0.1        0.104        0.149        0.126        0.261      0.00272\n","     13    15       0.0214       0.0211     0.000246       0.0873        0.112       0.0765        0.109       0.0927       0.0981        0.137        0.117          1.1       0.0115\n","     13    16       0.0228       0.0226     0.000274       0.0895        0.116       0.0788        0.111       0.0949        0.103        0.139        0.121         1.19       0.0124\n","     13    17       0.0224       0.0224     2.81e-05       0.0898        0.116        0.081        0.107       0.0943        0.104        0.137         0.12        0.254      0.00265\n","     13    18       0.0214       0.0214     1.59e-05       0.0871        0.113       0.0761        0.109       0.0926       0.0977        0.139        0.118        0.237      0.00246\n","     13    19       0.0189       0.0189     2.44e-05       0.0822        0.106       0.0705        0.106        0.088         0.09        0.133        0.112        0.279      0.00291\n","     13    20       0.0219       0.0219     1.36e-05       0.0883        0.114       0.0765        0.112       0.0942       0.0989         0.14         0.12        0.228      0.00237\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     13     1       0.0247       0.0246     3.48e-05       0.0926        0.121       0.0802        0.117       0.0988        0.104         0.15        0.127        0.339      0.00353\n","     13     2       0.0236       0.0235     2.08e-05       0.0909        0.119       0.0783        0.116       0.0972        0.102        0.146        0.124        0.271      0.00282\n","     13     3       0.0206       0.0205     2.25e-05       0.0857        0.111       0.0742        0.109       0.0915       0.0954        0.137        0.116        0.303      0.00316\n","     13     4       0.0217       0.0217      5.1e-05       0.0874        0.114       0.0757        0.111       0.0932       0.0989        0.139        0.119         0.47      0.00489\n","     13     5       0.0215       0.0215     4.53e-05       0.0875        0.113        0.075        0.112       0.0937        0.097         0.14        0.119        0.403      0.00419\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              13   48.987    0.005        0.022     0.000234       0.0222       0.0883        0.115       0.0771        0.111       0.0939       0.0998         0.14         0.12        0.903      0.00941\n","! Validation         13   48.987    0.005       0.0224     3.49e-05       0.0224       0.0888        0.116       0.0767        0.113       0.0949       0.0996        0.143        0.121        0.357      0.00372\n","Wall time: 48.987467269000035\n","! Best model       13    0.022\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     14     1       0.0237       0.0237     2.28e-05       0.0912        0.119       0.0765        0.121       0.0985          0.1         0.15        0.125        0.254      0.00264\n","     14     2         0.02         0.02     9.54e-06        0.082        0.109       0.0689        0.108       0.0886       0.0897        0.141        0.115        0.197      0.00205\n","     14     3       0.0187       0.0187     4.21e-05       0.0816        0.106       0.0692        0.106       0.0877       0.0892        0.133        0.111        0.376      0.00392\n","     14     4        0.021       0.0209     2.43e-05       0.0866        0.112       0.0746        0.111       0.0926       0.0982        0.135        0.117        0.304      0.00316\n","     14     5       0.0209       0.0208     0.000104       0.0858        0.111       0.0743        0.109       0.0915       0.0963        0.137        0.117        0.644      0.00671\n","     14     6       0.0195       0.0194     0.000101       0.0834        0.108        0.072        0.106       0.0891       0.0922        0.134        0.113        0.714      0.00743\n","     14     7       0.0187       0.0186      8.2e-05       0.0807        0.105        0.069        0.104       0.0865       0.0884        0.133        0.111        0.465      0.00484\n","     14     8         0.02         0.02     2.95e-05       0.0832        0.109        0.071        0.108       0.0893       0.0935        0.135        0.114        0.317       0.0033\n","     14     9       0.0192       0.0191     0.000127       0.0829        0.107        0.071        0.107       0.0888       0.0903        0.134        0.112        0.812      0.00845\n","     14    10       0.0201       0.0199     0.000171       0.0844        0.109       0.0702        0.113       0.0916       0.0909        0.139        0.115        0.925      0.00964\n","     14    11         0.02         0.02      2.4e-05       0.0813        0.109       0.0708        0.102       0.0865       0.0953        0.133        0.114        0.313      0.00326\n","     14    12       0.0216       0.0216     4.04e-05       0.0884        0.114       0.0814        0.102       0.0919        0.104        0.131        0.117        0.399      0.00416\n","     14    13       0.0219       0.0218     9.94e-05        0.088        0.114       0.0758        0.113       0.0941       0.0972        0.143         0.12        0.645      0.00672\n","     14    14        0.019        0.019     3.73e-05       0.0808        0.107       0.0679        0.107       0.0873       0.0877        0.137        0.112        0.413       0.0043\n","     14    15       0.0208       0.0207     3.22e-05       0.0856        0.111       0.0713        0.114       0.0927       0.0907        0.144        0.117        0.351      0.00365\n","     14    16       0.0246       0.0246     4.06e-05        0.094        0.121       0.0827        0.117       0.0997        0.105        0.148        0.127        0.438      0.00457\n","     14    17       0.0256       0.0255     8.82e-05       0.0969        0.123       0.0874        0.116        0.102        0.109        0.147        0.128        0.575      0.00599\n","     14    18       0.0186       0.0185     6.55e-06       0.0806        0.105       0.0684        0.105       0.0867       0.0881        0.133        0.111        0.174      0.00181\n","     14    19       0.0211       0.0209     0.000207        0.086        0.112        0.075        0.108       0.0916       0.0983        0.135        0.117        0.981       0.0102\n","     14    20       0.0301         0.03     6.15e-05        0.104        0.134       0.0896        0.131        0.111        0.114        0.167         0.14        0.504      0.00525\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     14     1       0.0228       0.0228     3.55e-05       0.0889        0.117       0.0765        0.114       0.0951       0.0994        0.145        0.122         0.32      0.00333\n","     14     2        0.022       0.0219     2.45e-05       0.0873        0.115       0.0746        0.113       0.0936       0.0981        0.142         0.12         0.29      0.00302\n","     14     3       0.0191       0.0191     2.77e-05       0.0825        0.107       0.0708        0.106       0.0883       0.0911        0.133        0.112        0.339      0.00353\n","     14     4       0.0203       0.0202     5.58e-05       0.0841         0.11       0.0726        0.107       0.0899        0.095        0.135        0.115        0.492      0.00512\n","     14     5         0.02       0.0199     5.08e-05       0.0842        0.109       0.0717        0.109       0.0905       0.0925        0.137        0.115        0.428      0.00446\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              14   52.026    0.005       0.0212     6.75e-05       0.0212       0.0864        0.113       0.0743         0.11       0.0924       0.0962         0.14        0.118         0.49       0.0051\n","! Validation         14   52.026    0.005       0.0208     3.89e-05       0.0208       0.0854        0.112       0.0732         0.11       0.0915       0.0952        0.138        0.117        0.374      0.00389\n","Wall time: 52.026962324000124\n","! Best model       14    0.021\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     15     1       0.0266       0.0266     4.94e-05       0.0984        0.126       0.0902        0.115        0.103        0.115        0.146         0.13        0.428      0.00445\n","     15     2       0.0164       0.0162     0.000186       0.0752       0.0985       0.0666       0.0924       0.0795       0.0863        0.119        0.103        0.996       0.0104\n","     15     3       0.0227       0.0226      0.00016       0.0892        0.116       0.0783        0.111       0.0946        0.103        0.139        0.121        0.836       0.0087\n","     15     4       0.0274       0.0274     8.43e-05        0.101        0.128       0.0907        0.122        0.106        0.114        0.152        0.133        0.491      0.00512\n","     15     5       0.0243       0.0243     7.77e-05       0.0942         0.12       0.0806        0.121        0.101        0.101        0.152        0.127         0.63      0.00656\n","     15     6       0.0176       0.0176      5.5e-05       0.0786        0.103       0.0682       0.0996       0.0839       0.0891        0.125        0.107        0.447      0.00465\n","     15     7       0.0227       0.0227     4.61e-05       0.0906        0.116        0.078        0.116       0.0969       0.0999        0.144        0.122        0.399      0.00415\n","     15     8       0.0238       0.0235     0.000327       0.0916        0.118       0.0816        0.112       0.0966        0.107        0.139        0.123         1.32       0.0137\n","     15     9       0.0185       0.0185     4.14e-05       0.0803        0.105        0.066        0.109       0.0875        0.085        0.137        0.111        0.421      0.00439\n","     15    10       0.0252        0.025     0.000197       0.0944        0.122       0.0847        0.114       0.0993        0.109        0.145        0.127        0.969       0.0101\n","     15    11       0.0193       0.0193     2.72e-05       0.0816        0.107       0.0694        0.106       0.0877       0.0909        0.134        0.113        0.334      0.00348\n","     15    12       0.0216       0.0216     4.81e-05        0.086        0.114       0.0731        0.112       0.0924       0.0969        0.141        0.119        0.478      0.00498\n","     15    13       0.0205       0.0205     2.43e-05       0.0852        0.111       0.0768        0.102       0.0894        0.097        0.134        0.116          0.3      0.00313\n","     15    14       0.0171       0.0171     5.07e-05       0.0798        0.101       0.0695        0.101        0.085       0.0888        0.122        0.105        0.479      0.00499\n","     15    15       0.0167       0.0166     6.16e-05       0.0781       0.0998       0.0671          0.1       0.0836       0.0854        0.124        0.105        0.536      0.00559\n","     15    16       0.0199       0.0199     3.64e-05       0.0841        0.109       0.0747        0.103       0.0889       0.0967         0.13        0.114        0.383      0.00399\n","     15    17       0.0185       0.0184     1.97e-05       0.0816        0.105       0.0694        0.106       0.0876       0.0892        0.131         0.11        0.262      0.00273\n","     15    18       0.0189       0.0189     2.89e-05       0.0792        0.106       0.0658        0.106       0.0859       0.0853        0.139        0.112        0.346       0.0036\n","     15    19       0.0211       0.0211     2.22e-05       0.0843        0.112       0.0712        0.111       0.0909       0.0942        0.142        0.118        0.296      0.00309\n","     15    20       0.0176       0.0175     7.71e-05       0.0773        0.102       0.0657        0.101       0.0832       0.0869        0.128        0.107        0.557      0.00581\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     15     1       0.0215       0.0215      3.5e-05       0.0862        0.113       0.0738        0.111       0.0924       0.0959        0.142        0.119        0.341      0.00355\n","     15     2       0.0207       0.0207      2.2e-05       0.0847        0.111       0.0719         0.11       0.0911       0.0947        0.139        0.117        0.282      0.00294\n","     15     3        0.018        0.018      2.4e-05         0.08        0.104       0.0683        0.103       0.0859       0.0879         0.13        0.109        0.312      0.00325\n","     15     4       0.0192       0.0192     5.12e-05       0.0818        0.107       0.0705        0.105       0.0875        0.092        0.132        0.112        0.469      0.00489\n","     15     5       0.0189       0.0189     4.63e-05       0.0817        0.106       0.0692        0.107       0.0879       0.0893        0.134        0.112        0.409      0.00426\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              15   55.074    0.005       0.0208      8.1e-05       0.0208       0.0855        0.111       0.0744        0.108       0.0911       0.0965        0.137        0.117        0.545      0.00568\n","! Validation         15   55.074    0.005       0.0196     3.57e-05       0.0197       0.0829        0.108       0.0707        0.107        0.089        0.092        0.135        0.114        0.362      0.00377\n","Wall time: 55.074531191999995\n","! Best model       15    0.020\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     16     1       0.0216       0.0215     3.18e-05       0.0879        0.114       0.0747        0.114       0.0945        0.096        0.142        0.119        0.371      0.00386\n","     16     2       0.0194       0.0194     5.82e-05       0.0819        0.108       0.0696        0.107       0.0881       0.0914        0.134        0.113        0.489      0.00509\n","     16     3       0.0188       0.0188     3.63e-05       0.0799        0.106       0.0672        0.105       0.0862        0.089        0.134        0.111        0.352      0.00366\n","     16     4       0.0175       0.0175     4.71e-05       0.0779        0.102       0.0668          0.1       0.0834       0.0874        0.127        0.107        0.436      0.00455\n","     16     5       0.0154       0.0154        4e-05       0.0754        0.096       0.0643       0.0975       0.0809       0.0821        0.119        0.101        0.415      0.00433\n","     16     6       0.0199       0.0198     9.58e-06       0.0825        0.109       0.0707        0.106       0.0883        0.094        0.134        0.114        0.207      0.00216\n","     16     7       0.0177       0.0176     6.79e-05       0.0798        0.103       0.0701       0.0992       0.0846       0.0892        0.125        0.107        0.559      0.00582\n","     16     8        0.016        0.016     1.43e-05       0.0745       0.0979       0.0636       0.0964         0.08       0.0828        0.123        0.103        0.222      0.00231\n","     16     9       0.0178       0.0177     4.81e-05       0.0796        0.103        0.068        0.103       0.0854       0.0868        0.129        0.108         0.39      0.00407\n","     16    10       0.0196       0.0195     7.65e-05        0.082        0.108       0.0692        0.108       0.0885       0.0916        0.135        0.113        0.572      0.00596\n","     16    11       0.0165       0.0165      1.3e-05       0.0768       0.0994       0.0651          0.1       0.0827       0.0836        0.125        0.104        0.226      0.00235\n","     16    12       0.0165       0.0165     8.29e-05       0.0761       0.0993        0.065       0.0983       0.0816       0.0846        0.123        0.104        0.644       0.0067\n","     16    13       0.0174       0.0174     1.14e-05       0.0773        0.102       0.0644        0.103       0.0838       0.0832        0.132        0.107        0.245      0.00255\n","     16    14       0.0183       0.0183     2.92e-05       0.0806        0.105        0.069        0.104       0.0865       0.0897        0.129        0.109         0.37      0.00386\n","     16    15       0.0159       0.0158     9.12e-05       0.0744       0.0973       0.0634       0.0965         0.08        0.082        0.122        0.102        0.619      0.00645\n","     16    16       0.0177       0.0176     9.87e-05       0.0791        0.103       0.0665        0.104       0.0854       0.0858         0.13        0.108        0.593      0.00618\n","     16    17       0.0163       0.0162     4.63e-05       0.0763       0.0986        0.064        0.101       0.0824         0.08        0.128        0.104        0.424      0.00441\n","     16    18       0.0169       0.0168     7.67e-05       0.0774          0.1        0.067       0.0981       0.0826       0.0856        0.125        0.105        0.627      0.00653\n","     16    19       0.0161       0.0161      3.8e-05       0.0751       0.0981       0.0635       0.0981       0.0808       0.0834        0.122        0.103        0.387      0.00403\n","     16    20       0.0155       0.0155     2.91e-05        0.073       0.0962       0.0618       0.0953       0.0786       0.0813        0.121        0.101        0.311      0.00323\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     16     1       0.0203       0.0202     3.54e-05       0.0837         0.11       0.0712        0.109       0.0899       0.0927        0.138        0.116        0.347      0.00362\n","     16     2       0.0196       0.0196     2.23e-05       0.0821        0.108       0.0693        0.108       0.0885       0.0914        0.136        0.114        0.286      0.00298\n","     16     3        0.017        0.017     2.42e-05       0.0777        0.101       0.0659        0.101       0.0836       0.0849        0.127        0.106        0.311      0.00324\n","     16     4       0.0182       0.0181     5.12e-05       0.0795        0.104       0.0682        0.102       0.0851       0.0891        0.129        0.109        0.468      0.00487\n","     16     5       0.0179       0.0178     4.63e-05       0.0792        0.103       0.0667        0.104       0.0855       0.0861        0.131        0.109        0.408      0.00425\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              16   58.114    0.005       0.0175     4.73e-05       0.0175       0.0784        0.102       0.0667        0.102       0.0842       0.0866        0.128        0.107        0.423      0.00441\n","! Validation         16   58.114    0.005       0.0186     3.59e-05       0.0186       0.0804        0.105       0.0683        0.105       0.0865       0.0889        0.132        0.111        0.364      0.00379\n","Wall time: 58.114671658000134\n","! Best model       16    0.019\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     17     1       0.0196       0.0195     8.05e-06       0.0792        0.108       0.0659        0.106       0.0859       0.0912        0.136        0.114        0.168      0.00175\n","     17     2       0.0163       0.0162     0.000145       0.0758       0.0985       0.0639       0.0996       0.0818       0.0813        0.126        0.104        0.864        0.009\n","     17     3       0.0156       0.0156      3.9e-05       0.0746       0.0966       0.0641       0.0956       0.0799        0.082        0.121        0.101        0.428      0.00446\n","     17     4       0.0156       0.0155     0.000106       0.0752       0.0963       0.0647       0.0964       0.0805       0.0808        0.122        0.101        0.688      0.00717\n","     17     5       0.0164       0.0163     5.82e-05       0.0756       0.0989       0.0633          0.1       0.0818       0.0807        0.128        0.104        0.515      0.00536\n","     17     6       0.0195       0.0195     1.39e-05       0.0822        0.108       0.0706        0.105        0.088       0.0929        0.133        0.113        0.229      0.00239\n","     17     7       0.0161       0.0159      0.00017       0.0758       0.0976       0.0653       0.0968        0.081       0.0823        0.123        0.102        0.929      0.00968\n","     17     8       0.0168       0.0167     0.000107       0.0769          0.1       0.0636        0.103       0.0836       0.0814         0.13        0.105        0.703      0.00733\n","     17     9       0.0169       0.0168     0.000101       0.0773          0.1       0.0659          0.1        0.083       0.0851        0.125        0.105         0.61      0.00636\n","     17    10       0.0159       0.0155     0.000377       0.0745       0.0965       0.0643       0.0949       0.0796       0.0833        0.118        0.101         1.37       0.0142\n","     17    11       0.0181        0.018     7.06e-05       0.0801        0.104        0.072       0.0963       0.0842       0.0931        0.122        0.108        0.537      0.00559\n","     17    12       0.0182       0.0181     7.18e-05       0.0795        0.104       0.0669        0.105       0.0858       0.0884         0.13        0.109        0.468      0.00488\n","     17    13       0.0185       0.0181      0.00035       0.0795        0.104       0.0653        0.108       0.0865       0.0846        0.135         0.11         1.32       0.0138\n","     17    14       0.0156       0.0155     0.000136       0.0748       0.0962       0.0647        0.095       0.0798        0.083        0.118        0.101        0.859      0.00895\n","     17    15       0.0168       0.0167     5.49e-05       0.0772          0.1        0.064        0.104       0.0838       0.0826        0.128        0.105        0.486      0.00507\n","     17    16       0.0172       0.0171     9.93e-05       0.0777        0.101       0.0687       0.0956       0.0821       0.0899         0.12        0.105        0.657      0.00685\n","     17    17       0.0149       0.0148     3.84e-05        0.072       0.0942       0.0599       0.0964       0.0781       0.0765        0.122       0.0993        0.385      0.00401\n","     17    18       0.0159       0.0159     1.51e-05       0.0747       0.0975       0.0614        0.101       0.0814       0.0803        0.125        0.103        0.243      0.00253\n","     17    19       0.0162        0.016     0.000161       0.0741        0.098       0.0617       0.0988       0.0803       0.0808        0.126        0.103        0.816       0.0085\n","     17    20       0.0171        0.017     8.54e-05       0.0781        0.101       0.0664        0.101       0.0839       0.0844        0.128        0.106        0.534      0.00556\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     17     1       0.0192       0.0192      3.6e-05       0.0813        0.107       0.0688        0.106       0.0876       0.0898        0.135        0.113        0.344      0.00359\n","     17     2       0.0186       0.0185     2.38e-05       0.0798        0.105        0.067        0.105       0.0862       0.0884        0.133        0.111        0.293      0.00306\n","     17     3       0.0162       0.0162     2.63e-05       0.0755       0.0983       0.0637       0.0993       0.0815       0.0821        0.125        0.103        0.325      0.00339\n","     17     4       0.0173       0.0173     5.33e-05       0.0775        0.102       0.0663       0.0998       0.0831       0.0865        0.127        0.107        0.477      0.00497\n","     17     5        0.017       0.0169     4.82e-05        0.077        0.101       0.0645        0.102       0.0833       0.0833        0.128        0.106        0.417      0.00435\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              17   61.179    0.005       0.0167      0.00011       0.0169       0.0767          0.1       0.0651          0.1       0.0825       0.0844        0.126        0.105         0.64      0.00667\n","! Validation         17   61.179    0.005       0.0176     3.75e-05       0.0176       0.0782        0.103       0.0661        0.103       0.0843       0.0861         0.13        0.108        0.372      0.00387\n","Wall time: 61.17937085900007\n","! Best model       17    0.018\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     18     1       0.0155       0.0154     9.06e-05       0.0721        0.096        0.062       0.0924       0.0772       0.0831        0.118          0.1        0.667      0.00695\n","     18     2       0.0146       0.0143      0.00031       0.0717       0.0925       0.0609       0.0932       0.0771       0.0783        0.116       0.0971         1.26       0.0131\n","     18     3       0.0163       0.0162     0.000105       0.0744       0.0986       0.0622       0.0988       0.0805        0.081        0.127        0.104        0.608      0.00633\n","     18     4       0.0167       0.0165     0.000218       0.0769       0.0993        0.068       0.0947       0.0813       0.0856        0.122        0.104         1.03       0.0108\n","     18     5       0.0194       0.0191     0.000271       0.0836        0.107       0.0723        0.106       0.0892        0.091        0.133        0.112         1.21       0.0126\n","     18     6       0.0176       0.0174     0.000201       0.0782        0.102       0.0675       0.0997       0.0836       0.0874        0.126        0.107        0.989       0.0103\n","     18     7       0.0164       0.0163     4.31e-05       0.0751       0.0988       0.0622        0.101       0.0816       0.0813        0.127        0.104        0.368      0.00384\n","     18     8       0.0189       0.0185     0.000429        0.082        0.105       0.0701        0.106       0.0879       0.0894        0.131         0.11         1.52       0.0159\n","     18     9       0.0236       0.0233     0.000259       0.0907        0.118       0.0782        0.116        0.097        0.102        0.145        0.124         1.14       0.0118\n","     18    10       0.0191       0.0191     3.55e-05       0.0831        0.107       0.0726        0.104       0.0884       0.0931         0.13        0.112        0.358      0.00373\n","     18    11        0.015       0.0148     0.000216       0.0733       0.0942       0.0625       0.0949       0.0787       0.0795        0.118       0.0989         1.04       0.0108\n","     18    12       0.0172        0.017     0.000198       0.0765        0.101       0.0644        0.101       0.0825       0.0827         0.13        0.106        0.994       0.0104\n","     18    13       0.0209       0.0208     6.73e-05       0.0855        0.112       0.0736        0.109       0.0914       0.0963        0.137        0.117         0.54      0.00563\n","     18    14       0.0178       0.0177     0.000118       0.0788        0.103       0.0677        0.101       0.0844       0.0877        0.128        0.108        0.717      0.00747\n","     18    15       0.0151        0.015     2.29e-05       0.0719       0.0948       0.0605       0.0947       0.0776       0.0797         0.12       0.0996        0.324      0.00337\n","     18    16       0.0167       0.0167     8.97e-05       0.0776       0.0998       0.0658        0.101       0.0834       0.0846        0.125        0.105        0.577      0.00601\n","     18    17       0.0158       0.0155     0.000233       0.0739       0.0964       0.0633       0.0951       0.0792       0.0821         0.12        0.101         1.13       0.0117\n","     18    18       0.0143       0.0142     5.09e-05       0.0708       0.0923       0.0587        0.095       0.0769       0.0757        0.119       0.0972        0.439      0.00458\n","     18    19       0.0155       0.0152      0.00034        0.073       0.0954       0.0628       0.0933       0.0781       0.0807        0.119          0.1         1.28       0.0133\n","     18    20       0.0152       0.0148     0.000356       0.0736       0.0943       0.0636       0.0936       0.0786       0.0811        0.116       0.0986         1.37       0.0143\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     18     1       0.0183       0.0183     3.66e-05       0.0793        0.105       0.0668        0.104       0.0855       0.0872        0.133         0.11        0.369      0.00385\n","     18     2       0.0177       0.0177     2.17e-05       0.0778        0.103       0.0651        0.103       0.0842       0.0859         0.13        0.108        0.282      0.00294\n","     18     3       0.0154       0.0154     2.25e-05       0.0736        0.096       0.0618       0.0973       0.0795       0.0797        0.122        0.101        0.292      0.00304\n","     18     4       0.0165       0.0165      4.9e-05       0.0757       0.0994       0.0646        0.098       0.0813       0.0842        0.124        0.104        0.454      0.00473\n","     18     5       0.0162       0.0161     4.33e-05       0.0751       0.0983       0.0626       0.0999       0.0813        0.081        0.126        0.103        0.393      0.00409\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              18   64.236    0.005       0.0169     0.000183       0.0171       0.0771        0.101       0.0659       0.0995       0.0827       0.0854        0.126        0.105        0.878      0.00914\n","! Validation         18   64.236    0.005       0.0168     3.46e-05       0.0168       0.0763          0.1       0.0642        0.101       0.0824       0.0836        0.127        0.105        0.358      0.00373\n","Wall time: 64.23677021900016\n","! Best model       18    0.017\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     19     1       0.0161       0.0161     7.44e-06       0.0744       0.0981       0.0631       0.0971       0.0801       0.0834        0.122        0.103        0.161      0.00167\n","     19     2       0.0174       0.0171     0.000336       0.0758        0.101       0.0617        0.104       0.0828       0.0823        0.131        0.107         1.31       0.0137\n","     19     3        0.016       0.0156      0.00043       0.0741       0.0965       0.0634       0.0954       0.0794       0.0818        0.121        0.101         1.53       0.0159\n","     19     4        0.016       0.0159     0.000118       0.0748       0.0975       0.0661       0.0923       0.0792       0.0861        0.117        0.102        0.703      0.00733\n","     19     5       0.0164       0.0161     0.000324       0.0742       0.0981       0.0632       0.0962       0.0797       0.0825        0.124        0.103         1.29       0.0135\n","     19     6       0.0151        0.015     0.000127       0.0732       0.0946       0.0618       0.0959       0.0789       0.0794        0.119       0.0994        0.777       0.0081\n","     19     7       0.0185       0.0184      1.9e-05       0.0814        0.105       0.0685        0.107       0.0878       0.0869        0.134        0.111        0.257      0.00267\n","     19     8       0.0181       0.0178     0.000311       0.0797        0.103       0.0689        0.101       0.0851       0.0873        0.129        0.108         1.24       0.0129\n","     19     9       0.0139       0.0137     0.000196       0.0706       0.0905       0.0618       0.0883        0.075       0.0783        0.111       0.0946        0.874       0.0091\n","     19    10       0.0135       0.0135     3.83e-05       0.0703       0.0897       0.0608       0.0894       0.0751       0.0756        0.113       0.0942        0.358      0.00372\n","     19    11       0.0153       0.0151     0.000184       0.0739       0.0952       0.0662       0.0892       0.0777       0.0844        0.114        0.099         0.97       0.0101\n","     19    12       0.0177       0.0176     0.000119       0.0781        0.103       0.0657        0.103       0.0843       0.0848        0.131        0.108        0.757      0.00788\n","     19    13       0.0156       0.0155     2.13e-05       0.0753       0.0964        0.063       0.0999       0.0815         0.08        0.123        0.101         0.32      0.00333\n","     19    14       0.0132       0.0129     0.000225       0.0689        0.088       0.0578       0.0913       0.0745       0.0723        0.113       0.0926         1.03       0.0108\n","     19    15       0.0168       0.0167     5.48e-05       0.0761          0.1       0.0636        0.101       0.0824       0.0834        0.127        0.105        0.432       0.0045\n","     19    16       0.0204       0.0204     4.01e-05       0.0844         0.11       0.0767          0.1       0.0883          0.1        0.128        0.114        0.439      0.00458\n","     19    17       0.0184       0.0182     0.000184       0.0803        0.104       0.0688        0.103        0.086        0.087        0.133         0.11        0.798      0.00831\n","     19    18       0.0166       0.0165     7.08e-05        0.076       0.0995       0.0632        0.102       0.0824       0.0803         0.13        0.105        0.554      0.00577\n","     19    19       0.0156       0.0156     3.69e-05       0.0725       0.0965       0.0597       0.0981       0.0789       0.0788        0.125        0.102        0.413       0.0043\n","     19    20       0.0201         0.02     9.93e-05       0.0843        0.109       0.0715         0.11       0.0907       0.0927        0.137        0.115         0.67      0.00697\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     19     1       0.0175       0.0175     3.65e-05       0.0774        0.102       0.0649        0.102       0.0836       0.0849         0.13        0.108        0.354      0.00368\n","     19     2       0.0169       0.0169     2.37e-05       0.0759          0.1       0.0632        0.101       0.0822       0.0835        0.128        0.106        0.292      0.00304\n","     19     3       0.0147       0.0147     2.62e-05       0.0719       0.0938         0.06       0.0956       0.0778       0.0775         0.12       0.0987        0.321      0.00334\n","     19     4       0.0159       0.0158     5.27e-05       0.0741       0.0974        0.063       0.0963       0.0796       0.0821        0.122        0.102        0.473      0.00492\n","     19     5       0.0155       0.0154     4.73e-05       0.0733       0.0962       0.0609       0.0982       0.0795       0.0788        0.124        0.101        0.414      0.00431\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              19   67.298    0.005       0.0164     0.000147       0.0165       0.0759        0.099       0.0648       0.0982       0.0815       0.0836        0.124        0.104        0.745      0.00776\n","! Validation         19   67.298    0.005       0.0161     3.73e-05       0.0161       0.0745       0.0981       0.0624       0.0987       0.0806       0.0814        0.125        0.103        0.371      0.00386\n","Wall time: 67.29879491099996\n","! Best model       19    0.016\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     20     1       0.0188       0.0187     0.000112       0.0833        0.106       0.0747          0.1       0.0875       0.0931        0.127         0.11        0.761      0.00793\n","     20     2       0.0184       0.0183     7.65e-05       0.0811        0.105       0.0685        0.106       0.0874       0.0871        0.133         0.11        0.571      0.00595\n","     20     3       0.0149       0.0147     0.000136       0.0717       0.0939       0.0611       0.0928       0.0769       0.0796        0.117       0.0985        0.793      0.00826\n","     20     4       0.0197       0.0197      6.4e-06       0.0852        0.109       0.0738        0.108       0.0908       0.0918        0.136        0.114        0.166      0.00173\n","     20     5       0.0165       0.0164     0.000111       0.0781        0.099       0.0692       0.0958       0.0825       0.0867         0.12        0.103        0.665      0.00692\n","     20     6       0.0145       0.0143     0.000153       0.0708       0.0926       0.0584       0.0957        0.077       0.0757        0.119       0.0975        0.857      0.00893\n","     20     7       0.0156       0.0155     5.65e-05       0.0739       0.0965       0.0641       0.0934       0.0788       0.0839        0.118        0.101        0.515      0.00536\n","     20     8       0.0179       0.0176     0.000337       0.0804        0.103       0.0708       0.0996       0.0852         0.09        0.124        0.107         1.27       0.0132\n","     20     9        0.015       0.0148     0.000191       0.0713       0.0941       0.0597       0.0946       0.0772       0.0782         0.12       0.0989        0.987       0.0103\n","     20    10       0.0138       0.0138     5.02e-05       0.0703       0.0908       0.0582       0.0946       0.0764       0.0754        0.116       0.0955        0.493      0.00514\n","     20    11       0.0145       0.0144     7.87e-05       0.0712       0.0929       0.0601       0.0932       0.0767        0.077        0.118       0.0977        0.609      0.00634\n","     20    12       0.0129       0.0128     0.000112       0.0671       0.0874        0.056       0.0893       0.0726       0.0724        0.111       0.0919        0.708      0.00738\n","     20    13       0.0165       0.0165     1.41e-05        0.075       0.0994       0.0616        0.102       0.0817       0.0822        0.127        0.105        0.199      0.00207\n","     20    14       0.0169       0.0168     8.48e-05       0.0747          0.1       0.0618          0.1       0.0812       0.0837        0.127        0.105        0.641      0.00668\n","     20    15       0.0139       0.0138     6.19e-05       0.0696       0.0909       0.0597       0.0894       0.0746       0.0761        0.115       0.0955        0.523      0.00545\n","     20    16       0.0141        0.014     2.28e-05       0.0703       0.0917         0.06       0.0907       0.0754       0.0771        0.115       0.0962        0.319      0.00332\n","     20    17       0.0171        0.017     0.000105       0.0784        0.101       0.0679       0.0995       0.0837       0.0866        0.124        0.105        0.587      0.00611\n","     20    18       0.0146       0.0146     5.26e-05       0.0717       0.0935       0.0601       0.0949       0.0775        0.076        0.121       0.0985        0.454      0.00473\n","     20    19       0.0149       0.0148     0.000113       0.0721       0.0941       0.0615       0.0933       0.0774       0.0807        0.116       0.0985        0.681      0.00709\n","     20    20       0.0153       0.0152     5.57e-05       0.0723       0.0955       0.0602       0.0964       0.0783       0.0789        0.122          0.1        0.486      0.00506\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     20     1       0.0168       0.0168     3.71e-05       0.0758          0.1       0.0633        0.101        0.082       0.0829        0.128        0.105        0.368      0.00383\n","     20     2       0.0162       0.0162     2.27e-05       0.0742       0.0984       0.0616       0.0994       0.0805       0.0815        0.126        0.104        0.287      0.00299\n","     20     3       0.0142       0.0141     2.44e-05       0.0704        0.092       0.0586       0.0941       0.0763       0.0757        0.118       0.0968        0.304      0.00317\n","     20     4       0.0153       0.0153      5.1e-05       0.0726       0.0956       0.0615       0.0948       0.0782       0.0802        0.121          0.1        0.463      0.00482\n","     20     5       0.0149       0.0148     4.47e-05       0.0718       0.0943       0.0594       0.0964       0.0779        0.077        0.122       0.0993        0.401      0.00418\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              20   70.360    0.005       0.0157     9.66e-05       0.0158       0.0744       0.0969       0.0634       0.0965       0.0799       0.0818        0.122        0.102        0.614       0.0064\n","! Validation         20   70.360    0.005       0.0154      3.6e-05       0.0155        0.073       0.0961       0.0609       0.0971        0.079       0.0795        0.123        0.101        0.365       0.0038\n","Wall time: 70.36056033299997\n","! Best model       20    0.015\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     21     1       0.0177       0.0176     0.000114       0.0796        0.103       0.0682        0.102       0.0853       0.0867        0.129        0.108        0.695      0.00724\n","     21     2        0.016        0.016     2.38e-05        0.075       0.0979       0.0664       0.0922       0.0793       0.0859        0.118        0.102        0.341      0.00355\n","     21     3       0.0148       0.0148     1.58e-05       0.0725       0.0941       0.0613       0.0947        0.078       0.0792        0.118       0.0988        0.252      0.00262\n","     21     4       0.0142       0.0142     2.29e-05       0.0691       0.0921       0.0577       0.0917       0.0747       0.0763        0.118       0.0969        0.284      0.00296\n","     21     5       0.0141        0.014     7.85e-05       0.0683       0.0916       0.0576       0.0897       0.0737       0.0775        0.115       0.0961        0.541      0.00564\n","     21     6       0.0133       0.0133     5.55e-05       0.0669       0.0891       0.0555       0.0899       0.0727        0.074        0.113       0.0937        0.475      0.00495\n","     21     7       0.0118       0.0117      6.2e-05       0.0653       0.0838       0.0565        0.083       0.0697       0.0721        0.103       0.0877         0.49      0.00511\n","     21     8       0.0119       0.0118     2.78e-05       0.0654       0.0841       0.0564       0.0833       0.0699        0.072        0.104       0.0881        0.317       0.0033\n","     21     9       0.0147       0.0147     2.16e-05       0.0717       0.0938       0.0607       0.0938       0.0772       0.0779        0.119       0.0987        0.305      0.00317\n","     21    10       0.0158       0.0158     4.14e-05       0.0726       0.0972       0.0592       0.0994       0.0793       0.0783        0.127        0.103         0.42      0.00437\n","     21    11       0.0157       0.0157     3.98e-05        0.074       0.0969       0.0627       0.0966       0.0796       0.0808        0.123        0.102        0.422       0.0044\n","     21    12       0.0161       0.0161     2.19e-05       0.0751       0.0981       0.0639       0.0976       0.0808       0.0836        0.122        0.103        0.321      0.00334\n","     21    13       0.0123       0.0122     3.98e-05       0.0662       0.0855       0.0559       0.0867       0.0713       0.0713        0.109       0.0899        0.421      0.00438\n","     21    14       0.0137       0.0137     2.35e-05       0.0687       0.0905       0.0572       0.0917       0.0744        0.074        0.117       0.0954         0.31      0.00323\n","     21    15       0.0198       0.0198     1.49e-05       0.0847        0.109       0.0751        0.104       0.0894       0.0952        0.132        0.114        0.239      0.00249\n","     21    16       0.0171        0.017     6.67e-05       0.0791        0.101         0.07       0.0973       0.0837       0.0881        0.123        0.105        0.523      0.00545\n","     21    17        0.014        0.014     3.78e-05       0.0702       0.0915       0.0599       0.0907       0.0753       0.0754        0.117       0.0964        0.417      0.00435\n","     21    18       0.0149       0.0149     2.66e-05       0.0716       0.0944       0.0588       0.0973        0.078       0.0773        0.122       0.0994        0.338      0.00352\n","     21    19       0.0129       0.0129     1.07e-05       0.0678       0.0878       0.0575       0.0884       0.0729       0.0728        0.112       0.0923        0.227      0.00236\n","     21    20       0.0128       0.0128     9.01e-05       0.0682       0.0874       0.0574       0.0898       0.0736       0.0725        0.111       0.0919        0.653       0.0068\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     21     1       0.0161       0.0161     3.75e-05       0.0743       0.0982       0.0619        0.099       0.0805       0.0811        0.126        0.103         0.37      0.00385\n","     21     2       0.0155       0.0155     2.31e-05       0.0726       0.0964       0.0601       0.0977       0.0789       0.0794        0.123        0.101        0.288        0.003\n","     21     3       0.0136       0.0136     2.52e-05        0.069       0.0903       0.0572       0.0926       0.0749       0.0739        0.116       0.0951        0.308      0.00321\n","     21     4       0.0148       0.0147     5.22e-05       0.0712       0.0938       0.0602       0.0932       0.0767       0.0784        0.119       0.0986        0.467      0.00487\n","     21     5       0.0143       0.0143     4.53e-05       0.0702       0.0924       0.0581       0.0946       0.0763       0.0753        0.119       0.0974        0.404      0.00421\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              21   73.424    0.005       0.0146     4.18e-05       0.0147       0.0716       0.0936       0.0609        0.093       0.0769       0.0788        0.118       0.0983          0.4      0.00416\n","! Validation         21   73.424    0.005       0.0148     3.67e-05       0.0149       0.0715       0.0942       0.0595       0.0954       0.0775       0.0777        0.121       0.0992        0.368      0.00383\n","Wall time: 73.42439689900016\n","! Best model       21    0.015\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     22     1       0.0145       0.0145     1.52e-05       0.0722       0.0933       0.0619        0.093       0.0774       0.0795        0.116       0.0977        0.239      0.00249\n","     22     2       0.0128       0.0127     8.61e-05       0.0668       0.0871       0.0553       0.0898       0.0725         0.07        0.114        0.092        0.582      0.00607\n","     22     3       0.0115       0.0115     3.14e-05       0.0631       0.0829       0.0532       0.0829        0.068       0.0693        0.105       0.0871        0.328      0.00342\n","     22     4       0.0118       0.0118     4.21e-05       0.0646        0.084       0.0544       0.0851       0.0698       0.0695        0.107       0.0885        0.451       0.0047\n","     22     5       0.0141        0.014     6.92e-05       0.0708       0.0916       0.0619       0.0884       0.0752       0.0787        0.113       0.0959         0.54      0.00563\n","     22     6       0.0153       0.0153     9.53e-06       0.0726       0.0957       0.0606       0.0966       0.0786       0.0794        0.122        0.101        0.217      0.00227\n","     22     7        0.014       0.0139     3.73e-05       0.0689       0.0913       0.0569        0.093       0.0749       0.0753        0.117       0.0961        0.362      0.00377\n","     22     8       0.0177       0.0175     0.000118       0.0799        0.102       0.0679        0.104        0.086       0.0858        0.129        0.108        0.737      0.00767\n","     22     9       0.0189       0.0188     0.000121       0.0829        0.106       0.0721        0.104       0.0883       0.0919         0.13        0.111          0.8      0.00833\n","     22    10       0.0147       0.0146     1.23e-05       0.0723       0.0936        0.062       0.0928       0.0774        0.079        0.117       0.0982        0.231      0.00241\n","     22    11       0.0142       0.0142     1.95e-05       0.0691       0.0922       0.0566        0.094       0.0753       0.0763        0.118        0.097        0.315      0.00328\n","     22    12       0.0169       0.0168     5.62e-05       0.0796          0.1       0.0697       0.0993       0.0845       0.0869        0.123        0.105        0.498      0.00519\n","     22    13       0.0182       0.0181     4.65e-05       0.0804        0.104       0.0708       0.0995       0.0852         0.09        0.128        0.109        0.456      0.00475\n","     22    14       0.0142       0.0142      1.8e-05       0.0698       0.0921       0.0572       0.0951       0.0762       0.0753        0.119       0.0971        0.276      0.00288\n","     22    15       0.0146       0.0146     3.29e-05       0.0725       0.0935        0.061       0.0953       0.0782       0.0777        0.119       0.0983          0.4      0.00417\n","     22    16       0.0188       0.0187     7.16e-05       0.0821        0.106       0.0723        0.102        0.087       0.0907        0.131        0.111        0.502      0.00523\n","     22    17       0.0177       0.0176     0.000112       0.0794        0.103       0.0721       0.0941       0.0831       0.0933        0.119        0.106        0.736      0.00767\n","     22    18       0.0143       0.0143     2.49e-05       0.0705       0.0924       0.0588       0.0939       0.0763       0.0769        0.117       0.0971        0.305      0.00318\n","     22    19       0.0144       0.0143     6.88e-05        0.071       0.0926       0.0612       0.0906       0.0759         0.08        0.114       0.0968        0.531      0.00553\n","     22    20       0.0169       0.0169     2.87e-05       0.0776        0.101       0.0664       0.0998       0.0831       0.0844        0.127        0.106        0.329      0.00342\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     22     1       0.0156       0.0155      3.8e-05       0.0729       0.0964       0.0605       0.0975        0.079       0.0794        0.124        0.101        0.374      0.00389\n","     22     2        0.015        0.015     2.31e-05       0.0712       0.0946       0.0588       0.0961       0.0774       0.0777        0.122       0.0997        0.287      0.00299\n","     22     3       0.0132       0.0131     2.53e-05       0.0677       0.0886        0.056       0.0911       0.0736       0.0723        0.114       0.0934        0.307       0.0032\n","     22     4       0.0143       0.0142     5.21e-05       0.0699       0.0923        0.059       0.0919       0.0754       0.0769        0.117       0.0971        0.466      0.00486\n","     22     5       0.0138       0.0138      4.5e-05        0.069       0.0908       0.0569        0.093        0.075       0.0739        0.118       0.0957        0.403      0.00419\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              22   76.494    0.005       0.0152     5.11e-05       0.0153       0.0733       0.0955       0.0626       0.0947       0.0786       0.0808        0.119          0.1        0.442       0.0046\n","! Validation         22   76.494    0.005       0.0143     3.67e-05       0.0144       0.0701       0.0926       0.0582       0.0939       0.0761       0.0761        0.119       0.0975        0.367      0.00383\n","Wall time: 76.49465867699996\n","! Best model       22    0.014\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     23     1       0.0158       0.0157     1.54e-05       0.0746       0.0971       0.0623       0.0993       0.0808       0.0795        0.125        0.102        0.274      0.00285\n","     23     2       0.0123       0.0122     0.000137       0.0651       0.0855       0.0534       0.0884       0.0709       0.0684        0.112       0.0902        0.728      0.00759\n","     23     3       0.0163       0.0162     7.32e-06       0.0759       0.0986       0.0666       0.0947       0.0806       0.0847        0.122        0.103         0.19      0.00198\n","     23     4       0.0198       0.0197      7.8e-05       0.0853        0.109       0.0775        0.101       0.0892       0.0984        0.126        0.112        0.567      0.00591\n","     23     5       0.0133       0.0133      3.1e-05       0.0681       0.0893        0.057       0.0902       0.0736       0.0751        0.112       0.0937        0.375      0.00391\n","     23     6       0.0154       0.0153     6.14e-05       0.0741       0.0957       0.0648       0.0926       0.0787        0.082        0.119          0.1        0.535      0.00557\n","     23     7       0.0237       0.0236     0.000108       0.0928        0.119       0.0819        0.115       0.0982        0.104        0.144        0.124        0.657      0.00684\n","     23     8       0.0203       0.0202     8.78e-05       0.0855         0.11       0.0745        0.108       0.0911       0.0943        0.136        0.115         0.58      0.00604\n","     23     9        0.015       0.0148     0.000161       0.0715       0.0941        0.059       0.0965       0.0778       0.0768        0.122       0.0992        0.872      0.00908\n","     23    10       0.0147       0.0147     4.64e-05       0.0707       0.0937       0.0612       0.0897       0.0754       0.0827        0.112       0.0976        0.441      0.00459\n","     23    11       0.0193       0.0193     1.82e-05       0.0834        0.107       0.0752       0.0997       0.0874       0.0962        0.127        0.112        0.264      0.00275\n","     23    12       0.0137       0.0135     0.000142       0.0677         0.09       0.0554       0.0923       0.0739       0.0719        0.118        0.095        0.821      0.00856\n","     23    13       0.0157       0.0156     0.000136       0.0749       0.0965       0.0661       0.0925       0.0793        0.083        0.119        0.101        0.854       0.0089\n","     23    14        0.018       0.0179     4.31e-05       0.0803        0.104       0.0691        0.103       0.0859       0.0886        0.128        0.108        0.357      0.00371\n","     23    15       0.0116       0.0115     0.000187        0.064       0.0828       0.0532       0.0855       0.0694       0.0675        0.107       0.0873        0.976       0.0102\n","     23    16        0.014        0.014      7.2e-05       0.0708       0.0914       0.0612       0.0898       0.0755       0.0777        0.114       0.0959        0.577      0.00601\n","     23    17        0.013       0.0129      5.4e-05       0.0673        0.088       0.0558       0.0903        0.073       0.0722        0.113       0.0927        0.494      0.00514\n","     23    18       0.0124       0.0123     0.000117       0.0664       0.0858       0.0557       0.0876       0.0717       0.0711        0.109       0.0903        0.738      0.00768\n","     23    19       0.0114       0.0112     0.000109       0.0642        0.082       0.0538       0.0849       0.0694       0.0675        0.105       0.0864         0.58      0.00604\n","     23    20       0.0143       0.0142      0.00011       0.0689       0.0921       0.0574       0.0918       0.0746       0.0771        0.116       0.0967         0.66      0.00687\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     23     1       0.0151        0.015     3.92e-05       0.0717       0.0949       0.0595       0.0961       0.0778        0.078        0.122       0.0999        0.397      0.00413\n","     23     2       0.0145       0.0145     2.22e-05       0.0701       0.0931       0.0576       0.0949       0.0763       0.0762         0.12       0.0981        0.279      0.00291\n","     23     3       0.0127       0.0127     2.33e-05       0.0666       0.0872        0.055       0.0898       0.0724        0.071        0.113       0.0919        0.293      0.00305\n","     23     4       0.0139       0.0138     5.05e-05       0.0689        0.091        0.058       0.0907       0.0743       0.0755        0.116       0.0957        0.453      0.00472\n","     23     5       0.0134       0.0133     4.26e-05       0.0678       0.0894        0.056       0.0915       0.0737       0.0726        0.116       0.0942        0.389      0.00405\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              23   79.577    0.005       0.0154      8.6e-05       0.0155       0.0736        0.096       0.0631       0.0946       0.0788       0.0816         0.12        0.101        0.577      0.00601\n","! Validation         23   79.577    0.005       0.0139     3.55e-05       0.0139        0.069       0.0911       0.0572       0.0926       0.0749       0.0747        0.117        0.096        0.362      0.00377\n","Wall time: 79.57758894300014\n","! Best model       23    0.014\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     24     1        0.012       0.0118     0.000232       0.0646        0.084       0.0529       0.0878       0.0704       0.0678        0.109       0.0886         1.08       0.0112\n","     24     2       0.0134       0.0133     7.14e-05       0.0677       0.0894       0.0566       0.0901       0.0733       0.0743        0.114        0.094        0.555      0.00578\n","     24     3       0.0123        0.012     0.000243       0.0644       0.0849       0.0531        0.087       0.0701       0.0685        0.111       0.0896         1.12       0.0117\n","     24     4       0.0117       0.0117     5.28e-05       0.0641       0.0837       0.0526       0.0871       0.0699       0.0677        0.109       0.0882        0.491      0.00511\n","     24     5       0.0135       0.0134      2.3e-05       0.0674       0.0897       0.0564       0.0893       0.0728       0.0725        0.117       0.0946        0.309      0.00322\n","     24     6       0.0117       0.0117     5.29e-05       0.0634       0.0836       0.0528       0.0847       0.0687       0.0682        0.108       0.0881        0.465      0.00484\n","     24     7       0.0111       0.0111     7.34e-06       0.0625       0.0815       0.0519       0.0838       0.0679       0.0665        0.105       0.0859        0.155      0.00161\n","     24     8       0.0124       0.0124      6.3e-06        0.066       0.0863       0.0538       0.0903       0.0721        0.069        0.113       0.0911        0.136      0.00142\n","     24     9       0.0105       0.0104     5.62e-05       0.0607       0.0791         0.05       0.0822       0.0661       0.0641        0.103       0.0834        0.478      0.00498\n","     24    10       0.0129       0.0129     3.45e-05       0.0664       0.0877       0.0558       0.0877       0.0718       0.0741         0.11       0.0921        0.367      0.00382\n","     24    11       0.0137       0.0136      6.5e-05       0.0689       0.0903       0.0587       0.0893        0.074       0.0766        0.113       0.0946        0.542      0.00565\n","     24    12       0.0131        0.013     4.42e-05       0.0677       0.0883        0.058       0.0872       0.0726       0.0742        0.111       0.0927        0.417      0.00434\n","     24    13       0.0136       0.0135     0.000124       0.0694       0.0898       0.0593       0.0895       0.0744       0.0767        0.111        0.094        0.787      0.00819\n","     24    14       0.0137       0.0137     4.21e-05       0.0698       0.0904       0.0584       0.0925       0.0754       0.0756        0.114        0.095        0.383      0.00399\n","     24    15       0.0129       0.0126     0.000245       0.0662       0.0869        0.054       0.0906       0.0723       0.0695        0.114       0.0918         1.09       0.0114\n","     24    16       0.0124       0.0124      5.1e-05       0.0666        0.086       0.0555       0.0887       0.0721         0.07        0.111       0.0907        0.459      0.00479\n","     24    17       0.0121        0.012      0.00013       0.0648       0.0846       0.0548       0.0848       0.0698       0.0703        0.108        0.089        0.664      0.00692\n","     24    18       0.0149       0.0147     0.000137       0.0713       0.0939       0.0606       0.0926       0.0766       0.0801        0.117       0.0984        0.816       0.0085\n","     24    19       0.0136       0.0136     2.96e-05       0.0691       0.0901       0.0585       0.0904       0.0744        0.074        0.116       0.0949        0.363      0.00378\n","     24    20       0.0134       0.0133     0.000157       0.0676       0.0892       0.0547       0.0935       0.0741        0.071        0.117       0.0942        0.891      0.00928\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     24     1       0.0146       0.0146      3.9e-05       0.0706       0.0933       0.0585       0.0948       0.0766       0.0766         0.12       0.0983        0.388      0.00404\n","     24     2       0.0141        0.014     2.31e-05       0.0689       0.0917       0.0565       0.0936       0.0751       0.0748        0.118       0.0966        0.285      0.00297\n","     24     3       0.0124       0.0123     2.52e-05       0.0656        0.086        0.054       0.0886       0.0713       0.0698        0.112       0.0906        0.305      0.00318\n","     24     4       0.0135       0.0134     5.26e-05       0.0678       0.0896       0.0569       0.0895       0.0732       0.0741        0.115       0.0943        0.466      0.00485\n","     24     5        0.013       0.0129     4.44e-05       0.0667        0.088        0.055       0.0901       0.0726       0.0714        0.114       0.0928        0.399      0.00416\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              24   82.656    0.005       0.0127     9.03e-05       0.0127       0.0664        0.087       0.0554       0.0885       0.0719       0.0716        0.112       0.0916        0.578      0.00602\n","! Validation         24   82.656    0.005       0.0135     3.69e-05       0.0135       0.0679       0.0898       0.0562       0.0913       0.0738       0.0734        0.116       0.0946        0.368      0.00384\n","Wall time: 82.65624843499995\n","! Best model       24    0.013\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     25     1       0.0125       0.0124     0.000107       0.0651        0.086       0.0546       0.0862       0.0704       0.0708         0.11       0.0906        0.649      0.00676\n","     25     2       0.0124       0.0124     3.52e-05       0.0661        0.086       0.0559       0.0863       0.0711       0.0705        0.111       0.0906        0.392      0.00408\n","     25     3       0.0112       0.0111     0.000116       0.0642       0.0815       0.0543       0.0841       0.0692       0.0675        0.104       0.0858        0.753      0.00784\n","     25     4       0.0133       0.0132     0.000129       0.0672       0.0888       0.0557       0.0902       0.0729       0.0728        0.114       0.0935        0.836       0.0087\n","     25     5       0.0122       0.0121     7.01e-05        0.066       0.0852       0.0546       0.0889       0.0717        0.069        0.111       0.0898        0.554      0.00577\n","     25     6       0.0135       0.0134     0.000102       0.0681       0.0897       0.0557       0.0928       0.0743       0.0725        0.117       0.0946        0.677      0.00705\n","     25     7       0.0125       0.0123     0.000165       0.0664       0.0859       0.0553       0.0886        0.072       0.0707         0.11       0.0904        0.823      0.00857\n","     25     8       0.0125       0.0125     7.15e-05        0.065       0.0864       0.0527       0.0896       0.0712       0.0694        0.113       0.0912          0.6      0.00625\n","     25     9       0.0147       0.0144     0.000367       0.0706       0.0927         0.06       0.0917       0.0759       0.0795        0.115       0.0971         1.41       0.0147\n","     25    10       0.0151       0.0149     0.000201       0.0735       0.0944        0.065       0.0903       0.0777       0.0836        0.113       0.0982        0.943      0.00982\n","     25    11       0.0124       0.0121     0.000287       0.0653       0.0852       0.0561       0.0838       0.0699       0.0726        0.106       0.0893          1.2       0.0126\n","     25    12        0.014       0.0136     0.000341       0.0686       0.0904       0.0564        0.093       0.0747       0.0725        0.118       0.0954          1.3       0.0136\n","     25    13       0.0151       0.0151     2.81e-05       0.0744        0.095       0.0656       0.0919       0.0788       0.0814        0.118       0.0995        0.349      0.00363\n","     25    14       0.0116       0.0115     5.85e-05        0.064       0.0831       0.0544       0.0833       0.0688       0.0691        0.106       0.0874        0.483      0.00504\n","     25    15       0.0127       0.0126     4.31e-05       0.0662        0.087       0.0563       0.0859       0.0711        0.072        0.111       0.0915        0.419      0.00436\n","     25    16       0.0168       0.0168     2.02e-05       0.0772          0.1       0.0673       0.0969       0.0821       0.0863        0.123        0.105        0.253      0.00263\n","     25    17       0.0169       0.0167     0.000193       0.0765          0.1       0.0665       0.0965       0.0815       0.0859        0.123        0.105        0.869      0.00905\n","     25    18       0.0149       0.0146      0.00021        0.071       0.0936       0.0596       0.0937       0.0767       0.0787        0.118       0.0983        0.984       0.0103\n","     25    19       0.0134       0.0133     0.000128        0.068       0.0891       0.0564       0.0911       0.0738       0.0732        0.114       0.0938        0.763      0.00794\n","     25    20       0.0146       0.0144     0.000133       0.0732        0.093       0.0639       0.0917       0.0778         0.08        0.115       0.0973        0.772      0.00804\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     25     1       0.0142       0.0141     3.96e-05       0.0696        0.092       0.0575       0.0936       0.0756       0.0754        0.118       0.0969        0.396      0.00413\n","     25     2       0.0136       0.0136      2.3e-05       0.0678       0.0903       0.0555       0.0924        0.074       0.0734        0.117       0.0952        0.283      0.00295\n","     25     3        0.012        0.012      2.5e-05       0.0646       0.0848       0.0532       0.0875       0.0704       0.0686         0.11       0.0895        0.303      0.00316\n","     25     4       0.0131       0.0131     5.26e-05       0.0668       0.0884       0.0559       0.0885       0.0722       0.0729        0.113       0.0931        0.463      0.00483\n","     25     5       0.0126       0.0125     4.39e-05       0.0656       0.0867       0.0541       0.0887       0.0714       0.0703        0.112       0.0914        0.396      0.00412\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              25   85.736    0.005       0.0135      0.00014       0.0136       0.0688       0.0898       0.0583       0.0898       0.0741       0.0751        0.114       0.0943        0.752      0.00783\n","! Validation         25   85.736    0.005       0.0131     3.68e-05       0.0131       0.0669       0.0885       0.0553       0.0901       0.0727       0.0722        0.114       0.0932        0.368      0.00384\n","Wall time: 85.73720220600012\n","! Best model       25    0.013\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     26     1       0.0145       0.0143     0.000214       0.0706       0.0924       0.0595       0.0929       0.0762       0.0755        0.119       0.0974         1.05       0.0109\n","     26     2       0.0108       0.0108     7.04e-06       0.0623       0.0805       0.0525       0.0818       0.0672       0.0662        0.103       0.0847        0.163       0.0017\n","     26     3       0.0122       0.0121     0.000128       0.0655       0.0852       0.0542       0.0881       0.0712       0.0691        0.111       0.0898        0.827      0.00861\n","     26     4       0.0122       0.0121     4.18e-05       0.0655       0.0852        0.055       0.0864       0.0707       0.0712        0.108       0.0896        0.411      0.00428\n","     26     5       0.0111       0.0111     4.32e-05       0.0618       0.0815         0.05       0.0854       0.0677       0.0649        0.107       0.0861        0.376      0.00392\n","     26     6       0.0144       0.0143        5e-05       0.0694       0.0926       0.0583       0.0918        0.075       0.0785        0.116       0.0971        0.403       0.0042\n","     26     7       0.0124       0.0123     5.01e-05       0.0673       0.0859       0.0592       0.0835       0.0714       0.0749        0.105       0.0897        0.485      0.00505\n","     26     8       0.0133       0.0133      6.7e-06       0.0668       0.0892       0.0529       0.0945       0.0737        0.071        0.118       0.0943        0.163       0.0017\n","     26     9       0.0126       0.0125     0.000158       0.0664       0.0864       0.0576       0.0841       0.0708       0.0739        0.107       0.0904        0.815      0.00849\n","     26    10       0.0145       0.0144     6.36e-05       0.0718       0.0929       0.0618       0.0919       0.0768        0.079        0.116       0.0973        0.536      0.00559\n","     26    11       0.0117       0.0116     4.19e-05       0.0629       0.0834        0.053       0.0828       0.0679       0.0682        0.108       0.0879        0.421      0.00439\n","     26    12       0.0105       0.0105     4.27e-05       0.0608       0.0791       0.0506       0.0812       0.0659       0.0647        0.102       0.0833        0.456      0.00474\n","     26    13       0.0118       0.0117     8.75e-05       0.0626       0.0837       0.0522       0.0834       0.0678       0.0697        0.106        0.088        0.643      0.00669\n","     26    14       0.0125       0.0125     3.09e-06       0.0658       0.0865       0.0556       0.0861       0.0708       0.0716         0.11        0.091          0.1      0.00104\n","     26    15       0.0121       0.0121     5.86e-05        0.064        0.085       0.0529       0.0862       0.0695       0.0695         0.11       0.0895        0.492      0.00513\n","     26    16       0.0115       0.0115     4.57e-05       0.0638       0.0828       0.0534       0.0845       0.0689       0.0681        0.106       0.0872        0.434      0.00452\n","     26    17       0.0112       0.0112     7.51e-06       0.0622       0.0818         0.05       0.0865       0.0683       0.0635         0.11       0.0866        0.175      0.00182\n","     26    18       0.0131        0.013     9.59e-05       0.0649       0.0881       0.0517       0.0914       0.0715       0.0691        0.117       0.0931        0.653      0.00681\n","     26    19       0.0149       0.0148     8.89e-05       0.0719       0.0942       0.0601       0.0954       0.0778       0.0774        0.121       0.0992        0.688      0.00716\n","     26    20       0.0177       0.0176     9.45e-05       0.0801        0.103       0.0684        0.104       0.0859       0.0859         0.13        0.108        0.529      0.00551\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     26     1       0.0138       0.0137     4.01e-05       0.0686       0.0906       0.0567       0.0925       0.0746       0.0742        0.117       0.0955        0.402      0.00419\n","     26     2       0.0133       0.0132     2.31e-05       0.0668        0.089       0.0546       0.0913        0.073       0.0722        0.116       0.0939        0.283      0.00294\n","     26     3       0.0117       0.0117     2.49e-05       0.0637       0.0837       0.0523       0.0864       0.0694       0.0675        0.109       0.0883        0.303      0.00315\n","     26     4       0.0128       0.0127     5.27e-05       0.0659       0.0873       0.0551       0.0875       0.0713       0.0717        0.112        0.092        0.464      0.00483\n","     26     5       0.0123       0.0122     4.36e-05       0.0647       0.0855       0.0533       0.0875       0.0704       0.0694        0.111       0.0902        0.394       0.0041\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              26   88.838    0.005       0.0127     6.64e-05       0.0127       0.0663       0.0871       0.0554       0.0881       0.0718       0.0718        0.112       0.0917        0.491      0.00511\n","! Validation         26   88.838    0.005       0.0127     3.69e-05       0.0128       0.0659       0.0873       0.0544        0.089       0.0717        0.071        0.113        0.092        0.369      0.00384\n","Wall time: 88.83883276400002\n","! Best model       26    0.013\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     27     1        0.015       0.0149     7.42e-05        0.073       0.0944       0.0626       0.0936       0.0781       0.0802        0.118        0.099        0.421      0.00438\n","     27     2       0.0112       0.0111     6.55e-05        0.062       0.0817       0.0501       0.0859        0.068       0.0642        0.109       0.0863        0.584      0.00608\n","     27     3        0.012       0.0119     2.83e-05       0.0642       0.0845       0.0537       0.0852       0.0695       0.0698        0.108       0.0889        0.392      0.00408\n","     27     4       0.0141       0.0138      0.00029       0.0687       0.0908       0.0587       0.0888       0.0737       0.0775        0.113       0.0952         1.23       0.0128\n","     27     5       0.0106       0.0105     9.85e-05       0.0617       0.0794       0.0529       0.0794       0.0661       0.0671       0.0995       0.0833        0.657      0.00684\n","     27     6       0.0139       0.0138     8.62e-05       0.0687       0.0909        0.055       0.0962       0.0756       0.0726        0.119        0.096        0.657      0.00684\n","     27     7        0.016        0.016     6.56e-05       0.0763       0.0978       0.0677       0.0936       0.0807       0.0864        0.117        0.102        0.519       0.0054\n","     27     8       0.0162       0.0161     5.52e-05       0.0777       0.0983        0.071       0.0911        0.081       0.0888        0.115        0.102        0.498      0.00519\n","     27     9       0.0125       0.0123     0.000171       0.0651       0.0858       0.0527       0.0899       0.0713       0.0673        0.114       0.0908         0.86      0.00896\n","     27    10       0.0112       0.0111     1.37e-05       0.0624       0.0816       0.0518       0.0836       0.0677       0.0676        0.104       0.0859        0.207      0.00215\n","     27    11       0.0106       0.0105     0.000122       0.0612       0.0792       0.0519       0.0797       0.0658       0.0655        0.101       0.0834        0.755      0.00787\n","     27    12       0.0108       0.0108     8.51e-05       0.0608       0.0803       0.0478        0.087       0.0674       0.0617        0.108        0.085        0.647      0.00674\n","     27    13       0.0108       0.0108     4.74e-05       0.0608       0.0802       0.0484       0.0855        0.067       0.0627        0.107       0.0849        0.454      0.00473\n","     27    14       0.0134       0.0133      5.3e-05        0.068       0.0892       0.0573       0.0894       0.0734       0.0734        0.114       0.0939         0.39      0.00406\n","     27    15       0.0149       0.0148     0.000121       0.0727        0.094       0.0624       0.0932       0.0778       0.0804        0.117       0.0985        0.713      0.00743\n","     27    16       0.0116       0.0116     9.58e-06       0.0634       0.0832       0.0546       0.0809       0.0677       0.0706        0.104       0.0873        0.222      0.00232\n","     27    17       0.0146       0.0144     0.000173       0.0697       0.0928        0.058       0.0931       0.0755       0.0777        0.117       0.0975        0.929      0.00968\n","     27    18       0.0161        0.016     6.23e-05        0.077       0.0979       0.0684       0.0943       0.0814       0.0859        0.118        0.102        0.525      0.00547\n","     27    19        0.014       0.0139     1.78e-05       0.0696       0.0913       0.0569        0.095       0.0759       0.0721        0.121       0.0965        0.214      0.00223\n","     27    20       0.0103       0.0102      8.7e-05       0.0604       0.0781         0.05       0.0811       0.0656       0.0636        0.101       0.0824        0.557       0.0058\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     27     1       0.0134       0.0133     4.11e-05       0.0677       0.0894       0.0558       0.0914       0.0736        0.073        0.115       0.0942        0.417      0.00434\n","     27     2       0.0129       0.0129     2.29e-05       0.0659       0.0879       0.0538       0.0903        0.072       0.0711        0.114       0.0927        0.279       0.0029\n","     27     3       0.0115       0.0114     2.38e-05       0.0629       0.0827       0.0516       0.0856       0.0686       0.0665        0.108       0.0873        0.294      0.00307\n","     27     4       0.0125       0.0125     5.19e-05       0.0651       0.0863       0.0543       0.0867       0.0705       0.0706        0.111        0.091        0.461       0.0048\n","     27     5        0.012       0.0119     4.22e-05       0.0638       0.0844       0.0526       0.0863       0.0695       0.0685         0.11        0.089        0.385      0.00401\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              27   91.917    0.005       0.0129     8.63e-05        0.013       0.0672       0.0878       0.0566       0.0883       0.0725       0.0732        0.111       0.0923        0.571      0.00595\n","! Validation         27   91.917    0.005       0.0124     3.64e-05       0.0124       0.0651       0.0862       0.0536       0.0881       0.0708         0.07        0.112       0.0909        0.367      0.00382\n","Wall time: 91.91809300900013\n","! Best model       27    0.012\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     28     1       0.0141       0.0138     0.000286        0.069       0.0909       0.0558       0.0952       0.0755       0.0731        0.119       0.0959         1.22       0.0127\n","     28     2       0.0141       0.0141     6.37e-05       0.0715       0.0918       0.0611       0.0922       0.0766       0.0767        0.116       0.0965        0.461       0.0048\n","     28     3       0.0154       0.0153     8.95e-05        0.075       0.0958       0.0664       0.0921       0.0792       0.0854        0.114       0.0996        0.642      0.00669\n","     28     4       0.0142       0.0139     0.000248       0.0709       0.0913       0.0615       0.0896       0.0756       0.0786        0.113       0.0956         1.12       0.0116\n","     28     5       0.0105       0.0104     5.85e-05       0.0601        0.079       0.0499       0.0806       0.0652       0.0645        0.102       0.0832        0.501      0.00522\n","     28     6       0.0121       0.0118     0.000249       0.0652       0.0841       0.0555       0.0847       0.0701       0.0714        0.105       0.0882         1.17       0.0122\n","     28     7       0.0117       0.0117      8.3e-06       0.0641       0.0837       0.0523       0.0876         0.07        0.066        0.111       0.0884        0.188      0.00195\n","     28     8       0.0117       0.0116     0.000101       0.0618       0.0833       0.0497       0.0859       0.0678       0.0654        0.111        0.088        0.634       0.0066\n","     28     9       0.0151       0.0149     0.000177       0.0716       0.0945       0.0565        0.102       0.0792       0.0737        0.126       0.0999        0.967       0.0101\n","     28    10       0.0131       0.0131     4.41e-05       0.0665       0.0885        0.056       0.0874       0.0717       0.0737        0.112       0.0931        0.396      0.00412\n","     28    11       0.0116       0.0112     0.000413       0.0627       0.0817       0.0534       0.0813       0.0673       0.0682        0.104       0.0859         1.44        0.015\n","     28    12       0.0105       0.0102     0.000286         0.06       0.0783        0.051        0.078       0.0645       0.0661       0.0982       0.0821         1.17       0.0122\n","     28    13       0.0124       0.0124     6.09e-05       0.0678       0.0861       0.0603       0.0828       0.0715       0.0756        0.104       0.0897        0.501      0.00522\n","     28    14       0.0125       0.0123     0.000264       0.0642       0.0857       0.0522       0.0884       0.0703       0.0688        0.112       0.0904         1.12       0.0117\n","     28    15       0.0131       0.0131     7.59e-05       0.0686       0.0884       0.0596       0.0866       0.0731       0.0766        0.108       0.0924        0.605       0.0063\n","     28    16       0.0118       0.0118     2.89e-05       0.0642        0.084       0.0534       0.0858       0.0696        0.068        0.109       0.0886        0.368      0.00383\n","     28    17       0.0121        0.012     0.000129       0.0632       0.0847       0.0507       0.0882       0.0694        0.066        0.113       0.0896        0.762      0.00794\n","     28    18       0.0107       0.0105     0.000173       0.0601       0.0792       0.0489       0.0826       0.0657       0.0623        0.105       0.0838        0.861      0.00897\n","     28    19        0.012       0.0119     6.17e-05       0.0648       0.0845       0.0549       0.0844       0.0697       0.0703        0.107       0.0888        0.421      0.00439\n","     28    20       0.0113        0.011      0.00031       0.0624       0.0812       0.0512       0.0847        0.068       0.0652        0.106       0.0856         1.25       0.0131\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     28     1        0.013        0.013     4.08e-05       0.0668       0.0882        0.055       0.0905       0.0727       0.0719        0.114        0.093        0.409      0.00426\n","     28     2       0.0126       0.0126     2.33e-05       0.0651       0.0868       0.0529       0.0894       0.0711         0.07        0.113       0.0916        0.281      0.00293\n","     28     3       0.0112       0.0112     2.52e-05       0.0621       0.0818       0.0509       0.0846       0.0677       0.0656        0.107       0.0863        0.304      0.00316\n","     28     4       0.0122       0.0122     5.32e-05       0.0643       0.0854       0.0535       0.0859       0.0697       0.0696         0.11         0.09        0.467      0.00487\n","     28     5       0.0117       0.0116     4.38e-05        0.063       0.0834       0.0519       0.0853       0.0686       0.0676        0.108        0.088        0.395      0.00411\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              28   95.020    0.005       0.0123     0.000156       0.0125       0.0657        0.086        0.055        0.087        0.071        0.071         0.11       0.0905         0.79      0.00823\n","! Validation         28   95.020    0.005       0.0121     3.73e-05       0.0121       0.0643       0.0851       0.0528       0.0871         0.07        0.069        0.111       0.0898        0.371      0.00387\n","Wall time: 95.02085218599996\n","! Best model       28    0.012\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     29     1       0.0127       0.0126        5e-05        0.067       0.0868       0.0568       0.0875       0.0721       0.0739        0.108        0.091        0.393      0.00409\n","     29     2       0.0122       0.0121     4.89e-05        0.064       0.0853       0.0525        0.087       0.0697       0.0693        0.111       0.0899        0.471      0.00491\n","     29     3       0.0104       0.0103     1.71e-05       0.0601       0.0786       0.0507        0.079       0.0648       0.0644        0.101       0.0828        0.287      0.00299\n","     29     4       0.0115       0.0115     2.88e-05       0.0611        0.083       0.0488       0.0856       0.0672       0.0664        0.109       0.0877        0.328      0.00342\n","     29     5       0.0126       0.0126      2.5e-05       0.0658       0.0869       0.0519       0.0937       0.0728       0.0659        0.118        0.092        0.294      0.00306\n","     29     6       0.0124       0.0123     2.53e-05       0.0657       0.0859       0.0549       0.0873       0.0711       0.0717        0.109       0.0903        0.359      0.00374\n","     29     7       0.0124       0.0124     5.34e-05       0.0656        0.086       0.0541       0.0886       0.0713       0.0707         0.11       0.0905        0.419      0.00437\n","     29     8       0.0105       0.0104     4.88e-05       0.0594       0.0789       0.0489       0.0804       0.0646       0.0631        0.104       0.0833        0.391      0.00407\n","     29     9       0.0114       0.0113     7.56e-05        0.063       0.0823       0.0526       0.0838       0.0682       0.0674        0.106       0.0867         0.61      0.00635\n","     29    10       0.0115       0.0115     1.58e-05       0.0636        0.083       0.0547       0.0813        0.068       0.0707        0.103        0.087        0.265      0.00276\n","     29    11       0.0141       0.0141     3.78e-05       0.0714       0.0918        0.063       0.0884       0.0757       0.0805        0.111       0.0958        0.391      0.00407\n","     29    12       0.0121        0.012     1.21e-05       0.0655       0.0849       0.0555       0.0853       0.0704       0.0714        0.107       0.0892        0.241      0.00251\n","     29    13       0.0102       0.0101     0.000189       0.0593       0.0776       0.0481       0.0816       0.0649       0.0609        0.103        0.082        0.873      0.00909\n","     29    14       0.0109       0.0108     6.23e-05       0.0615       0.0805       0.0519       0.0807       0.0663       0.0662        0.103       0.0847        0.498      0.00519\n","     29    15       0.0114       0.0113     0.000152       0.0626       0.0821       0.0525       0.0829       0.0677       0.0674        0.106       0.0865        0.865      0.00901\n","     29    16        0.011       0.0108     0.000187       0.0616       0.0804       0.0512       0.0823       0.0667        0.065        0.105       0.0848        0.951       0.0099\n","     29    17      0.00997      0.00991     5.65e-05       0.0592        0.077        0.049       0.0796       0.0643       0.0631       0.0992       0.0811        0.498      0.00519\n","     29    18       0.0116       0.0111     0.000478       0.0623       0.0815       0.0531       0.0806       0.0669        0.069        0.102       0.0855         1.54       0.0161\n","     29    19       0.0136       0.0135     0.000142         0.07       0.0898       0.0628       0.0844       0.0736       0.0797        0.107       0.0934        0.772      0.00804\n","     29    20       0.0159       0.0158     0.000119       0.0747       0.0972       0.0643       0.0954       0.0799       0.0827        0.121        0.102        0.678      0.00706\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     29     1       0.0127       0.0127      4.2e-05        0.066       0.0871       0.0542       0.0895       0.0719       0.0709        0.113       0.0918        0.426      0.00444\n","     29     2       0.0123       0.0123     2.31e-05       0.0643       0.0858       0.0523       0.0885       0.0704        0.069        0.112       0.0905        0.281      0.00293\n","     29     3        0.011       0.0109      2.4e-05       0.0614       0.0809       0.0502       0.0837        0.067       0.0648        0.106       0.0854        0.294      0.00307\n","     29     4        0.012       0.0119     5.25e-05       0.0635       0.0844       0.0528       0.0851       0.0689       0.0686        0.109        0.089        0.464      0.00483\n","     29     5       0.0114       0.0114     4.23e-05       0.0622       0.0824       0.0512       0.0842       0.0677       0.0668        0.107       0.0869        0.386      0.00402\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              29   98.136    0.005       0.0118     9.13e-05       0.0119       0.0642       0.0841       0.0539       0.0848       0.0693       0.0697        0.107       0.0885        0.556       0.0058\n","! Validation         29   98.136    0.005       0.0118     3.68e-05       0.0119       0.0635       0.0842       0.0522       0.0862       0.0692       0.0681        0.109       0.0888         0.37      0.00386\n","Wall time: 98.1369455680001\n","! Best model       29    0.012\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     30     1       0.0126        0.012     0.000621       0.0642       0.0848       0.0547       0.0831       0.0689       0.0707        0.108       0.0892         1.83        0.019\n","     30     2      0.00964      0.00959     4.87e-05       0.0582       0.0758       0.0457       0.0831       0.0644       0.0586        0.102       0.0802        0.423      0.00441\n","     30     3        0.011       0.0109      0.00014        0.063       0.0808       0.0524       0.0842       0.0683       0.0658        0.104       0.0851        0.854       0.0089\n","     30     4       0.0137       0.0136     0.000128        0.069       0.0903       0.0589        0.089        0.074       0.0769        0.112       0.0946        0.686      0.00715\n","     30     5       0.0137       0.0137     2.35e-05       0.0697       0.0904         0.06        0.089       0.0745       0.0761        0.114       0.0949        0.325      0.00338\n","     30     6       0.0128       0.0128     7.77e-05       0.0663       0.0874       0.0543       0.0904       0.0723       0.0709        0.113       0.0921        0.598      0.00623\n","     30     7       0.0127       0.0126     9.06e-05       0.0647       0.0867       0.0533       0.0877       0.0705       0.0699        0.113       0.0915        0.642      0.00669\n","     30     8        0.014       0.0138      0.00019       0.0701        0.091       0.0582        0.094       0.0761       0.0737        0.118        0.096        0.966       0.0101\n","     30     9       0.0152       0.0152     3.62e-05       0.0732       0.0952       0.0628       0.0939       0.0784       0.0805        0.119       0.0999        0.329      0.00343\n","     30    10       0.0133       0.0133     2.78e-05       0.0681       0.0891       0.0573       0.0898       0.0735        0.074        0.113       0.0937         0.33      0.00343\n","     30    11       0.0119       0.0119     2.27e-05       0.0632       0.0842       0.0521       0.0855       0.0688       0.0702        0.107       0.0886        0.326      0.00339\n","     30    12       0.0132       0.0131     5.46e-05       0.0686       0.0886       0.0586       0.0886       0.0736       0.0756         0.11       0.0928        0.388      0.00405\n","     30    13       0.0152       0.0151     8.97e-05       0.0751       0.0951       0.0642        0.097       0.0806       0.0799         0.12       0.0998        0.687      0.00715\n","     30    14       0.0119       0.0119     3.06e-05       0.0631       0.0844       0.0523       0.0849       0.0686       0.0693        0.109       0.0889        0.364      0.00379\n","     30    15      0.00924      0.00918     5.46e-05       0.0559       0.0741       0.0469       0.0741       0.0605        0.059       0.0976       0.0783        0.452       0.0047\n","     30    16       0.0122       0.0121      8.4e-05       0.0645       0.0851       0.0538       0.0859       0.0698       0.0682        0.111       0.0898         0.62      0.00646\n","     30    17       0.0136       0.0135     0.000125        0.071       0.0899        0.062       0.0891       0.0755        0.079        0.108       0.0937        0.776      0.00809\n","     30    18       0.0117       0.0115     0.000266       0.0634       0.0828       0.0534       0.0836       0.0685       0.0681        0.106       0.0872         1.16        0.012\n","     30    19       0.0119       0.0118     0.000127       0.0639       0.0839       0.0535       0.0847       0.0691       0.0682        0.109       0.0884        0.697      0.00726\n","     30    20       0.0124       0.0124     5.74e-05       0.0658        0.086       0.0545       0.0884       0.0715       0.0689        0.113       0.0908        0.431      0.00449\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     30     1       0.0124       0.0124     4.17e-05       0.0652       0.0861       0.0535       0.0887       0.0711         0.07        0.112       0.0908        0.421      0.00438\n","     30     2        0.012        0.012     2.33e-05       0.0636       0.0848       0.0516       0.0876       0.0696       0.0681        0.111       0.0895        0.282      0.00294\n","     30     3       0.0107       0.0107     2.48e-05       0.0607         0.08       0.0496       0.0829       0.0663       0.0639        0.105       0.0845          0.3      0.00312\n","     30     4       0.0117       0.0117     5.31e-05       0.0629       0.0836       0.0521       0.0844       0.0682       0.0677        0.109       0.0881        0.467      0.00487\n","     30     5       0.0111       0.0111     4.31e-05       0.0615       0.0815       0.0507       0.0833        0.067        0.066        0.106        0.086        0.389      0.00405\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              30  101.242    0.005       0.0125     0.000115       0.0126       0.0661       0.0864       0.0554       0.0873       0.0714       0.0714        0.111        0.091        0.644      0.00671\n","! Validation         30  101.242    0.005       0.0116     3.72e-05       0.0116       0.0628       0.0833       0.0515       0.0854       0.0684       0.0672        0.108       0.0878        0.372      0.00387\n","Wall time: 101.24250946100005\n","! Best model       30    0.012\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     31     1       0.0147       0.0146     7.68e-05       0.0739       0.0935       0.0641       0.0933       0.0787       0.0799        0.116       0.0979        0.557       0.0058\n","     31     2       0.0116       0.0115     2.64e-05       0.0648       0.0831       0.0551       0.0842       0.0697       0.0703        0.104       0.0872         0.33      0.00344\n","     31     3       0.0102       0.0102     4.01e-05       0.0577        0.078        0.046       0.0812       0.0636       0.0597        0.106       0.0826        0.403       0.0042\n","     31     4       0.0133       0.0132     6.81e-05         0.07       0.0889       0.0647       0.0807       0.0727       0.0805        0.104       0.0921        0.551      0.00574\n","     31     5       0.0162       0.0161     6.53e-05       0.0762       0.0982       0.0651       0.0985       0.0818       0.0825        0.124        0.103        0.575      0.00599\n","     31     6       0.0108       0.0108     5.28e-05       0.0608       0.0802       0.0511       0.0803       0.0657       0.0658        0.103       0.0845        0.453      0.00471\n","     31     7       0.0148       0.0148     1.19e-05       0.0724        0.094       0.0629       0.0915       0.0772       0.0804        0.116       0.0984        0.205      0.00214\n","     31     8        0.017        0.017      4.4e-05       0.0793        0.101       0.0669        0.104       0.0855       0.0831        0.129        0.106        0.365       0.0038\n","     31     9        0.011       0.0109     7.77e-05       0.0622       0.0807       0.0533       0.0798       0.0666       0.0672        0.102       0.0848        0.531      0.00553\n","     31    10        0.014        0.014     4.72e-05       0.0704       0.0915       0.0561       0.0991       0.0776       0.0711        0.122       0.0968        0.463      0.00482\n","     31    11       0.0179       0.0179     5.23e-05       0.0815        0.103       0.0708        0.103       0.0868       0.0887        0.128        0.108         0.45      0.00469\n","     31    12       0.0146       0.0146     4.29e-05       0.0728       0.0934       0.0654       0.0875       0.0765       0.0829        0.111       0.0972        0.448      0.00467\n","     31    13       0.0112       0.0111     5.73e-05       0.0605       0.0815       0.0491       0.0834       0.0662       0.0652        0.107       0.0861        0.387      0.00403\n","     31    14       0.0135       0.0134     7.99e-05       0.0699       0.0897       0.0616       0.0865       0.0741       0.0783        0.109       0.0936        0.484      0.00504\n","     31    15       0.0154       0.0154        4e-05       0.0723        0.096       0.0598       0.0974       0.0786        0.077        0.126        0.101        0.406      0.00423\n","     31    16         0.01      0.00976     0.000267       0.0584       0.0764       0.0485       0.0781       0.0633       0.0621       0.0991       0.0806         1.18       0.0123\n","     31    17       0.0128       0.0127     0.000122       0.0668       0.0872       0.0547       0.0909       0.0728       0.0698        0.114       0.0921        0.747      0.00778\n","     31    18       0.0159       0.0157     0.000185       0.0764       0.0969       0.0684       0.0922       0.0803       0.0851        0.117        0.101        0.932      0.00971\n","     31    19       0.0135       0.0134      0.00016       0.0706       0.0894       0.0634        0.085       0.0742       0.0793        0.107        0.093        0.809      0.00842\n","     31    20       0.0119       0.0118     0.000125        0.065        0.084       0.0554       0.0843       0.0698       0.0698        0.107       0.0883        0.759      0.00791\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     31     1       0.0122       0.0121     4.15e-05       0.0646       0.0852       0.0529       0.0879       0.0704       0.0691        0.111       0.0899        0.417      0.00435\n","     31     2       0.0118       0.0118     2.35e-05        0.063       0.0841       0.0511       0.0868        0.069       0.0673         0.11       0.0887        0.282      0.00294\n","     31     3       0.0105       0.0105     2.53e-05       0.0602       0.0793       0.0491       0.0823       0.0657       0.0632        0.104       0.0838        0.303      0.00315\n","     31     4       0.0115       0.0115     5.34e-05       0.0623       0.0829       0.0515       0.0839       0.0677       0.0669        0.108       0.0874        0.469      0.00489\n","     31     5       0.0109       0.0109     4.35e-05       0.0609       0.0808       0.0501       0.0825       0.0663       0.0654        0.105       0.0852        0.392      0.00409\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              31  104.348    0.005       0.0134     8.21e-05       0.0135       0.0691       0.0897       0.0591       0.0891       0.0741       0.0754        0.113       0.0942        0.552      0.00575\n","! Validation         31  104.348    0.005       0.0114     3.74e-05       0.0114       0.0622       0.0825        0.051       0.0847       0.0678       0.0664        0.108        0.087        0.373      0.00388\n","Wall time: 104.34829417000014\n","! Best model       31    0.011\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     32     1       0.0196       0.0195      0.00011        0.086        0.108       0.0799       0.0981        0.089          0.1        0.123        0.111        0.719      0.00749\n","     32     2        0.015       0.0142     0.000812       0.0706       0.0923       0.0606       0.0906       0.0756       0.0772        0.117       0.0969          2.1       0.0219\n","     32     3      0.00971      0.00967     4.27e-05       0.0584       0.0761        0.049       0.0773       0.0631       0.0605          0.1       0.0803        0.435      0.00453\n","     32     4       0.0121       0.0117      0.00038       0.0643       0.0839        0.056        0.081       0.0685       0.0725        0.103       0.0877         1.38       0.0143\n","     32     5       0.0146       0.0145     9.63e-05       0.0703       0.0931       0.0569       0.0971        0.077       0.0759         0.12       0.0981        0.653       0.0068\n","     32     6      0.00898      0.00892      5.3e-05       0.0551       0.0731       0.0443       0.0766       0.0605       0.0558        0.099       0.0774        0.454      0.00473\n","     32     7        0.012       0.0119     0.000109       0.0656       0.0845       0.0574       0.0819       0.0697       0.0737        0.103       0.0882        0.714      0.00744\n","     32     8       0.0108       0.0107     5.74e-05       0.0617       0.0801       0.0527       0.0796       0.0662       0.0674        0.101       0.0841        0.473      0.00493\n","     32     9       0.0104       0.0104     2.26e-05       0.0595       0.0787       0.0477       0.0831       0.0654       0.0617        0.105       0.0832        0.314      0.00327\n","     32    10       0.0118       0.0118     4.95e-05       0.0631       0.0839       0.0519       0.0855       0.0687       0.0665        0.111       0.0887        0.451       0.0047\n","     32    11       0.0129       0.0128     4.15e-05       0.0671       0.0876       0.0565       0.0883       0.0724        0.072        0.112       0.0922         0.41      0.00427\n","     32    12       0.0121       0.0121     3.28e-05       0.0648       0.0851       0.0547        0.085       0.0699       0.0718        0.107       0.0893        0.313      0.00326\n","     32    13      0.00993      0.00991     2.17e-05       0.0592        0.077        0.048       0.0817       0.0648       0.0602        0.103       0.0815         0.24       0.0025\n","     32    14       0.0119       0.0117     0.000192       0.0631       0.0835       0.0524       0.0845       0.0684       0.0689        0.107       0.0879        0.943      0.00982\n","     32    15       0.0116       0.0115     0.000106       0.0627       0.0828        0.051       0.0862       0.0686       0.0659        0.109       0.0875        0.635      0.00661\n","     32    16       0.0096       0.0095     0.000103       0.0573       0.0754       0.0467       0.0783       0.0625       0.0598       0.0995       0.0797        0.705      0.00734\n","     32    17      0.00954      0.00941     0.000127        0.058       0.0751       0.0472       0.0797       0.0634       0.0593       0.0994       0.0793        0.809      0.00843\n","     32    18       0.0114       0.0114     2.12e-05        0.063       0.0826       0.0522       0.0847       0.0684       0.0671        0.107       0.0871        0.261      0.00272\n","     32    19       0.0103       0.0103     2.06e-05       0.0598       0.0786       0.0494       0.0807       0.0651       0.0636        0.102       0.0829        0.261      0.00272\n","     32    20      0.00927      0.00923      4.1e-05       0.0572       0.0743       0.0471       0.0773       0.0622       0.0597       0.0972       0.0784        0.433      0.00451\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     32     1       0.0119       0.0119      4.2e-05        0.064       0.0844       0.0523       0.0872       0.0698       0.0684         0.11        0.089        0.424      0.00442\n","     32     2       0.0116       0.0116     2.33e-05       0.0625       0.0833       0.0507       0.0862       0.0684       0.0666        0.109        0.088        0.282      0.00294\n","     32     3       0.0104       0.0104     2.48e-05       0.0597       0.0787       0.0487       0.0817       0.0652       0.0627        0.104       0.0832        0.299      0.00312\n","     32     4       0.0113       0.0113     5.33e-05       0.0617       0.0822       0.0509       0.0833       0.0671       0.0662        0.107       0.0867        0.468      0.00488\n","     32     5       0.0107       0.0107      4.3e-05       0.0603         0.08       0.0496       0.0817       0.0656       0.0648        0.104       0.0843        0.389      0.00406\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              32  107.456    0.005       0.0116     0.000122       0.0117       0.0633       0.0832       0.0531       0.0839       0.0685       0.0686        0.106       0.0875        0.635      0.00662\n","! Validation         32  107.456    0.005       0.0112     3.73e-05       0.0112       0.0616       0.0818       0.0504        0.084       0.0672       0.0658        0.107       0.0863        0.373      0.00388\n","Wall time: 107.45628719199999\n","! Best model       32    0.011\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     33     1      0.00973      0.00971     1.51e-05        0.058       0.0762       0.0475       0.0788       0.0632       0.0621       0.0986       0.0804        0.198      0.00207\n","     33     2       0.0099      0.00985     5.29e-05       0.0585       0.0768       0.0502       0.0752       0.0627        0.065       0.0961       0.0805        0.452      0.00471\n","     33     3       0.0101       0.0101     2.66e-05        0.059       0.0776        0.049        0.079        0.064       0.0637       0.0998       0.0817        0.354      0.00369\n","     33     4       0.0109       0.0109     2.14e-05       0.0621       0.0809       0.0516       0.0829       0.0673       0.0659        0.105       0.0852         0.32      0.00333\n","     33     5       0.0102       0.0101     3.83e-05       0.0594       0.0779       0.0492       0.0799       0.0645       0.0626        0.102       0.0822        0.371      0.00387\n","     33     6       0.0093      0.00922     8.05e-05       0.0563       0.0743       0.0462       0.0766       0.0614        0.059       0.0979       0.0785        0.598      0.00622\n","     33     7       0.0113       0.0113     5.04e-05       0.0617       0.0822       0.0509       0.0834       0.0672       0.0659        0.108       0.0867        0.447      0.00466\n","     33     8       0.0124       0.0123     0.000154        0.067       0.0857       0.0592       0.0826       0.0709        0.075        0.104       0.0894        0.838      0.00873\n","     33     9       0.0113       0.0113     5.42e-05       0.0626       0.0821       0.0529       0.0821       0.0675       0.0685        0.104       0.0863        0.441      0.00459\n","     33    10      0.00982      0.00953     0.000289        0.058       0.0755       0.0487       0.0764       0.0626       0.0621       0.0969       0.0795         1.21       0.0126\n","     33    11       0.0125       0.0125     2.19e-05       0.0676       0.0865       0.0596       0.0837       0.0716       0.0753        0.105       0.0903        0.289      0.00301\n","     33    12       0.0118       0.0117     5.42e-05       0.0631       0.0839       0.0502        0.089       0.0696       0.0653        0.112       0.0887        0.515      0.00536\n","     33    13      0.00865      0.00858     6.16e-05        0.056       0.0717        0.046        0.076        0.061       0.0581       0.0931       0.0756        0.532      0.00554\n","     33    14       0.0103       0.0103      5.3e-05       0.0589       0.0784       0.0462       0.0843       0.0652       0.0597        0.106        0.083        0.491      0.00512\n","     33    15       0.0111       0.0111     1.64e-05       0.0621       0.0814       0.0515       0.0833       0.0674       0.0653        0.106       0.0859        0.216      0.00225\n","     33    16       0.0131       0.0129     0.000202       0.0649       0.0878       0.0522       0.0902       0.0712       0.0706        0.115       0.0926            1       0.0104\n","     33    17       0.0107       0.0107     6.94e-05       0.0613       0.0799       0.0521       0.0797       0.0659       0.0663        0.102       0.0841        0.559      0.00582\n","     33    18       0.0105       0.0104     0.000134       0.0604       0.0789       0.0487       0.0838       0.0662       0.0612        0.106       0.0835        0.755      0.00787\n","     33    19       0.0111       0.0109      0.00021       0.0616       0.0806       0.0502       0.0845       0.0673       0.0636        0.107       0.0852         1.02       0.0107\n","     33    20       0.0105       0.0105     1.25e-05       0.0601       0.0792       0.0511       0.0782       0.0647       0.0652        0.102       0.0834        0.209      0.00218\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     33     1       0.0117       0.0117     4.21e-05       0.0634       0.0836       0.0518       0.0866       0.0692       0.0676        0.109       0.0882        0.423      0.00441\n","     33     2       0.0114       0.0114     2.35e-05       0.0619       0.0826       0.0501       0.0855       0.0678       0.0658        0.109       0.0872        0.284      0.00296\n","     33     3       0.0102       0.0102     2.53e-05       0.0592       0.0781       0.0482       0.0812       0.0647        0.062        0.103       0.0825        0.302      0.00314\n","     33     4       0.0111       0.0111      5.4e-05       0.0612       0.0815       0.0504       0.0828       0.0666       0.0655        0.106        0.086        0.471      0.00491\n","     33     5       0.0105       0.0105     4.35e-05       0.0597       0.0792       0.0491       0.0809        0.065       0.0641        0.103       0.0835        0.392      0.00408\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              33  110.563    0.005       0.0107     8.08e-05       0.0108       0.0609         0.08       0.0507       0.0815       0.0661       0.0652        0.103       0.0843        0.541      0.00564\n","! Validation         33  110.563    0.005        0.011     3.77e-05        0.011       0.0611        0.081       0.0499       0.0834       0.0666       0.0651        0.106       0.0855        0.374       0.0039\n","Wall time: 110.5637871020001\n","! Best model       33    0.011\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     34     1        0.012       0.0118     0.000186        0.063       0.0839       0.0516       0.0859       0.0687        0.068        0.109       0.0885        0.993       0.0103\n","     34     2      0.00921      0.00881     0.000398       0.0549       0.0726        0.046       0.0729       0.0594       0.0588       0.0943       0.0766         1.47       0.0154\n","     34     3       0.0103       0.0102     6.57e-05       0.0598       0.0782       0.0497       0.0799       0.0648       0.0637        0.101       0.0824        0.553      0.00576\n","     34     4       0.0101      0.00962     0.000511       0.0571       0.0759       0.0473       0.0767        0.062       0.0612       0.0989       0.0801         1.65       0.0172\n","     34     5       0.0103      0.00956     0.000688       0.0587       0.0757       0.0488       0.0784       0.0636       0.0612       0.0984       0.0798         1.92         0.02\n","     34     6      0.00999      0.00998     1.68e-05       0.0582       0.0773       0.0471       0.0803       0.0637       0.0606        0.103       0.0817         0.24       0.0025\n","     34     7       0.0105       0.0102     0.000281       0.0598       0.0781       0.0506       0.0781       0.0643       0.0656       0.0984        0.082         1.22       0.0127\n","     34     8       0.0134       0.0133     0.000144       0.0676       0.0892       0.0546       0.0935       0.0741       0.0724        0.116        0.094        0.883       0.0092\n","     34     9       0.0103       0.0103     1.79e-05       0.0592       0.0785       0.0488         0.08       0.0644       0.0643        0.101       0.0827        0.279      0.00291\n","     34    10       0.0106       0.0106      7.1e-05       0.0617       0.0796       0.0531       0.0789        0.066       0.0676       0.0993       0.0834        0.556       0.0058\n","     34    11      0.00996       0.0099     6.14e-05       0.0596        0.077       0.0493       0.0802       0.0647       0.0624       0.0999       0.0812        0.523      0.00544\n","     34    12       0.0115       0.0115     3.23e-05       0.0623       0.0828       0.0492       0.0885       0.0689       0.0636        0.112       0.0877        0.331      0.00345\n","     34    13       0.0101      0.00999     0.000141       0.0583       0.0773        0.047       0.0808       0.0639       0.0606        0.103       0.0818        0.746      0.00777\n","     34    14      0.00892       0.0088     0.000112       0.0549       0.0726       0.0461       0.0726       0.0593       0.0588       0.0943       0.0766        0.705      0.00735\n","     34    15      0.00958      0.00953     4.38e-05       0.0575       0.0755       0.0474       0.0776       0.0625       0.0609       0.0984       0.0797        0.359      0.00373\n","     34    16       0.0114        0.011     0.000344       0.0613       0.0812       0.0497       0.0845       0.0671        0.065        0.106       0.0857         1.34        0.014\n","     34    17       0.0103       0.0102     0.000136         0.06       0.0782       0.0509        0.078       0.0645       0.0649       0.0995       0.0822        0.773      0.00805\n","     34    18        0.009      0.00866     0.000343       0.0555        0.072       0.0472       0.0721       0.0596       0.0593       0.0923       0.0758         1.37       0.0142\n","     34    19       0.0104         0.01      0.00037       0.0594       0.0775       0.0467       0.0849       0.0658       0.0591        0.105       0.0821         1.36       0.0142\n","     34    20       0.0103       0.0102     0.000132       0.0601       0.0782       0.0499       0.0806       0.0652       0.0634        0.101       0.0824        0.751      0.00782\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     34     1       0.0115       0.0115     4.27e-05       0.0628       0.0828       0.0512       0.0859       0.0686       0.0669        0.108       0.0874        0.431      0.00449\n","     34     2       0.0112       0.0112     2.36e-05       0.0614       0.0818       0.0496       0.0849       0.0672       0.0651        0.108       0.0864        0.287      0.00299\n","     34     3       0.0101         0.01     2.49e-05       0.0587       0.0775       0.0477       0.0807       0.0642       0.0614        0.102       0.0819        0.298      0.00311\n","     34     4        0.011       0.0109     5.37e-05       0.0607       0.0808       0.0498       0.0823       0.0661       0.0648        0.106       0.0853         0.47       0.0049\n","     34     5       0.0104       0.0103      4.3e-05       0.0592       0.0786       0.0487       0.0802       0.0644       0.0636        0.102       0.0829        0.393      0.00409\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              34  113.668    0.005       0.0102     0.000205       0.0104       0.0594       0.0782       0.0491       0.0802       0.0646       0.0632        0.102       0.0824        0.901      0.00939\n","! Validation         34  113.668    0.005       0.0108     3.76e-05       0.0108       0.0605       0.0803       0.0494       0.0828       0.0661       0.0644        0.105       0.0848        0.376      0.00391\n","Wall time: 113.66859525200016\n","! Best model       34    0.011\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     35     1       0.0116       0.0112     0.000348       0.0623       0.0819       0.0512       0.0844       0.0678       0.0662        0.107       0.0864         1.35       0.0141\n","     35     2       0.0109       0.0108     7.21e-05       0.0606       0.0805       0.0501       0.0817       0.0659       0.0664        0.103       0.0847        0.595       0.0062\n","     35     3       0.0135       0.0129     0.000603       0.0672       0.0877       0.0606       0.0804       0.0705       0.0796        0.102       0.0908         1.76       0.0183\n","     35     4       0.0113       0.0111      0.00017       0.0634       0.0816       0.0561        0.078       0.0671       0.0709       0.0996       0.0853        0.942      0.00982\n","     35     5       0.0105       0.0103      0.00013       0.0598       0.0787       0.0485       0.0823       0.0654       0.0616        0.105       0.0832        0.808      0.00842\n","     35     6       0.0131       0.0126     0.000503       0.0663       0.0868       0.0562       0.0866       0.0714       0.0729         0.11       0.0912         1.65       0.0172\n","     35     7       0.0126       0.0126     4.44e-05       0.0665       0.0868       0.0535       0.0925        0.073       0.0673        0.116       0.0918        0.437      0.00455\n","     35     8       0.0127       0.0122     0.000439       0.0647       0.0856       0.0532       0.0875       0.0704       0.0687        0.112       0.0903         1.55       0.0162\n","     35     9       0.0117       0.0113     0.000467       0.0634       0.0821       0.0538       0.0824       0.0681       0.0682        0.104       0.0863         1.56       0.0163\n","     35    10      0.00939      0.00929     9.71e-05       0.0573       0.0746       0.0478       0.0762        0.062       0.0607       0.0965       0.0786        0.698      0.00727\n","     35    11        0.012       0.0116     0.000419        0.065       0.0832       0.0587       0.0777       0.0682       0.0744       0.0985       0.0865         1.43       0.0149\n","     35    12      0.00957      0.00918     0.000394       0.0569       0.0741       0.0477       0.0753       0.0615       0.0614       0.0945        0.078         1.39       0.0145\n","     35    13         0.01       0.0099     0.000105       0.0584        0.077       0.0473       0.0805       0.0639       0.0603        0.102       0.0814         0.65      0.00677\n","     35    14      0.00925      0.00843     0.000818       0.0545        0.071       0.0444       0.0745       0.0595       0.0574       0.0925       0.0749         2.12        0.022\n","     35    15       0.0115       0.0115     3.46e-05       0.0636       0.0828       0.0528       0.0853        0.069       0.0676        0.107       0.0872        0.345      0.00359\n","     35    16      0.00822      0.00818     3.29e-05       0.0536         0.07       0.0446       0.0717       0.0582       0.0564       0.0913       0.0738        0.348      0.00363\n","     35    17       0.0107       0.0105     0.000177         0.06       0.0792        0.049       0.0822       0.0656       0.0635        0.104       0.0836        0.897      0.00935\n","     35    18       0.0105       0.0104      5.7e-05       0.0596        0.079       0.0483       0.0821       0.0652       0.0636        0.103       0.0833        0.486      0.00506\n","     35    19      0.00974      0.00974     6.87e-06       0.0576       0.0763       0.0458       0.0812       0.0635       0.0587        0.103       0.0808        0.142      0.00148\n","     35    20       0.0104       0.0103     8.56e-05       0.0596       0.0785       0.0503       0.0784       0.0643       0.0654       0.0997       0.0826        0.554      0.00577\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     35     1       0.0113       0.0113     4.24e-05       0.0622       0.0821       0.0507       0.0853        0.068       0.0662        0.107       0.0866        0.426      0.00444\n","     35     2        0.011        0.011     2.39e-05       0.0608       0.0811       0.0491       0.0842       0.0667       0.0644        0.107       0.0857        0.286      0.00298\n","     35     3      0.00992      0.00989     2.56e-05       0.0582       0.0769       0.0472       0.0802       0.0637       0.0608        0.102       0.0813        0.303      0.00316\n","     35     4       0.0108       0.0107     5.43e-05       0.0601       0.0802       0.0493       0.0818       0.0656       0.0641        0.105       0.0847        0.473      0.00493\n","     35     5       0.0102       0.0101     4.38e-05       0.0586       0.0779       0.0482       0.0795       0.0638        0.063        0.101       0.0821        0.394      0.00411\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              35  116.775    0.005       0.0107      0.00025       0.0109        0.061         0.08        0.051       0.0811        0.066       0.0658        0.103       0.0843        0.986       0.0103\n","! Validation         35  116.775    0.005       0.0106      3.8e-05       0.0106         0.06       0.0797       0.0489       0.0822       0.0655       0.0637        0.105       0.0841        0.377      0.00392\n","Wall time: 116.77592633800009\n","! Best model       35    0.011\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     36     1       0.0106       0.0106     1.19e-05       0.0614       0.0797        0.053       0.0782       0.0656       0.0679       0.0992       0.0836        0.219      0.00229\n","     36     2       0.0107       0.0106     6.71e-05       0.0608       0.0798       0.0491       0.0842       0.0667       0.0632        0.105       0.0843        0.534      0.00556\n","     36     3      0.00941      0.00936     4.61e-05       0.0577       0.0749       0.0473       0.0784       0.0629       0.0594       0.0988       0.0791        0.405      0.00421\n","     36     4       0.0132       0.0131     3.72e-05       0.0675       0.0887       0.0558        0.091       0.0734       0.0723        0.115       0.0935        0.399      0.00415\n","     36     5       0.0109       0.0108     0.000161       0.0616       0.0803       0.0529       0.0789       0.0659        0.068          0.1       0.0842        0.807      0.00841\n","     36     6        0.011        0.011     1.49e-05        0.061       0.0811       0.0503       0.0826       0.0664       0.0651        0.106       0.0856        0.218      0.00227\n","     36     7       0.0101       0.0101     3.41e-05       0.0592       0.0776       0.0496       0.0785       0.0641        0.065       0.0981       0.0816        0.395      0.00412\n","     36     8        0.012        0.012     9.73e-06       0.0656       0.0846       0.0576       0.0816       0.0696        0.073        0.104       0.0885        0.172       0.0018\n","     36     9        0.011       0.0108      0.00026        0.061       0.0803       0.0498       0.0835       0.0666       0.0633        0.106       0.0849         1.18       0.0123\n","     36    10      0.00857      0.00854     2.75e-05       0.0544       0.0715       0.0436        0.076       0.0598       0.0557       0.0956       0.0756        0.293      0.00306\n","     36    11       0.0117       0.0115     0.000142       0.0642       0.0831       0.0524       0.0879       0.0701       0.0658         0.11       0.0878        0.858      0.00893\n","     36    12       0.0186       0.0184     0.000126       0.0825        0.105       0.0721        0.103       0.0877       0.0904         0.13         0.11        0.652      0.00679\n","     36    13       0.0168       0.0167     6.18e-05       0.0784          0.1         0.07       0.0952       0.0826        0.089        0.119        0.104        0.422      0.00439\n","     36    14      0.00841       0.0083     0.000107       0.0548       0.0705       0.0447       0.0751       0.0599       0.0562       0.0927       0.0744        0.689      0.00718\n","     36    15       0.0151        0.015     3.24e-05       0.0744       0.0948       0.0665       0.0902       0.0784       0.0843        0.113       0.0987        0.331      0.00345\n","     36    16       0.0152       0.0152     1.27e-05       0.0754       0.0954       0.0681       0.0902       0.0791        0.085        0.113       0.0992        0.197      0.00206\n","     36    17       0.0094      0.00936     4.38e-05       0.0571       0.0748       0.0469       0.0774       0.0622       0.0598       0.0982        0.079        0.417      0.00435\n","     36    18       0.0139       0.0137     0.000222       0.0698       0.0905       0.0603        0.089       0.0746       0.0775        0.112       0.0948         1.08       0.0113\n","     36    19       0.0132       0.0132      1.6e-05       0.0687       0.0889       0.0582       0.0896       0.0739       0.0733        0.114       0.0936        0.276      0.00288\n","     36    20      0.00958      0.00956     2.19e-05       0.0576       0.0756        0.048        0.077       0.0625       0.0611       0.0985       0.0798         0.32      0.00333\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     36     1       0.0111       0.0111     4.22e-05       0.0617       0.0815       0.0502       0.0848       0.0675       0.0655        0.106        0.086        0.423      0.00441\n","     36     2       0.0109       0.0108     2.39e-05       0.0603       0.0805       0.0487       0.0836       0.0662       0.0638        0.106       0.0851        0.287      0.00299\n","     36     3      0.00978      0.00975     2.61e-05       0.0578       0.0764       0.0468       0.0797       0.0632       0.0603        0.101       0.0807        0.306      0.00319\n","     36     4       0.0106       0.0106     5.46e-05       0.0597       0.0796       0.0488       0.0813       0.0651       0.0635        0.105       0.0841        0.475      0.00495\n","     36     5         0.01      0.00997     4.41e-05       0.0581       0.0772       0.0477       0.0789       0.0633       0.0625          0.1       0.0815        0.396      0.00412\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              36  119.879    0.005       0.0119     7.27e-05        0.012       0.0647       0.0844       0.0548       0.0844       0.0696       0.0705        0.107       0.0887        0.493      0.00514\n","! Validation         36  119.879    0.005       0.0104     3.82e-05       0.0105       0.0595       0.0791       0.0485       0.0817       0.0651       0.0631        0.104       0.0835        0.377      0.00393\n","Wall time: 119.87991640900009\n","! Best model       36    0.010\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     37     1       0.0117       0.0117      2.2e-05       0.0642       0.0838       0.0529       0.0869       0.0699       0.0685        0.108       0.0883         0.26      0.00271\n","     37     2       0.0137       0.0136     7.96e-05       0.0707       0.0904       0.0636       0.0851       0.0743       0.0801        0.108        0.094        0.558      0.00581\n","     37     3      0.00996      0.00993     2.85e-05        0.059       0.0771       0.0492       0.0784       0.0638       0.0638       0.0984       0.0811        0.329      0.00342\n","     37     4      0.00966      0.00963     3.31e-05       0.0578       0.0759        0.049       0.0754       0.0622       0.0628        0.097       0.0799        0.259       0.0027\n","     37     5       0.0122       0.0121     2.27e-05       0.0663       0.0853       0.0591       0.0807       0.0699       0.0746        0.103        0.089        0.293      0.00305\n","     37     6       0.0117       0.0117     4.55e-05        0.063       0.0835       0.0505       0.0882       0.0693       0.0657        0.111       0.0883        0.377      0.00393\n","     37     7       0.0102       0.0102     6.59e-05       0.0588        0.078       0.0483       0.0798       0.0641       0.0621        0.103       0.0823        0.551      0.00574\n","     37     8       0.0161       0.0161     3.31e-05       0.0767       0.0981       0.0628        0.104       0.0836       0.0781        0.129        0.104          0.4      0.00417\n","     37     9       0.0181        0.018     8.75e-05       0.0829        0.104        0.075       0.0987       0.0869       0.0932        0.122        0.108        0.609      0.00634\n","     37    10       0.0116       0.0116     4.03e-05       0.0642       0.0832       0.0569       0.0787       0.0678       0.0725        0.101       0.0869        0.277      0.00288\n","     37    11       0.0127       0.0124     0.000279        0.068       0.0863       0.0628       0.0784       0.0706       0.0787       0.0997       0.0892         1.21       0.0126\n","     37    12       0.0207       0.0207     3.98e-05       0.0906        0.111        0.089       0.0938       0.0914        0.107        0.119        0.113        0.441       0.0046\n","     37    13       0.0137       0.0137     3.17e-05       0.0684       0.0906       0.0547       0.0957       0.0752       0.0701        0.122       0.0958        0.349      0.00363\n","     37    14       0.0146       0.0145      3.1e-05       0.0707       0.0933       0.0617       0.0888       0.0752       0.0823        0.112       0.0972         0.41      0.00427\n","     37    15       0.0225       0.0224     5.73e-05       0.0913        0.116       0.0778        0.118       0.0981       0.0953        0.148        0.122        0.424      0.00442\n","     37    16       0.0132       0.0131     7.19e-05       0.0686       0.0887       0.0608       0.0843       0.0725       0.0772        0.108       0.0926        0.581      0.00605\n","     37    17       0.0135       0.0134     5.12e-05       0.0687       0.0896       0.0554       0.0951       0.0753       0.0707        0.119       0.0947        0.481      0.00501\n","     37    18       0.0181       0.0179     0.000243       0.0821        0.104       0.0768       0.0927       0.0848       0.0956        0.118        0.107         1.12       0.0117\n","     37    19       0.0124       0.0123      3.3e-05       0.0677       0.0859       0.0612       0.0806       0.0709        0.077        0.102       0.0893        0.332      0.00346\n","     37    20       0.0165       0.0165     2.93e-05       0.0797       0.0993       0.0743       0.0906       0.0824       0.0924        0.112        0.102        0.362      0.00377\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     37     1        0.011       0.0109     4.19e-05       0.0613       0.0809       0.0498       0.0843        0.067        0.065        0.106       0.0854        0.418      0.00436\n","     37     2       0.0107       0.0107     2.39e-05         0.06       0.0801       0.0484       0.0831       0.0657       0.0633        0.106       0.0846        0.288        0.003\n","     37     3      0.00966      0.00963     2.64e-05       0.0574       0.0759       0.0465       0.0792       0.0628       0.0598        0.101       0.0802        0.308      0.00321\n","     37     4       0.0105       0.0105     5.48e-05       0.0593       0.0791       0.0484       0.0811       0.0647       0.0629        0.104       0.0836        0.476      0.00496\n","     37     5      0.00989      0.00984     4.45e-05       0.0578       0.0768       0.0474       0.0785       0.0629        0.062       0.0999        0.081        0.397      0.00414\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              37  122.973    0.005       0.0141     6.63e-05       0.0141        0.071       0.0918       0.0621       0.0887       0.0754       0.0794        0.113        0.096        0.481      0.00501\n","! Validation         37  122.973    0.005       0.0103     3.83e-05       0.0104       0.0591       0.0786       0.0481       0.0812       0.0647       0.0626        0.103        0.083        0.378      0.00393\n","Wall time: 122.973443266\n","! Best model       37    0.010\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     38     1       0.0171       0.0171     6.08e-05       0.0804        0.101       0.0754       0.0906        0.083       0.0947        0.113        0.104        0.451       0.0047\n","     38     2       0.0107       0.0105     0.000208       0.0606       0.0793       0.0502       0.0815       0.0658       0.0642        0.103       0.0837        0.929      0.00968\n","     38     3        0.014       0.0139     6.51e-05        0.072       0.0912       0.0658       0.0844       0.0751       0.0825        0.106       0.0945        0.525      0.00547\n","     38     4       0.0145       0.0142     0.000278       0.0704       0.0922       0.0557       0.0999       0.0778       0.0709        0.124       0.0976         1.21       0.0126\n","     38     5      0.00991      0.00976     0.000151       0.0571       0.0764       0.0464       0.0787       0.0625       0.0615       0.0997       0.0806        0.825      0.00859\n","     38     6       0.0129       0.0129     3.21e-05       0.0675       0.0879       0.0558        0.091       0.0734       0.0717        0.114       0.0926        0.333      0.00347\n","     38     7      0.00973       0.0096     0.000138       0.0582       0.0758       0.0504       0.0737        0.062        0.064       0.0951       0.0795        0.755      0.00787\n","     38     8       0.0115       0.0113      0.00022       0.0631       0.0821       0.0507       0.0878       0.0693       0.0641         0.11       0.0869            1       0.0104\n","     38     9       0.0119       0.0118     8.44e-05       0.0664       0.0841       0.0619       0.0754       0.0687       0.0772       0.0963       0.0868        0.546      0.00569\n","     38    10       0.0102       0.0101     0.000131       0.0579       0.0777       0.0469       0.0799       0.0634       0.0614        0.103       0.0821        0.793      0.00826\n","     38    11        0.012       0.0117     0.000259       0.0653       0.0838       0.0593       0.0772       0.0683       0.0753       0.0986        0.087         1.11       0.0115\n","     38    12       0.0129       0.0129     1.34e-05       0.0657       0.0877       0.0539       0.0894       0.0716       0.0727        0.112       0.0923        0.248      0.00259\n","     38    13         0.01      0.00991     0.000137       0.0593        0.077       0.0482       0.0816       0.0649       0.0606        0.102       0.0814        0.685      0.00714\n","     38    14       0.0103       0.0103      9.4e-06       0.0588       0.0787       0.0477       0.0809       0.0643       0.0634        0.103        0.083        0.181      0.00188\n","     38    15       0.0101       0.0101     2.06e-05       0.0581       0.0778        0.047       0.0802       0.0636       0.0603        0.104       0.0823        0.238      0.00248\n","     38    16       0.0124       0.0124     1.96e-05       0.0661       0.0862       0.0544       0.0894       0.0719       0.0699        0.112       0.0909        0.315      0.00328\n","     38    17      0.00981      0.00979     1.35e-05       0.0591       0.0766       0.0494       0.0784       0.0639       0.0621       0.0993       0.0807        0.242      0.00252\n","     38    18       0.0101         0.01      3.3e-05        0.059       0.0775        0.049       0.0791        0.064       0.0638       0.0993       0.0815        0.355       0.0037\n","     38    19      0.00988      0.00982      6.2e-05       0.0579       0.0766       0.0469       0.0798       0.0634       0.0613        0.101       0.0809        0.447      0.00466\n","     38    20       0.0101         0.01      2.1e-05        0.058       0.0775       0.0474        0.079       0.0632       0.0616        0.102       0.0819        0.258      0.00268\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     38     1       0.0109       0.0108     4.18e-05       0.0609       0.0805       0.0494       0.0839       0.0667       0.0646        0.105        0.085        0.416      0.00434\n"]}],"source":["!ls /content/drive/MyDrive/Colab\\ \\Notebooks/nequip/configs/my-full-example.yaml\n","!rm -rf ./results\n","!nequip-train /content/drive/MyDrive/Colab\\ \\Notebooks/nequip/configs/my-full-example.yaml"]},{"cell_type":"markdown","metadata":{"id":"5EwuQZLDO4Cr"},"source":["We see that the model has converged to an energy accuarcy < 1meV/atom and a force accuracy of approx. 40 meV/A within 5 minutes and trained on only 100 samples. That should give us a good first potential! Note that these numbers will decrease significantly if you increase the training set size and the number of epochs to train. "]},{"cell_type":"markdown","metadata":{"id":"kJitSZgLYNNF"},"source":["### Deploy the model"]},{"cell_type":"markdown","metadata":{"id":"Lo_kIpYV00as"},"source":["<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/deploy.png?raw=true\" width=\"60%\">"]},{"cell_type":"markdown","metadata":{"id":"7VoeGtlA02KQ"},"source":["We now convert the model to a potential file. This makes it independent of NequIP and we can use it any downstream application, such as LAMMPS. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6107,"status":"ok","timestamp":1644071858191,"user":{"displayName":"Simon Batzner","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01516403350906604395"},"user_tz":300},"id":"Y3NJJgtDIDNc","outputId":"b5d86b6b-c649-4d46-e1aa-457b2e7ceb13"},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:root:Atomic outputs are scaled by: 1.0, shifted by 0.0.\n","INFO:root:Compiled & optimized model.\n"]}],"source":["!nequip-deploy build results/toluene/example-run-toluene toluene-deployed.pth"]},{"cell_type":"markdown","metadata":{"id":"UXpcE3oP0LyD"},"source":["## Evaluate Test Error on all remaining frames"]},{"cell_type":"markdown","metadata":{"id":"4wRKKCZ2PRl3"},"source":["Before running inference, we'd like to know how well the model is doing on a hold-out test set. We run the nequip-evaluate command to compute the test error on all data that we didn't use for training or validation. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14415,"status":"ok","timestamp":1644071880764,"user":{"displayName":"Simon Batzner","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01516403350906604395"},"user_tz":300},"id":"mB54WSrN0PaS","outputId":"96734101-62b1-4b9a-8c23-1c5c1387fbd5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","WARNING: please note that models running on CUDA are usually nondeterministc and that this manifests in the final test errors; for a _more_ deterministic result, please use `--device cpu`\n","Loading model... \n","loaded model from training session\n","Loading original dataset...\n","Loaded dataset specified in config.yaml.\n","Using origial training dataset (1000 frames) minus training (100 frames) and validation frames (50 frames), yielding a test set size of 850 frames.\n","Starting...\n","  0% 0/850 [00:00<?, ?it/s]\n","\u001b[A\n","  6% 50/850 [00:00<00:04, 198.25it/s]\n"," 12% 100/850 [00:00<00:07, 95.56it/s]\n"," 18% 150/850 [00:02<00:14, 46.79it/s]\n"," 24% 200/850 [00:04<00:17, 37.76it/s]\n","\u001b[A\n"," 35% 300/850 [00:04<00:07, 76.45it/s]\n","\u001b[A\n"," 47% 400/850 [00:04<00:03, 125.72it/s]\n","\u001b[A\n","\u001b[A\n"," 65% 550/850 [00:04<00:01, 217.01it/s]\n","\u001b[A\n","\u001b[A\n"," 82% 700/850 [00:04<00:00, 319.80it/s]\n","\u001b[A\n","\u001b[A\n","100% 850/850 [00:05<00:00, 167.55it/s]\n","\n","\n","--- Final result: ---\n","             0_f_mae =  0.796981           \n","             1_f_mae =  1.318640           \n","           all_f_mae =  1.057810           \n","             e/N_mae =  0.022260           \n","             0_f_mae =  0.796981           \n","             1_f_mae =  1.318640           \n","           all_f_mae =  1.057810           \n","             e/N_mae =  0.022260           \n"]}],"source":["!nequip-evaluate --train-dir results/toluene/example-run-toluene --batch-size 50"]},{"cell_type":"markdown","metadata":{"id":"HQHrMMnsPaJO"},"source":["Again, energy errors of < 1meV/atom (converted from kcal/mol to eV), and force errors of ~45 meV/A 🎉"]},{"cell_type":"markdown","metadata":{"id":"H4r5FBXaum9n"},"source":["# LAMMPS"]},{"cell_type":"markdown","metadata":{"id":"0qIYIYyr1B4O"},"source":["We are now in a position to run MD with our potential. Here, we will minimize the geometry of the toluene molecule we trained on from a perturbed initial state. "]},{"cell_type":"markdown","metadata":{"id":"UirNBTlJ1BNZ"},"source":["<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/run.png?raw=true\" width=\"60%\">"]},{"cell_type":"markdown","metadata":{"id":"JQs0ijPhvAGb"},"source":["Set up a simple LAMMPS input file\n","\n","CAUTION: the reference data here are in kcal/mol for the energies and kcal/mol/A for the forces. The NequIP model will therefore also be predicting outputs in these units. We are therefore using `units real` in LAMMPS (see [docs](https://docs.lammps.org/units.html)). If your reference data are in other units, you should using the corresponding units command in LAMMPS (e.g. if you use eV, A then `units metal` would be appropriate, which would then also change time units from `fs` to `ps`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W090KfMsd2Do"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tAHO8ODrpwG"},"outputs":[],"source":["lammps_input_minimize = \"\"\"\n","units\treal\n","atom_style atomic\n","newton off\n","thermo 1\n","read_data structure.data\n","\n","pair_style\tnequip\n","pair_coeff\t* * ../toluene-deployed.pth C H \n","mass            1 15.9994\n","mass            2 1.00794\n","\n","neighbor 1.0 bin\n","neigh_modify delay 5 every 1\n","\n","minimize 0.0 1.0e-8 10000 1000000\n","write_dump all custom output.dump id type x y z fx fy fz\n","\"\"\"\n","!mkdir lammps_run\n","with open(\"lammps_run/toluene_minimize.in\", \"w\") as f:\n","    f.write(lammps_input_minimize)"]},{"cell_type":"markdown","metadata":{"id":"AvWZCw1zvjRc"},"source":["Here's starting configuration for Toluene at CCSD(T) accuracy. We will strongly perturb the inital positions by sampling from a uniform distribution $\\mathcal{U}([0, 0.5])$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEPfMeGnJUVH"},"outputs":[],"source":["toluene_example = \"\"\"15\n"," Lattice=\"100.0 0.0 0.0 0.0 100.0 0.0 0.0 0.0 100.0\" Properties=species:S:1:pos:R:3 -169777.5840406276=T pbc=\"F F F\"\n"," C       52.48936904      49.86911725      50.09520748\n"," C       51.01088202      49.89609925      50.17978049\n"," C       50.36647401      50.04650925      48.96054247\n"," C       48.95673398      50.29576626      48.71580846\n"," C       48.04533296      50.26023426      49.82589448\n"," C       48.70932398      49.85770925      51.01923950\n"," C       50.06326400      49.77782925      51.25691751\n"," H       52.94467905      50.48672926      50.86545150\n"," H       52.89060405      48.87175023      50.14480949\n"," H       53.02173405      50.05890725      49.03968247\n"," H       51.01439802      50.38234726      48.05314045\n"," H       48.80598498      50.64314926      47.68195744\n"," H       46.96754695      50.20586626      49.53998848\n"," H       48.16716997      49.75850325      51.88622952\n"," H       50.45791001      49.55387424      52.15303052\n"," \"\"\"\n","\n","with open('toluene.xyz', 'w') as f: \n","    f.write(toluene_example)\n","\n","# read as ASE objects\n","atoms = read('toluene.xyz', format='extxyz')\n","\n","# perturb positions\n","p = atoms.get_positions()\n","p += np.random.rand(15, 3) * 0.5\n","atoms.set_positions(p)\n","atoms.set_pbc(False)\n","\n","# write to a LAMMPS file\n","write(\"lammps_run/structure.data\", atoms, format=\"lammps-data\")"]},{"cell_type":"markdown","metadata":{"id":"QDuyueY11YBF"},"source":["### Run the LAMMPS command: "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18042,"status":"ok","timestamp":1644071930560,"user":{"displayName":"Simon Batzner","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01516403350906604395"},"user_tz":300},"id":"gurLjNK5upvq","outputId":"13e735a0-ea9a-4108-c0a8-c5ea6bb1de6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["LAMMPS (29 Sep 2021 - Update 2)\n","OMP_NUM_THREADS environment is not set. Defaulting to 1 thread. (src/comm.cpp:98)\n","  using 1 OpenMP thread(s) per MPI task\n","Reading data file ...\n","  orthogonal box = (0.0000000 0.0000000 0.0000000) to (100.00000 100.00000 100.00000)\n","  1 by 1 by 1 MPI processor grid\n","  reading atoms ...\n","  15 atoms\n","  read_data CPU = 0.002 seconds\n","NEQUIP is using device cuda\n","NequIP Coeff: type 1 is element C\n","NequIP Coeff: type 2 is element H\n","Loading model from ../toluene-deployed.pth\n","Freezing TorchScript model...\n","WARNING: Using 'neigh_modify every 1 delay 0 check yes' setting during minimization (src/min.cpp:188)\n","Neighbor list info ...\n","  update every 1 steps, delay 0 steps, check yes\n","  max neighbors/atom: 2000, page size: 100000\n","  master list distance cutoff = 5\n","  ghost atom cutoff = 5\n","  binsize = 2.5, bins = 40 40 40\n","  1 neighbor lists, perpetual/occasional/extra = 1 0 0\n","  (1) pair nequip, perpetual\n","      attributes: full, newton off, ghost\n","      pair build: full/bin/ghost\n","      stencil: full/ghost/bin/3d\n","      bin: standard\n","Setting up cg style minimization ...\n","  Unit style    : real\n","  Current step  : 0\n","Per MPI rank memory allocation (min/avg/max) = 4.603 | 4.603 | 4.603 Mbytes\n","Step Temp E_pair E_mol TotEng Press \n","       0            0   -169569.11            0   -169569.11            0 \n","       1            0   -169648.28            0   -169648.28            0 \n","       2            0   -169719.86            0   -169719.86            0 \n","       3            0    -169769.8            0    -169769.8            0 \n","       4            0   -169792.31            0   -169792.31            0 \n","       5            0   -169803.03            0   -169803.03            0 \n","       6            0   -169806.83            0   -169806.83            0 \n","       7            0   -169810.16            0   -169810.16            0 \n","       8            0   -169812.75            0   -169812.75            0 \n","       9            0   -169813.66            0   -169813.66            0 \n","      10            0    -169814.3            0    -169814.3            0 \n","      11            0   -169814.77            0   -169814.77            0 \n","      12            0   -169815.05            0   -169815.05            0 \n","      13            0   -169815.28            0   -169815.28            0 \n","      14            0    -169815.7            0    -169815.7            0 \n","      15            0   -169815.88            0   -169815.88            0 \n","      16            0   -169816.05            0   -169816.05            0 \n","      17            0   -169816.23            0   -169816.23            0 \n","      18            0   -169816.31            0   -169816.31            0 \n","      19            0   -169816.36            0   -169816.36            0 \n","      20            0   -169816.42            0   -169816.42            0 \n","      21            0   -169816.44            0   -169816.44            0 \n","      22            0   -169816.47            0   -169816.47            0 \n","      23            0   -169816.52            0   -169816.52            0 \n","      24            0   -169816.52            0   -169816.52            0 \n","      25            0   -169816.52            0   -169816.52            0 \n","      26            0   -169816.52            0   -169816.52            0 \n","      27            0   -169816.52            0   -169816.52            0 \n","      28            0   -169816.52            0   -169816.52            0 \n","      29            0   -169816.53            0   -169816.53            0 \n","      30            0   -169816.55            0   -169816.55            0 \n","      31            0   -169816.55            0   -169816.55            0 \n","      32            0   -169816.56            0   -169816.56            0 \n","      33            0   -169816.56            0   -169816.56            0 \n","Loop time of 11.5642 on 1 procs for 33 steps with 15 atoms\n","\n","98.9% CPU use with 1 MPI tasks x 1 OpenMP threads\n","\n","Minimization stats:\n","  Stopping criterion = linesearch alpha is zero\n","  Energy initial, next-to-last, final = \n","        -169569.109375       -169816.5625       -169816.5625\n","  Force two-norm initial, final = 485.27448 2.6820958\n","  Force max component initial, final = 212.56668 1.0711311\n","  Final line search alpha, max atom move = 1.3603356e-09 1.4570978e-09\n","  Iterations, force evaluations = 33 119\n","\n","MPI task timing breakdown:\n","Section |  min time  |  avg time  |  max time  |%varavg| %total\n","---------------------------------------------------------------\n","Pair    | 11.562     | 11.562     | 11.562     |   0.0 | 99.98\n","Neigh   | 9.3531e-05 | 9.3531e-05 | 9.3531e-05 |   0.0 |  0.00\n","Comm    | 5.6191e-05 | 5.6191e-05 | 5.6191e-05 |   0.0 |  0.00\n","Output  | 0.00098602 | 0.00098602 | 0.00098602 |   0.0 |  0.01\n","Modify  | 0          | 0          | 0          |   0.0 |  0.00\n","Other   |            | 0.0006101  |            |       |  0.01\n","\n","Nlocal:        15.0000 ave          15 max          15 min\n","Histogram: 1 0 0 0 0 0 0 0 0 0\n","Nghost:         0.00000 ave           0 max           0 min\n","Histogram: 1 0 0 0 0 0 0 0 0 0\n","Neighs:         0.00000 ave           0 max           0 min\n","Histogram: 1 0 0 0 0 0 0 0 0 0\n","FullNghs:      190.000 ave         190 max         190 min\n","Histogram: 1 0 0 0 0 0 0 0 0 0\n","\n","Total # of neighbors = 190\n","Ave neighs/atom = 12.666667\n","Neighbor list builds = 1\n","Dangerous builds = 0\n","Total wall time: 0:00:16\n","[6bc2016184fb:03199] *** Process received signal ***\n","[6bc2016184fb:03199] Signal: Segmentation fault (11)\n","[6bc2016184fb:03199] Signal code: Address not mapped (1)\n","[6bc2016184fb:03199] Failing at address: 0x7f331cd2820d\n","[6bc2016184fb:03199] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f331f9d3980]\n","[6bc2016184fb:03199] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7f331f6128a5]\n","[6bc2016184fb:03199] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7f331fe7de44]\n","[6bc2016184fb:03199] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7f331f613735]\n","[6bc2016184fb:03199] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7f331fe7bcb3]\n","[6bc2016184fb:03199] *** End of error message ***\n"]}],"source":["!cd lammps_run/ && ../lammps/build/lmp -in toluene_minimize.in"]},{"cell_type":"markdown","metadata":{"id":"BOKfQ83JQESc"},"source":["We see LAMMPS converges quickly to a minimum. Let's check how well we did. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qPcQU9HbsaVl"},"outputs":[],"source":["# read the final structure back in \n","minimized = read('./lammps_run/output.dump', format='lammps-dump-text')"]},{"cell_type":"markdown","metadata":{"id":"brqGqVtdWpCF"},"source":["### Compare optimized bond length to true coupled cluster reference from CCCBDB"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1644071930562,"user":{"displayName":"Simon Batzner","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01516403350906604395"},"user_tz":300},"id":"ltFlaHrTRn97","outputId":"1c6a131b-01a5-4b39-b972-945508bb7c6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Relative Error in bond length w.r.t. Coupled Cluster from CCCBDB: 0.100%\n"]}],"source":["# get distances of optimized geometry (reference data: CCSD(T) [Psi4, cc-pVDZ])\n","d_12 = minimized.get_distances(1, 2)\n","\n","# reference: https://cccbdb.nist.gov/geom3x.asp?method=6&basis=2, coupled cluster\n","d_12_ccd = 1.4086\n","\n","print('Relative Error in bond length w.r.t. Coupled Cluster from CCCBDB: {:.3f}%'.format((100 * np.abs(d_12 - d_12_ccd) / d_12_ccd)[0]))"]},{"cell_type":"markdown","metadata":{"id":"iT64gDUeQOvO"},"source":["We find a final relative error close to Coupled Cluster accuracy 🎉"]},{"cell_type":"markdown","metadata":{"id":"T4ZD6U0EkIp5"},"source":["## Next Steps\n","\n","This concludes our tutorial. A next step would be to head over to https://github.com/mir-group/nequip, install NequIP and get started with your own system. If you have questions, please don't hesitate to reach out to batzner@g.harvard.edu, we're happy to help! \n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"my-short-nequip-tutorial.ipynb","provenance":[{"file_id":"1_r348f6oIyKxH4FnpKeD8g4QjwDhP8mT","timestamp":1656689687220}]},"gpuClass":"standard","interpreter":{"hash":"c9be9acec9edbd902b751bf46a8fbd7b71bbc5f0438c72d3ebaee4bffeb5e5e4"},"kernelspec":{"display_name":"Python 3.7.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}