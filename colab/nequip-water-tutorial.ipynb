{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFpAi8g9XmUU"
      },
      "source": [
        "# Molecular Dynamics with NequIP \n",
        "\n",
        "### NequIP Authors: Simon Batzner, Albert Musaelian, Lixin Sun, Anders Johansson, Boris Kozinsky"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qtq3KJqLq9F"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/nequip3.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m1W9W9yvuKA"
      },
      "source": [
        "### Tutorial for MD for bulk water modified by Gabriele Tocci"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMg3YvVF2aBQ"
      },
      "source": [
        "### The water trajectory is obtained with the SCAN functional at 300 K and is the last 5 ps (10k frames) of a trajectory used in the paper by Herrero et al.: [Connection between water's dynamical and structural properties: Insights from ab initio simulations ](https://www.pnas.org/doi/10.1073/pnas.2121641119)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2shNisM863Wy"
      },
      "source": [
        "### Open in colab and change the runtime to use the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7bW4JWmmuyD",
        "outputId": "98ec98f8-e989-4b32-cab2-a0c3dbe18c8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.10\n",
            "  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:42tcmalloc: large alloc 1147494400 bytes == 0x3f26000 @  0x7f9a4a475615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 19 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10) (4.1.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOLIFOJZaeZ5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import os\n",
        "\n",
        "data_dir = \"/content/nequip/datasets\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZpOvFtImsy2",
        "outputId": "5dfdcd90-33de-43fa-bfb8-6ba9472d92ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.10.0+cu102\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.__version__)\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7Uh7nyHnR-w",
        "outputId": "48a198c2-75d4-42c9-c5c4-05eed2eeee08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIMyrDOEm2IB",
        "outputId": "2a908e2e-3546-4c90-e252-0fd8e5c5b8c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'lammps': No such file or directory\n",
            "Cloning into 'lammps'...\n",
            "remote: Enumerating objects: 11732, done.\u001b[K\n",
            "remote: Counting objects: 100% (11732/11732), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8609/8609), done.\u001b[K\n",
            "remote: Total 11732 (delta 3932), reused 6311 (delta 2924), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (11732/11732), 110.00 MiB | 15.08 MiB/s, done.\n",
            "Resolving deltas: 100% (3932/3932), done.\n",
            "Note: checking out '7586adbb6a61254125992709ef2fda9134cfca6c'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "Checking out files: 100% (11058/11058), done.\n",
            "Cloning into 'pair_nequip'...\n",
            "remote: Enumerating objects: 418, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 418 (delta 79), reused 85 (delta 65), pack-reused 302\u001b[K\n",
            "Receiving objects: 100% (418/418), 427.14 KiB | 356.00 KiB/s, done.\n",
            "Resolving deltas: 100% (208/208), done.\n",
            "Copying files...\n",
            "Updating CMakeLists.txt...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "!rm -r lammps\n",
        "!git clone -b \"stable_29Sep2021_update2\" --depth 1 \"https://github.com/lammps/lammps.git\"\n",
        "!git clone https://github.com/mir-group/pair_nequip\n",
        "!cd pair_nequip && ./patch_lammps.sh /content/lammps/\n",
        "!cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "100Be8B6m5am"
      },
      "outputs": [],
      "source": [
        "!cp /content/pair_nequip/*.cpp /content/lammps/src/\n",
        "!cp /content/pair_nequip/*.h /content/lammps/src/\n",
        "! sed -i 's/CMAKE_CXX_STANDARD 11/CMAKE_CXX_STANDARD 14/g'  /content/lammps/cmake/CMakeLists.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlgRSmyom9VQ",
        "outputId": "42dc9d4e-f83f-4601-c6bc-e6766d792a7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.7/dist-packages (2019.0)\n",
            "Collecting mkl-include\n",
            "  Downloading mkl_include-2022.1.0-py2.py3-none-manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: intel-openmp in /usr/local/lib/python3.7/dist-packages (from mkl) (2022.1.0)\n",
            "Installing collected packages: mkl-include\n",
            "Successfully installed mkl-include-2022.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mkl mkl-include"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8vY6rwbnEgK",
        "outputId": "095c042e-d44c-4d7d-d449-4383ee29ca26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.17.1\") \n",
            "-- Appending /usr/local/cuda/lib64/stubs to CMAKE_LIBRARY_PATH: /usr/local/cuda/lib64/stubs\n",
            "-- Running check for auto-generated files from make-based build system\n",
            "-- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so (found version \"3.1\") \n",
            "-- Found MPI: TRUE (found version \"3.1\")  \n",
            "-- Looking for C++ include omp.h\n",
            "-- Looking for C++ include omp.h - found\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
            "-- Found JPEG: /usr/lib/x86_64-linux-gnu/libjpeg.so (found version \"80\") \n",
            "-- Found PNG: /usr/lib/x86_64-linux-gnu/libpng.so (found version \"1.6.34\") \n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
            "-- Found GZIP: /bin/gzip  \n",
            "-- Found FFMPEG: /usr/bin/ffmpeg  \n",
            "-- Looking for C++ include cmath\n",
            "-- Looking for C++ include cmath - found\n",
            "-- Generating style headers...\n",
            "-- Generating package headers...\n",
            "-- Generating lmpinstalledpkgs.h...\n",
            "-- Could NOT find ClangFormat (missing: ClangFormat_EXECUTABLE) (Required is at least version \"8.0\")\n",
            "-- The following tools and libraries have been found and configured:\n",
            " * Git\n",
            " * MPI\n",
            " * OpenMP\n",
            " * JPEG\n",
            " * PNG\n",
            " * ZLIB\n",
            "\n",
            "-- <<< Build configuration >>>\n",
            "   Operating System: Linux Ubuntu 18.04\n",
            "   Build type:       RelWithDebInfo\n",
            "   Install path:     /root/.local\n",
            "   Generator:        Unix Makefiles using /usr/bin/make\n",
            "-- Enabled packages: <None>\n",
            "-- <<< Compilers and Flags: >>>\n",
            "-- C++ Compiler:     /usr/bin/c++\n",
            "      Type:          GNU\n",
            "      Version:       7.5.0\n",
            "      C++ Flags:     -O2 -g -DNDEBUG\n",
            "      Defines:       LAMMPS_SMALLBIG;LAMMPS_MEMALIGN=64;LAMMPS_OMP_COMPAT=3;LAMMPS_JPEG;LAMMPS_PNG;LAMMPS_GZIP;LAMMPS_FFMPEG\n",
            "-- <<< Linker flags: >>>\n",
            "-- Executable name:  lmp\n",
            "-- Static library flags:    \n",
            "-- <<< MPI flags >>>\n",
            "-- MPI_defines:      MPICH_SKIP_MPICXX;OMPI_SKIP_MPICXX;_MPICC_H\n",
            "-- MPI includes:     /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include;/usr/lib/x86_64-linux-gnu/openmpi/include\n",
            "-- MPI libraries:    /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so;\n",
            "-- Looking for C++ include pthread.h\n",
            "-- Looking for C++ include pthread.h - found\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
            "-- Looking for pthread_create in pthreads\n",
            "-- Looking for pthread_create in pthreads - not found\n",
            "-- Looking for pthread_create in pthread\n",
            "-- Looking for pthread_create in pthread - found\n",
            "-- Found Threads: TRUE  \n",
            "-- Found CUDA: /usr/local/cuda (found version \"11.1\") \n",
            "-- Caffe2: CUDA detected: 11.1\n",
            "-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\n",
            "-- Caffe2: CUDA toolkit directory: /usr/local/cuda\n",
            "-- Caffe2: Header version is: 11.1\n",
            "-- Found CUDNN: /usr/lib/x86_64-linux-gnu/libcudnn.so  \n",
            "-- Found cuDNN: v7.6.5  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)\n",
            "-- /usr/local/cuda/lib64/libnvrtc.so shorthash is 3a20f2b6\n",
            "-- Autodetected CUDA architecture(s):  7.5\n",
            "-- Added CUDA NVCC flags for: -gencode;arch=compute_75,code=sm_75\n",
            "\u001b[33mCMake Warning at /usr/local/lib/python3.7/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\n",
            "  static library kineto_LIBRARY-NOTFOUND not found.\n",
            "Call Stack (most recent call first):\n",
            "  /usr/local/lib/python3.7/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:127 (append_torchlib_if_found)\n",
            "  CMakeLists.txt:922 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch: /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so  \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/lammps/build\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/compute.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/bond.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/citeme.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/comm.h\u001b[0m\n",
            "[  1%] Built target compute.h\n",
            "[  1%] Built target citeme.h\n",
            "[  1%] Built target bond.h\n",
            "[  1%] Built target comm.h\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/dihedral.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/domain.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/error.h\u001b[0m\n",
            "[  1%] Built target domain.h\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/fix.h\u001b[0m\n",
            "[  1%] Built target dihedral.h\n",
            "[  1%] Built target fix.h\n",
            "[  1%] Built target error.h\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/force.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/group.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/improper.h\u001b[0m\n",
            "[  2%] Built target force.h\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/input.h\u001b[0m\n",
            "[  2%] Built target group.h\n",
            "[  2%] Built target improper.h\n",
            "[  2%] Built target input.h\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/kspace.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/info.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/lammps.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/lattice.h\u001b[0m\n",
            "[  3%] Built target kspace.h\n",
            "[  3%] Built target info.h\n",
            "[  3%] Built target lammps.h\n",
            "[  3%] Built target lattice.h\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/lmppython.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/library.h\u001b[0m\n",
            "[  4%] Built target lmppython.h\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lmptype.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/memory.h\u001b[0m\n",
            "[  5%] Built target library.h\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/modify.h\u001b[0m\n",
            "[  5%] Built target lmptype.h\n",
            "[  5%] Built target memory.h\n",
            "[  5%] Built target modify.h\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/neighbor.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/output.h\u001b[0m\n",
            "[  6%] Built target neighbor.h\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/neigh_list.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/pair.h\u001b[0m\n",
            "[  6%] Built target output.h\n",
            "[  6%] Built target neigh_list.h\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/pointers.h\u001b[0m\n",
            "[  6%] Built target pair.h\n",
            "[  6%] Built target pointers.h\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/region.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/universe.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/timer.h\u001b[0m\n",
            "[  7%] Built target region.h\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/update.h\u001b[0m\n",
            "[  7%] Built target universe.h\n",
            "[  7%] Built target timer.h\n",
            "[  7%] Built target update.h\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/utils.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/variable.h\u001b[0m\n",
            "-- Git Directory: /content/lammps/.git\n",
            "[  8%] Built target utils.h\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/angle.h\u001b[0m\n",
            "[  8%] Built target variable.h\n",
            "[  8%] Built target angle.h\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/atom.h\u001b[0m\n",
            "[  8%] Built target atom.h\n",
            "-- Generating lmpgitversion.h...\n",
            "[  8%] Built target gitversion\n",
            "[  8%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_hybrid.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_zero.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_deprecated.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/arg_info.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_map.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_atomic.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_body.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_charge.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_ellipsoid.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_hybrid.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_line.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_sphere.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_tri.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/balance.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/body.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_deprecated.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_hybrid.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_zero.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/change_box.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/citeme.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_brick.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_tiled.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_aggregate_atom.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle_local.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angmom_chunk.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond_local.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centro_atom.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centroid_stress_atom.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_atom.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_spread_atom.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cluster_atom.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cna_atom.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com_chunk.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_coord_atom.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_deprecated.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral_local.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole_chunk.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_displace_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_fragment_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_global_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_group_group.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration_chunk.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_heat_flux.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper_local.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_inertia_chunk.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke_atom.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd_chunk.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_omega_chunk.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_orientorder_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair_local.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe_atom.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pressure.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_atom.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_chunk.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_local.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_rdf.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_chunk.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_region.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_slice.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_stress_atom.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_chunk.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_com.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_deform.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_partial.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_profile.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_ramp.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_region.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_sphere.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_torque_chunk.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vacf.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vcm_chunk.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_atoms.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_bonds.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_box.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_atoms.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_bonds.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/deprecated.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_deprecated.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_hybrid.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_zero.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/displace_atoms.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/domain.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_atom.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_cfg.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_custom.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_deprecated.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_image.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_local.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_movie.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_xyz.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/error.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/finish.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_adapt.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_addforce.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_atom.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_chunk.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_correlate.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo_weight.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_time.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_aveforce.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_balance.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_box_relax.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deform.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deposit.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deprecated.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dt_reset.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dummy.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_efield.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_enforce2d.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_evaporate.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_external.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_gravity.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_group.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_halt.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_heat.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_indent.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_langevin.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_lineforce.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_minimize.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_momentum.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_move.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_neigh_history.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh_sphere.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph_sphere.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt_sphere.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_limit.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_noforce.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_sphere.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sllod.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sphere.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_planeforce.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_press_berendsen.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_print.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_property_atom.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_read_restart.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_recenter.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_respa.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_restrain.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_setforce.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_chunk.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_self.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_force.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_state.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_berendsen.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_rescale.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_thermal_conductivity.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_vector.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_viscous.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_harmonic.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj1043.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj126.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj93.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_morse.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_reflect.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_region.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_format.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_os.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/force.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/gridcomm.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/group.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/hashlittle.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/image.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_group.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_neigh.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_store.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_time.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_var.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_deprecated.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_hybrid.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_zero.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/info.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/input.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/integrate.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/irregular.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace_deprecated.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lammps.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lattice.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/library.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lmppython.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_eigen.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_extra.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_special.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/memory.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_cg.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_fire.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_fire_old.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_hftn.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_linesearch.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_quickmin.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_sd.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/minimize.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/modify.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/molecule.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_page.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_pool_chunk.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_multi.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_standard.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_list.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_request.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neighbor.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_copy.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin_atomonly.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin_ghost.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_multi.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_multi_old.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_nsq.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_nsq_ghost.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_atomonly_newton.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newtoff.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newtoff_ghost.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newton.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newton_tri.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newtoff.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newton.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newton_tri.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newtoff.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newton.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newton_tri.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newtoff.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newtoff_ghost.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newton.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newtoff.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newton.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newton_tri.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_nsq_newtoff.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_nsq_newton.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newtoff.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newton.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newton_tri.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newtoff.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newton.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newton_tri.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newtoff.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newton.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newton_tri.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_nsq_newtoff.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_nsq_newton.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_halffull_newtoff.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_halffull_newton.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_respa.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on_oneside.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_bin_2d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_bin_3d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_ghost_bin_2d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_ghost_bin_3d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_2d.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_3d.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_old_2d.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_old_3d.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_2d.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_2d_tri.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_3d.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_3d_tri.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_2d.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_2d_tri.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_3d.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_3d_tri.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_2d.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_2d_tri.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_3d.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_3d_tri.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_all.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_partial.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_template.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_all.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_partial.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_template.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_all.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_partial.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_template.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_all.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_partial.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_template.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/output.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_born.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck_coul_cut.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_cut.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_debye.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_dsf.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_wolf.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_deprecated.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_overlay.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_scaled.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut_coul_cut.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_expand.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_morse.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_nequip.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_soft.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_table.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_yukawa.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zbl.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zero.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/potential_file_reader.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/procmap.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_mars.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_park.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rcb.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_data.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_dump.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_restart.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_native.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_xyz.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_block.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cone.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cylinder.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_deprecated.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_intersect.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_plane.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_prism.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_sphere.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_union.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/replicate.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rerun.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atom_ids.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_mol_ids.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/respa.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/run.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/set.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/special.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/table_file_reader.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tabular_function.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/text_file_reader.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/thermo.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/timer.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tokenizer.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/universe.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/update.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/utils.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/variable.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/lammps/src/variable.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint LAMMPS_NS::Variable::next(int, char**)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/lammps/src/variable.cpp:714:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "         \u001b[01;35m\u001b[Kfread(buf,1,64,fp)\u001b[m\u001b[K;\n",
            "         \u001b[01;35m\u001b[K~~~~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/velocity.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/verlet.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_coeff.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_data.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_dump.cpp.o\u001b[0m\n",
            "[100%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_restart.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX static library liblammps.a\u001b[0m\n",
            "[100%] Built target lammps\n",
            "[100%] \u001b[32mBuilding CXX object CMakeFiles/lmp.dir/content/lammps/src/main.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable lmp\u001b[0m\n",
            "[100%] Built target lmp\n"
          ]
        }
      ],
      "source": [
        "!cd lammps && mkdir -p build && cd build && cmake ../cmake -DCMAKE_PREFIX_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` && make -j4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG8wqxg43tiZ"
      },
      "source": [
        "### Clone nequip repository with the AIMD data file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoYyUGf70c4H",
        "outputId": "f9434afa-31a3-45fb-bcae-983e9c2e1526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'nequip'...\n",
            "remote: Enumerating objects: 184, done.\u001b[K\n",
            "remote: Counting objects: 100% (184/184), done.\u001b[K\n",
            "remote: Compressing objects: 100% (178/178), done.\u001b[K\n",
            "remote: Total 184 (delta 6), reused 85 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (184/184), 39.53 MiB | 28.35 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "! git clone --depth 1 \"https://github.com/gabriele16/nequip.git\"\n",
        "! cd nequip && git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrjXfT6L3S51"
      },
      "source": [
        "### Extract configurations and forces of AIMD trajectory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHCh2aC7WfKj",
        "outputId": "8cd387be-964f-4828-815c-2f8c6f11985b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "._AIMD_data\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n",
            "AIMD_data/\n",
            "AIMD_data/._WATER-frc-10k-1.xyz\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n",
            "AIMD_data/WATER-frc-10k-1.xyz\n",
            "AIMD_data/._celldata.dat\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n",
            "AIMD_data/celldata.dat\n",
            "AIMD_data/._WATER-pos-10k-1.xyz\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n",
            "AIMD_data/WATER-pos-10k-1.xyz\n",
            "celldata.dat  WATER-frc-10k-1.xyz  WATER-pos-10k-1.xyz\n"
          ]
        }
      ],
      "source": [
        "! tar -xzvf  /content/nequip/data/AIMD_data.tar.gz -C /content/nequip/data/\n",
        "! ls /content/nequip/data/aimd_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J51CA0Bod1Jv",
        "outputId": "266e76f8-0a53-478c-da1a-79b9435a8be7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.7.0-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 63.7 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 68.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=0fba950e3c3377f47321b26122bfd73e95c31bdcbe4ae9ecabd9dc745ff174b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.7.0 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.21\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./nequip\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (1.21.6)\n",
            "Collecting ase\n",
            "  Downloading ase-3.22.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (4.64.0)\n",
            "Requirement already satisfied: torch!=1.9.0,<=1.12,>=1.8 in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (1.10.0)\n",
            "Collecting e3nn<0.6.0,>=0.3.5\n",
            "  Downloading e3nn-0.5.0-py3-none-any.whl (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 72.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (3.13)\n",
            "Collecting torch-runstats>=0.2.0\n",
            "  Downloading torch_runstats-0.2.0-py3-none-any.whl (8.1 kB)\n",
            "Collecting torch-ema>=0.3.0\n",
            "  Downloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
            "Collecting scikit_learn<=1.0.1\n",
            "  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 60.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (4.1.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.7.1)\n",
            "Collecting opt-einsum-fx>=0.1.4\n",
            "  Downloading opt_einsum_fx-0.1.4-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (21.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn<=1.0.1->nequip==0.5.5) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn<=1.0.1->nequip==0.5.5) (1.1.0)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from ase->nequip==0.5.5) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (1.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1.0->ase->nequip==0.5.5) (1.15.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.2.1)\n",
            "Building wheels for collected packages: nequip\n",
            "  Building wheel for nequip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nequip: filename=nequip-0.5.5-py3-none-any.whl size=138681 sha256=09ece140cd18262a93151b10d3bb732450b7244564da8bb11e86c998e372b358\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-of6ec5ks/wheels/a8/8f/18/b30c4402c2d6ab52853310650b85822afe26aaae5f6d00356b\n",
            "Successfully built nequip\n",
            "Installing collected packages: opt-einsum-fx, torch-runstats, torch-ema, scikit-learn, e3nn, ase, nequip\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "Successfully installed ase-3.22.1 e3nn-0.5.0 nequip-0.5.5 opt-einsum-fx-0.1.4 scikit-learn-1.0.1 torch-ema-0.3 torch-runstats-0.2.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f72c67d1dd0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# install wandb\n",
        "!pip install wandb\n",
        "# install nequip\n",
        "!pip install nequip/\n",
        "# fix colab imports\n",
        "import site\n",
        "\n",
        "site.main()\n",
        "# set to allow anonymous WandB\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "import numpy as np\n",
        "from ase.io import read, write\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8c489d789b554721b386c847f58fb18c"
          ]
        },
        "id": "ZixXbCiPcMGg",
        "outputId": "1536f815-ddf5-4b59-cd6f-11a996ee680a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nglview\n",
            "  Downloading nglview-3.0.3.tar.gz (5.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7 MB 6.7 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nglview) (1.21.6)\n",
            "Requirement already satisfied: jupyterlab-widgets in /usr/local/lib/python3.7/dist-packages (from nglview) (1.1.0)\n",
            "Requirement already satisfied: ipywidgets>=7 in /usr/local/lib/python3.7/dist-packages (from nglview) (7.7.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (5.1.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (3.6.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (4.10.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (5.4.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (5.5.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7->nglview) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7->nglview) (5.3.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (57.4.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7->nglview) (4.10.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7->nglview) (2.15.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7->nglview) (4.3.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (4.11.4)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (5.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (0.18.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (4.1.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (21.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7->nglview) (3.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7->nglview) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7->nglview) (1.15.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (5.3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (1.8.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.13.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7->nglview) (2.8.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7->nglview) (23.1.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (2.0.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.6.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (5.0.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.5.1)\n",
            "Building wheels for collected packages: nglview\n",
            "  Building wheel for nglview (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nglview: filename=nglview-3.0.3-py3-none-any.whl size=8057551 sha256=d5e84a553f3b42105f2773e9a9ac6c288d710ba2a9b549366863f5b11e54cb64\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/0c/49/c6f79d8edba8fe89752bf20de2d99040bfa57db0548975c5d5\n",
            "Successfully built nglview\n",
            "Installing collected packages: nglview\n",
            "Successfully installed nglview-3.0.3\n",
            "Enabling notebook extension nglview-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/etc/jupyter/nbconfig/notebook.json\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/etc/jupyter/nbconfig/notebook.json\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c489d789b554721b386c847f58fb18c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip3 install nglview\n",
        "!jupyter-nbextension enable nglview --py --sys-prefix\n",
        "import nglview as nv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDJ9Re0arOb0"
      },
      "outputs": [],
      "source": [
        "def MD_reader_xyz(f, data_dir, no_skip=0):\n",
        "    filename = os.path.join(data_dir, f)\n",
        "    fo = open(filename, \"r\")\n",
        "    natoms_str = fo.read().rsplit(\" i = \")[0]\n",
        "    natoms = int(natoms_str.split(\"\\n\")[0])\n",
        "    fo.close()\n",
        "    fo = open(filename, \"r\")\n",
        "    samples = fo.read().split(natoms_str)[1:]\n",
        "    steps = []\n",
        "    xyz = []\n",
        "    temperatures = []\n",
        "    energies = []\n",
        "    for sample in samples[::no_skip]:\n",
        "        entries = sample.split(\"\\n\")[:-1]\n",
        "        energies.append(float(entries[0].split(\"=\")[-1]))\n",
        "        temp = np.array([list(map(float, lv.split()[1:])) for lv in entries[1:]])\n",
        "        xyz.append(temp[:, :])\n",
        "    return natoms_str, np.array(xyz), np.array(energies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DziQT_zjMKS4"
      },
      "outputs": [],
      "source": [
        "from ase.build import sort\n",
        "\n",
        "\n",
        "def MD_writer_xyz(\n",
        "    positions, forces, cell_vec_abc, energies, data_dir, f, conv_frc=1.0, conv_ener=1.0\n",
        "):\n",
        "\n",
        "    filename = os.path.join(data_dir, f)\n",
        "    fo = open(filename, \"w\")\n",
        "\n",
        "    for it, frame in enumerate(positions):\n",
        "        natoms = len(frame)\n",
        "        fo.write(\"{:5d}\\n\".format(natoms))\n",
        "        fo.write(\n",
        "            'Lattice=\"{:.5f} 0.0 0.0 0.0 {:.5f} 0.0 0.0 0.0 {:.5f}\" \\\n",
        "    Properties=\"species:S:1:pos:R:3:forces:R:3\" \\\n",
        "    energy={:.10f} pbc=\"T T T\"\\n'.format(\n",
        "                cell_vec_abc[0],\n",
        "                cell_vec_abc[1],\n",
        "                cell_vec_abc[2],\n",
        "                energies[it] * conv_ener,\n",
        "            )\n",
        "        )\n",
        "        if it % 1000 == 0.0:\n",
        "            print(it)\n",
        "\n",
        "        sorted_frame = sort(frame)\n",
        "        sorted_forces = sort(forces[it])\n",
        "\n",
        "        fo.write(\n",
        "            \"\".join(\n",
        "                \"{:8s} {:.8f} {:16.8f} {:16.8f}\\\n",
        "     {:16.8f} {:16.8f} {:16.8f}\\n\".format(\n",
        "                    sorted_frame[iat].symbol,\n",
        "                    sorted_frame[iat].position[0],\n",
        "                    sorted_frame[iat].position[1],\n",
        "                    sorted_frame[iat].position[2],\n",
        "                    sorted_forces[iat].position[0] * conv_frc,\n",
        "                    sorted_forces[iat].position[1] * conv_frc,\n",
        "                    sorted_forces[iat].position[2] * conv_frc,\n",
        "                )\n",
        "                for iat in range(len(frame))\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2U5i3IZqLBI",
        "outputId": "362448e0-367e-43df-d34f-a697f7afdcb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([9.85, 9.85, 9.85])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def read_cell(f, data_dir):\n",
        "    filename = os.path.join(data_dir, f)\n",
        "    fo = open(filename, \"r\")\n",
        "    cell_list_abc = fo.read().split(\"\\n\")[:-1]\n",
        "    cell_vec_abc = np.array(\n",
        "        [list(map(float, lv.split())) for lv in cell_list_abc]\n",
        "    ).squeeze()\n",
        "    return cell_vec_abc\n",
        "\n",
        "\n",
        "cell_vec_abc = read_cell(\"celldata.dat\", data_dir + \"/AIMD_data\")\n",
        "cell_vec_abc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUcVzREa7XNR"
      },
      "source": [
        "### Read positions, energies, forces from of a 32 water molecules box in .xyz format generated with CP2K using the SCAN functional\n",
        "### The Energy is in Hartree while the forces are in eV/angstrom, therefore we convert also the energy in eV to make it consistent with LAMMPS \"metal\" units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9fcRMYnu5-V"
      },
      "outputs": [],
      "source": [
        "wat_traj = read(data_dir + \"/AIMD_data/WATER-pos-10k-1.xyz\", index=\":\")\n",
        "wat_frc = read(data_dir + \"/AIMD_data/WATER-frc-10k-1.xyz\", index=\":\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFK_vOgL7ul8"
      },
      "source": [
        "### The reader below is required to get the energies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT7B4ryYu82t"
      },
      "outputs": [],
      "source": [
        "natoms, positions, energies = MD_reader_xyz(\n",
        "    \"WATER-pos-10k-1.xyz\", data_dir + \"/AIMD_data/\", no_skip=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BivsnMaz8NoG"
      },
      "source": [
        "### The writer below is useful to convert 2 separate ase Atom objects of the positions and forces, and np.arrays of energies and cell, to an .extxyz file that can be read by nequip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgmmJAU5l5F2",
        "outputId": "b27ecc94-a9fe-4d30-cd32-23e9cdeb5447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n"
          ]
        }
      ],
      "source": [
        "MD_writer_xyz(\n",
        "    wat_traj,\n",
        "    wat_frc,\n",
        "    cell_vec_abc,\n",
        "    energies,\n",
        "    data_dir + \"/AIMD_data/\",\n",
        "    \"wat_pos_frc-10k.extxyz\",\n",
        "    conv_frc=1.0,\n",
        "    conv_ener=27.211399,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPqnt-SAXyvL"
      },
      "source": [
        "### Turn on GPU\n",
        "\n",
        "Make sure Runtime --> Change runtime type is set to GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HDmxkn3z8_m"
      },
      "source": [
        "## 3 Steps: \n",
        "* Train: using a data set, train the neural network 🧠 \n",
        "* Deploy: convert the Python-based model into a stand-alone potential file for fast execution ⚡\n",
        "* Run: run Molecular Dynamics, Monte Carlo, Structural Minimization, ...  with it in LAMMPS 🏃"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OD71eeDz7dA"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/all.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62aEgq6QYFIn"
      },
      "source": [
        "### Train a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELdBzH_8z4_2"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/train.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KuOIippfVfd"
      },
      "source": [
        "Here, we will train a NequIP potential on the following system\n",
        "\n",
        "* Water\n",
        "* sampled at T=300K from AIMD\n",
        "* Using 1000 training configurations\n",
        "* The units of the reference data are in eV and A."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mgoydrJW5lg0",
        "outputId": "22e4379b-7ebc-4eef-ac70-9ab4e9cbba02"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.5.5'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nequip\n",
        "\n",
        "nequip.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff4YA7aK1o9d"
      },
      "source": [
        "### Below is the configuration file used by nequip. We provide the dataset in an .extxyz file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFCszShRk2RP",
        "outputId": "ab9cef70-901b-40de-eb1b-aa233891f5dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'root': 'results/water', 'run_name': 'example-run-water', 'seed': 123, 'dataset_seed': 456, 'append': True, 'default_dtype': 'float32', 'allow_tf32': False, 'r_max': 4.0, 'num_layers': 4, 'l_max': 1, 'parity': True, 'num_features': 32, 'nonlinearity_type': 'gate', 'resnet': False, 'nonlinearity_scalars': {'e': 'silu', 'o': 'tanh'}, 'nonlinearity_gates': {'e': 'silu', 'o': 'tanh'}, 'num_basis': 8, 'BesselBasis_trainable': True, 'PolynomialCutoff_p': 6, 'invariant_layers': 2, 'invariant_neurons': 64, 'avg_num_neighbors': 'auto', 'use_sc': True, 'dataset': 'ase', 'dataset_file_name': './nequip/data/AIMD_data/wat_pos_frc-10k.extxyz', 'ase_args': {'format': 'extxyz'}, 'include_keys': ['user_label'], 'key_mapping': {'user_label': 'label0'}, 'chemical_symbols': ['H', 'O'], 'verbose': 'info', 'log_batch_freq': 1, 'log_epoch_freq': 1, 'save_checkpoint_freq': -1, 'save_ema_checkpoint_freq': -1, 'n_train': 100, 'n_val': 50, 'learning_rate': 0.005, 'batch_size': 5, 'validation_batch_size': 10, 'max_epochs': 100000, 'train_val_split': 'random', 'shuffle': True, 'metrics_key': 'validation_loss', 'use_ema': True, 'ema_decay': 0.99, 'ema_use_num_updates': True, 'report_init_validation': True, 'early_stopping_patiences': {'validation_loss': 50}, 'early_stopping_delta': {'validation_loss': 0.005}, 'early_stopping_cumulative_delta': False, 'early_stopping_lower_bounds': {'LR': 1e-05}, 'early_stopping_upper_bounds': {'cumulative_wall': 1e+100}, 'loss_coeffs': {'forces': 1, 'total_energy': [1, 'PerAtomMSELoss']}, 'metrics_components': [['forces', 'mae'], ['forces', 'rmse'], ['forces', 'mae', {'PerSpecies': True, 'report_per_component': False}], ['forces', 'rmse', {'PerSpecies': True, 'report_per_component': False}], ['total_energy', 'mae'], ['total_energy', 'mae', {'PerAtom': True}]], 'optimizer_name': 'Adam', 'optimizer_amsgrad': False, 'optimizer_betas': (0.9, 0.999), 'optimizer_eps': 1e-08, 'optimizer_weight_decay': 0, 'max_gradient_norm': None, 'lr_scheduler_name': 'ReduceLROnPlateau', 'lr_scheduler_patience': 100, 'lr_scheduler_factor': 0.5, 'per_species_rescale_scales_trainable': False, 'per_species_rescale_shifts_trainable': False, 'per_species_rescale_shifts': 'dataset_per_atom_total_energy_mean', 'per_species_rescale_scales': 'dataset_forces_rms', 'global_rescale_shift': None, 'global_rescale_scale': 'dataset_forces_rms', 'global_rescale_shift_trainable': False, 'global_rescale_scale_trainable': False}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nequip.utils import Config\n",
        "\n",
        "config = Config.from_file(\"/content/nequip/configs/water-example.yaml\")\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukSnt_QD5avu",
        "outputId": "6e7d2f03-fff9-49ec-ef09-f72e7f3dd100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch device: cuda\n",
            "Processing dataset...\n",
            "Loaded data: Batch(atomic_numbers=[960000, 1], batch=[960000], cell=[10000, 3, 3], edge_cell_shift=[25556838, 3], edge_index=[2, 25556838], forces=[960000, 3], pbc=[10000, 3], pos=[960000, 3], ptr=[10001], total_energy=[10000, 1])\n",
            "Cached processed data to disk\n",
            "Done!\n",
            "Successfully loaded the data set of type ASEDataset(10000)...\n",
            "Replace string dataset_forces_rms to 0.7736424803733826\n",
            "Replace string dataset_per_atom_total_energy_mean to -156.0919189453125\n",
            "Atomic outputs are scaled by: [H, O: 0.773642], shifted by [H, O: -156.091919].\n",
            "Replace string dataset_forces_rms to 0.7736424803733826\n",
            "Initially outputs are globally scaled by: 0.7736424803733826, total_energy are globally shifted by None.\n",
            "Successfully built the network...\n",
            "Number of weights: 154200\n",
            "! Starting training ...\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      0     1          1.1         1.09       0.0142        0.606        0.807        0.515        0.787        0.651        0.688            1        0.846         8.82       0.0919\n",
            "      0     2         1.02         1.01       0.0135        0.577        0.776        0.491        0.749         0.62        0.662        0.965        0.813         8.61       0.0896\n",
            "      0     3        0.992        0.979       0.0132        0.574        0.765        0.494        0.734        0.614        0.659        0.943        0.801         8.52       0.0888\n",
            "      0     4        0.973         0.96       0.0134        0.564        0.758         0.48        0.733        0.606        0.653        0.933        0.793         8.58       0.0894\n",
            "      0     5        0.969        0.956       0.0131        0.571        0.756        0.491        0.732        0.612         0.65        0.934        0.792          8.5       0.0885\n",
            "\n",
            "\n",
            "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Initial Validation          0    4.292    0.005        0.998       0.0135         1.01        0.578        0.773        0.494        0.747        0.621        0.663        0.956        0.809         8.61       0.0896\n",
            "Wall time: 4.292088351000075\n",
            "! Best model        0    1.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      1     1        0.886        0.873       0.0135         0.55        0.723        0.481        0.687        0.584        0.633        0.875        0.754          8.6       0.0896\n",
            "      1     2         1.02         1.01       0.0064        0.576        0.778        0.498        0.733        0.616        0.664        0.968        0.816         5.93       0.0618\n",
            "      1     3         1.05         1.05      0.00212        0.586        0.792        0.499        0.759        0.629        0.675        0.986         0.83         3.37       0.0351\n",
            "      1     4         1.01         1.01     0.000403        0.573        0.777        0.492        0.735        0.613        0.672        0.954        0.813          1.4       0.0146\n",
            "      1     5        0.962        0.962        5e-05        0.576        0.759        0.499         0.73        0.615        0.655        0.933        0.794        0.481      0.00501\n",
            "      1     6        0.956        0.956      2.6e-05        0.573        0.756        0.491        0.737        0.614        0.647        0.938        0.792         0.33      0.00344\n",
            "      1     7        0.904        0.904     6.71e-05        0.553        0.736        0.472        0.715        0.594        0.632        0.908         0.77        0.529      0.00552\n",
            "      1     8        0.891        0.891     3.62e-05         0.53         0.73        0.457        0.677        0.567        0.626        0.903        0.764        0.413       0.0043\n",
            "      1     9        0.826        0.826     0.000137        0.533        0.703        0.457        0.685        0.571        0.597        0.878        0.738        0.817      0.00851\n",
            "      1    10        0.893        0.893     0.000659        0.558        0.731        0.474        0.726          0.6        0.627        0.903        0.765         1.89       0.0197\n",
            "      1    11        0.769        0.767      0.00131        0.505        0.678        0.435        0.646        0.541        0.581        0.839         0.71         2.69        0.028\n",
            "      1    12        0.654        0.651        0.003        0.474        0.624        0.403        0.615        0.509        0.526        0.785        0.655         4.05       0.0422\n",
            "      1    13        0.596        0.591      0.00545        0.458        0.595        0.394        0.586         0.49         0.51        0.735        0.623         5.47        0.057\n",
            "      1    14         0.59        0.581      0.00867        0.445         0.59        0.381        0.574        0.477          0.5        0.737        0.619          6.9       0.0719\n",
            "      1    15        0.528        0.516       0.0123         0.41        0.556        0.343        0.544        0.444        0.463        0.705        0.584         8.23       0.0858\n",
            "      1    16         0.46        0.445       0.0148        0.394        0.516        0.331         0.52        0.425        0.434         0.65        0.542         9.04       0.0941\n",
            "      1    17        0.346        0.333       0.0131        0.345        0.446        0.303        0.428        0.365        0.388        0.545        0.466         8.49       0.0884\n",
            "      1    18        0.335        0.325      0.00993         0.34        0.441        0.302        0.415        0.359        0.386        0.535         0.46         7.38       0.0769\n",
            "      1    19        0.295        0.293      0.00247         0.33        0.419        0.287        0.416        0.351         0.36        0.516        0.438         3.67       0.0382\n",
            "      1    20        0.273        0.272     0.000177        0.313        0.404        0.279        0.381         0.33        0.355        0.487        0.421        0.965       0.0101\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      1     1        0.298        0.298     0.000639        0.326        0.422        0.281        0.415        0.348        0.357        0.529        0.443         1.84       0.0191\n",
            "      1     2        0.273        0.272     0.000526        0.312        0.404        0.272        0.391        0.332        0.348        0.497        0.422         1.65       0.0172\n",
            "      1     3        0.253        0.253     0.000496        0.304        0.389        0.272        0.368         0.32        0.343        0.467        0.405         1.62       0.0169\n",
            "      1     4        0.264        0.263     0.000611        0.311        0.397        0.274        0.385        0.329        0.344        0.487        0.415         1.79       0.0187\n",
            "      1     5        0.265        0.264     0.000581        0.308        0.398        0.266        0.392        0.329        0.338        0.496        0.417         1.77       0.0184\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               1    8.744    0.005        0.707      0.00473        0.712        0.481        0.651        0.414        0.616        0.515        0.558        0.805        0.681         4.03        0.042\n",
            "! Validation          1    8.744    0.005         0.27     0.000571        0.271        0.312        0.402        0.273         0.39        0.332        0.346        0.495        0.421         1.73       0.0181\n",
            "Wall time: 8.744555983999817\n",
            "! Best model        1    0.271\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      2     1         0.27        0.266      0.00439        0.312        0.399        0.277        0.382         0.33         0.35        0.482        0.416         4.87       0.0508\n",
            "      2     2        0.263        0.254      0.00901        0.299         0.39        0.269        0.358        0.313        0.351        0.457        0.404         7.02       0.0731\n",
            "      2     3        0.212          0.2       0.0126        0.265        0.346         0.23        0.335        0.283        0.297        0.426        0.362         8.31       0.0866\n",
            "      2     4        0.207        0.199       0.0088        0.266        0.345        0.233        0.333        0.283        0.298        0.424        0.361         6.94       0.0723\n",
            "      2     5        0.204        0.202      0.00223        0.268        0.347        0.232        0.338        0.285        0.299        0.428        0.364         3.49       0.0364\n",
            "      2     6        0.161        0.161     0.000149        0.239         0.31         0.21        0.299        0.254        0.269        0.379        0.324        0.696      0.00725\n",
            "      2     7        0.167        0.166     0.000341        0.246        0.315        0.213        0.312        0.263        0.272        0.389         0.33         1.21       0.0126\n",
            "      2     8        0.152        0.152     0.000398         0.23        0.301        0.197        0.295        0.246        0.253         0.38        0.316         1.25        0.013\n",
            "      2     9        0.174        0.173     0.000924        0.246        0.322        0.211        0.317        0.264        0.272        0.404        0.338         2.24       0.0233\n",
            "      2    10        0.154        0.154     0.000707        0.236        0.303        0.204          0.3        0.252         0.26        0.375        0.317         1.93       0.0201\n",
            "      2    11        0.146        0.145     0.000939        0.231        0.295        0.204        0.284        0.244        0.256         0.36        0.308         2.23       0.0232\n",
            "      2    12        0.163        0.162     0.000592        0.246        0.312        0.215        0.309        0.262        0.271         0.38        0.325         1.78       0.0186\n",
            "      2    13        0.142        0.141     0.000747        0.226         0.29        0.195        0.289        0.242        0.245        0.364        0.305         1.99       0.0207\n",
            "      2    14         0.13        0.128      0.00154        0.214        0.277        0.184        0.274        0.229        0.233        0.349        0.291         2.88         0.03\n",
            "      2    15        0.138        0.136      0.00201         0.22        0.286        0.194        0.273        0.233        0.247        0.351        0.299          3.3       0.0344\n",
            "      2    16        0.147        0.146      0.00121        0.224        0.295        0.189        0.292        0.241        0.242         0.38        0.311         2.54       0.0264\n",
            "      2    17        0.137        0.136     0.000426        0.222        0.286        0.194        0.279        0.236        0.244        0.355        0.299         1.48       0.0155\n",
            "      2    18        0.137        0.137     3.24e-05         0.22        0.286         0.19        0.278        0.234        0.244        0.356          0.3        0.353      0.00367\n",
            "      2    19        0.122        0.122     4.68e-05        0.208         0.27        0.182        0.262        0.222        0.233        0.332        0.282        0.402      0.00419\n",
            "      2    20        0.127        0.126     0.000242        0.211        0.275        0.179        0.274        0.227        0.231        0.347        0.289         1.14       0.0119\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      2     1        0.128        0.128     4.02e-05        0.211        0.277        0.181        0.272        0.226        0.229        0.353        0.291        0.433      0.00451\n",
            "      2     2        0.132        0.132      3.8e-05        0.215        0.281        0.186        0.275         0.23        0.239         0.35        0.294         0.42      0.00437\n",
            "      2     3        0.114        0.114     2.33e-05        0.203        0.261        0.178        0.253        0.216        0.226        0.319        0.272         0.29      0.00302\n",
            "      2     4         0.12         0.12     4.85e-05        0.205        0.268        0.179        0.257        0.218        0.229        0.333        0.281        0.392      0.00408\n",
            "      2     5        0.128        0.127     3.44e-05        0.211        0.276        0.179        0.274        0.227        0.229        0.353        0.291        0.366      0.00381\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               2   12.437    0.005        0.165      0.00237        0.168        0.241        0.314         0.21        0.304        0.257        0.271        0.388        0.329          2.8       0.0292\n",
            "! Validation          2   12.437    0.005        0.124     3.69e-05        0.124        0.209        0.273        0.181        0.266        0.223         0.23        0.342        0.286         0.38      0.00396\n",
            "Wall time: 12.438124148000043\n",
            "! Best model        2    0.124\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      3     1        0.123        0.122      0.00115        0.205        0.271        0.169        0.278        0.223        0.221        0.349        0.285         2.48       0.0258\n",
            "      3     2        0.126        0.124      0.00129        0.209        0.273        0.181        0.265        0.223        0.231        0.341        0.286         2.63       0.0274\n",
            "      3     3        0.128        0.127     0.000888        0.212        0.275        0.186        0.265        0.225        0.239        0.336        0.288         2.16       0.0225\n",
            "      3     4        0.104        0.104      0.00017        0.192        0.249        0.165        0.245        0.205        0.212        0.311        0.261        0.896      0.00933\n",
            "      3     5        0.122        0.122     0.000148        0.204         0.27        0.176        0.261        0.218        0.233        0.332        0.283        0.863      0.00898\n",
            "      3     6       0.0961       0.0959     0.000121        0.184         0.24        0.157        0.238        0.198        0.199        0.305        0.252        0.734      0.00764\n",
            "      3     7        0.104        0.103      0.00029        0.193        0.249        0.166        0.246        0.206        0.211        0.311        0.261         1.15        0.012\n",
            "      3     8        0.114        0.114     0.000186        0.199        0.261        0.171        0.256        0.214        0.221        0.325        0.273        0.973       0.0101\n",
            "      3     9        0.109        0.109     4.67e-05        0.196        0.255        0.169        0.252         0.21         0.22        0.314        0.267        0.334      0.00348\n",
            "      3    10       0.0941       0.0939     0.000195        0.184        0.237        0.162        0.228        0.195        0.205         0.29        0.248         1.01       0.0105\n",
            "      3    11        0.106        0.106     0.000142        0.189        0.252        0.164        0.239        0.201        0.213        0.314        0.264        0.786      0.00818\n",
            "      3    12       0.0905       0.0905     1.81e-05        0.181        0.233        0.157        0.228        0.193        0.199        0.289        0.244        0.252      0.00262\n",
            "      3    13        0.102        0.102     4.53e-05        0.193        0.247        0.169         0.24        0.204        0.214        0.302        0.258        0.485      0.00506\n",
            "      3    14       0.0898       0.0896     0.000191         0.18        0.232         0.16        0.221        0.191        0.205        0.277        0.241        0.887      0.00924\n",
            "      3    15       0.0931        0.093     3.22e-05        0.185        0.236        0.159        0.236        0.197        0.201        0.294        0.247        0.389      0.00405\n",
            "      3    16       0.0853       0.0851     0.000143        0.174        0.226        0.146         0.23        0.188        0.188        0.287        0.237        0.841      0.00876\n",
            "      3    17       0.0794       0.0791     0.000293        0.169        0.218        0.149         0.21        0.179        0.194        0.259        0.226         1.21       0.0126\n",
            "      3    18       0.0847       0.0844     0.000328        0.174        0.225        0.149        0.226        0.187        0.191         0.28        0.236         1.25       0.0131\n",
            "      3    19        0.098        0.098     3.49e-05        0.187        0.242        0.161        0.238          0.2        0.206        0.302        0.254         0.38      0.00396\n",
            "      3    20       0.0859       0.0858     8.84e-05        0.175        0.227         0.15        0.226        0.188        0.193        0.282        0.238        0.653      0.00681\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      3     1       0.0955       0.0955     2.49e-05        0.182        0.239        0.155        0.234        0.195        0.199        0.304        0.251        0.297      0.00309\n",
            "      3     2       0.0993       0.0992     3.77e-05        0.186        0.244        0.161        0.238        0.199         0.21          0.3        0.255        0.396      0.00412\n",
            "      3     3       0.0819       0.0819     2.86e-05        0.171        0.221        0.151        0.212        0.182        0.194        0.268        0.231        0.308       0.0032\n",
            "      3     4       0.0904       0.0904     1.67e-05        0.177        0.233        0.154        0.224        0.189        0.199        0.289        0.244        0.283      0.00295\n",
            "      3     5       0.0939       0.0939     1.97e-05        0.181        0.237        0.154        0.235        0.195        0.198        0.301        0.249        0.247      0.00258\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               3   16.132    0.005        0.101      0.00029        0.102        0.189        0.246        0.163        0.241        0.202         0.21        0.306        0.258         1.02       0.0106\n",
            "! Validation          3   16.132    0.005       0.0922     2.55e-05       0.0922         0.18        0.235        0.155        0.228        0.192          0.2        0.292        0.246        0.306      0.00319\n",
            "Wall time: 16.1325652229998\n",
            "! Best model        3    0.092\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      4     1       0.0771        0.077     4.66e-05        0.165        0.215        0.142        0.211        0.177        0.183        0.268        0.225        0.425      0.00443\n",
            "      4     2       0.0834       0.0834     5.56e-05        0.175        0.223         0.15        0.227        0.188        0.187        0.282        0.235        0.492      0.00512\n",
            "      4     3       0.0764       0.0763     8.12e-05        0.169        0.214        0.146        0.215        0.181        0.183        0.265        0.224        0.609      0.00634\n",
            "      4     4       0.0777       0.0775      0.00013        0.166        0.215        0.148        0.202        0.175        0.195        0.252        0.223        0.795      0.00828\n",
            "      4     5       0.0776       0.0775     9.66e-05        0.166        0.215        0.144        0.208        0.176        0.184        0.267        0.226         0.69      0.00719\n",
            "      4     6       0.0768       0.0762     0.000654        0.163        0.214        0.142        0.205        0.174        0.183        0.264        0.223         1.89       0.0196\n",
            "      4     7       0.0787       0.0782     0.000551        0.166        0.216        0.144        0.211        0.177        0.185        0.268        0.226         1.72       0.0179\n",
            "      4     8       0.0756       0.0756     3.23e-05        0.165        0.213        0.143        0.209        0.176        0.183        0.262        0.223        0.362      0.00377\n",
            "      4     9        0.074       0.0738     0.000186        0.161         0.21        0.138        0.207        0.172        0.177        0.265        0.221        0.937      0.00976\n",
            "      4    10       0.0778       0.0773     0.000587        0.166        0.215        0.142        0.215        0.178        0.185        0.265        0.225         1.77       0.0184\n",
            "      4    11       0.0726       0.0725     7.92e-05        0.159        0.208        0.135        0.207        0.171        0.175        0.263        0.219        0.619      0.00645\n",
            "      4    12       0.0609       0.0609     6.43e-05        0.148        0.191        0.128        0.189        0.158        0.164        0.235          0.2        0.546      0.00569\n",
            "      4    13       0.0593       0.0592     6.22e-05        0.146        0.188        0.127        0.185        0.156        0.162        0.231        0.197        0.561      0.00584\n",
            "      4    14       0.0751       0.0751     9.74e-06        0.163        0.212        0.141        0.207        0.174        0.182        0.262        0.222        0.196      0.00205\n",
            "      4    15       0.0636       0.0634     0.000251        0.152        0.195        0.131        0.195        0.163        0.168         0.24        0.204         1.16       0.0121\n",
            "      4    16        0.078       0.0776     0.000349        0.163        0.216        0.142        0.206        0.174        0.185        0.267        0.226         1.32       0.0138\n",
            "      4    17        0.053        0.053     5.15e-05        0.137        0.178         0.12         0.17        0.145        0.158        0.213        0.185        0.433      0.00451\n",
            "      4    18        0.065       0.0648     0.000272        0.151        0.197         0.13        0.195        0.162        0.168        0.244        0.206         1.17       0.0122\n",
            "      4    19       0.0543       0.0542     0.000137         0.14         0.18        0.124        0.172        0.148        0.158        0.218        0.188        0.864        0.009\n",
            "      4    20       0.0703       0.0702     9.19e-05         0.16        0.205         0.14        0.201         0.17         0.18        0.248        0.214        0.629      0.00655\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      4     1       0.0703       0.0703     1.68e-05        0.156        0.205        0.135          0.2        0.167        0.174        0.257        0.215        0.258      0.00268\n",
            "      4     2       0.0727       0.0727     2.65e-05        0.159        0.209         0.14        0.199        0.169        0.184        0.251        0.217        0.314      0.00327\n",
            "      4     3       0.0591       0.0591     1.29e-05        0.146        0.188         0.13        0.178        0.154        0.166        0.225        0.196        0.227      0.00237\n",
            "      4     4       0.0644       0.0644     1.17e-05        0.151        0.196        0.131        0.191        0.161        0.168        0.243        0.206        0.206      0.00214\n",
            "      4     5       0.0637       0.0637     1.16e-05        0.149        0.195        0.127        0.192         0.16        0.167        0.243        0.205        0.193      0.00201\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               4   19.818    0.005       0.0712     0.000189       0.0714        0.159        0.206        0.138        0.202         0.17        0.178        0.254        0.216         0.86      0.00895\n",
            "! Validation          4   19.818    0.005        0.066     1.59e-05        0.066        0.152        0.199        0.132        0.192        0.162        0.172        0.244        0.208        0.239      0.00249\n",
            "Wall time: 19.818296705999728\n",
            "! Best model        4    0.066\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      5     1        0.066       0.0657     0.000242        0.151        0.198        0.133        0.186        0.159        0.174         0.24        0.207          1.1       0.0114\n",
            "      5     2        0.068        0.068     5.88e-05        0.155        0.202        0.136        0.193        0.164        0.175        0.246        0.211         0.52      0.00542\n",
            "      5     3       0.0584       0.0581     0.000315        0.142        0.186        0.122         0.18        0.151        0.161         0.23        0.195         1.24       0.0129\n",
            "      5     4        0.055       0.0545     0.000554        0.139        0.181         0.12        0.177        0.148        0.154        0.224        0.189         1.73        0.018\n",
            "      5     5       0.0494       0.0493      0.00014        0.133        0.172        0.117        0.166        0.141        0.149        0.209        0.179        0.838      0.00873\n",
            "      5     6       0.0516       0.0515     8.55e-05        0.136        0.176        0.118        0.173        0.146        0.153        0.214        0.183        0.619      0.00645\n",
            "      5     7       0.0525       0.0522     0.000236        0.138        0.177        0.119        0.178        0.148        0.149        0.222        0.186         1.11       0.0116\n",
            "      5     8       0.0561       0.0557     0.000401        0.142        0.183        0.125        0.175         0.15        0.161         0.22         0.19         1.47       0.0154\n",
            "      5     9       0.0497       0.0496     1.23e-05        0.133        0.172        0.116        0.166        0.141        0.152        0.208         0.18        0.226      0.00236\n",
            "      5    10       0.0543       0.0541      0.00018         0.14         0.18        0.124        0.171        0.148        0.159        0.216        0.187        0.971       0.0101\n",
            "      5    11       0.0636       0.0634     0.000142         0.15        0.195         0.13        0.189        0.159        0.169        0.239        0.204        0.874       0.0091\n",
            "      5    12       0.0526       0.0526     8.18e-06        0.138        0.177        0.122        0.171        0.146        0.157        0.213        0.185         0.16      0.00167\n",
            "      5    13       0.0531       0.0529     0.000262        0.137        0.178        0.122        0.167        0.145        0.158        0.212        0.185         1.18       0.0122\n",
            "      5    14       0.0441       0.0438     0.000332        0.127        0.162        0.112        0.158        0.135        0.142        0.196        0.169         1.35       0.0141\n",
            "      5    15       0.0456       0.0454     0.000246        0.127        0.165        0.111         0.16        0.136        0.142        0.203        0.172         1.16       0.0121\n",
            "      5    16       0.0502       0.0502     7.74e-06        0.135        0.173        0.123        0.158        0.141         0.16        0.198        0.179        0.159      0.00166\n",
            "      5    17       0.0464       0.0457     0.000677        0.126        0.165        0.108        0.162        0.135        0.137        0.211        0.174         1.92         0.02\n",
            "      5    18       0.0435       0.0423      0.00118        0.122        0.159        0.108        0.149        0.129        0.142        0.188        0.165         2.55       0.0265\n",
            "      5    19       0.0496       0.0493     0.000295        0.134        0.172        0.117        0.169        0.143        0.151        0.208        0.179         1.26       0.0131\n",
            "      5    20       0.0523       0.0523     1.44e-05        0.135        0.177        0.121        0.163        0.142        0.157        0.211        0.184        0.276      0.00287\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      5     1       0.0568       0.0568     2.65e-05        0.141        0.184        0.123        0.177         0.15        0.159        0.227        0.193        0.315      0.00328\n",
            "      5     2        0.058       0.0579     3.86e-05        0.143        0.186        0.127        0.175        0.151        0.167         0.22        0.193        0.408      0.00425\n",
            "      5     3       0.0471       0.0471     5.02e-05         0.13        0.168        0.117        0.156        0.136        0.151        0.198        0.174        0.469      0.00488\n",
            "      5     4        0.051        0.051     3.31e-05        0.135        0.175        0.117         0.17        0.144        0.151        0.214        0.183        0.369      0.00385\n",
            "      5     5       0.0494       0.0494     4.74e-05        0.132        0.172        0.114        0.167        0.141        0.149         0.21         0.18        0.459      0.00479\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               5   23.524    0.005       0.0528     0.000269       0.0531        0.137        0.178         0.12        0.171        0.145        0.155        0.216        0.186         1.04       0.0108\n",
            "! Validation          5   23.524    0.005       0.0524     3.92e-05       0.0525        0.136        0.177        0.119        0.169        0.144        0.155        0.214        0.185        0.404      0.00421\n",
            "Wall time: 23.525153133000003\n",
            "! Best model        5    0.052\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      6     1       0.0443       0.0442     0.000107        0.125        0.163        0.108         0.16        0.134         0.14        0.201         0.17        0.749       0.0078\n",
            "      6     2       0.0422        0.042     0.000277        0.124        0.158        0.109        0.152        0.131        0.138        0.193        0.165         1.23       0.0129\n",
            "      6     3       0.0457       0.0455     0.000197        0.128        0.165        0.115        0.155        0.135        0.149        0.192        0.171         1.02       0.0107\n",
            "      6     4       0.0522       0.0522     8.34e-06        0.136        0.177        0.121        0.164        0.143        0.157        0.211        0.184         0.21      0.00219\n",
            "      6     5       0.0449       0.0448      7.6e-05        0.127        0.164        0.113        0.156        0.135        0.144        0.197        0.171        0.576        0.006\n",
            "      6     6       0.0492       0.0492     9.71e-06         0.13        0.172        0.115         0.16        0.137        0.153        0.203        0.178        0.212      0.00221\n",
            "      6     7       0.0431       0.0431     1.13e-05        0.126        0.161        0.113        0.151        0.132        0.143        0.191        0.167        0.225      0.00234\n",
            "      6     8       0.0443        0.044     0.000307        0.125        0.162        0.108        0.157        0.133        0.139          0.2         0.17         1.27       0.0133\n",
            "      6     9       0.0378       0.0377     6.48e-05        0.118         0.15        0.103        0.148        0.126        0.132        0.181        0.157        0.595       0.0062\n",
            "      6    10       0.0475       0.0474     1.04e-05        0.129        0.169        0.112        0.162        0.137        0.147        0.205        0.176        0.183      0.00191\n",
            "      6    11       0.0531        0.053     5.87e-05        0.136        0.178        0.119        0.169        0.144        0.157        0.215        0.186        0.451       0.0047\n",
            "      6    12       0.0462       0.0458     0.000342        0.129        0.166        0.115        0.157        0.136        0.146        0.199        0.173         1.36       0.0141\n",
            "      6    13       0.0472       0.0468     0.000329         0.13        0.167        0.115        0.158        0.137        0.149          0.2        0.174         1.29       0.0134\n",
            "      6    14        0.045        0.045     2.38e-05        0.128        0.164        0.114        0.156        0.135        0.147        0.193         0.17        0.322      0.00336\n",
            "      6    15       0.0397       0.0397     9.63e-06        0.118        0.154        0.104        0.146        0.125        0.133        0.189        0.161        0.192        0.002\n",
            "      6    16       0.0443       0.0442     7.93e-05        0.126        0.163        0.111        0.156        0.134        0.142        0.197         0.17         0.65      0.00677\n",
            "      6    17       0.0428       0.0425     0.000283        0.122         0.16        0.112        0.144        0.128        0.147        0.182        0.164         1.24       0.0129\n",
            "      6    18       0.0357       0.0357     1.29e-05        0.115        0.146        0.101        0.141        0.121        0.129        0.175        0.152        0.231       0.0024\n",
            "      6    19       0.0489       0.0487     0.000193        0.131        0.171        0.116        0.161        0.139        0.151        0.205        0.178         1.01       0.0105\n",
            "      6    20       0.0429       0.0429     1.11e-05        0.125         0.16        0.111        0.153        0.132        0.143         0.19        0.167        0.214      0.00223\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      6     1       0.0488       0.0488     8.01e-06        0.131        0.171        0.115        0.163        0.139        0.149        0.208        0.179        0.175      0.00182\n",
            "      6     2        0.049        0.049      1.6e-05        0.132        0.171        0.118         0.16        0.139        0.154        0.202        0.178        0.246      0.00256\n",
            "      6     3       0.0404       0.0404     1.72e-05        0.121        0.156        0.109        0.144        0.127         0.14        0.182        0.161        0.265      0.00276\n",
            "      6     4       0.0433       0.0433     1.15e-05        0.125        0.161        0.108        0.157        0.133         0.14        0.197        0.168        0.189      0.00197\n",
            "      6     5        0.042        0.042     1.92e-05        0.121        0.158        0.106        0.152        0.129        0.139        0.191        0.165        0.269       0.0028\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               6   27.239    0.005       0.0447     0.000121       0.0449        0.126        0.164        0.112        0.155        0.134        0.145        0.196         0.17        0.662      0.00689\n",
            "! Validation          6   27.239    0.005       0.0447     1.44e-05       0.0447        0.126        0.164        0.111        0.155        0.133        0.144        0.196         0.17        0.229      0.00238\n",
            "Wall time: 27.239269653000065\n",
            "! Best model        6    0.045\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      7     1       0.0443       0.0442     0.000101        0.125        0.163        0.112        0.152        0.132        0.146        0.192        0.169        0.732      0.00762\n",
            "      7     2       0.0428       0.0428      5.5e-05        0.123         0.16        0.107        0.156        0.131        0.139        0.196        0.167        0.526      0.00548\n",
            "      7     3       0.0455       0.0453     0.000155        0.127        0.165         0.11        0.161        0.135        0.141        0.204        0.172        0.921      0.00959\n",
            "      7     4       0.0437       0.0437     5.97e-05        0.125        0.162        0.115        0.146         0.13        0.147        0.187        0.167        0.522      0.00544\n",
            "      7     5       0.0417       0.0414     0.000279         0.12        0.157        0.108        0.145        0.127        0.143        0.183        0.163         1.19       0.0124\n",
            "      7     6       0.0383       0.0377     0.000554        0.118         0.15        0.105        0.142        0.124        0.136        0.176        0.156         1.75       0.0182\n",
            "      7     7       0.0407       0.0406     2.55e-05        0.121        0.156        0.109        0.145        0.127        0.138        0.186        0.162         0.34      0.00354\n",
            "      7     8       0.0393       0.0393     5.55e-06        0.118        0.153        0.105        0.143        0.124        0.137        0.182         0.16        0.132      0.00138\n",
            "      7     9       0.0413       0.0413     1.74e-05         0.12        0.157        0.109        0.143        0.126        0.139        0.188        0.164        0.264      0.00275\n",
            "      7    10       0.0417       0.0415     0.000165        0.122        0.158        0.106        0.154         0.13        0.135        0.195        0.165        0.926      0.00965\n",
            "      7    11       0.0487       0.0484     0.000286        0.131         0.17         0.12        0.153        0.137        0.159        0.191        0.175         1.25        0.013\n",
            "      7    12       0.0407       0.0407     2.26e-05         0.12        0.156        0.107        0.147        0.127         0.14        0.185        0.162        0.316      0.00329\n",
            "      7    13       0.0411       0.0408     0.000251         0.12        0.156        0.109        0.143        0.126        0.142        0.182        0.162         1.17       0.0122\n",
            "      7    14       0.0446       0.0446     6.82e-06        0.127        0.163        0.114        0.154        0.134        0.146        0.193         0.17        0.158      0.00165\n",
            "      7    15       0.0349       0.0349     5.37e-05        0.112        0.144       0.0978         0.14        0.119        0.125        0.177        0.151        0.529      0.00551\n",
            "      7    16       0.0436       0.0436     7.98e-06        0.124        0.162        0.115        0.143        0.129        0.151         0.18        0.166        0.167      0.00174\n",
            "      7    17       0.0332       0.0331     4.75e-05        0.109        0.141       0.0959        0.135        0.115        0.122        0.172        0.147        0.471      0.00491\n",
            "      7    18       0.0403       0.0402     6.06e-05        0.121        0.155         0.11        0.142        0.126         0.14        0.181        0.161        0.565      0.00589\n",
            "      7    19       0.0379       0.0378     0.000113        0.116         0.15        0.101        0.145        0.123        0.131        0.183        0.157        0.777      0.00809\n",
            "      7    20       0.0397       0.0395     0.000182        0.118        0.154        0.104        0.146        0.125        0.134        0.187         0.16        0.971       0.0101\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      7     1       0.0437       0.0437     5.73e-06        0.124        0.162        0.109        0.155        0.132        0.141        0.197        0.169        0.154      0.00161\n",
            "      7     2        0.043        0.043     1.14e-05        0.124         0.16         0.11        0.151        0.131        0.143         0.19        0.167        0.199      0.00207\n",
            "      7     3       0.0362       0.0362     1.29e-05        0.114        0.147        0.103        0.138         0.12        0.132        0.174        0.153        0.232      0.00242\n",
            "      7     4       0.0385       0.0385      8.2e-06        0.117        0.152        0.102        0.148        0.125        0.132        0.185        0.159        0.156      0.00162\n",
            "      7     5       0.0376       0.0376     1.47e-05        0.115         0.15        0.101        0.143        0.122        0.132        0.181        0.156        0.234      0.00244\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               7   30.938    0.005       0.0411     0.000122       0.0412        0.121        0.157        0.108        0.147        0.127         0.14        0.186        0.163        0.684      0.00712\n",
            "! Validation          7   30.938    0.005       0.0398     1.06e-05       0.0398        0.119        0.154        0.105        0.147        0.126        0.136        0.185        0.161        0.195      0.00203\n",
            "Wall time: 30.938919614000042\n",
            "! Best model        7    0.040\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      8     1       0.0302       0.0302     4.87e-07        0.104        0.134       0.0922        0.127         0.11        0.119        0.162         0.14       0.0443     0.000462\n",
            "      8     2       0.0361       0.0361     4.16e-05        0.112        0.147        0.101        0.134        0.117        0.133        0.172        0.152        0.462      0.00482\n",
            "      8     3       0.0392       0.0391     5.96e-05        0.119        0.153        0.108        0.141        0.124        0.141        0.175        0.158        0.502      0.00523\n",
            "      8     4       0.0396       0.0395     5.87e-05         0.12        0.154        0.106        0.147        0.127        0.137        0.183         0.16        0.548       0.0057\n",
            "      8     5       0.0413       0.0413     5.74e-05         0.12        0.157        0.105         0.15        0.128        0.137        0.191        0.164        0.498      0.00519\n",
            "      8     6       0.0398       0.0397     4.95e-05        0.117        0.154        0.105        0.142        0.123        0.136        0.185        0.161        0.459      0.00479\n",
            "      8     7       0.0334       0.0333     0.000188         0.11        0.141        0.095        0.139        0.117        0.122        0.174        0.148            1       0.0105\n",
            "      8     8        0.039       0.0389     5.89e-05        0.116        0.153        0.103        0.142        0.122        0.134        0.184        0.159        0.525      0.00547\n",
            "      8     9       0.0291        0.029     7.09e-05        0.103        0.132       0.0926        0.125        0.109        0.117        0.158        0.137        0.622      0.00648\n",
            "      8    10       0.0377       0.0376     8.18e-06        0.116         0.15        0.104        0.141        0.123        0.134        0.178        0.156        0.121      0.00126\n",
            "      8    11       0.0377       0.0375     0.000189        0.116         0.15        0.103        0.144        0.123        0.131        0.181        0.156        0.999       0.0104\n",
            "      8    12       0.0327       0.0324      0.00035        0.108        0.139       0.0938        0.135        0.114         0.12        0.171        0.146         1.38       0.0144\n",
            "      8    13       0.0296       0.0295     0.000109        0.104        0.133       0.0933        0.124        0.109        0.122        0.153        0.137        0.755      0.00786\n",
            "      8    14       0.0333       0.0333     1.16e-05        0.109        0.141       0.0955        0.135        0.115        0.124         0.17        0.147         0.23       0.0024\n",
            "      8    15       0.0361        0.036     0.000134        0.112        0.147       0.0994        0.136        0.118        0.132        0.173        0.152         0.85      0.00886\n",
            "      8    16       0.0339       0.0336      0.00029         0.11        0.142       0.0984        0.134        0.116        0.128        0.166        0.147         1.26       0.0131\n",
            "      8    17       0.0305       0.0305     4.11e-05        0.106        0.135        0.092        0.133        0.113        0.117        0.166        0.141        0.412       0.0043\n",
            "      8    18       0.0375       0.0374     0.000108        0.115         0.15       0.0992        0.146        0.122        0.129        0.184        0.156        0.765      0.00797\n",
            "      8    19       0.0355       0.0354     9.54e-05        0.112        0.146       0.0989        0.139        0.119        0.129        0.175        0.152        0.707      0.00736\n",
            "      8    20       0.0295       0.0295     4.83e-06        0.104        0.133       0.0906        0.131        0.111        0.115        0.162        0.139         0.13      0.00135\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      8     1       0.0394       0.0394     4.67e-06        0.118        0.154        0.103        0.147        0.125        0.134        0.186         0.16        0.135      0.00141\n",
            "      8     2       0.0382       0.0382     8.06e-06        0.117        0.151        0.104        0.144        0.124        0.135         0.18        0.157        0.162      0.00169\n",
            "      8     3       0.0328       0.0327     9.33e-06        0.109         0.14       0.0975        0.132        0.115        0.125        0.166        0.146        0.201       0.0021\n",
            "      8     4       0.0347       0.0347     5.59e-06        0.111        0.144       0.0969         0.14        0.118        0.126        0.175         0.15        0.117      0.00121\n",
            "      8     5        0.034        0.034     1.02e-05         0.11        0.143       0.0963        0.136        0.116        0.126        0.171        0.149        0.187      0.00194\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               8   34.646    0.005        0.035     9.63e-05       0.0351        0.112        0.145       0.0988        0.137        0.118        0.128        0.173        0.151        0.614      0.00639\n",
            "! Validation          8   34.646    0.005       0.0358     7.57e-06       0.0358        0.113        0.146       0.0997         0.14         0.12        0.129        0.176        0.153         0.16      0.00167\n",
            "Wall time: 34.646915862999776\n",
            "! Best model        8    0.036\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      9     1       0.0371        0.037     0.000153        0.113        0.149          0.1        0.139         0.12        0.131        0.179        0.155        0.886      0.00922\n",
            "      9     2       0.0337       0.0332     0.000567        0.109        0.141       0.0963        0.134        0.115        0.123        0.171        0.147         1.76       0.0183\n",
            "      9     3        0.031       0.0309     0.000145        0.106        0.136        0.095        0.129        0.112        0.121        0.162        0.141        0.866      0.00903\n",
            "      9     4        0.034       0.0338     0.000212        0.109        0.142          0.1        0.127        0.114        0.132        0.161        0.146         1.07       0.0112\n",
            "      9     5       0.0257       0.0253     0.000391       0.0966        0.123       0.0859        0.118        0.102        0.111        0.145        0.128         1.46       0.0152\n",
            "      9     6       0.0362       0.0361     0.000175        0.114        0.147        0.103        0.138         0.12        0.132        0.173        0.152        0.956      0.00996\n",
            "      9     7       0.0319       0.0318     9.92e-05        0.107        0.138       0.0944        0.132        0.113        0.122        0.166        0.144        0.732      0.00763\n",
            "      9     8       0.0346       0.0346     4.62e-06        0.112        0.144       0.0976        0.141        0.119        0.126        0.175         0.15        0.133      0.00139\n",
            "      9     9       0.0312        0.031     0.000217        0.104        0.136       0.0928        0.128         0.11        0.121        0.162        0.142         1.08       0.0112\n",
            "      9    10       0.0322       0.0322     1.43e-05        0.108        0.139       0.0937        0.136        0.115        0.119        0.171        0.145        0.241      0.00251\n",
            "      9    11        0.033        0.033     1.38e-05        0.109        0.141        0.097        0.133        0.115        0.123         0.17        0.147        0.253      0.00263\n",
            "      9    12       0.0354       0.0353     4.76e-05        0.113        0.145          0.1        0.138        0.119        0.129        0.174        0.151        0.503      0.00524\n",
            "      9    13       0.0302       0.0299     0.000234        0.103        0.134       0.0908        0.126        0.109        0.117        0.163         0.14         1.12       0.0116\n",
            "      9    14       0.0348       0.0348     2.58e-05        0.113        0.144        0.104        0.131        0.118        0.132        0.166        0.149        0.351      0.00365\n",
            "      9    15       0.0305       0.0305     6.21e-06        0.104        0.135       0.0932        0.127         0.11        0.119        0.162        0.141        0.144       0.0015\n",
            "      9    16       0.0369       0.0368     4.71e-05        0.114        0.149       0.0991        0.143        0.121        0.129        0.181        0.155        0.502      0.00522\n",
            "      9    17       0.0306       0.0302     0.000393        0.105        0.134       0.0943        0.125         0.11        0.121        0.157        0.139         1.47       0.0153\n",
            "      9    18       0.0338       0.0336     0.000177        0.109        0.142       0.0964        0.135        0.116        0.125        0.171        0.148        0.979       0.0102\n",
            "      9    19       0.0325       0.0325     9.86e-06        0.106        0.139       0.0961        0.126        0.111        0.128         0.16        0.144        0.181      0.00188\n",
            "      9    20       0.0244       0.0244     5.28e-05       0.0944        0.121       0.0814        0.121        0.101        0.103         0.15        0.127        0.531      0.00553\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      9     1       0.0357       0.0357     5.26e-06        0.112        0.146       0.0984         0.14        0.119        0.128        0.178        0.153        0.139      0.00145\n",
            "      9     2       0.0344       0.0344     6.35e-06        0.111        0.143       0.0985        0.137        0.118        0.127        0.171        0.149        0.136      0.00142\n",
            "      9     3       0.0296       0.0296     7.23e-06        0.104        0.133       0.0925        0.126        0.109        0.118        0.159        0.139        0.171      0.00178\n",
            "      9     4       0.0313       0.0313      4.6e-06        0.105        0.137       0.0922        0.132        0.112         0.12        0.166        0.143        0.115      0.00119\n",
            "      9     5       0.0308       0.0308     6.52e-06        0.105        0.136       0.0916        0.131        0.111         0.12        0.164        0.142        0.147      0.00154\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               9   38.345    0.005       0.0323     0.000149       0.0325        0.107        0.139       0.0956        0.131        0.113        0.123        0.166        0.145        0.761      0.00793\n",
            "! Validation          9   38.345    0.005       0.0324     5.99e-06       0.0324        0.107        0.139       0.0946        0.133        0.114        0.123        0.168        0.145        0.142      0.00148\n",
            "Wall time: 38.345292122000046\n",
            "! Best model        9    0.032\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     10     1       0.0302       0.0302     1.31e-05        0.104        0.134       0.0914        0.128         0.11        0.121        0.158        0.139         0.24       0.0025\n",
            "     10     2       0.0306       0.0305     2.54e-05        0.105        0.135       0.0935        0.128        0.111        0.121         0.16         0.14        0.333      0.00347\n",
            "     10     3       0.0313       0.0313     1.33e-05        0.104        0.137       0.0908        0.129         0.11         0.12        0.166        0.143        0.215      0.00224\n",
            "     10     4       0.0314       0.0314     9.15e-06        0.105        0.137       0.0924        0.131        0.112        0.121        0.165        0.143        0.193      0.00201\n",
            "     10     5        0.029       0.0289     3.63e-05        0.104        0.132       0.0911        0.129         0.11        0.116        0.158        0.137        0.413       0.0043\n",
            "     10     6       0.0286       0.0285     9.94e-05        0.101        0.131       0.0918        0.119        0.105        0.118        0.152        0.135        0.736      0.00767\n",
            "     10     7       0.0252       0.0252     4.42e-06       0.0951        0.123       0.0853        0.115          0.1        0.109        0.146        0.128        0.142      0.00148\n",
            "     10     8       0.0291       0.0291     2.75e-05        0.101        0.132       0.0849        0.133        0.109        0.108         0.17        0.139        0.352      0.00367\n",
            "     10     9       0.0265       0.0264     5.67e-05       0.0961        0.126       0.0839         0.12        0.102        0.109        0.154        0.131        0.516      0.00537\n",
            "     10    10         0.03       0.0299     7.54e-05        0.102        0.134       0.0901        0.127        0.108        0.118         0.16        0.139        0.625      0.00651\n",
            "     10    11       0.0288       0.0288     1.04e-05       0.0998        0.131       0.0868        0.126        0.106        0.114         0.16        0.137        0.228      0.00237\n",
            "     10    12       0.0254       0.0253     2.64e-05       0.0951        0.123        0.083        0.119        0.101        0.106        0.151        0.129        0.345       0.0036\n",
            "     10    13       0.0268       0.0268     2.58e-05       0.0985        0.127       0.0874        0.121        0.104        0.112        0.152        0.132        0.366      0.00381\n",
            "     10    14        0.029        0.029     7.49e-06        0.102        0.132       0.0883        0.129        0.109        0.113        0.162        0.138        0.159      0.00165\n",
            "     10    15       0.0252       0.0252     1.86e-05       0.0963        0.123       0.0824        0.124        0.103        0.104        0.153        0.129        0.297      0.00309\n",
            "     10    16       0.0324       0.0324     2.85e-05        0.107        0.139       0.0925        0.135        0.114        0.121        0.169        0.145        0.379      0.00395\n",
            "     10    17        0.024        0.024     1.87e-05       0.0931         0.12       0.0817        0.116       0.0989        0.103        0.148        0.125        0.271      0.00282\n",
            "     10    18       0.0243       0.0241     0.000286       0.0951         0.12       0.0873        0.111       0.0991        0.111        0.136        0.123         1.25       0.0131\n",
            "     10    19       0.0259       0.0257     0.000132       0.0952        0.124       0.0858        0.114          0.1         0.11        0.149        0.129        0.849      0.00884\n",
            "     10    20       0.0258       0.0258     1.12e-05       0.0951        0.124        0.084        0.117        0.101        0.109        0.151         0.13        0.205      0.00213\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     10     1       0.0324       0.0324     4.94e-06        0.107        0.139       0.0934        0.133        0.113        0.121         0.17        0.145        0.132      0.00138\n",
            "     10     2       0.0309       0.0309     5.37e-06        0.105        0.136       0.0928        0.131        0.112         0.12        0.164        0.142        0.136      0.00141\n",
            "     10     3       0.0268       0.0268      6.8e-06       0.0985        0.127       0.0873        0.121        0.104        0.112        0.153        0.132        0.164      0.00171\n",
            "     10     4       0.0283       0.0283     3.85e-06          0.1         0.13       0.0876        0.126        0.107        0.114        0.158        0.136        0.106       0.0011\n",
            "     10     5       0.0279       0.0279      5.6e-06       0.0996        0.129       0.0869        0.125        0.106        0.113        0.156        0.135        0.138      0.00144\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              10   42.064    0.005       0.0279     4.63e-05        0.028       0.0997        0.129       0.0877        0.124        0.106        0.113        0.156        0.135        0.406      0.00423\n",
            "! Validation         10   42.064    0.005       0.0293     5.31e-06       0.0293        0.102        0.132       0.0896        0.127        0.108        0.116         0.16        0.138        0.135      0.00141\n",
            "Wall time: 42.064363075000074\n",
            "! Best model       10    0.029\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     11     1       0.0268       0.0267     0.000115       0.0954        0.126       0.0817        0.123        0.102        0.107        0.159        0.133        0.777      0.00809\n",
            "     11     2       0.0264       0.0263     0.000102       0.0978        0.126       0.0852        0.123        0.104        0.108        0.155        0.131        0.718      0.00748\n",
            "     11     3       0.0248       0.0248     1.53e-05       0.0943        0.122       0.0825        0.118          0.1        0.106        0.148        0.127        0.238      0.00248\n",
            "     11     4       0.0266       0.0266      1.3e-05       0.0978        0.126        0.088        0.117        0.103        0.113        0.149        0.131        0.256      0.00267\n",
            "     11     5       0.0234       0.0233      5.3e-06       0.0917        0.118       0.0811        0.113        0.097        0.105        0.142        0.123         0.14      0.00146\n",
            "     11     6       0.0257       0.0256     0.000103       0.0959        0.124       0.0832        0.122        0.102        0.107        0.152         0.13        0.746      0.00777\n",
            "     11     7       0.0256       0.0255     0.000119       0.0962        0.124        0.085        0.119        0.102         0.11        0.148        0.129        0.798      0.00831\n",
            "     11     8        0.022        0.022     3.09e-05        0.087        0.115       0.0766        0.108       0.0922          0.1         0.14         0.12          0.4      0.00417\n",
            "     11     9       0.0242       0.0238     0.000418       0.0919        0.119       0.0799        0.116       0.0978        0.103        0.147        0.125         1.51       0.0158\n",
            "     11    10       0.0251       0.0246     0.000468       0.0928        0.121       0.0809        0.117       0.0988        0.106        0.147        0.127          1.6       0.0167\n",
            "     11    11        0.023       0.0229     0.000151       0.0916        0.117       0.0815        0.112       0.0967        0.104        0.139        0.122        0.906      0.00943\n",
            "     11    12       0.0225       0.0223     0.000166       0.0897        0.116       0.0778        0.113       0.0956       0.0994        0.142        0.121         0.94      0.00979\n",
            "     11    13       0.0255       0.0249     0.000655       0.0953        0.122       0.0833        0.119        0.101        0.107        0.147        0.127          1.9       0.0198\n",
            "     11    14       0.0276       0.0274     0.000138       0.0978        0.128       0.0846        0.124        0.104        0.111        0.156        0.134        0.858      0.00894\n",
            "     11    15       0.0231       0.0231     2.26e-05       0.0898        0.118       0.0807        0.108       0.0943        0.105        0.139        0.122        0.349      0.00364\n",
            "     11    16       0.0278       0.0277     6.46e-05       0.0972        0.129       0.0856         0.12        0.103        0.113        0.155        0.134        0.584      0.00609\n",
            "     11    17       0.0278       0.0277     8.94e-05       0.0969        0.129       0.0849        0.121        0.103        0.113        0.156        0.134        0.692      0.00721\n",
            "     11    18       0.0268       0.0266     0.000122       0.0966        0.126       0.0831        0.124        0.103        0.109        0.156        0.132        0.813      0.00847\n",
            "     11    19       0.0237       0.0235     0.000167       0.0916        0.119       0.0794        0.116       0.0977        0.101        0.147        0.124         0.95       0.0099\n",
            "     11    20       0.0211       0.0211     1.01e-05       0.0868        0.112       0.0747        0.111       0.0928       0.0966        0.139        0.118        0.159      0.00165\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     11     1       0.0292       0.0292     6.03e-06        0.101        0.132       0.0883        0.127        0.108        0.115        0.162        0.138         0.14      0.00146\n",
            "     11     2       0.0278       0.0278     5.08e-06       0.0997        0.129       0.0872        0.125        0.106        0.113        0.156        0.135        0.146      0.00152\n",
            "     11     3       0.0242       0.0242     6.16e-06       0.0935         0.12       0.0822        0.116       0.0991        0.105        0.146        0.126        0.157      0.00163\n",
            "     11     4       0.0255       0.0255      4.2e-06        0.095        0.124       0.0829        0.119        0.101        0.108         0.15        0.129        0.125       0.0013\n",
            "     11     5       0.0253       0.0253     4.09e-06       0.0949        0.123       0.0824         0.12        0.101        0.107         0.15        0.129        0.122      0.00127\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              11   45.778    0.005       0.0248     0.000149        0.025       0.0937        0.122        0.082        0.117       0.0996        0.106        0.148        0.127        0.767      0.00798\n",
            "! Validation         11   45.778    0.005       0.0264     5.11e-06       0.0264       0.0968        0.126       0.0846        0.121        0.103         0.11        0.153        0.131        0.138      0.00144\n",
            "Wall time: 45.778213137999956\n",
            "! Best model       11    0.026\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     12     1       0.0248       0.0245     0.000266       0.0951        0.121       0.0815        0.122        0.102        0.103        0.151        0.127         1.21       0.0126\n",
            "     12     2       0.0345       0.0339     0.000634        0.112        0.142        0.102        0.131        0.116         0.13        0.164        0.147         1.86       0.0194\n",
            "     12     3       0.0285       0.0285     2.93e-05        0.101        0.131       0.0907        0.123        0.107        0.117        0.154        0.136         0.39      0.00406\n",
            "     12     4         0.02       0.0199     0.000128        0.085        0.109       0.0748        0.105       0.0901       0.0949        0.133        0.114        0.815      0.00849\n",
            "     12     5       0.0297       0.0293      0.00041        0.102        0.132       0.0882        0.129        0.108        0.113        0.164        0.139          1.5       0.0156\n",
            "     12     6       0.0348       0.0344     0.000388        0.112        0.143        0.105        0.127        0.116        0.132        0.163        0.148         1.46       0.0152\n",
            "     12     7       0.0294       0.0294     4.55e-05        0.104        0.133       0.0935        0.124        0.109        0.119        0.157        0.138        0.464      0.00484\n",
            "     12     8       0.0224       0.0221     0.000211       0.0896        0.115       0.0782        0.112       0.0952       0.0995        0.141         0.12         1.08       0.0112\n",
            "     12     9       0.0334       0.0334     9.19e-06        0.111        0.141        0.105        0.123        0.114        0.135        0.154        0.144        0.181      0.00188\n",
            "     12    10       0.0279       0.0278     3.69e-05          0.1        0.129       0.0855        0.129        0.107        0.109        0.162        0.135        0.439      0.00457\n",
            "     12    11       0.0208       0.0207     0.000121       0.0856        0.111       0.0749        0.107        0.091       0.0957        0.137        0.116         0.81      0.00843\n",
            "     12    12        0.028        0.028     3.36e-05        0.102        0.129       0.0881        0.129        0.108        0.112        0.158        0.135        0.352      0.00366\n",
            "     12    13       0.0273       0.0272     0.000118       0.0967        0.128       0.0826        0.125        0.104        0.108         0.16        0.134        0.772      0.00804\n",
            "     12    14       0.0215       0.0213     0.000153       0.0857        0.113       0.0755        0.106       0.0908       0.0991        0.136        0.118        0.906      0.00943\n",
            "     12    15       0.0217       0.0217     7.98e-05       0.0884        0.114       0.0764        0.112       0.0944       0.0988        0.139        0.119        0.657      0.00684\n",
            "     12    16       0.0207       0.0207     8.25e-06       0.0866        0.111       0.0786        0.103       0.0906        0.101        0.129        0.115        0.209      0.00217\n",
            "     12    17       0.0226       0.0226     7.78e-05       0.0893        0.116       0.0774        0.113       0.0952          0.1        0.143        0.122        0.646      0.00673\n",
            "     12    18       0.0249       0.0247     0.000177       0.0925        0.122       0.0811        0.115       0.0983        0.109        0.144        0.126        0.988       0.0103\n",
            "     12    19       0.0213       0.0213     1.08e-05       0.0873        0.113       0.0753        0.111       0.0933       0.0976        0.139        0.118        0.207      0.00215\n",
            "     12    20       0.0239       0.0239     4.02e-05       0.0911         0.12       0.0802        0.113       0.0965        0.105        0.144        0.125        0.459      0.00478\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     12     1       0.0266       0.0266     7.63e-06       0.0964        0.126       0.0837        0.122        0.103        0.109        0.155        0.132        0.163       0.0017\n",
            "     12     2       0.0254       0.0254     5.44e-06       0.0947        0.123       0.0822         0.12        0.101        0.107        0.151        0.129        0.157      0.00164\n",
            "     12     3       0.0222       0.0221     6.37e-06       0.0891        0.115       0.0777        0.112       0.0948       0.0997        0.141         0.12        0.154       0.0016\n",
            "     12     4       0.0234       0.0234     5.77e-06       0.0908        0.118       0.0789        0.115       0.0967        0.103        0.144        0.123        0.146      0.00152\n",
            "     12     5       0.0231       0.0231     3.55e-06       0.0908        0.118       0.0784        0.116        0.097        0.102        0.145        0.123        0.116      0.00121\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              12   49.486    0.005       0.0258     0.000149       0.0259       0.0958        0.124       0.0847        0.118        0.101         0.11        0.149        0.129         0.77      0.00802\n",
            "! Validation         12   49.486    0.005       0.0241     5.75e-06       0.0241       0.0923         0.12       0.0802        0.117       0.0984        0.104        0.147        0.126        0.147      0.00153\n",
            "Wall time: 49.48701511799982\n",
            "! Best model       12    0.024\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     13     1        0.024        0.024     1.04e-06        0.092         0.12       0.0791        0.118       0.0985        0.104        0.147        0.125       0.0662      0.00069\n",
            "     13     2       0.0206       0.0205     6.28e-05       0.0865        0.111       0.0742        0.111       0.0926       0.0958        0.136        0.116        0.583      0.00608\n",
            "     13     3       0.0239       0.0239     4.24e-05       0.0912         0.12       0.0767         0.12       0.0984        0.101         0.15        0.126        0.456      0.00475\n",
            "     13     4       0.0197       0.0197     8.27e-06       0.0849        0.109       0.0745        0.106       0.0901       0.0943        0.133        0.113        0.199      0.00207\n",
            "     13     5         0.02         0.02     4.94e-05       0.0848        0.109       0.0731        0.108       0.0907        0.093        0.136        0.115        0.513      0.00534\n",
            "     13     6       0.0272        0.027     0.000157       0.0999        0.127        0.092        0.116        0.104        0.117        0.146        0.131        0.921       0.0096\n",
            "     13     7       0.0232       0.0231     5.45e-05       0.0926        0.118       0.0795        0.119       0.0991          0.1        0.146        0.123        0.542      0.00565\n",
            "     13     8       0.0202       0.0202     1.04e-05       0.0837         0.11       0.0731        0.105        0.089       0.0962        0.133        0.115         0.22      0.00229\n",
            "     13     9       0.0249       0.0249     4.59e-05       0.0938        0.122       0.0805         0.12          0.1        0.105        0.151        0.128        0.483      0.00503\n",
            "     13    10       0.0209       0.0207     0.000104       0.0849        0.111       0.0764        0.102       0.0891       0.0999        0.132        0.116        0.746      0.00777\n",
            "     13    11       0.0234       0.0234      1.8e-05       0.0916        0.118       0.0793        0.116       0.0978        0.103        0.145        0.124        0.251      0.00261\n",
            "     13    12       0.0252       0.0251     7.73e-05       0.0957        0.123       0.0856        0.116        0.101         0.11        0.145        0.127        0.651      0.00678\n",
            "     13    13       0.0207       0.0206     5.46e-05       0.0855        0.111       0.0741        0.108       0.0912        0.095        0.138        0.116        0.541      0.00564\n",
            "     13    14       0.0247       0.0247     2.33e-06        0.092        0.122       0.0785        0.119       0.0988        0.106        0.149        0.127         0.11      0.00115\n",
            "     13    15       0.0221       0.0221     6.21e-06       0.0876        0.115       0.0764         0.11       0.0932       0.0989        0.142         0.12        0.179      0.00187\n",
            "     13    16       0.0191        0.019     2.29e-05       0.0827        0.107       0.0717        0.105       0.0882       0.0921        0.131        0.112         0.29      0.00302\n",
            "     13    17       0.0219       0.0219     1.22e-05        0.087        0.114       0.0743        0.113       0.0934        0.098        0.142         0.12        0.228      0.00237\n",
            "     13    18       0.0218       0.0217     5.08e-05       0.0864        0.114       0.0767        0.106       0.0913       0.0992        0.139        0.119        0.503      0.00523\n",
            "     13    19       0.0203       0.0203     1.21e-05       0.0854         0.11       0.0743        0.108       0.0909        0.095        0.135        0.115        0.228      0.00237\n",
            "     13    20       0.0231        0.023     9.23e-05       0.0905        0.117       0.0792        0.113       0.0961        0.103        0.141        0.122        0.709      0.00738\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     13     1       0.0245       0.0244     9.96e-06       0.0923        0.121       0.0798        0.117       0.0986        0.104         0.15        0.127         0.19      0.00198\n",
            "     13     2       0.0236       0.0236     6.27e-06       0.0908        0.119       0.0781        0.116       0.0971        0.102        0.146        0.124        0.166      0.00173\n",
            "     13     3       0.0205       0.0205     7.23e-06       0.0855        0.111       0.0739        0.109       0.0913       0.0952        0.137        0.116        0.161      0.00167\n",
            "     13     4       0.0216       0.0216     8.03e-06       0.0872        0.114       0.0756         0.11        0.093       0.0987        0.139        0.119        0.173      0.00181\n",
            "     13     5       0.0214       0.0214     4.21e-06       0.0872        0.113       0.0748        0.112       0.0934       0.0967         0.14        0.118        0.123      0.00128\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              13   53.193    0.005       0.0223     4.42e-05       0.0223       0.0889        0.116       0.0775        0.112       0.0947          0.1        0.141        0.121        0.421      0.00438\n",
            "! Validation         13   53.193    0.005       0.0223     7.14e-06       0.0223       0.0886        0.115       0.0764        0.113       0.0947       0.0993        0.142        0.121        0.163      0.00169\n",
            "Wall time: 53.193432992\n",
            "! Best model       13    0.022\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     14     1       0.0214       0.0213     8.33e-05       0.0866        0.113        0.075         0.11       0.0924       0.0962        0.141        0.118        0.674      0.00702\n",
            "     14     2       0.0196       0.0195     7.52e-05        0.084        0.108       0.0742        0.103       0.0889       0.0945        0.131        0.113        0.615      0.00641\n",
            "     14     3       0.0184       0.0184     2.43e-06       0.0816        0.105       0.0706        0.104       0.0871       0.0894        0.131         0.11         0.11      0.00114\n",
            "     14     4       0.0242        0.024     0.000163       0.0931         0.12       0.0812        0.117        0.099        0.105        0.146        0.125        0.929      0.00968\n",
            "     14     5       0.0218       0.0216     0.000202       0.0887        0.114       0.0749        0.116       0.0956       0.0963        0.143        0.119         1.03       0.0107\n",
            "     14     6       0.0209       0.0209     6.55e-05       0.0876        0.112       0.0794        0.104       0.0917        0.101        0.131        0.116        0.586      0.00611\n",
            "     14     7       0.0187       0.0187     5.13e-06       0.0814        0.106       0.0689        0.106       0.0876       0.0886        0.134        0.111        0.139      0.00145\n",
            "     14     8       0.0218       0.0217     5.52e-05       0.0878        0.114       0.0782        0.107       0.0926        0.101        0.136        0.119        0.534      0.00556\n",
            "     14     9       0.0253       0.0253      8.4e-05       0.0938        0.123       0.0795        0.122        0.101        0.101        0.158         0.13        0.662      0.00689\n",
            "     14    10       0.0209       0.0208     5.44e-05       0.0839        0.112       0.0725        0.107       0.0896       0.0975        0.135        0.116        0.543      0.00566\n",
            "     14    11       0.0189       0.0189     3.24e-05       0.0817        0.106       0.0696        0.106       0.0878       0.0893        0.134        0.112        0.404      0.00421\n",
            "     14    12       0.0175       0.0175      2.5e-06       0.0795        0.102       0.0693          0.1       0.0847       0.0894        0.124        0.107        0.116      0.00121\n",
            "     14    13       0.0165       0.0164     5.68e-05       0.0758       0.0991        0.065       0.0976       0.0813       0.0852        0.122        0.104        0.554      0.00577\n",
            "     14    14       0.0223       0.0222      3.7e-05       0.0861        0.115       0.0727        0.113       0.0928       0.0981        0.144        0.121        0.445      0.00464\n",
            "     14    15       0.0201       0.0201      1.8e-06       0.0834         0.11       0.0719        0.107       0.0892       0.0941        0.136        0.115       0.0926     0.000964\n",
            "     14    16       0.0177       0.0177     1.34e-05       0.0791        0.103       0.0692       0.0988        0.084         0.09        0.125        0.107        0.243      0.00253\n",
            "     14    17       0.0212       0.0212     3.13e-06       0.0867        0.113       0.0738        0.112       0.0931       0.0956        0.141        0.118        0.111      0.00116\n",
            "     14    18       0.0197       0.0196     6.51e-06       0.0837        0.108       0.0729        0.105       0.0891       0.0937        0.133        0.113        0.171      0.00179\n",
            "     14    19       0.0196       0.0195     3.88e-05       0.0821        0.108       0.0703        0.106        0.088       0.0919        0.135        0.113        0.455      0.00474\n",
            "     14    20       0.0169       0.0169     1.92e-05       0.0785        0.101       0.0675          0.1        0.084        0.085        0.126        0.105        0.308      0.00321\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     14     1       0.0228       0.0227     8.52e-06       0.0889        0.117       0.0764        0.114       0.0952       0.0992        0.145        0.122        0.176      0.00184\n",
            "     14     2        0.022        0.022     5.16e-06       0.0875        0.115       0.0747        0.113       0.0939       0.0981        0.142         0.12        0.157      0.00164\n",
            "     14     3       0.0191       0.0191     6.02e-06       0.0825        0.107       0.0708        0.106       0.0884       0.0911        0.133        0.112        0.148      0.00154\n",
            "     14     4       0.0201       0.0201     6.59e-06        0.084         0.11       0.0725        0.107       0.0897       0.0948        0.135        0.115         0.15      0.00156\n",
            "     14     5         0.02         0.02     3.46e-06       0.0842        0.109       0.0717        0.109       0.0904       0.0925        0.137        0.115         0.11      0.00115\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              14   56.920    0.005       0.0201     5.01e-05       0.0202       0.0843         0.11       0.0728        0.107         0.09       0.0943        0.135        0.115        0.436      0.00454\n",
            "! Validation         14   56.920    0.005       0.0208     5.95e-06       0.0208       0.0854        0.112       0.0732         0.11       0.0915       0.0952        0.139        0.117        0.148      0.00154\n",
            "Wall time: 56.920410077999804\n",
            "! Best model       14    0.021\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     15     1       0.0193       0.0193     1.54e-05        0.082        0.108       0.0702        0.106       0.0879       0.0923        0.133        0.113        0.274      0.00285\n",
            "     15     2       0.0182       0.0181     0.000113       0.0801        0.104       0.0689        0.103       0.0857        0.089        0.129        0.109        0.775      0.00807\n",
            "     15     3       0.0174       0.0173     2.26e-05       0.0786        0.102       0.0669        0.102       0.0844       0.0854        0.129        0.107        0.332      0.00346\n",
            "     15     4       0.0201       0.0201     9.15e-06       0.0846         0.11       0.0738        0.106       0.0899       0.0949        0.135        0.115         0.19      0.00198\n",
            "     15     5       0.0197       0.0197      2.2e-05       0.0834        0.109       0.0713        0.108       0.0895       0.0921        0.136        0.114        0.324      0.00337\n",
            "     15     6       0.0182       0.0182     1.59e-06       0.0817        0.104       0.0721        0.101       0.0865       0.0912        0.126        0.109       0.0822     0.000857\n",
            "     15     7       0.0202       0.0202     3.22e-06       0.0837         0.11       0.0721        0.107       0.0894       0.0938        0.136        0.115        0.113      0.00118\n",
            "     15     8       0.0194       0.0194     2.66e-06        0.083        0.108       0.0713        0.106       0.0888       0.0914        0.134        0.113        0.118      0.00122\n",
            "     15     9       0.0215       0.0215      1.4e-05       0.0879        0.113       0.0776        0.109        0.093        0.101        0.135        0.118        0.241      0.00251\n",
            "     15    10       0.0215       0.0214       0.0001       0.0866        0.113       0.0773        0.105       0.0913          0.1        0.136        0.118        0.738      0.00769\n",
            "     15    11       0.0197       0.0197     2.47e-05       0.0848        0.109       0.0731        0.108       0.0906       0.0923        0.135        0.114        0.329      0.00342\n",
            "     15    12       0.0221        0.022     6.32e-05        0.088        0.115       0.0772         0.11       0.0934          0.1         0.14         0.12        0.583      0.00608\n",
            "     15    13       0.0268       0.0266     0.000138        0.098        0.126        0.088        0.118        0.103        0.112         0.15        0.131        0.866      0.00902\n",
            "     15    14       0.0189       0.0188     0.000149       0.0809        0.106       0.0705        0.102       0.0861       0.0904        0.132        0.111        0.903      0.00941\n",
            "     15    15        0.021        0.021     1.26e-06       0.0854        0.112       0.0727        0.111       0.0918       0.0966        0.138        0.117       0.0705     0.000734\n",
            "     15    16       0.0218       0.0217     0.000127       0.0904        0.114       0.0812        0.109        0.095        0.101        0.135        0.118         0.82      0.00854\n",
            "     15    17       0.0168       0.0167      8.3e-05       0.0774          0.1       0.0659          0.1       0.0832       0.0854        0.124        0.105        0.669      0.00697\n",
            "     15    18       0.0222       0.0222      8.6e-07       0.0867        0.115       0.0738        0.113       0.0932       0.0981        0.143        0.121       0.0498     0.000519\n",
            "     15    19        0.019        0.019     1.32e-05       0.0833        0.107       0.0748          0.1       0.0875       0.0951        0.126        0.111        0.248      0.00258\n",
            "     15    20       0.0198       0.0198     2.18e-05       0.0816        0.109       0.0675         0.11       0.0887       0.0884        0.141        0.115        0.315      0.00328\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     15     1       0.0213       0.0213     8.35e-06        0.086        0.113       0.0735        0.111       0.0922       0.0954        0.142        0.119        0.177      0.00184\n",
            "     15     2       0.0207       0.0207     4.98e-06       0.0846        0.111       0.0717         0.11       0.0911       0.0945        0.139        0.117        0.155      0.00162\n",
            "     15     3       0.0179       0.0179      5.7e-06       0.0799        0.104        0.068        0.104       0.0858       0.0876         0.13        0.109        0.146      0.00152\n",
            "     15     4        0.019        0.019     6.66e-06       0.0814        0.107         0.07        0.104       0.0871       0.0916        0.132        0.112        0.151      0.00157\n",
            "     15     5       0.0188       0.0188     3.42e-06       0.0815        0.106        0.069        0.107       0.0878       0.0889        0.134        0.111        0.109      0.00114\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              15   60.653    0.005       0.0201     4.63e-05       0.0202       0.0844         0.11       0.0733        0.107         0.09       0.0948        0.135        0.115        0.402      0.00419\n",
            "! Validation         15   60.653    0.005       0.0195     5.82e-06       0.0195       0.0827        0.108       0.0704        0.107       0.0888       0.0916        0.135        0.113        0.148      0.00154\n",
            "Wall time: 60.65376408999964\n",
            "! Best model       15    0.020\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     16     1       0.0185       0.0185     4.52e-06        0.083        0.105        0.074        0.101       0.0874        0.094        0.125        0.109        0.153       0.0016\n",
            "     16     2       0.0212       0.0212      5.7e-05       0.0863        0.113       0.0728        0.113       0.0931        0.094        0.143        0.118        0.547       0.0057\n",
            "     16     3       0.0212       0.0211     3.28e-06       0.0849        0.113       0.0702        0.114       0.0923       0.0934        0.143        0.118        0.108      0.00113\n",
            "     16     4       0.0186       0.0186      3.2e-05       0.0813        0.105       0.0684        0.107       0.0877       0.0872        0.135        0.111        0.414      0.00431\n",
            "     16     5       0.0181        0.018     3.59e-05       0.0796        0.104       0.0694          0.1       0.0847       0.0891        0.128        0.109        0.427      0.00445\n",
            "     16     6       0.0177       0.0177     2.34e-05       0.0789        0.103       0.0677        0.101       0.0845       0.0871        0.129        0.108        0.325      0.00339\n",
            "     16     7       0.0183       0.0183      2.6e-05       0.0811        0.105       0.0719       0.0995       0.0857        0.093        0.124        0.109        0.372      0.00387\n",
            "     16     8       0.0207       0.0206     0.000102       0.0858        0.111        0.074        0.109       0.0917       0.0942        0.138        0.116        0.747      0.00778\n",
            "     16     9       0.0208       0.0208     1.92e-05       0.0837        0.112        0.071        0.109       0.0901       0.0956        0.138        0.117        0.272      0.00284\n",
            "     16    10       0.0175       0.0174     9.52e-06       0.0774        0.102        0.065        0.102       0.0836       0.0853         0.13        0.107        0.213      0.00222\n",
            "     16    11        0.024        0.024     9.04e-05       0.0916         0.12       0.0778        0.119       0.0985          0.1        0.152        0.126        0.666      0.00693\n",
            "     16    12       0.0174       0.0174     5.39e-05       0.0782        0.102       0.0691       0.0964       0.0827       0.0896        0.123        0.106        0.528       0.0055\n",
            "     16    13        0.017        0.017     1.35e-06       0.0768        0.101       0.0649          0.1       0.0827        0.086        0.126        0.106       0.0695     0.000724\n",
            "     16    14       0.0182       0.0182     4.41e-06        0.079        0.104       0.0675        0.102       0.0848       0.0882        0.131        0.109        0.133      0.00138\n",
            "     16    15       0.0154       0.0154     1.98e-06       0.0738       0.0959       0.0622       0.0971       0.0796       0.0812         0.12        0.101       0.0967      0.00101\n",
            "     16    16       0.0166       0.0166     1.69e-06       0.0768       0.0995        0.066       0.0983       0.0822       0.0856        0.123        0.104       0.0738     0.000769\n",
            "     16    17       0.0176       0.0176     4.13e-06       0.0787        0.103       0.0654        0.105       0.0853       0.0851        0.131        0.108        0.137      0.00143\n",
            "     16    18       0.0157       0.0157     4.78e-06       0.0741       0.0969       0.0622       0.0978         0.08        0.081        0.123        0.102        0.124       0.0013\n",
            "     16    19       0.0139       0.0138      3.5e-05       0.0707        0.091       0.0611       0.0899       0.0755       0.0765        0.115       0.0956        0.431      0.00449\n",
            "     16    20       0.0165       0.0165      7.1e-06       0.0759       0.0994       0.0648       0.0981       0.0815       0.0838        0.125        0.104        0.178      0.00185\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     16     1       0.0202       0.0202        7e-06       0.0836         0.11        0.071        0.109       0.0898       0.0923        0.138        0.115         0.16      0.00166\n",
            "     16     2       0.0196       0.0196     4.11e-06       0.0821        0.108       0.0691        0.108       0.0886       0.0913        0.136        0.114        0.142      0.00148\n",
            "     16     3        0.017        0.017     4.83e-06       0.0776        0.101       0.0658        0.101       0.0835       0.0848        0.127        0.106        0.135      0.00141\n",
            "     16     4        0.018        0.018     5.27e-06       0.0793        0.104        0.068        0.102       0.0849       0.0888        0.129        0.109        0.138      0.00144\n",
            "     16     5       0.0178       0.0178     2.86e-06        0.079        0.103       0.0665        0.104       0.0853       0.0858        0.131        0.108        0.106       0.0011\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              16   64.374    0.005       0.0182     2.59e-05       0.0182       0.0799        0.104       0.0683        0.103       0.0857       0.0887         0.13        0.109        0.301      0.00313\n",
            "! Validation         16   64.374    0.005       0.0185     4.81e-06       0.0185       0.0803        0.105       0.0681        0.105       0.0864       0.0887        0.132         0.11        0.136      0.00142\n",
            "Wall time: 64.37428521999982\n",
            "! Best model       16    0.019\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     17     1       0.0154       0.0154     4.29e-06       0.0744       0.0961       0.0623       0.0984       0.0804       0.0796        0.123        0.101        0.121      0.00126\n",
            "     17     2       0.0151       0.0151     2.16e-05       0.0734       0.0949       0.0626       0.0949       0.0788       0.0797         0.12       0.0997        0.329      0.00343\n",
            "     17     3       0.0167       0.0167     5.98e-05       0.0775       0.0999       0.0663          0.1       0.0831       0.0844        0.125        0.105        0.562      0.00586\n",
            "     17     4       0.0188       0.0187     3.32e-05       0.0802        0.106       0.0685        0.103        0.086       0.0906        0.131        0.111        0.383      0.00399\n",
            "     17     5       0.0159       0.0159     9.56e-06       0.0742       0.0975       0.0642       0.0943       0.0792       0.0843         0.12        0.102        0.218      0.00227\n",
            "     17     6       0.0159       0.0158     5.56e-05        0.074       0.0972       0.0618       0.0984       0.0801       0.0795        0.125        0.102        0.523      0.00545\n",
            "     17     7       0.0164       0.0163      6.2e-05       0.0761       0.0988       0.0635        0.101       0.0825       0.0818        0.126        0.104        0.575      0.00599\n",
            "     17     8       0.0192       0.0192     5.71e-06       0.0808        0.107       0.0676        0.107       0.0874       0.0895        0.136        0.113        0.145      0.00151\n",
            "     17     9       0.0151        0.015     0.000129        0.072       0.0947       0.0601       0.0957       0.0779       0.0797        0.119       0.0994        0.842      0.00877\n",
            "     17    10       0.0157       0.0155     0.000231       0.0748       0.0963       0.0639       0.0967       0.0803       0.0816         0.12        0.101         1.13       0.0118\n",
            "     17    11       0.0183       0.0182     1.24e-05        0.078        0.104       0.0643        0.105       0.0848       0.0852        0.135         0.11        0.205      0.00213\n",
            "     17    12       0.0164       0.0161      0.00022       0.0763       0.0983       0.0645       0.0998       0.0822        0.082        0.125        0.103         1.09       0.0113\n",
            "     17    13       0.0164       0.0161     0.000381        0.074        0.098       0.0636       0.0949       0.0792       0.0838        0.122        0.103         1.45       0.0151\n",
            "     17    14       0.0153       0.0153     1.51e-06       0.0745       0.0958       0.0636       0.0961       0.0799       0.0798        0.122        0.101       0.0811     0.000844\n",
            "     17    15       0.0178       0.0173     0.000497       0.0774        0.102       0.0671       0.0981       0.0826       0.0878        0.125        0.106         1.65       0.0172\n",
            "     17    16       0.0154       0.0151     0.000378       0.0733        0.095       0.0599          0.1         0.08       0.0755        0.125          0.1         1.44        0.015\n",
            "     17    17       0.0163       0.0163     6.41e-06       0.0756       0.0988       0.0647       0.0974        0.081       0.0833        0.124        0.104         0.16      0.00167\n",
            "     17    18       0.0174        0.017     0.000414       0.0766        0.101       0.0652       0.0996       0.0824       0.0856        0.126        0.106         1.51       0.0157\n",
            "     17    19       0.0187       0.0184     0.000327       0.0799        0.105       0.0667        0.106       0.0865       0.0887        0.132         0.11         1.34        0.014\n",
            "     17    20       0.0154       0.0154     7.56e-06       0.0738       0.0961       0.0627        0.096       0.0793       0.0808        0.121        0.101        0.181      0.00188\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     17     1       0.0191       0.0191     7.22e-06       0.0813        0.107       0.0687        0.106       0.0875       0.0895        0.135        0.112        0.166      0.00172\n",
            "     17     2       0.0185       0.0185     3.95e-06       0.0797        0.105       0.0667        0.106       0.0862       0.0883        0.133        0.111        0.137      0.00143\n",
            "     17     3       0.0161       0.0161     4.91e-06       0.0754       0.0982       0.0635       0.0992       0.0814        0.082        0.124        0.103        0.138      0.00144\n",
            "     17     4       0.0171       0.0171     5.56e-06       0.0772        0.101        0.066       0.0996       0.0828       0.0861        0.126        0.106        0.141      0.00147\n",
            "     17     5       0.0169       0.0169     2.94e-06       0.0769          0.1       0.0643        0.102       0.0831       0.0831        0.128        0.106          0.1      0.00104\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              17   68.106    0.005       0.0164     0.000143       0.0166       0.0758       0.0992       0.0641       0.0992       0.0817       0.0832        0.125        0.104        0.696      0.00725\n",
            "! Validation         17   68.106    0.005       0.0176     4.91e-06       0.0176       0.0781        0.102       0.0658        0.103       0.0842       0.0858         0.13        0.108        0.136      0.00142\n",
            "Wall time: 68.10658523899974\n",
            "! Best model       17    0.018\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     18     1       0.0173        0.017     0.000271       0.0756        0.101       0.0662       0.0945       0.0804       0.0879        0.123        0.105         1.22       0.0127\n",
            "     18     2       0.0149       0.0148     0.000147       0.0734        0.094       0.0622       0.0958        0.079       0.0782         0.12       0.0989        0.898      0.00935\n",
            "     18     3       0.0187       0.0187     3.45e-06       0.0803        0.106       0.0656         0.11       0.0876       0.0865        0.136        0.111        0.099      0.00103\n",
            "     18     4       0.0169       0.0168      0.00015       0.0765          0.1       0.0633        0.103       0.0831       0.0816         0.13        0.106        0.887      0.00924\n",
            "     18     5       0.0156       0.0155     0.000135       0.0723       0.0963       0.0595       0.0979       0.0787       0.0795        0.123        0.101        0.855      0.00891\n",
            "     18     6       0.0149       0.0149     7.12e-06       0.0714       0.0945       0.0602       0.0938        0.077       0.0792        0.119       0.0993         0.15      0.00156\n",
            "     18     7        0.016       0.0159     0.000112       0.0743       0.0976       0.0615          0.1       0.0808       0.0804        0.125        0.103        0.772      0.00804\n",
            "     18     8       0.0166       0.0165     2.73e-05       0.0772       0.0995       0.0666       0.0984       0.0825       0.0847        0.124        0.104        0.381      0.00397\n",
            "     18     9       0.0158       0.0158     2.09e-06       0.0755       0.0972       0.0654       0.0958       0.0806        0.083        0.121        0.102       0.0889     0.000926\n",
            "     18    10       0.0169       0.0169     2.66e-05       0.0773          0.1       0.0662       0.0993       0.0828       0.0864        0.124        0.105        0.366      0.00381\n",
            "     18    11        0.015        0.015     1.15e-06       0.0738       0.0947       0.0648       0.0917       0.0782       0.0822        0.116       0.0989        0.068     0.000708\n",
            "     18    12       0.0147       0.0147     2.75e-06       0.0726       0.0938       0.0612       0.0953       0.0783        0.077        0.121       0.0988        0.109      0.00114\n",
            "     18    13        0.016        0.016      9.7e-06       0.0762       0.0978       0.0661       0.0965       0.0813       0.0855        0.119        0.102        0.219      0.00228\n",
            "     18    14        0.017        0.017     5.35e-06       0.0785        0.101       0.0681       0.0992       0.0836       0.0856        0.126        0.106        0.138      0.00144\n",
            "     18    15       0.0178       0.0178     6.46e-06       0.0794        0.103       0.0664        0.105       0.0858       0.0845        0.133        0.109        0.179      0.00187\n",
            "     18    16       0.0154       0.0153     3.94e-05       0.0747       0.0958       0.0655       0.0931       0.0793       0.0829        0.117          0.1        0.457      0.00476\n",
            "     18    17       0.0144       0.0143     6.05e-05       0.0704       0.0926       0.0591        0.093        0.076       0.0766        0.118       0.0974        0.563      0.00586\n",
            "     18    18       0.0152       0.0152     6.29e-06       0.0726       0.0955       0.0636       0.0906       0.0771       0.0829        0.117       0.0998        0.154       0.0016\n",
            "     18    19        0.019        0.019     8.72e-06       0.0824        0.107       0.0691        0.109       0.0891        0.088        0.136        0.112        0.202      0.00211\n",
            "     18    20       0.0177       0.0176      3.1e-05        0.079        0.103        0.069        0.099        0.084       0.0898        0.124        0.107        0.411      0.00428\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     18     1       0.0181       0.0181     7.01e-06       0.0791        0.104       0.0665        0.104       0.0853       0.0867        0.132         0.11        0.164      0.00171\n",
            "     18     2       0.0176       0.0176     3.64e-06       0.0776        0.103       0.0647        0.103        0.084       0.0855         0.13        0.108        0.131      0.00137\n",
            "     18     3       0.0153       0.0153     4.72e-06       0.0734       0.0958       0.0614       0.0973       0.0794       0.0794        0.122        0.101        0.135      0.00141\n",
            "     18     4       0.0164       0.0164     5.43e-06       0.0754        0.099       0.0642       0.0977       0.0809       0.0838        0.124        0.104        0.141      0.00146\n",
            "     18     5        0.016        0.016     2.84e-06       0.0748        0.098       0.0622       0.0999       0.0811       0.0806        0.126        0.103       0.0969      0.00101\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              18   71.844    0.005       0.0162     5.26e-05       0.0163       0.0757       0.0986       0.0645       0.0981       0.0813       0.0832        0.124        0.103        0.411      0.00428\n",
            "! Validation         18   71.844    0.005       0.0167     4.73e-06       0.0167        0.076          0.1       0.0638        0.101       0.0822       0.0832        0.127        0.105        0.134      0.00139\n",
            "Wall time: 71.84474003600008\n",
            "! Best model       18    0.017\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     19     1       0.0156       0.0155     3.99e-05       0.0732       0.0964       0.0646       0.0905       0.0775       0.0856        0.115          0.1        0.462      0.00481\n",
            "     19     2       0.0133       0.0133     4.34e-05       0.0674       0.0891       0.0555       0.0912       0.0734       0.0726        0.115        0.094        0.481      0.00501\n",
            "     19     3       0.0135       0.0135      9.3e-07       0.0695         0.09       0.0613       0.0861       0.0737       0.0784         0.11        0.094       0.0518     0.000539\n",
            "     19     4       0.0143       0.0142     8.93e-05       0.0709       0.0923       0.0591       0.0947       0.0769       0.0755        0.119       0.0973          0.7      0.00729\n",
            "     19     5       0.0158       0.0158     7.74e-05       0.0742       0.0971        0.063       0.0965       0.0798       0.0829        0.121        0.102        0.648      0.00675\n",
            "     19     6       0.0162       0.0162     2.41e-05       0.0758       0.0985       0.0642        0.099       0.0816       0.0825        0.125        0.103        0.324      0.00337\n",
            "     19     7       0.0148       0.0148     1.58e-05       0.0718        0.094       0.0583        0.099       0.0786       0.0757        0.123       0.0992        0.283      0.00295\n",
            "     19     8       0.0132        0.013     0.000199       0.0675       0.0882       0.0579       0.0867       0.0723       0.0747         0.11       0.0926         1.04       0.0108\n",
            "     19     9       0.0142       0.0141     0.000141       0.0703       0.0918       0.0584        0.094       0.0762       0.0756        0.118       0.0966        0.877      0.00914\n",
            "     19    10       0.0188       0.0188     5.73e-06       0.0804        0.106       0.0669        0.108       0.0872       0.0874        0.136        0.112        0.155      0.00161\n",
            "     19    11       0.0242       0.0241     3.25e-05       0.0932         0.12       0.0817        0.116        0.099        0.105        0.146        0.125        0.402      0.00419\n",
            "     19    12       0.0244       0.0241     0.000251       0.0959         0.12       0.0882        0.111       0.0997         0.11        0.139        0.124         1.17       0.0122\n",
            "     19    13       0.0173       0.0173     2.36e-06       0.0782        0.102       0.0693       0.0961       0.0827       0.0901        0.122        0.106       0.0926     0.000964\n",
            "     19    14       0.0173       0.0173     1.87e-05       0.0784        0.102       0.0676          0.1       0.0838       0.0877        0.125        0.106        0.315      0.00328\n",
            "     19    15       0.0243       0.0242     6.35e-05       0.0959         0.12       0.0913        0.105       0.0981        0.114        0.133        0.123        0.591      0.00615\n",
            "     19    16       0.0213       0.0213     3.05e-06       0.0888        0.113       0.0798        0.107       0.0933       0.0995        0.136        0.118        0.109      0.00114\n",
            "     19    17       0.0157       0.0157     1.59e-06       0.0741        0.097       0.0619       0.0984       0.0801       0.0814        0.122        0.102       0.0791     0.000824\n",
            "     19    18       0.0158       0.0158     3.84e-05       0.0746       0.0972       0.0636       0.0967       0.0802       0.0827        0.121        0.102        0.433      0.00451\n",
            "     19    19       0.0164       0.0163     0.000106       0.0749       0.0987       0.0646       0.0956       0.0801       0.0843        0.122        0.103        0.762      0.00794\n",
            "     19    20       0.0174       0.0174     5.74e-06       0.0768        0.102       0.0645        0.101       0.0829       0.0855        0.129        0.107        0.168      0.00175\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     19     1       0.0173       0.0173     5.44e-06       0.0772        0.102       0.0646        0.102       0.0834       0.0844         0.13        0.107         0.14      0.00146\n",
            "     19     2       0.0168       0.0168     2.91e-06       0.0757          0.1       0.0629        0.101       0.0821       0.0832        0.128        0.105        0.118      0.00123\n",
            "     19     3       0.0146       0.0146     3.68e-06       0.0716       0.0935       0.0597       0.0953       0.0775       0.0773        0.119       0.0984        0.123      0.00128\n",
            "     19     4       0.0157       0.0157     3.73e-06       0.0737       0.0969       0.0626        0.096       0.0793       0.0816        0.122        0.102        0.119      0.00124\n",
            "     19     5       0.0154       0.0154     2.19e-06        0.073       0.0959       0.0606       0.0979       0.0792       0.0785        0.123        0.101       0.0938     0.000977\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              19   75.580    0.005       0.0171     5.79e-05       0.0172       0.0776        0.101       0.0671       0.0986       0.0829       0.0873        0.125        0.106        0.457      0.00476\n",
            "! Validation         19   75.580    0.005        0.016     3.59e-06        0.016       0.0742       0.0977       0.0621       0.0985       0.0803        0.081        0.125        0.103        0.119      0.00124\n",
            "Wall time: 75.58098972499965\n",
            "! Best model       19    0.016\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     20     1       0.0157       0.0157     6.54e-06       0.0746       0.0968       0.0635       0.0968       0.0801       0.0823        0.121        0.102        0.138      0.00144\n",
            "     20     2       0.0134       0.0134     6.23e-06       0.0687       0.0896       0.0579       0.0905       0.0742       0.0741        0.114       0.0942        0.171      0.00178\n",
            "     20     3       0.0169       0.0169     2.39e-05        0.076          0.1       0.0652       0.0976       0.0814       0.0857        0.125        0.105        0.335      0.00349\n",
            "     20     4       0.0175       0.0175     4.62e-06       0.0776        0.102        0.065        0.103       0.0839       0.0855         0.13        0.107        0.139      0.00145\n",
            "     20     5       0.0143       0.0143      1.8e-05       0.0704       0.0926       0.0584       0.0943       0.0763       0.0776        0.117       0.0973        0.301      0.00314\n",
            "     20     6       0.0139       0.0139     2.38e-05       0.0705       0.0911       0.0594       0.0927       0.0761       0.0762        0.115       0.0958        0.354      0.00369\n",
            "     20     7       0.0141       0.0141     9.46e-06       0.0696       0.0918       0.0578       0.0933       0.0755       0.0751        0.118       0.0967        0.208      0.00216\n",
            "     20     8       0.0125       0.0125     4.06e-06       0.0665       0.0866       0.0543       0.0908       0.0725       0.0694        0.114       0.0914        0.116       0.0012\n",
            "     20     9       0.0132       0.0132     5.65e-06        0.068       0.0888       0.0581       0.0878        0.073       0.0754        0.111       0.0931        0.162      0.00169\n",
            "     20    10       0.0178       0.0178     9.27e-05       0.0803        0.103       0.0676        0.106       0.0867        0.085        0.132        0.109        0.698      0.00727\n",
            "     20    11       0.0169       0.0169     1.32e-05       0.0768          0.1       0.0662        0.098       0.0821       0.0869        0.123        0.105        0.245      0.00255\n",
            "     20    12       0.0136       0.0136     3.95e-06       0.0693       0.0903       0.0582       0.0915       0.0749       0.0752        0.115       0.0949        0.123      0.00129\n",
            "     20    13       0.0131       0.0131     2.73e-05        0.067       0.0885       0.0549       0.0912       0.0731       0.0704        0.117       0.0935        0.384        0.004\n",
            "     20    14       0.0133       0.0132     3.13e-05       0.0687        0.089        0.058       0.0901       0.0741       0.0736        0.114       0.0937        0.391      0.00407\n",
            "     20    15       0.0137       0.0137     1.11e-05       0.0695       0.0904       0.0578       0.0928       0.0753        0.075        0.115       0.0951        0.231      0.00241\n",
            "     20    16       0.0136       0.0136     7.53e-06       0.0678       0.0903       0.0555       0.0924        0.074       0.0731        0.117       0.0953        0.197      0.00205\n",
            "     20    17       0.0163       0.0162     1.06e-05       0.0753       0.0986        0.064       0.0978       0.0809       0.0824        0.125        0.104         0.24       0.0025\n",
            "     20    18       0.0177       0.0176     6.47e-06       0.0808        0.103       0.0722        0.098       0.0851       0.0918        0.122        0.107        0.124      0.00129\n",
            "     20    19       0.0199       0.0199     1.07e-05       0.0827        0.109       0.0702        0.108        0.089       0.0936        0.135        0.114        0.219      0.00228\n",
            "     20    20       0.0132       0.0132     3.35e-05       0.0682       0.0889       0.0577       0.0892       0.0735       0.0748        0.112       0.0933        0.419      0.00436\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     20     1       0.0166       0.0166     5.03e-06       0.0755       0.0997       0.0631          0.1       0.0817       0.0824        0.127        0.105        0.133      0.00139\n",
            "     20     2       0.0161       0.0161     2.68e-06        0.074       0.0981       0.0613       0.0993       0.0803        0.081        0.125        0.103        0.112      0.00117\n",
            "     20     3        0.014        0.014     3.48e-06         0.07       0.0916       0.0582       0.0937       0.0759       0.0754        0.118       0.0965         0.12      0.00125\n",
            "     20     4       0.0151       0.0151     3.38e-06       0.0722        0.095       0.0612       0.0944       0.0778       0.0797         0.12       0.0998        0.115      0.00119\n",
            "     20     5       0.0147       0.0147     2.14e-06       0.0713       0.0938        0.059        0.096       0.0775       0.0766        0.121       0.0989        0.093     0.000968\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              20   79.337    0.005        0.015     1.75e-05        0.015       0.0724       0.0948       0.0611       0.0951       0.0781       0.0794         0.12       0.0996         0.26      0.00271\n",
            "! Validation         20   79.337    0.005       0.0153     3.34e-06       0.0153       0.0726       0.0957       0.0606       0.0967       0.0786       0.0791        0.122        0.101        0.115      0.00119\n",
            "Wall time: 79.33739603599997\n",
            "! Best model       20    0.015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     21     1       0.0124       0.0123     7.47e-05       0.0663       0.0858        0.056       0.0868       0.0714       0.0708         0.11       0.0904        0.636      0.00662\n",
            "     21     2       0.0172       0.0172     1.69e-06        0.078        0.101       0.0656        0.103       0.0842       0.0861        0.126        0.106       0.0746     0.000777\n",
            "     21     3        0.017       0.0169     4.76e-05       0.0767        0.101       0.0654       0.0992       0.0823       0.0847        0.126        0.106         0.49       0.0051\n",
            "     21     4       0.0175       0.0174     0.000151       0.0794        0.102       0.0697        0.099       0.0843       0.0887        0.124        0.106        0.907      0.00945\n",
            "     21     5        0.015        0.015     6.13e-06       0.0735       0.0948       0.0616       0.0973       0.0795       0.0787        0.121       0.0997        0.166      0.00173\n",
            "     21     6       0.0147       0.0147     4.94e-06       0.0717       0.0937       0.0607       0.0939       0.0773       0.0787        0.118       0.0984        0.152      0.00159\n",
            "     21     7        0.016       0.0159      7.5e-05        0.073       0.0975       0.0609        0.097        0.079       0.0788        0.127        0.103         0.63      0.00657\n",
            "     21     8       0.0189       0.0188     0.000146       0.0832        0.106       0.0741        0.102       0.0878       0.0931        0.128        0.111        0.895      0.00932\n",
            "     21     9       0.0121       0.0121     2.09e-06       0.0662       0.0852       0.0566       0.0855        0.071        0.071        0.108       0.0895       0.0834     0.000869\n",
            "     21    10       0.0124       0.0123     5.69e-05       0.0669       0.0859       0.0569        0.087        0.072       0.0717        0.109       0.0903         0.55      0.00573\n",
            "     21    11       0.0156       0.0155     8.48e-05       0.0752       0.0964       0.0675       0.0906        0.079       0.0849        0.116          0.1        0.673      0.00701\n",
            "     21    12       0.0151       0.0151      7.3e-05       0.0722        0.095       0.0597       0.0971       0.0784       0.0789        0.121       0.0999        0.621      0.00647\n",
            "     21    13       0.0138       0.0138     3.07e-06       0.0685        0.091       0.0576       0.0903       0.0739       0.0755        0.116       0.0957        0.127      0.00132\n",
            "     21    14       0.0161       0.0161     4.42e-05       0.0751       0.0981       0.0608        0.104       0.0823       0.0796        0.127        0.103         0.49      0.00511\n",
            "     21    15       0.0147       0.0147     2.59e-05       0.0717       0.0939       0.0625       0.0902       0.0763       0.0801        0.117       0.0984        0.364      0.00379\n",
            "     21    16        0.015        0.015     8.45e-06       0.0728       0.0947       0.0607        0.097       0.0788       0.0786        0.121       0.0996         0.19      0.00198\n",
            "     21    17       0.0129       0.0129     3.42e-06       0.0666       0.0877       0.0538       0.0923       0.0731       0.0693        0.116       0.0927        0.121      0.00126\n",
            "     21    18       0.0148       0.0148      1.6e-06       0.0727       0.0942       0.0629       0.0922       0.0775       0.0799        0.118       0.0988       0.0908     0.000946\n",
            "     21    19       0.0163       0.0163     2.74e-05       0.0753       0.0987       0.0652       0.0956       0.0804       0.0836        0.123        0.104         0.38      0.00396\n",
            "     21    20        0.015        0.015     2.49e-05       0.0727       0.0947       0.0632       0.0916       0.0774       0.0821        0.116        0.099        0.365       0.0038\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     21     1        0.016        0.016     5.26e-06        0.074       0.0977       0.0616       0.0986       0.0801       0.0806        0.125        0.103        0.139      0.00145\n",
            "     21     2       0.0154       0.0154     2.51e-06       0.0724       0.0961       0.0598       0.0976       0.0787       0.0791        0.123        0.101        0.107      0.00112\n",
            "     21     3       0.0135       0.0135     3.57e-06       0.0686       0.0898       0.0569       0.0921       0.0745       0.0736        0.116       0.0946        0.121      0.00126\n",
            "     21     4       0.0146       0.0146     3.67e-06       0.0709       0.0933       0.0598       0.0929       0.0764       0.0779        0.118       0.0981        0.117      0.00122\n",
            "     21     5       0.0142       0.0142     2.26e-06         0.07       0.0921       0.0578       0.0943       0.0761        0.075        0.119       0.0971       0.0917     0.000955\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              21   83.089    0.005       0.0151     4.31e-05       0.0151       0.0729        0.095       0.0621       0.0945       0.0783         0.08         0.12       0.0998          0.4      0.00417\n",
            "! Validation         21   83.089    0.005       0.0147     3.46e-06       0.0147       0.0712       0.0939       0.0592       0.0951       0.0771       0.0773         0.12       0.0988        0.115       0.0012\n",
            "Wall time: 83.08959570199977\n",
            "! Best model       21    0.015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     22     1       0.0139       0.0138     8.47e-05       0.0687       0.0909       0.0574       0.0911       0.0743       0.0751        0.116       0.0957        0.674      0.00703\n",
            "     22     2       0.0168       0.0168     6.26e-05       0.0764          0.1       0.0655       0.0981       0.0818       0.0849        0.125        0.105        0.575      0.00599\n",
            "     22     3       0.0183       0.0183     4.91e-06       0.0796        0.105       0.0703       0.0982       0.0842       0.0937        0.123        0.109        0.111      0.00115\n",
            "     22     4       0.0135       0.0134     8.82e-05       0.0695       0.0894       0.0597        0.089       0.0744       0.0752        0.113       0.0939        0.695      0.00723\n",
            "     22     5       0.0129       0.0128     9.47e-05       0.0679       0.0875       0.0581       0.0875       0.0728       0.0739         0.11       0.0919        0.713      0.00743\n",
            "     22     6       0.0148       0.0148     4.09e-06       0.0718       0.0942       0.0596       0.0964        0.078       0.0759        0.123       0.0993        0.121      0.00126\n",
            "     22     7       0.0157       0.0157     8.21e-07       0.0737        0.097       0.0606          0.1       0.0803        0.078        0.127        0.102       0.0609     0.000635\n",
            "     22     8       0.0154       0.0153     5.74e-05       0.0736       0.0958       0.0621       0.0967       0.0794       0.0786        0.123        0.101        0.562      0.00585\n",
            "     22     9        0.013        0.013     3.19e-05       0.0682       0.0881       0.0564       0.0917       0.0741       0.0706        0.116        0.093        0.411      0.00428\n",
            "     22    10       0.0153       0.0153     5.61e-06        0.073       0.0958       0.0613       0.0964       0.0788       0.0802        0.121        0.101        0.136      0.00142\n",
            "     22    11       0.0127       0.0127     2.39e-06       0.0657       0.0871       0.0548       0.0876       0.0712        0.072        0.111       0.0916       0.0947     0.000987\n",
            "     22    12       0.0137       0.0137      5.2e-06        0.069       0.0906       0.0575        0.092       0.0747        0.075        0.116       0.0953        0.164      0.00171\n",
            "     22    13       0.0121       0.0121     3.03e-05       0.0638        0.085       0.0524       0.0866       0.0695       0.0695         0.11       0.0895        0.403       0.0042\n",
            "     22    14        0.012        0.012     2.65e-06        0.066       0.0849       0.0551       0.0877       0.0714       0.0703        0.108       0.0893       0.0939     0.000979\n",
            "     22    15       0.0135       0.0135     1.25e-05       0.0693       0.0899       0.0565       0.0949       0.0757       0.0735        0.116       0.0947        0.254      0.00264\n",
            "     22    16       0.0117       0.0117      5.6e-06        0.064       0.0835       0.0539       0.0842        0.069       0.0692        0.107       0.0879        0.165      0.00172\n",
            "     22    17       0.0127       0.0126     3.24e-05       0.0657       0.0869       0.0544       0.0883       0.0713       0.0718        0.111       0.0915        0.408      0.00425\n",
            "     22    18       0.0118       0.0118     1.71e-05       0.0634       0.0841       0.0526       0.0849       0.0688       0.0688        0.108       0.0887        0.293      0.00305\n",
            "     22    19        0.012        0.012     9.93e-06       0.0651       0.0847       0.0561       0.0832       0.0697        0.071        0.107        0.089        0.222      0.00232\n",
            "     22    20        0.013        0.013     4.47e-05       0.0669       0.0881       0.0537       0.0933       0.0735       0.0685        0.118       0.0932        0.492      0.00512\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     22     1       0.0153       0.0153     4.77e-06       0.0725       0.0958       0.0602        0.097       0.0786       0.0788        0.123        0.101         0.13      0.00136\n",
            "     22     2       0.0148       0.0148     2.19e-06        0.071       0.0942       0.0584        0.096       0.0772       0.0772        0.121       0.0992       0.0993      0.00103\n",
            "     22     3        0.013        0.013     3.21e-06       0.0673       0.0882       0.0557       0.0907       0.0732        0.072        0.114        0.093        0.116      0.00121\n",
            "     22     4       0.0141       0.0141     3.19e-06       0.0695       0.0917       0.0586       0.0915        0.075       0.0762        0.117       0.0965        0.111      0.00116\n",
            "     22     5       0.0137       0.0137     2.13e-06       0.0686       0.0904       0.0566       0.0927       0.0746       0.0735        0.117       0.0953       0.0904     0.000942\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              22   86.837    0.005       0.0137     2.99e-05       0.0137       0.0691       0.0906       0.0579       0.0914       0.0746        0.075        0.116       0.0953        0.332      0.00346\n",
            "! Validation         22   86.837    0.005       0.0142      3.1e-06       0.0142       0.0698       0.0921       0.0579       0.0936       0.0757       0.0756        0.118        0.097        0.109      0.00114\n",
            "Wall time: 86.83750201399971\n",
            "! Best model       22    0.014\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     23     1       0.0115       0.0115     3.91e-07       0.0641        0.083       0.0533       0.0855       0.0694       0.0693        0.105       0.0873       0.0455     0.000474\n",
            "     23     2       0.0127       0.0126     2.57e-05       0.0683       0.0869       0.0583       0.0883       0.0733       0.0735        0.109       0.0912        0.369      0.00385\n",
            "     23     3       0.0134       0.0134     5.11e-05       0.0682       0.0894       0.0562       0.0922       0.0742       0.0735        0.115       0.0942        0.528       0.0055\n",
            "     23     4       0.0141       0.0141     1.55e-05       0.0693       0.0918       0.0567       0.0946       0.0756       0.0747        0.119       0.0968         0.28      0.00292\n",
            "     23     5       0.0128       0.0127     9.12e-05       0.0658       0.0872       0.0547       0.0881       0.0714       0.0713        0.113       0.0919          0.7       0.0073\n",
            "     23     6       0.0142       0.0142     1.82e-05       0.0702        0.092       0.0584        0.094       0.0762       0.0758        0.118       0.0969        0.312      0.00325\n",
            "     23     7       0.0127       0.0127     6.23e-05       0.0664        0.087       0.0554       0.0885       0.0719       0.0726         0.11       0.0915        0.571      0.00594\n",
            "     23     8       0.0141       0.0141     7.53e-05       0.0697       0.0917       0.0571       0.0951       0.0761       0.0747        0.119       0.0967        0.624       0.0065\n",
            "     23     9       0.0132       0.0132     6.21e-06        0.067        0.089       0.0561       0.0887       0.0724       0.0728        0.115       0.0938        0.158      0.00165\n",
            "     23    10       0.0125       0.0124     8.11e-05       0.0656       0.0862        0.055       0.0869       0.0709        0.072        0.109       0.0906        0.652      0.00679\n",
            "     23    11       0.0135       0.0135     1.14e-05       0.0691         0.09       0.0587       0.0898       0.0743       0.0747        0.115       0.0946        0.233      0.00243\n",
            "     23    12       0.0132       0.0131     5.75e-06       0.0686       0.0887       0.0583       0.0892       0.0738        0.074        0.112       0.0932        0.154       0.0016\n",
            "     23    13       0.0116       0.0115     0.000104       0.0635        0.083       0.0528       0.0849       0.0688       0.0688        0.106       0.0874        0.754      0.00785\n",
            "     23    14       0.0127       0.0127     3.55e-06       0.0671       0.0871       0.0569       0.0876       0.0722       0.0729         0.11       0.0916        0.123      0.00128\n",
            "     23    15       0.0166       0.0165     7.49e-05       0.0778       0.0994       0.0717       0.0899       0.0808       0.0898        0.116        0.103        0.633       0.0066\n",
            "     23    16        0.016        0.016     5.05e-06       0.0773       0.0978       0.0702       0.0915       0.0808       0.0883        0.115        0.101        0.141      0.00147\n",
            "     23    17       0.0144       0.0144      4.3e-06       0.0715       0.0927       0.0595       0.0953       0.0774       0.0749        0.121       0.0978        0.152      0.00158\n",
            "     23    18       0.0146       0.0146     4.25e-05       0.0701       0.0934       0.0582       0.0939        0.076       0.0777        0.119       0.0982        0.475      0.00495\n",
            "     23    19       0.0141       0.0141     1.19e-06       0.0707        0.092       0.0571       0.0979       0.0775       0.0723        0.122       0.0972       0.0619     0.000645\n",
            "     23    20       0.0187       0.0186     9.46e-05       0.0827        0.106       0.0733        0.102       0.0875       0.0914        0.129         0.11        0.715      0.00745\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     23     1       0.0148       0.0148      4.5e-06       0.0712       0.0941        0.059       0.0955       0.0772       0.0772        0.121       0.0991         0.13      0.00135\n",
            "     23     2       0.0143       0.0143     1.96e-06       0.0695       0.0924       0.0571       0.0945       0.0758       0.0755        0.119       0.0974       0.0929     0.000967\n",
            "     23     3       0.0126       0.0126     3.06e-06       0.0661       0.0867       0.0545       0.0892       0.0719       0.0705        0.112       0.0914        0.113      0.00118\n",
            "     23     4       0.0136       0.0136     2.99e-06       0.0683       0.0902       0.0574       0.0901       0.0738       0.0747        0.115       0.0949        0.108      0.00113\n",
            "     23     5       0.0132       0.0132     2.04e-06       0.0673       0.0888       0.0554       0.0911       0.0733       0.0721        0.115       0.0937       0.0875     0.000911\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              23   90.617    0.005       0.0138     3.87e-05       0.0138       0.0696       0.0909       0.0589       0.0912        0.075        0.076        0.115       0.0955        0.384        0.004\n",
            "! Validation         23   90.617    0.005       0.0137     2.91e-06       0.0137       0.0685       0.0905       0.0567       0.0921       0.0744        0.074        0.117       0.0953        0.106      0.00111\n",
            "Wall time: 90.61759600799996\n",
            "! Best model       23    0.014\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     24     1       0.0198       0.0198     1.24e-06       0.0858        0.109       0.0782        0.101       0.0896        0.098        0.128        0.113       0.0691      0.00072\n",
            "     24     2       0.0144       0.0144     1.59e-05       0.0712       0.0927       0.0614       0.0908       0.0761       0.0782        0.116       0.0973        0.289      0.00301\n",
            "     24     3       0.0117       0.0117     4.46e-05       0.0631       0.0836       0.0519       0.0854       0.0686       0.0675        0.109       0.0882        0.479      0.00499\n",
            "     24     4       0.0149       0.0149     4.81e-06       0.0737       0.0944       0.0633       0.0944       0.0789       0.0794        0.119       0.0991        0.158      0.00165\n",
            "     24     5       0.0137       0.0136     9.45e-05       0.0688       0.0901       0.0584       0.0897        0.074        0.075        0.115       0.0948        0.721      0.00751\n",
            "     24     6       0.0122       0.0122     2.68e-05       0.0642       0.0855       0.0526       0.0874         0.07       0.0701         0.11       0.0901         0.37      0.00386\n",
            "     24     7       0.0126       0.0126     2.21e-05       0.0672       0.0869        0.057       0.0877       0.0723       0.0729         0.11       0.0913        0.338      0.00352\n",
            "     24     8       0.0138       0.0138     2.53e-05       0.0699       0.0908       0.0593       0.0911       0.0752       0.0761        0.115       0.0954        0.359      0.00374\n",
            "     24     9       0.0137       0.0136     1.91e-05        0.068       0.0904       0.0555       0.0931       0.0743       0.0742        0.116       0.0952         0.31      0.00323\n",
            "     24    10       0.0132       0.0132     7.73e-06       0.0689        0.089        0.058       0.0908       0.0744       0.0745        0.112       0.0935        0.198      0.00207\n",
            "     24    11       0.0145       0.0144     7.76e-05       0.0716        0.093       0.0581       0.0986       0.0784       0.0732        0.123       0.0983        0.648      0.00675\n",
            "     24    12       0.0126       0.0126     4.67e-06       0.0655       0.0869        0.056       0.0847       0.0703       0.0732        0.109       0.0913        0.157      0.00163\n",
            "     24    13       0.0126       0.0126     1.64e-05       0.0679       0.0868       0.0591       0.0856       0.0723       0.0748        0.107       0.0908        0.282      0.00294\n",
            "     24    14       0.0128       0.0128     1.38e-05       0.0671       0.0874       0.0555       0.0901       0.0728       0.0711        0.113       0.0921        0.255      0.00266\n",
            "     24    15       0.0164       0.0164     7.39e-06       0.0758       0.0989       0.0647       0.0978       0.0813       0.0834        0.124        0.104        0.198      0.00206\n",
            "     24    16       0.0134       0.0133     7.02e-05       0.0687       0.0892       0.0557       0.0946       0.0751       0.0701        0.119       0.0944        0.614      0.00639\n",
            "     24    17        0.014        0.014     4.48e-06       0.0707       0.0916       0.0575       0.0972       0.0773       0.0733         0.12       0.0967        0.125       0.0013\n",
            "     24    18       0.0125       0.0124     3.91e-05       0.0671       0.0863       0.0573       0.0867        0.072       0.0734        0.108       0.0905        0.463      0.00483\n",
            "     24    19       0.0122       0.0121     2.77e-05       0.0641       0.0852       0.0532        0.086       0.0696       0.0696         0.11       0.0898        0.384        0.004\n",
            "     24    20       0.0111       0.0111     5.06e-07       0.0626       0.0816        0.053       0.0816       0.0673       0.0684        0.103       0.0857       0.0396     0.000413\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     24     1       0.0143       0.0143     3.96e-06         0.07       0.0925       0.0579       0.0941        0.076       0.0758        0.119       0.0974        0.122      0.00127\n",
            "     24     2       0.0138       0.0138     1.78e-06       0.0684        0.091        0.056       0.0932       0.0746       0.0741        0.118       0.0959       0.0875     0.000911\n",
            "     24     3       0.0122       0.0122     2.69e-06        0.065       0.0853       0.0535       0.0879       0.0707       0.0692        0.111         0.09        0.107      0.00112\n",
            "     24     4       0.0132       0.0132     2.41e-06       0.0672       0.0889       0.0564       0.0889       0.0726       0.0733        0.114       0.0936       0.0997      0.00104\n",
            "     24     5       0.0128       0.0128      1.9e-06       0.0662       0.0875       0.0545       0.0897       0.0721       0.0709        0.114       0.0922       0.0858     0.000894\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              24   94.387    0.005       0.0136     2.62e-05       0.0136       0.0691       0.0901       0.0583       0.0907       0.0745       0.0751        0.114       0.0948        0.323      0.00336\n",
            "! Validation         24   94.387    0.005       0.0133     2.55e-06       0.0133       0.0674       0.0891       0.0557       0.0907       0.0732       0.0727        0.115       0.0939          0.1      0.00105\n",
            "Wall time: 94.3873906089998\n",
            "! Best model       24    0.013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     25     1       0.0149       0.0149     4.62e-05       0.0716       0.0944       0.0598       0.0951       0.0775        0.079        0.119       0.0992        0.496      0.00517\n",
            "     25     2        0.014        0.014      6.4e-06       0.0698       0.0914       0.0575       0.0945        0.076       0.0726         0.12       0.0965        0.133      0.00138\n",
            "     25     3       0.0133       0.0133     2.47e-06       0.0692       0.0893       0.0601       0.0876       0.0738       0.0752        0.112       0.0938       0.0816      0.00085\n",
            "     25     4        0.012       0.0119     5.18e-05        0.066       0.0845       0.0564       0.0852       0.0708       0.0715        0.106       0.0886        0.506      0.00527\n",
            "     25     5       0.0109       0.0109     1.14e-06       0.0606       0.0807       0.0503       0.0813       0.0658       0.0659        0.104        0.085       0.0676     0.000704\n",
            "     25     6        0.012       0.0118     0.000196       0.0652       0.0842        0.056       0.0835       0.0697       0.0716        0.105       0.0883         1.04       0.0108\n",
            "     25     7       0.0136       0.0135     6.58e-05       0.0657       0.0899       0.0524       0.0924       0.0724       0.0722        0.117       0.0949        0.602      0.00627\n",
            "     25     8       0.0117       0.0116      3.9e-05       0.0639       0.0835       0.0537       0.0844        0.069       0.0684        0.107       0.0879        0.456      0.00475\n",
            "     25     9       0.0111        0.011     0.000103       0.0626       0.0812        0.052       0.0838       0.0679        0.066        0.105       0.0856        0.752      0.00783\n",
            "     25    10       0.0121       0.0121     7.76e-05       0.0653       0.0849       0.0545       0.0871       0.0708       0.0692         0.11       0.0895        0.649      0.00676\n",
            "     25    11       0.0127       0.0127     2.07e-05       0.0676       0.0873       0.0588       0.0853        0.072       0.0744        0.109       0.0915        0.335      0.00349\n",
            "     25    12       0.0121        0.012     6.88e-05        0.065       0.0847       0.0537       0.0875       0.0706       0.0684         0.11       0.0894        0.612      0.00637\n",
            "     25    13       0.0134       0.0134     6.94e-07       0.0677       0.0895       0.0566       0.0898       0.0732       0.0747        0.113       0.0941       0.0516     0.000537\n",
            "     25    14       0.0114       0.0113     2.29e-05       0.0631       0.0824       0.0536       0.0823       0.0679       0.0682        0.105       0.0867         0.34      0.00354\n",
            "     25    15        0.012       0.0119     1.46e-05       0.0645       0.0845       0.0539       0.0857       0.0698       0.0701        0.108       0.0889        0.261      0.00272\n",
            "     25    16       0.0117       0.0116     2.07e-05        0.062       0.0834       0.0492       0.0876       0.0684       0.0652        0.111       0.0882        0.322      0.00335\n",
            "     25    17       0.0125       0.0124     4.73e-05       0.0654       0.0862       0.0529       0.0904       0.0716       0.0674        0.115       0.0912          0.5      0.00521\n",
            "     25    18        0.012        0.012      2.1e-06       0.0644       0.0847       0.0526       0.0879       0.0702       0.0688         0.11       0.0892         0.09     0.000938\n",
            "     25    19       0.0125       0.0125     2.87e-05       0.0661       0.0865        0.054       0.0904       0.0722       0.0711        0.111       0.0911        0.389      0.00405\n",
            "     25    20       0.0106       0.0106     9.37e-06       0.0616       0.0796       0.0502       0.0842       0.0672       0.0628        0.105       0.0841        0.226      0.00235\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     25     1       0.0138       0.0138     3.65e-06       0.0689        0.091       0.0569       0.0928       0.0749       0.0745        0.117       0.0959        0.116      0.00121\n",
            "     25     2       0.0134       0.0134     1.65e-06       0.0672       0.0895       0.0549       0.0919       0.0734       0.0726        0.116       0.0944        0.085     0.000885\n",
            "     25     3       0.0118       0.0118      2.5e-06        0.064       0.0841       0.0526       0.0867       0.0697        0.068        0.109       0.0887        0.104      0.00108\n",
            "     25     4       0.0128       0.0128     1.99e-06       0.0662       0.0877       0.0553       0.0878       0.0716       0.0719        0.113       0.0923       0.0919     0.000957\n",
            "     25     5       0.0124       0.0124     1.85e-06       0.0652       0.0861       0.0536       0.0883       0.0709       0.0698        0.112       0.0908       0.0842     0.000877\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              25   98.152    0.005       0.0123     4.12e-05       0.0123       0.0654       0.0857       0.0544       0.0873       0.0709       0.0702         0.11       0.0903        0.395      0.00412\n",
            "! Validation         25   98.152    0.005       0.0129     2.33e-06       0.0129       0.0663       0.0877       0.0547       0.0895       0.0721       0.0714        0.114       0.0925       0.0962        0.001\n",
            "Wall time: 98.15232986000001\n",
            "! Best model       25    0.013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     26     1       0.0135       0.0135     1.21e-05       0.0681         0.09       0.0562       0.0919        0.074        0.074        0.116       0.0948        0.187      0.00194\n",
            "     26     2       0.0128       0.0127     5.11e-06       0.0676       0.0873       0.0596       0.0836       0.0716       0.0759        0.107       0.0912        0.149      0.00156\n",
            "     26     3       0.0121       0.0121     7.89e-06        0.066       0.0851       0.0559       0.0862        0.071       0.0703        0.109       0.0896        0.203      0.00211\n",
            "     26     4       0.0124       0.0124     1.58e-05       0.0654       0.0862       0.0536        0.089       0.0713       0.0703        0.112       0.0909        0.285      0.00297\n",
            "     26     5         0.01         0.01     2.17e-05       0.0587       0.0773       0.0469       0.0824       0.0647       0.0606        0.103       0.0818        0.342      0.00356\n",
            "     26     6       0.0104       0.0104     1.97e-06       0.0601       0.0787       0.0483       0.0837        0.066        0.062        0.104       0.0832       0.0861     0.000897\n",
            "     26     7       0.0119       0.0119     1.69e-05       0.0631       0.0845       0.0517        0.086       0.0688       0.0684         0.11       0.0891        0.302      0.00315\n",
            "     26     8       0.0126       0.0126     4.18e-05       0.0666       0.0868       0.0567       0.0865       0.0716       0.0718        0.111       0.0914        0.475      0.00495\n",
            "     26     9       0.0137       0.0137     2.37e-06       0.0695       0.0904       0.0597       0.0891       0.0744       0.0762        0.114       0.0949       0.0982      0.00102\n",
            "     26    10       0.0139       0.0139     1.79e-05        0.069       0.0913       0.0569       0.0931        0.075        0.075        0.117       0.0962        0.302      0.00314\n",
            "     26    11       0.0109       0.0109     1.95e-05       0.0612       0.0806       0.0501       0.0835       0.0668       0.0642        0.106       0.0852        0.326       0.0034\n",
            "     26    12       0.0106       0.0105     9.94e-06       0.0618       0.0795       0.0519       0.0816       0.0668       0.0655        0.102       0.0836        0.204      0.00212\n",
            "     26    13       0.0135       0.0135     5.19e-06       0.0681         0.09       0.0565       0.0911       0.0738       0.0723        0.118        0.095        0.157      0.00164\n",
            "     26    14       0.0169       0.0169     1.91e-05       0.0788        0.101       0.0711        0.094       0.0826       0.0902        0.118        0.104        0.314      0.00328\n",
            "     26    15        0.016        0.016     2.25e-06        0.077       0.0977       0.0712       0.0886       0.0799       0.0894        0.112        0.101       0.0863     0.000899\n",
            "     26    16       0.0115       0.0115      1.3e-05        0.064        0.083       0.0538       0.0844       0.0691       0.0684        0.106       0.0874        0.263      0.00274\n",
            "     26    17       0.0118       0.0118     4.34e-05       0.0644        0.084       0.0547       0.0838       0.0693       0.0709        0.105       0.0881        0.478      0.00498\n",
            "     26    18       0.0168       0.0168     1.33e-05       0.0777          0.1       0.0681        0.097       0.0825       0.0886         0.12        0.105        0.264      0.00275\n",
            "     26    19       0.0191       0.0189     0.000186       0.0844        0.106        0.077       0.0991        0.088       0.0956        0.125         0.11         1.01       0.0105\n",
            "     26    20       0.0187       0.0186     7.87e-05        0.083        0.106       0.0712        0.107       0.0889       0.0887        0.133        0.111        0.653       0.0068\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     26     1       0.0134       0.0134      3.4e-06       0.0678       0.0896        0.056       0.0915       0.0737       0.0732        0.116       0.0944         0.11      0.00115\n",
            "     26     2        0.013        0.013     1.63e-06       0.0662       0.0881       0.0539       0.0907       0.0723       0.0713        0.115        0.093       0.0841     0.000876\n",
            "     26     3       0.0115       0.0115     2.22e-06        0.063       0.0829       0.0517       0.0855       0.0686       0.0668        0.108       0.0875       0.0977      0.00102\n",
            "     26     4       0.0125       0.0125     1.67e-06       0.0652       0.0864       0.0544       0.0868       0.0706       0.0707        0.111       0.0911       0.0846     0.000881\n",
            "     26     5        0.012        0.012     1.84e-06       0.0641       0.0849       0.0527       0.0869       0.0698       0.0688         0.11       0.0895       0.0836     0.000871\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              26  101.930    0.005       0.0134     2.67e-05       0.0135       0.0687       0.0897       0.0586       0.0891       0.0738       0.0756        0.113       0.0941        0.309      0.00322\n",
            "! Validation         26  101.930    0.005       0.0125     2.15e-06       0.0125       0.0653       0.0864       0.0538       0.0883        0.071       0.0702        0.112       0.0911        0.092     0.000958\n",
            "Wall time: 101.93056835699963\n",
            "! Best model       26    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     27     1       0.0139       0.0139     6.45e-06       0.0703       0.0913       0.0596       0.0916       0.0756        0.075        0.117       0.0961        0.183      0.00191\n",
            "     27     2       0.0111       0.0109     0.000203       0.0603       0.0809       0.0503       0.0803       0.0653       0.0649        0.106       0.0854         1.06        0.011\n",
            "     27     3       0.0142        0.014      0.00022       0.0707       0.0916       0.0589       0.0944       0.0766       0.0751        0.118       0.0965          1.1       0.0114\n",
            "     27     4       0.0226       0.0225     6.01e-05       0.0928        0.116       0.0867        0.105       0.0958        0.109         0.13        0.119        0.571      0.00595\n",
            "     27     5       0.0164       0.0164     1.96e-05       0.0768        0.099       0.0688       0.0929       0.0809       0.0873        0.119        0.103        0.319      0.00332\n",
            "     27     6       0.0142       0.0141     3.07e-05       0.0679        0.092       0.0542       0.0954       0.0748       0.0747        0.119        0.097         0.41      0.00427\n",
            "     27     7       0.0143       0.0143     2.24e-06       0.0723       0.0925       0.0644       0.0882       0.0763       0.0816        0.111       0.0964       0.0982      0.00102\n",
            "     27     8       0.0193       0.0193     5.31e-05       0.0851        0.107       0.0755        0.104       0.0899        0.093        0.132        0.112        0.531      0.00554\n",
            "     27     9       0.0169       0.0169     1.43e-05       0.0775        0.101       0.0669       0.0988       0.0828        0.085        0.126        0.105        0.238      0.00248\n",
            "     27    10       0.0111       0.0111     4.68e-06       0.0618       0.0814        0.051       0.0833       0.0672       0.0662        0.105       0.0858        0.124      0.00129\n",
            "     27    11       0.0175       0.0175     4.19e-05       0.0808        0.102       0.0717       0.0989       0.0853       0.0901        0.123        0.107        0.468      0.00488\n",
            "     27    12       0.0149       0.0149     1.59e-06       0.0733       0.0946       0.0657       0.0884       0.0771       0.0845        0.112       0.0983       0.0795     0.000828\n",
            "     27    13       0.0116       0.0116     2.36e-05       0.0643       0.0832        0.052        0.089       0.0705        0.066         0.11       0.0879        0.344      0.00358\n",
            "     27    14       0.0177       0.0177     3.99e-05       0.0811        0.103       0.0753       0.0925       0.0839       0.0947        0.118        0.106        0.462      0.00481\n",
            "     27    15       0.0176       0.0175     6.57e-05       0.0804        0.102       0.0728       0.0957       0.0843       0.0905        0.123        0.107        0.598      0.00623\n",
            "     27    16       0.0111       0.0111     5.02e-05       0.0616       0.0813       0.0515       0.0817       0.0666       0.0667        0.105       0.0857        0.521      0.00542\n",
            "     27    17       0.0119       0.0119     7.94e-06        0.066       0.0843       0.0572       0.0837       0.0705       0.0727        0.104       0.0883        0.202       0.0021\n",
            "     27    18       0.0155       0.0155      6.3e-06       0.0766       0.0963       0.0651       0.0997       0.0824       0.0804        0.122        0.101        0.179      0.00186\n",
            "     27    19       0.0142       0.0141     2.29e-05         0.07        0.092       0.0596       0.0909       0.0752       0.0787        0.114       0.0964        0.355       0.0037\n",
            "     27    20       0.0139       0.0139     4.53e-05       0.0692       0.0911       0.0568       0.0941       0.0755       0.0742        0.118        0.096        0.484      0.00504\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     27     1       0.0131       0.0131     3.38e-06        0.067       0.0885       0.0552       0.0906       0.0729       0.0721        0.115       0.0933        0.112      0.00117\n",
            "     27     2       0.0127       0.0127     1.43e-06       0.0653       0.0871       0.0531       0.0896       0.0714       0.0703        0.113       0.0919         0.08     0.000833\n",
            "     27     3       0.0112       0.0112      2.2e-06       0.0622       0.0819       0.0511       0.0845       0.0678       0.0659        0.107       0.0864       0.0977      0.00102\n",
            "     27     4       0.0122       0.0122     1.78e-06       0.0645       0.0855       0.0536       0.0862       0.0699       0.0697        0.111       0.0902       0.0871     0.000907\n",
            "     27     5       0.0118       0.0118     1.78e-06       0.0633       0.0839        0.052       0.0857       0.0689        0.068        0.109       0.0884       0.0801     0.000834\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              27  105.701    0.005        0.015      4.6e-05        0.015        0.073       0.0946       0.0632       0.0924       0.0778       0.0813        0.117        0.099        0.416      0.00433\n",
            "! Validation         27  105.701    0.005       0.0122     2.11e-06       0.0122       0.0644       0.0854        0.053       0.0873       0.0702       0.0692        0.111       0.0901       0.0914     0.000952\n",
            "Wall time: 105.70138208299977\n",
            "! Best model       27    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     28     1       0.0165       0.0165      1.6e-06       0.0753       0.0992       0.0634       0.0989       0.0812       0.0839        0.124        0.104       0.0859     0.000895\n",
            "     28     2       0.0134       0.0134     3.91e-05       0.0684       0.0896       0.0576         0.09       0.0738       0.0754        0.113        0.094        0.443      0.00461\n",
            "     28     3       0.0127       0.0126     1.09e-06       0.0656        0.087       0.0557       0.0855       0.0706       0.0729         0.11       0.0914       0.0629     0.000655\n",
            "     28     4       0.0151       0.0151     4.75e-05       0.0742       0.0949       0.0641       0.0945       0.0793       0.0813        0.118       0.0994        0.498      0.00518\n",
            "     28     5       0.0157       0.0157     1.97e-05       0.0755       0.0969       0.0648       0.0967       0.0808        0.082        0.121        0.102         0.32      0.00333\n",
            "     28     6       0.0129       0.0129     7.65e-06       0.0666       0.0877       0.0556       0.0886       0.0721       0.0719        0.113       0.0924        0.185      0.00193\n",
            "     28     7       0.0117       0.0117     3.46e-05       0.0653       0.0837       0.0542       0.0875       0.0708       0.0688        0.108       0.0882        0.428      0.00446\n",
            "     28     8       0.0134       0.0133     8.35e-05       0.0686       0.0892       0.0574        0.091       0.0742       0.0729        0.115        0.094        0.671      0.00699\n",
            "     28     9       0.0121       0.0121     1.41e-06       0.0652       0.0852       0.0521       0.0915       0.0718       0.0659        0.114       0.0902       0.0742     0.000773\n",
            "     28    10       0.0103       0.0103     2.64e-05       0.0588       0.0783       0.0489       0.0786       0.0637       0.0646          0.1       0.0824         0.37      0.00385\n",
            "     28    11       0.0108       0.0108      1.3e-05       0.0611       0.0805       0.0519       0.0794       0.0657       0.0677        0.101       0.0845         0.25       0.0026\n",
            "     28    12        0.012        0.012      1.9e-06       0.0652       0.0846       0.0542       0.0873       0.0707       0.0679        0.111       0.0893       0.0857     0.000893\n",
            "     28    13       0.0101       0.0101     2.42e-05       0.0588       0.0776        0.049       0.0784       0.0637       0.0637       0.0996       0.0817        0.356      0.00371\n",
            "     28    14      0.00913      0.00912     6.24e-06       0.0564       0.0739       0.0461       0.0769       0.0615       0.0585       0.0976       0.0781        0.176      0.00183\n",
            "     28    15       0.0114       0.0113     7.41e-05       0.0633       0.0822       0.0519        0.086        0.069        0.067        0.106       0.0867        0.633       0.0066\n",
            "     28    16       0.0113       0.0113     6.35e-06       0.0628       0.0824       0.0529       0.0827       0.0678       0.0682        0.105       0.0867         0.17      0.00177\n",
            "     28    17      0.00976       0.0097     5.82e-05       0.0587       0.0762        0.049       0.0782       0.0636       0.0628       0.0976       0.0802        0.561      0.00584\n",
            "     28    18       0.0127       0.0126     7.83e-05       0.0668        0.087       0.0558       0.0888       0.0723       0.0717        0.111       0.0916        0.657      0.00684\n",
            "     28    19       0.0136       0.0136     1.94e-06        0.068       0.0902       0.0568       0.0904       0.0736       0.0748        0.115       0.0949       0.0844     0.000879\n",
            "     28    20        0.011       0.0109     0.000152       0.0618       0.0807       0.0512        0.083       0.0671       0.0659        0.104       0.0851        0.912       0.0095\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     28     1       0.0128       0.0128     3.08e-06       0.0662       0.0875       0.0545       0.0897       0.0721       0.0712        0.113       0.0922        0.103      0.00107\n",
            "     28     2       0.0124       0.0124     1.67e-06       0.0646       0.0861       0.0525       0.0888       0.0707       0.0693        0.113       0.0909       0.0825      0.00086\n",
            "     28     3        0.011        0.011     2.08e-06       0.0615       0.0811       0.0505       0.0837       0.0671       0.0651        0.106       0.0856       0.0921     0.000959\n",
            "     28     4        0.012        0.012     1.36e-06       0.0638       0.0847       0.0529       0.0854       0.0692       0.0688         0.11       0.0893       0.0775     0.000808\n",
            "     28     5       0.0115       0.0115     1.99e-06       0.0625       0.0829       0.0514       0.0847       0.0681       0.0672        0.108       0.0874       0.0861     0.000897\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              28  109.479    0.005       0.0122     3.39e-05       0.0123       0.0653       0.0856       0.0546       0.0867       0.0707       0.0707         0.11       0.0901        0.351      0.00366\n",
            "! Validation         28  109.479    0.005       0.0119     2.04e-06       0.0119       0.0637       0.0845       0.0524       0.0865       0.0694       0.0684         0.11       0.0891       0.0882     0.000919\n",
            "Wall time: 109.48017858399999\n",
            "! Best model       28    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     29     1       0.0108       0.0107     6.94e-05       0.0611       0.0802       0.0506        0.082       0.0663       0.0654        0.104       0.0845        0.617      0.00643\n",
            "     29     2       0.0138       0.0138     2.94e-06       0.0705        0.091       0.0603       0.0909       0.0756        0.077        0.114       0.0955        0.104      0.00108\n",
            "     29     3       0.0116       0.0116     3.37e-05       0.0631       0.0833       0.0527       0.0838       0.0683       0.0684        0.107       0.0877        0.416      0.00433\n",
            "     29     4      0.00981      0.00969     0.000112       0.0582       0.0762       0.0474       0.0798       0.0636       0.0611       0.0996       0.0804        0.781      0.00814\n",
            "     29     5       0.0123       0.0123     2.89e-05        0.066       0.0857        0.055        0.088       0.0715       0.0685        0.112       0.0905        0.375       0.0039\n",
            "     29     6       0.0118       0.0117     6.08e-05       0.0634       0.0838       0.0528       0.0844       0.0686       0.0683        0.108       0.0883        0.562      0.00585\n",
            "     29     7       0.0094      0.00933     7.27e-05        0.057       0.0747       0.0463       0.0783       0.0623        0.059        0.099        0.079         0.63      0.00657\n",
            "     29     8       0.0103       0.0103     1.02e-06         0.06       0.0784        0.049       0.0821       0.0655       0.0623        0.103       0.0829       0.0539     0.000562\n",
            "     29     9       0.0145       0.0145      1.8e-05       0.0715       0.0932       0.0612       0.0921       0.0766       0.0781        0.118       0.0979        0.293      0.00305\n",
            "     29    10       0.0131       0.0131     1.66e-05       0.0665       0.0886       0.0554       0.0886        0.072       0.0728        0.114       0.0933        0.298       0.0031\n",
            "     29    11       0.0128       0.0128     5.79e-06       0.0657       0.0875       0.0541       0.0889       0.0715       0.0734         0.11       0.0919        0.168      0.00175\n",
            "     29    12       0.0149       0.0149     4.73e-05       0.0729       0.0944       0.0666       0.0855       0.0761       0.0863        0.109       0.0976        0.509       0.0053\n",
            "     29    13       0.0118       0.0118      2.7e-06       0.0663       0.0839       0.0583       0.0821       0.0702       0.0722        0.103       0.0878       0.0842     0.000877\n",
            "     29    14      0.00956      0.00955     9.93e-06       0.0574       0.0756       0.0475       0.0772       0.0623       0.0604       0.0992       0.0798        0.211       0.0022\n",
            "     29    15       0.0166       0.0165     3.66e-05       0.0776       0.0995       0.0669       0.0989       0.0829       0.0849        0.124        0.104        0.443      0.00461\n",
            "     29    16       0.0182       0.0182     7.18e-05       0.0812        0.104       0.0718          0.1       0.0859       0.0911        0.127        0.109        0.626      0.00652\n",
            "     29    17       0.0124       0.0123     5.15e-05       0.0656       0.0859       0.0552       0.0863       0.0707       0.0713         0.11       0.0904        0.527      0.00549\n",
            "     29    18       0.0108       0.0107     0.000114       0.0607       0.0801       0.0491       0.0838       0.0664       0.0634        0.106       0.0847        0.782      0.00815\n",
            "     29    19       0.0129       0.0129     8.88e-07       0.0696        0.088       0.0636       0.0817       0.0726       0.0791        0.104       0.0913       0.0672       0.0007\n",
            "     29    20       0.0139       0.0138     2.22e-05       0.0698        0.091       0.0592        0.091       0.0751       0.0749        0.117       0.0958        0.346      0.00361\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     29     1       0.0125       0.0125     3.08e-06       0.0655       0.0865       0.0538       0.0888       0.0713       0.0704        0.112       0.0912        0.106       0.0011\n",
            "     29     2       0.0121       0.0121     1.37e-06       0.0639       0.0852       0.0518        0.088       0.0699       0.0683        0.111       0.0899       0.0763     0.000794\n",
            "     29     3       0.0108       0.0108     2.07e-06       0.0609       0.0803       0.0499       0.0829       0.0664       0.0643        0.105       0.0847       0.0938     0.000977\n",
            "     29     4       0.0117       0.0117     1.51e-06       0.0631       0.0838       0.0523       0.0847       0.0685       0.0679        0.109       0.0884       0.0806     0.000839\n",
            "     29     5       0.0112       0.0112     1.82e-06       0.0618        0.082       0.0508       0.0838       0.0673       0.0664        0.107       0.0865       0.0798     0.000831\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              29  113.271    0.005       0.0125     3.89e-05       0.0126       0.0662       0.0866       0.0562       0.0863       0.0712       0.0724         0.11        0.091        0.395      0.00411\n",
            "! Validation         29  113.271    0.005       0.0117     1.97e-06       0.0117        0.063       0.0836       0.0517       0.0856       0.0687       0.0675        0.109       0.0882       0.0872     0.000908\n",
            "Wall time: 113.27218560999972\n",
            "! Best model       29    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     30     1        0.013       0.0129     6.93e-05       0.0674       0.0879       0.0552       0.0917       0.0734       0.0711        0.114       0.0927        0.615      0.00641\n",
            "     30     2        0.011        0.011     6.71e-06       0.0626       0.0813       0.0547       0.0784       0.0666       0.0693        0.101       0.0852        0.176      0.00183\n",
            "     30     3       0.0137       0.0137     4.24e-05       0.0699       0.0905       0.0594       0.0909       0.0751       0.0741        0.117       0.0953        0.479      0.00499\n",
            "     30     4        0.013        0.013     1.23e-05       0.0657       0.0881       0.0527       0.0918       0.0722       0.0702        0.116        0.093        0.244      0.00254\n",
            "     30     5      0.00991      0.00991     1.93e-07       0.0584        0.077       0.0474       0.0802       0.0638       0.0611        0.102       0.0814       0.0277     0.000289\n",
            "     30     6       0.0108       0.0108      1.2e-05       0.0618       0.0803       0.0517       0.0818       0.0668       0.0667        0.102       0.0844        0.246      0.00257\n",
            "     30     7      0.00944      0.00944     2.76e-06        0.058       0.0752       0.0489       0.0762       0.0626       0.0614       0.0969       0.0792       0.0908     0.000946\n",
            "     30     8        0.011        0.011     2.37e-06        0.062        0.081       0.0531       0.0799       0.0665       0.0683        0.102        0.085       0.0947     0.000987\n",
            "     30     9       0.0116       0.0116     1.44e-06       0.0634       0.0832       0.0518       0.0864       0.0691        0.067        0.109       0.0878       0.0709     0.000739\n",
            "     30    10       0.0119       0.0118     5.43e-05       0.0626       0.0842       0.0509        0.086       0.0684       0.0671        0.111       0.0889        0.546      0.00569\n",
            "     30    11       0.0123       0.0123     3.73e-05       0.0655       0.0858        0.054       0.0887       0.0713       0.0699        0.111       0.0904         0.45      0.00469\n",
            "     30    12       0.0113       0.0113     4.18e-06        0.062       0.0821       0.0516       0.0827       0.0672       0.0679        0.105       0.0864        0.133      0.00138\n",
            "     30    13       0.0106       0.0106     1.72e-05       0.0607       0.0795        0.049       0.0841       0.0666        0.063        0.105        0.084        0.274      0.00286\n",
            "     30    14       0.0103       0.0102      6.9e-05       0.0601       0.0781       0.0516        0.077       0.0643       0.0666       0.0972       0.0819        0.616      0.00641\n",
            "     30    15       0.0111       0.0111     5.79e-06       0.0624       0.0816       0.0521        0.083       0.0675       0.0672        0.105       0.0859        0.154       0.0016\n",
            "     30    16       0.0121       0.0121     8.09e-07       0.0647        0.085       0.0559       0.0821        0.069       0.0718        0.107       0.0892       0.0621     0.000647\n",
            "     30    17       0.0163       0.0163     1.32e-06       0.0786       0.0988       0.0691       0.0976       0.0833       0.0851        0.122        0.103       0.0719     0.000749\n",
            "     30    18       0.0148       0.0148     5.95e-05       0.0723        0.094       0.0603       0.0963       0.0783       0.0758        0.123       0.0992        0.571      0.00595\n",
            "     30    19       0.0105       0.0105     1.05e-05       0.0604       0.0793       0.0493       0.0827        0.066       0.0637        0.104       0.0837        0.225      0.00234\n",
            "     30    20       0.0138       0.0138     6.67e-06       0.0706       0.0907       0.0592       0.0934       0.0763       0.0744        0.117       0.0955        0.149      0.00155\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     30     1       0.0122       0.0122        3e-06       0.0648       0.0856       0.0531        0.088       0.0706       0.0695        0.111       0.0902        0.105      0.00109\n",
            "     30     2       0.0119       0.0119     1.29e-06       0.0633       0.0843       0.0513       0.0872       0.0693       0.0675        0.111        0.089       0.0729     0.000759\n",
            "     30     3       0.0106       0.0106     1.98e-06       0.0603       0.0796       0.0493       0.0822       0.0658       0.0636        0.104        0.084       0.0915     0.000953\n",
            "     30     4       0.0115       0.0115     1.38e-06       0.0624       0.0831       0.0516       0.0841       0.0679       0.0671        0.108       0.0876       0.0768       0.0008\n",
            "     30     5        0.011        0.011     1.88e-06       0.0611       0.0812       0.0503       0.0828       0.0666       0.0658        0.105       0.0856       0.0812     0.000845\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              30  117.065    0.005       0.0119     2.08e-05       0.0119       0.0645       0.0844       0.0539       0.0855       0.0697       0.0693        0.108       0.0889        0.265      0.00276\n",
            "! Validation         30  117.065    0.005       0.0114      1.9e-06       0.0114       0.0624       0.0828       0.0511       0.0849        0.068       0.0667        0.108       0.0873       0.0854      0.00089\n",
            "Wall time: 117.06559690899985\n",
            "! Best model       30    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     31     1       0.0144       0.0144     4.31e-06       0.0718       0.0928       0.0632       0.0889       0.0761       0.0802        0.114        0.097         0.15      0.00156\n",
            "     31     2       0.0123       0.0123     2.33e-05       0.0665       0.0857       0.0575       0.0845        0.071       0.0741        0.105       0.0897        0.349      0.00363\n",
            "     31     3       0.0101       0.0101     3.34e-05       0.0587       0.0777       0.0484       0.0793       0.0639       0.0622        0.102        0.082        0.427      0.00445\n",
            "     31     4       0.0112       0.0112     1.09e-05       0.0632        0.082       0.0551       0.0793       0.0672       0.0704        0.101       0.0858         0.24       0.0025\n",
            "     31     5       0.0107       0.0107     9.17e-06       0.0614       0.0799         0.05       0.0841        0.067        0.064        0.105       0.0844        0.187      0.00195\n",
            "     31     6      0.00969      0.00968     3.34e-06       0.0592       0.0761       0.0503       0.0772       0.0637       0.0631       0.0971       0.0801        0.106      0.00111\n",
            "     31     7       0.0121       0.0121     5.59e-05       0.0648        0.085        0.054       0.0864       0.0702       0.0701        0.109       0.0895        0.553      0.00576\n",
            "     31     8       0.0106       0.0106     4.11e-05       0.0607       0.0795       0.0486        0.085       0.0668       0.0623        0.106       0.0841        0.461       0.0048\n",
            "     31     9       0.0106       0.0106     3.68e-06       0.0606       0.0797       0.0497       0.0824        0.066       0.0639        0.104       0.0841        0.138      0.00143\n",
            "     31    10       0.0121       0.0121     2.46e-06       0.0656       0.0852       0.0559        0.085       0.0705       0.0703        0.109       0.0897        0.102      0.00106\n",
            "     31    11         0.01         0.01     3.91e-05       0.0591       0.0774       0.0491        0.079        0.064       0.0626        0.101       0.0816        0.458      0.00477\n",
            "     31    12       0.0119       0.0119     3.55e-05       0.0637       0.0844       0.0527       0.0859       0.0693       0.0694        0.108       0.0888        0.439      0.00457\n",
            "     31    13       0.0108       0.0108     8.57e-07        0.061       0.0803       0.0508       0.0813        0.066       0.0657        0.103       0.0846       0.0621     0.000647\n",
            "     31    14       0.0101       0.0101     9.71e-06       0.0575       0.0778       0.0448        0.083       0.0639       0.0601        0.105       0.0823        0.206      0.00215\n",
            "     31    15       0.0099      0.00988     1.79e-05       0.0581       0.0769       0.0466       0.0813       0.0639       0.0606        0.102       0.0813        0.307       0.0032\n",
            "     31    16      0.00975      0.00975     2.73e-06       0.0575       0.0764       0.0475       0.0774       0.0625       0.0614       0.0998       0.0806       0.0986      0.00103\n",
            "     31    17       0.0107       0.0107     2.15e-05       0.0603         0.08        0.049       0.0829        0.066       0.0644        0.105       0.0845        0.336       0.0035\n",
            "     31    18      0.00931      0.00931     4.62e-07       0.0573       0.0746       0.0475       0.0767       0.0621       0.0605        0.097       0.0787       0.0369     0.000385\n",
            "     31    19        0.011       0.0109     2.37e-05       0.0612       0.0809       0.0499       0.0839       0.0669       0.0646        0.106       0.0854        0.343      0.00358\n",
            "     31    20         0.01         0.01     1.46e-06        0.059       0.0775       0.0479        0.081       0.0645       0.0614        0.102       0.0819       0.0641     0.000667\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     31     1        0.012        0.012     2.87e-06       0.0641       0.0847       0.0525       0.0873       0.0699       0.0686         0.11       0.0893          0.1      0.00104\n",
            "     31     2       0.0116       0.0116     1.29e-06       0.0626       0.0835       0.0506       0.0865       0.0686       0.0666         0.11       0.0881       0.0717     0.000747\n",
            "     31     3       0.0104       0.0104      1.9e-06       0.0597       0.0789       0.0488       0.0816       0.0652       0.0629        0.104       0.0833        0.088     0.000917\n",
            "     31     4       0.0113       0.0113     1.24e-06       0.0618       0.0823        0.051       0.0835       0.0672       0.0662        0.107       0.0868       0.0744     0.000775\n",
            "     31     5       0.0108       0.0108     1.92e-06       0.0605       0.0804       0.0497        0.082       0.0659       0.0651        0.104       0.0847       0.0801     0.000834\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              31  120.880    0.005       0.0109      1.7e-05       0.0109       0.0614       0.0806       0.0509       0.0822       0.0666       0.0658        0.104       0.0849        0.253      0.00264\n",
            "! Validation         31  120.880    0.005       0.0112     1.84e-06       0.0112       0.0617       0.0819       0.0505       0.0842       0.0674       0.0659        0.107       0.0865       0.0829     0.000863\n",
            "Wall time: 120.88097278899977\n",
            "! Best model       31    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     32     1       0.0108       0.0108     1.22e-05       0.0609       0.0803       0.0504       0.0818       0.0661        0.067        0.102       0.0844        0.248      0.00258\n",
            "     32     2      0.00861       0.0086     1.94e-05       0.0556       0.0717       0.0443       0.0783       0.0613       0.0558       0.0959       0.0759        0.321      0.00334\n",
            "     32     3        0.013        0.013      1.3e-06       0.0677       0.0881       0.0581        0.087       0.0725       0.0745         0.11       0.0925       0.0709     0.000739\n",
            "     32     4       0.0112       0.0111     5.63e-05       0.0641       0.0816       0.0567        0.079       0.0679        0.071       0.0994       0.0852        0.545      0.00568\n",
            "     32     5      0.00981      0.00979     2.34e-05       0.0581       0.0765       0.0464       0.0814       0.0639       0.0592        0.103        0.081        0.358      0.00373\n",
            "     32     6       0.0105       0.0105     5.06e-06        0.061       0.0792       0.0513       0.0805       0.0659       0.0651        0.102       0.0835        0.143      0.00149\n",
            "     32     7       0.0128       0.0127      8.3e-05       0.0653       0.0872       0.0534       0.0893       0.0713       0.0707        0.113        0.092        0.672        0.007\n",
            "     32     8        0.011       0.0109     0.000126       0.0619       0.0807       0.0513       0.0831       0.0672       0.0648        0.106       0.0852        0.833      0.00868\n",
            "     32     9       0.0112       0.0111     2.62e-05       0.0625       0.0816       0.0526       0.0824       0.0675       0.0675        0.104       0.0859        0.379      0.00395\n",
            "     32    10       0.0119       0.0117     0.000174        0.064       0.0836       0.0528       0.0864       0.0696       0.0674        0.109       0.0882        0.976       0.0102\n",
            "     32    11       0.0122       0.0122     4.62e-05       0.0655       0.0854        0.056       0.0847       0.0703       0.0735        0.105       0.0893        0.495      0.00516\n",
            "     32    12       0.0098       0.0098     3.01e-06       0.0586       0.0766       0.0475       0.0809       0.0642       0.0608        0.101       0.0809        0.112      0.00117\n",
            "     32    13      0.00984      0.00976      7.7e-05       0.0583       0.0764       0.0506       0.0737       0.0622       0.0656       0.0945         0.08        0.646      0.00673\n",
            "     32    14       0.0103       0.0103     3.87e-05       0.0605       0.0783       0.0509       0.0796       0.0652       0.0639        0.101       0.0825        0.449      0.00468\n",
            "     32    15       0.0107       0.0107     1.49e-05         0.06         0.08       0.0493       0.0813       0.0653       0.0648        0.104       0.0844        0.284      0.00296\n",
            "     32    16      0.00999      0.00994     5.14e-05       0.0586       0.0771       0.0489       0.0781       0.0635       0.0618        0.101       0.0814        0.521      0.00543\n",
            "     32    17       0.0114       0.0114     8.27e-06       0.0619       0.0825       0.0498       0.0859       0.0679       0.0641        0.111       0.0873        0.208      0.00217\n",
            "     32    18        0.013        0.013     2.74e-05       0.0681       0.0881         0.06       0.0842       0.0721       0.0764        0.108       0.0921        0.385      0.00401\n",
            "     32    19       0.0106       0.0105     3.91e-05       0.0614       0.0794       0.0527       0.0788       0.0658       0.0679       0.0983       0.0831         0.45      0.00469\n",
            "     32    20       0.0118       0.0118     9.74e-06       0.0627       0.0839       0.0508       0.0865       0.0686       0.0673         0.11       0.0886         0.22      0.00229\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     32     1       0.0117       0.0117      2.9e-06       0.0634       0.0838       0.0518       0.0865       0.0692       0.0678        0.109       0.0884        0.103      0.00107\n",
            "     32     2       0.0114       0.0114      1.2e-06        0.062       0.0826       0.0501       0.0858       0.0679       0.0658        0.109       0.0873       0.0697     0.000726\n",
            "     32     3       0.0102       0.0102     1.92e-06       0.0592       0.0782       0.0483        0.081       0.0646       0.0622        0.103       0.0826       0.0879     0.000916\n",
            "     32     4       0.0111       0.0111      1.3e-06       0.0612       0.0815       0.0504       0.0829       0.0666       0.0655        0.107       0.0861       0.0762     0.000793\n",
            "     32     5       0.0106       0.0106     1.83e-06       0.0599       0.0796       0.0492       0.0812       0.0652       0.0645        0.103       0.0839       0.0771     0.000804\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              32  124.666    0.005        0.011     4.22e-05        0.011       0.0618        0.081       0.0517       0.0821       0.0669       0.0666        0.104       0.0853        0.416      0.00433\n",
            "! Validation         32  124.666    0.005        0.011     1.83e-06        0.011       0.0611       0.0812         0.05       0.0835       0.0667       0.0652        0.106       0.0857       0.0828     0.000862\n",
            "Wall time: 124.66642670800002\n",
            "! Best model       32    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     33     1       0.0105       0.0103     0.000176       0.0614       0.0785       0.0532       0.0778       0.0655       0.0671       0.0974       0.0823        0.983       0.0102\n",
            "     33     2       0.0132       0.0132     2.71e-06       0.0667        0.089       0.0519       0.0964       0.0742       0.0661        0.123       0.0943        0.111      0.00116\n",
            "     33     3       0.0109       0.0109     2.79e-07       0.0624       0.0806       0.0518       0.0835       0.0676       0.0641        0.106       0.0852       0.0348     0.000362\n",
            "     33     4      0.00956      0.00949     6.52e-05       0.0577       0.0754       0.0477       0.0776       0.0626       0.0604       0.0988       0.0796        0.596      0.00621\n",
            "     33     5       0.0134       0.0134     1.83e-05       0.0672       0.0896       0.0534        0.095       0.0742       0.0695         0.12       0.0948        0.312      0.00325\n",
            "     33     6        0.017       0.0169     5.63e-05       0.0795        0.101       0.0726       0.0932       0.0829       0.0907        0.118        0.104        0.555      0.00578\n",
            "     33     7       0.0147       0.0147     8.56e-06       0.0739       0.0939       0.0675       0.0868       0.0771       0.0845         0.11       0.0974        0.209      0.00218\n",
            "     33     8         0.01         0.01     4.86e-06       0.0591       0.0774       0.0485       0.0804       0.0644       0.0617        0.102       0.0818         0.15      0.00156\n",
            "     33     9       0.0127       0.0127     3.52e-06       0.0676       0.0873       0.0596       0.0835       0.0715       0.0756        0.107       0.0913        0.138      0.00143\n",
            "     33    10       0.0144       0.0144     1.08e-05       0.0709       0.0928       0.0592       0.0943       0.0768       0.0761        0.119       0.0977        0.234      0.00244\n",
            "     33    11       0.0113       0.0112      5.8e-05       0.0613       0.0819       0.0498       0.0842        0.067       0.0642        0.109       0.0866        0.564      0.00588\n",
            "     33    12        0.011        0.011     2.42e-05       0.0617       0.0811       0.0486       0.0878       0.0682       0.0628        0.109       0.0858        0.359      0.00374\n",
            "     33    13       0.0132       0.0131     5.65e-05       0.0682       0.0886       0.0574       0.0899       0.0737       0.0739        0.112       0.0931        0.556      0.00579\n",
            "     33    14       0.0123       0.0123     1.24e-05       0.0667       0.0857        0.059       0.0821       0.0706       0.0741        0.105       0.0896        0.237      0.00247\n",
            "     33    15      0.00947      0.00947     3.04e-06       0.0574       0.0753       0.0478       0.0767       0.0623       0.0614       0.0973       0.0793        0.112      0.00116\n",
            "     33    16      0.00965      0.00964     9.29e-06       0.0588        0.076       0.0498       0.0767       0.0633       0.0638       0.0958       0.0798        0.219      0.00228\n",
            "     33    17       0.0111        0.011     2.37e-05       0.0621       0.0813       0.0502       0.0858        0.068       0.0662        0.105       0.0857        0.337      0.00351\n",
            "     33    18       0.0113       0.0113     4.24e-06        0.062       0.0821       0.0507       0.0848       0.0677       0.0668        0.106       0.0866        0.131      0.00137\n",
            "     33    19       0.0113       0.0113     5.85e-06       0.0631       0.0823       0.0525       0.0843       0.0684       0.0681        0.105       0.0866        0.155      0.00161\n",
            "     33    20      0.00992      0.00992      5.5e-07       0.0575        0.077       0.0477       0.0771       0.0624       0.0633        0.099       0.0811       0.0404     0.000421\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     33     1       0.0115       0.0115     2.83e-06       0.0628        0.083       0.0512       0.0859       0.0685        0.067        0.108       0.0875          0.1      0.00105\n",
            "     33     2       0.0112       0.0112     1.21e-06       0.0614       0.0819       0.0496       0.0851       0.0673       0.0652        0.108       0.0865       0.0682      0.00071\n",
            "     33     3         0.01         0.01     1.82e-06       0.0587       0.0775       0.0478       0.0804       0.0641       0.0616        0.102       0.0819       0.0854     0.000889\n",
            "     33     4       0.0109       0.0109     1.23e-06       0.0607       0.0809       0.0498       0.0824       0.0661       0.0647        0.106       0.0854       0.0749      0.00078\n",
            "     33     5       0.0104       0.0104     1.78e-06       0.0593       0.0789       0.0488       0.0805       0.0646       0.0639        0.103       0.0832       0.0757     0.000788\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              33  128.476    0.005       0.0118     2.72e-05       0.0118       0.0643       0.0841       0.0539       0.0849       0.0694       0.0695        0.108       0.0885        0.302      0.00314\n",
            "! Validation         33  128.476    0.005       0.0108     1.77e-06       0.0108       0.0606       0.0805       0.0494       0.0828       0.0661       0.0645        0.105       0.0849       0.0809     0.000843\n",
            "Wall time: 128.476427565\n",
            "! Best model       33    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     34     1       0.0102       0.0102     1.58e-06       0.0596       0.0782       0.0497       0.0794       0.0646        0.063        0.102       0.0826       0.0684     0.000712\n",
            "     34     2       0.0124       0.0124     4.28e-06       0.0659       0.0861       0.0546       0.0883       0.0715       0.0697        0.112       0.0908        0.144       0.0015\n",
            "     34     3      0.00927      0.00927      1.2e-06       0.0583       0.0745       0.0498       0.0752       0.0625       0.0622       0.0944       0.0783       0.0555     0.000578\n",
            "     34     4       0.0113       0.0113     6.51e-06       0.0618       0.0823       0.0502        0.085       0.0676       0.0668        0.107       0.0868         0.17      0.00177\n",
            "     34     5      0.00994      0.00994     2.13e-06       0.0578       0.0771       0.0484       0.0767       0.0626       0.0644       0.0977       0.0811       0.0918     0.000956\n",
            "     34     6       0.0102       0.0102     3.36e-05       0.0601       0.0781       0.0511       0.0781       0.0646       0.0654       0.0989       0.0821        0.429      0.00447\n",
            "     34     7      0.00904      0.00904     4.75e-06       0.0555       0.0735       0.0443       0.0778       0.0611       0.0575       0.0981       0.0778        0.148      0.00154\n",
            "     34     8      0.00976      0.00975     6.91e-06       0.0583       0.0764       0.0478       0.0793       0.0636       0.0619       0.0993       0.0806        0.183       0.0019\n",
            "     34     9      0.00992      0.00991      1.3e-06       0.0587        0.077       0.0475       0.0811       0.0643       0.0607        0.102       0.0814       0.0811     0.000844\n",
            "     34    10      0.00897      0.00896     7.89e-06        0.056       0.0732       0.0463       0.0755       0.0609       0.0593       0.0951       0.0772        0.188      0.00195\n",
            "     34    11       0.0105       0.0105     6.13e-06       0.0606       0.0792       0.0511       0.0795       0.0653       0.0644        0.103       0.0835        0.165      0.00172\n",
            "     34    12       0.0102       0.0102     3.06e-06       0.0592        0.078       0.0471       0.0833       0.0652       0.0615        0.103       0.0824       0.0973      0.00101\n",
            "     34    13       0.0106       0.0106     4.28e-07       0.0598       0.0795       0.0484       0.0825       0.0654       0.0635        0.104        0.084        0.043     0.000448\n",
            "     34    14       0.0106       0.0106     1.04e-06        0.059       0.0798       0.0469       0.0832        0.065       0.0616        0.107       0.0844       0.0672       0.0007\n",
            "     34    15      0.00892      0.00892     1.27e-06       0.0566       0.0731       0.0466       0.0765       0.0615       0.0593       0.0948        0.077        0.066     0.000688\n",
            "     34    16      0.00966      0.00966     4.08e-06       0.0571        0.076       0.0451       0.0812       0.0631        0.058        0.103       0.0805        0.138      0.00144\n",
            "     34    17       0.0104       0.0104     2.39e-05       0.0591        0.079       0.0486         0.08       0.0643       0.0618        0.105       0.0835         0.36      0.00375\n",
            "     34    18       0.0121       0.0121     1.19e-06       0.0653        0.085       0.0562       0.0836       0.0699       0.0715        0.107       0.0893       0.0508     0.000529\n",
            "     34    19        0.012        0.012     3.33e-06       0.0646       0.0847       0.0546       0.0845       0.0696       0.0702        0.108       0.0891        0.114      0.00119\n",
            "     34    20      0.00991      0.00986      4.6e-05       0.0587       0.0768       0.0479       0.0802       0.0641       0.0612        0.101       0.0811        0.502      0.00523\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     34     1       0.0113       0.0113     2.73e-06       0.0622       0.0822       0.0507       0.0852       0.0679       0.0662        0.107       0.0867       0.0989      0.00103\n",
            "     34     2        0.011        0.011     1.19e-06       0.0608       0.0812       0.0491       0.0844       0.0667       0.0644        0.107       0.0857       0.0663     0.000691\n",
            "     34     3      0.00989      0.00989     1.79e-06       0.0582       0.0769       0.0473         0.08       0.0636        0.061        0.102       0.0813       0.0835      0.00087\n",
            "     34     4       0.0107       0.0107     1.14e-06       0.0601       0.0802       0.0493       0.0818       0.0656        0.064        0.105       0.0847        0.072      0.00075\n",
            "     34     5       0.0102       0.0102     1.84e-06       0.0588       0.0782       0.0483       0.0797        0.064       0.0633        0.102       0.0824       0.0758     0.000789\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              34  132.273    0.005       0.0103     8.03e-06       0.0103       0.0596       0.0785       0.0491       0.0805       0.0648       0.0633        0.102       0.0828        0.158      0.00165\n",
            "! Validation         34  132.273    0.005       0.0106     1.74e-06       0.0106         0.06       0.0798       0.0489       0.0822       0.0656       0.0638        0.105       0.0842       0.0793     0.000826\n",
            "Wall time: 132.27400382099995\n",
            "! Best model       34    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     35     1      0.00981      0.00974     6.52e-05       0.0575       0.0764       0.0462       0.0803       0.0632       0.0604        0.101       0.0807        0.595       0.0062\n",
            "     35     2      0.00997      0.00996     1.98e-06       0.0591       0.0772       0.0509       0.0756       0.0632        0.065       0.0972       0.0811       0.0822     0.000857\n",
            "     35     3       0.0103       0.0103     2.45e-05        0.059       0.0786       0.0477       0.0816       0.0646       0.0632        0.103       0.0829        0.364      0.00379\n",
            "     35     4      0.00981      0.00975     5.25e-05       0.0578       0.0764       0.0477        0.078       0.0628       0.0619       0.0992       0.0806        0.532      0.00555\n",
            "     35     5       0.0115       0.0115     8.74e-06       0.0628       0.0829       0.0512        0.086       0.0686       0.0664        0.109       0.0875        0.196      0.00204\n",
            "     35     6        0.011       0.0109     7.93e-05       0.0613       0.0807       0.0504        0.083       0.0667       0.0648        0.106       0.0852        0.659      0.00687\n",
            "     35     7        0.011       0.0109     1.37e-06       0.0609        0.081       0.0502       0.0823       0.0663       0.0651        0.106       0.0854       0.0762     0.000793\n",
            "     35     8       0.0101       0.0101     1.52e-05       0.0583       0.0776       0.0471       0.0807       0.0639       0.0617        0.102        0.082        0.281      0.00293\n",
            "     35     9       0.0103       0.0103     1.29e-06       0.0609       0.0786       0.0513       0.0801       0.0657       0.0653          0.1       0.0826       0.0816      0.00085\n",
            "     35    10       0.0132       0.0132     2.16e-06       0.0683        0.089       0.0577       0.0894       0.0736       0.0744        0.112       0.0935        0.099      0.00103\n",
            "     35    11       0.0132       0.0132     2.58e-05       0.0684       0.0888       0.0552       0.0949        0.075       0.0696        0.118       0.0939        0.362      0.00377\n",
            "     35    12       0.0129       0.0129     1.38e-05       0.0674        0.088       0.0563       0.0895       0.0729       0.0716        0.114       0.0927        0.255      0.00266\n",
            "     35    13       0.0108       0.0108     1.08e-05       0.0607       0.0803       0.0497       0.0827       0.0662       0.0645        0.105       0.0847        0.237      0.00247\n",
            "     35    14       0.0082      0.00819      6.3e-06       0.0543         0.07       0.0444       0.0741       0.0593       0.0564       0.0914       0.0739        0.182      0.00189\n",
            "     35    15      0.00997      0.00997     2.04e-07        0.058       0.0773       0.0476       0.0787       0.0632       0.0613        0.102       0.0816       0.0299     0.000311\n",
            "     35    16       0.0107       0.0106     6.89e-05        0.062       0.0797       0.0523       0.0813       0.0668       0.0659        0.102       0.0839         0.61      0.00635\n",
            "     35    17       0.0124       0.0124     1.78e-06       0.0664       0.0863       0.0586       0.0821       0.0704       0.0748        0.106       0.0902       0.0826     0.000861\n",
            "     35    18      0.00933      0.00931     2.15e-05       0.0577       0.0746       0.0485       0.0761       0.0623       0.0615       0.0957       0.0786        0.341      0.00355\n",
            "     35    19      0.00814      0.00808     5.69e-05       0.0535       0.0696       0.0449       0.0707       0.0578       0.0569       0.0897       0.0733        0.552      0.00575\n",
            "     35    20       0.0108       0.0108     1.08e-05       0.0614       0.0802       0.0521       0.0802       0.0661       0.0666        0.102       0.0844        0.221       0.0023\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     35     1       0.0111       0.0111     2.77e-06       0.0616       0.0814       0.0502       0.0846       0.0674       0.0655        0.106       0.0859        0.101      0.00105\n",
            "     35     2       0.0108       0.0108     1.11e-06       0.0603       0.0805       0.0486       0.0837       0.0662       0.0637        0.106        0.085        0.065     0.000677\n",
            "     35     3      0.00975      0.00975      1.7e-06       0.0577       0.0764       0.0469       0.0795       0.0632       0.0604        0.101       0.0807       0.0819     0.000853\n",
            "     35     4       0.0106       0.0106     1.21e-06       0.0596       0.0796       0.0487       0.0813        0.065       0.0633        0.105        0.084       0.0737     0.000768\n",
            "     35     5         0.01         0.01     1.78e-06       0.0583       0.0775       0.0479       0.0791       0.0635       0.0628        0.101       0.0818       0.0737     0.000768\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              35  136.076    0.005       0.0106     2.35e-05       0.0107       0.0608       0.0798       0.0505       0.0814       0.0659        0.065        0.103       0.0841        0.292      0.00304\n",
            "! Validation         35  136.076    0.005       0.0105     1.71e-06       0.0105       0.0595       0.0791       0.0485       0.0816       0.0651       0.0632        0.104       0.0835       0.0791     0.000824\n",
            "Wall time: 136.07652323000002\n",
            "! Best model       35    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     36     1       0.0112       0.0111     0.000106       0.0634       0.0815       0.0531       0.0841       0.0686       0.0689        0.102       0.0856        0.763      0.00795\n",
            "     36     2      0.00975      0.00969      6.1e-05       0.0589       0.0761       0.0502       0.0763       0.0633       0.0641       0.0958       0.0799        0.578      0.00602\n",
            "     36     3      0.00913      0.00913     2.31e-06       0.0564       0.0739        0.047       0.0752       0.0611       0.0599        0.096        0.078        0.104      0.00109\n",
            "     36     4       0.0125       0.0124     2.08e-05       0.0662       0.0863       0.0585       0.0817       0.0701       0.0756        0.104         0.09        0.328      0.00342\n",
            "     36     5       0.0112       0.0112     2.09e-05       0.0627       0.0819       0.0544       0.0795       0.0669         0.07        0.102       0.0858        0.329      0.00343\n",
            "     36     6      0.00946      0.00946     3.03e-06       0.0571       0.0752       0.0457       0.0799       0.0628       0.0591          0.1       0.0796       0.0861     0.000897\n",
            "     36     7       0.0109       0.0109     3.63e-06       0.0621       0.0806       0.0524       0.0815        0.067       0.0681        0.101       0.0846        0.115       0.0012\n",
            "     36     8       0.0106       0.0106     3.18e-05       0.0612       0.0797       0.0501       0.0835       0.0668       0.0641        0.104       0.0841         0.41      0.00427\n",
            "     36     9      0.00897      0.00897     4.14e-06       0.0554       0.0733       0.0426       0.0808       0.0617       0.0537        0.102       0.0777        0.138      0.00144\n",
            "     36    10      0.00885      0.00885     1.02e-06       0.0557       0.0728       0.0455       0.0761       0.0608       0.0572       0.0967       0.0769       0.0689     0.000718\n",
            "     36    11      0.00848      0.00848     6.91e-07        0.054       0.0712       0.0439       0.0742       0.0591       0.0556       0.0951       0.0753       0.0455     0.000474\n",
            "     36    12      0.00966      0.00965     7.21e-06       0.0575        0.076       0.0476       0.0773       0.0625       0.0623       0.0979       0.0801        0.193      0.00201\n",
            "     36    13      0.00954      0.00954     1.14e-06       0.0576       0.0756       0.0468       0.0792        0.063       0.0588        0.101       0.0799        0.059     0.000614\n",
            "     36    14      0.00942      0.00942     3.16e-06       0.0562       0.0751       0.0448       0.0791       0.0619       0.0575        0.102       0.0795        0.103      0.00107\n",
            "     36    15      0.00891      0.00889     1.29e-05       0.0558        0.073        0.046       0.0755       0.0607       0.0573        0.097       0.0771        0.252      0.00263\n",
            "     36    16       0.0117       0.0117     2.52e-06       0.0627       0.0837        0.051       0.0859       0.0685       0.0677        0.109       0.0882        0.103      0.00107\n",
            "     36    17       0.0118       0.0118     3.29e-05       0.0638        0.084       0.0533       0.0847        0.069       0.0692        0.108       0.0884        0.423      0.00441\n",
            "     36    18       0.0103       0.0103     2.05e-05       0.0596       0.0785       0.0502       0.0784       0.0643       0.0643        0.101       0.0827        0.317      0.00331\n",
            "     36    19       0.0089      0.00889      1.4e-05       0.0555       0.0729       0.0459       0.0749       0.0604       0.0586       0.0953        0.077        0.271      0.00282\n",
            "     36    20       0.0105       0.0105     2.72e-06       0.0607       0.0792       0.0522       0.0778        0.065        0.067       0.0992       0.0831        0.101      0.00105\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     36     1       0.0109       0.0109     2.77e-06       0.0611       0.0807       0.0497        0.084       0.0668       0.0649        0.106       0.0852        0.102      0.00107\n",
            "     36     2       0.0106       0.0106     1.02e-06       0.0598       0.0798       0.0481       0.0832       0.0656       0.0631        0.106       0.0843       0.0615     0.000641\n",
            "     36     3      0.00961      0.00961     1.72e-06       0.0573       0.0758       0.0464        0.079       0.0627       0.0598          0.1       0.0801       0.0824     0.000859\n",
            "     36     4       0.0104       0.0104     1.22e-06       0.0591       0.0789       0.0483       0.0808       0.0645       0.0627        0.104       0.0834       0.0725     0.000755\n",
            "     36     5      0.00988      0.00988     1.72e-06       0.0579       0.0769       0.0475       0.0786        0.063       0.0622       0.0999       0.0811       0.0701      0.00073\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              36  139.889    0.005       0.0101     1.76e-05       0.0101       0.0591       0.0776       0.0491       0.0793       0.0642       0.0632          0.1       0.0818        0.239      0.00249\n",
            "! Validation         36  139.889    0.005       0.0103     1.69e-06       0.0103        0.059       0.0785        0.048       0.0811       0.0645       0.0626        0.103       0.0829       0.0778      0.00081\n",
            "Wall time: 139.88975545899984\n",
            "! Best model       36    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     37     1       0.0112       0.0112     3.84e-06       0.0632        0.082       0.0525       0.0847       0.0686       0.0665        0.106       0.0864        0.139      0.00145\n",
            "     37     2       0.0107       0.0107     2.07e-05       0.0609       0.0801       0.0479       0.0871       0.0675       0.0618        0.108       0.0847        0.316       0.0033\n",
            "     37     3       0.0102       0.0102     9.14e-06       0.0583       0.0783       0.0467       0.0816       0.0641       0.0612        0.104       0.0828        0.219      0.00228\n",
            "     37     4       0.0105       0.0105      5.2e-06       0.0589       0.0794       0.0453        0.086       0.0656       0.0604        0.108       0.0841        0.164      0.00171\n",
            "     37     5       0.0097      0.00965     4.86e-05       0.0571        0.076       0.0464       0.0785       0.0625       0.0606       0.0999       0.0802        0.513      0.00534\n",
            "     37     6       0.0097      0.00968     2.32e-05       0.0574       0.0761       0.0471       0.0781       0.0626       0.0594        0.102       0.0805        0.354      0.00369\n",
            "     37     7       0.0101       0.0101     4.77e-05       0.0585       0.0776       0.0475       0.0806        0.064       0.0604        0.104       0.0821        0.508      0.00529\n",
            "     37     8      0.00961      0.00951     0.000104       0.0567       0.0754       0.0464       0.0774       0.0619       0.0611        0.098       0.0796         0.75      0.00781\n",
            "     37     9       0.0101         0.01     2.88e-05       0.0591       0.0775       0.0501        0.077       0.0636       0.0636       0.0997       0.0817        0.397      0.00414\n",
            "     37    10         0.01         0.01     2.44e-05        0.059       0.0774       0.0482       0.0807       0.0645        0.062        0.101       0.0817        0.361      0.00376\n",
            "     37    11       0.0091      0.00895     0.000151       0.0561       0.0732       0.0457       0.0769       0.0613       0.0587       0.0958       0.0772        0.911      0.00949\n",
            "     37    12       0.0101       0.0101     8.36e-06       0.0597       0.0776       0.0484       0.0825       0.0654       0.0626        0.101       0.0819        0.206      0.00215\n",
            "     37    13      0.00878      0.00877     7.82e-06       0.0564       0.0724       0.0477       0.0738       0.0608       0.0593       0.0934       0.0763        0.173      0.00181\n",
            "     37    14      0.00871      0.00869     1.83e-05        0.055       0.0721       0.0438       0.0773       0.0605       0.0558       0.0969       0.0763        0.304      0.00317\n",
            "     37    15       0.0086      0.00858     1.01e-05       0.0548       0.0717       0.0447        0.075       0.0599       0.0575       0.0939       0.0757        0.221      0.00231\n",
            "     37    16       0.0107       0.0107     8.98e-06       0.0605         0.08       0.0483        0.085       0.0666        0.063        0.106       0.0846        0.202      0.00211\n",
            "     37    17      0.00855      0.00855     6.17e-06       0.0542       0.0715       0.0455       0.0718       0.0586       0.0591       0.0915       0.0753        0.161      0.00167\n",
            "     37    18      0.00855      0.00854     1.35e-05       0.0546       0.0715       0.0442       0.0753       0.0598       0.0563       0.0949       0.0756         0.26       0.0027\n",
            "     37    19      0.00932      0.00932     2.84e-06       0.0569       0.0747       0.0463       0.0781       0.0622       0.0594       0.0983       0.0789        0.104      0.00109\n",
            "     37    20      0.00936      0.00933     3.69e-05       0.0567       0.0747       0.0458       0.0787       0.0622       0.0588       0.0992        0.079        0.448      0.00467\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     37     1       0.0107       0.0107     2.84e-06       0.0606       0.0801       0.0493       0.0834       0.0663       0.0642        0.105       0.0845        0.105      0.00109\n",
            "     37     2       0.0105       0.0105     9.24e-07       0.0592       0.0792       0.0476       0.0825       0.0651       0.0624        0.105       0.0837       0.0587     0.000611\n",
            "     37     3      0.00948      0.00948     1.72e-06       0.0569       0.0753        0.046       0.0785       0.0623       0.0593       0.0999       0.0796       0.0823     0.000858\n",
            "     37     4       0.0103       0.0103     1.29e-06       0.0587       0.0784       0.0478       0.0804       0.0641        0.062        0.104       0.0828       0.0751     0.000782\n",
            "     37     5      0.00972      0.00972      1.7e-06       0.0574       0.0763       0.0471        0.078       0.0626       0.0617       0.0992       0.0804       0.0691      0.00072\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              37  143.695    0.005      0.00966      2.9e-05      0.00969       0.0577        0.076       0.0469       0.0793       0.0631       0.0604          0.1       0.0803        0.336       0.0035\n",
            "! Validation         37  143.695    0.005       0.0101     1.69e-06       0.0101       0.0586       0.0779       0.0476       0.0806       0.0641        0.062        0.103       0.0822       0.0781     0.000813\n",
            "Wall time: 143.69548737399964\n",
            "! Best model       37    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     38     1       0.0086      0.00856     3.51e-05       0.0548       0.0716       0.0444       0.0756         0.06       0.0565       0.0948       0.0756        0.436      0.00454\n",
            "     38     2       0.0102       0.0102     5.37e-06       0.0586       0.0782       0.0459       0.0842        0.065       0.0597        0.106       0.0828        0.149      0.00155\n",
            "     38     3      0.00959      0.00955     3.47e-05       0.0579       0.0756       0.0475       0.0786       0.0631       0.0602       0.0995       0.0798        0.432       0.0045\n",
            "     38     4       0.0101         0.01     2.88e-05       0.0586       0.0775       0.0487       0.0784       0.0636       0.0638       0.0994       0.0816        0.392      0.00408\n",
            "     38     5      0.00873      0.00873      1.4e-06       0.0556       0.0723        0.046       0.0748       0.0604        0.059       0.0934       0.0762       0.0834     0.000869\n",
            "     38     6       0.0103       0.0103     1.17e-06       0.0605       0.0786       0.0519       0.0776       0.0648       0.0664       0.0985       0.0825       0.0729     0.000759\n",
            "     38     7      0.00859      0.00859     3.42e-06       0.0541       0.0717       0.0447       0.0728       0.0588       0.0572       0.0942       0.0757        0.135      0.00141\n",
            "     38     8      0.00908      0.00907     1.47e-05       0.0562       0.0737       0.0457       0.0771       0.0614       0.0587       0.0969       0.0778         0.26       0.0027\n",
            "     38     9       0.0104       0.0103     4.23e-05       0.0596       0.0785       0.0493         0.08       0.0647       0.0634        0.102       0.0829        0.467      0.00486\n",
            "     38    10       0.0108       0.0108     1.51e-05       0.0602       0.0803       0.0483       0.0841       0.0662       0.0633        0.106       0.0849        0.279      0.00291\n",
            "     38    11       0.0105       0.0104     5.59e-05       0.0614        0.079       0.0516       0.0812       0.0664       0.0663       0.0996        0.083        0.552      0.00575\n",
            "     38    12      0.00996      0.00993     2.17e-05       0.0597       0.0771       0.0515       0.0763       0.0639       0.0645       0.0976        0.081         0.34      0.00354\n",
            "     38    13       0.0105       0.0105     7.66e-07       0.0607       0.0793       0.0492       0.0838       0.0665       0.0638        0.104       0.0837       0.0488     0.000509\n",
            "     38    14      0.00941      0.00938     3.67e-05       0.0572       0.0749       0.0465       0.0788       0.0626       0.0594       0.0989       0.0792        0.447      0.00465\n",
            "     38    15       0.0111        0.011     7.64e-06       0.0614       0.0813        0.049       0.0861       0.0676       0.0635        0.109        0.086         0.19      0.00198\n",
            "     38    16      0.00887      0.00876     0.000103       0.0549       0.0724       0.0438       0.0769       0.0604       0.0552       0.0982       0.0767        0.752      0.00783\n",
            "     38    17      0.00866      0.00863     3.11e-05       0.0543       0.0719       0.0441       0.0748       0.0595       0.0563       0.0957        0.076        0.409      0.00426\n",
            "     38    18      0.00954      0.00952     1.99e-05       0.0555       0.0755       0.0453        0.076       0.0606       0.0597       0.0998       0.0798        0.326      0.00339\n",
            "     38    19       0.0082      0.00808      0.00012       0.0528       0.0695       0.0435       0.0713       0.0574       0.0552       0.0917       0.0734        0.809      0.00842\n",
            "     38    20      0.00886      0.00883        3e-05       0.0558       0.0727       0.0461       0.0752       0.0607       0.0592        0.094       0.0766         0.39      0.00407\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     38     1       0.0105       0.0105     2.76e-06       0.0602       0.0794       0.0488       0.0829       0.0658       0.0636        0.104       0.0839        0.104      0.00108\n",
            "     38     2       0.0103       0.0103     9.43e-07       0.0587       0.0785       0.0472       0.0819       0.0645       0.0618        0.104        0.083        0.059     0.000614\n",
            "     38     3      0.00936      0.00935     1.65e-06       0.0565       0.0748       0.0456       0.0781       0.0619       0.0588       0.0994       0.0791       0.0796     0.000829\n",
            "     38     4       0.0101       0.0101     1.23e-06       0.0582       0.0778       0.0474         0.08       0.0637       0.0614        0.103       0.0822       0.0725     0.000755\n",
            "     38     5      0.00956      0.00956      1.7e-06        0.057       0.0756       0.0467       0.0774       0.0621       0.0612       0.0984       0.0798       0.0691      0.00072\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              38  147.502    0.005      0.00956     3.04e-05      0.00959       0.0575       0.0757       0.0471       0.0782       0.0627       0.0607       0.0991       0.0799        0.348      0.00363\n",
            "! Validation         38  147.502    0.005      0.00997     1.66e-06      0.00998       0.0581       0.0773       0.0471       0.0801       0.0636       0.0614        0.102       0.0816       0.0768       0.0008\n",
            "Wall time: 147.5027447069997\n",
            "! Best model       38    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     39     1       0.0106       0.0106     8.19e-05       0.0607       0.0795       0.0502       0.0817        0.066        0.065        0.103       0.0838        0.667      0.00695\n",
            "     39     2       0.0109       0.0109     5.88e-05       0.0612       0.0807       0.0501       0.0834       0.0668       0.0644        0.106       0.0853        0.566       0.0059\n",
            "     39     3       0.0108       0.0108      2.1e-06       0.0608       0.0805       0.0493       0.0838       0.0665       0.0639        0.106        0.085       0.0854     0.000889\n",
            "     39     4       0.0106       0.0104     0.000106       0.0606       0.0791       0.0492       0.0835       0.0663       0.0619        0.105       0.0836        0.763      0.00795\n",
            "     39     5        0.009        0.009     3.56e-06       0.0556       0.0734       0.0445        0.078       0.0612       0.0574       0.0978       0.0776        0.114      0.00119\n",
            "     39     6       0.0102       0.0102     3.55e-05       0.0592        0.078       0.0502       0.0773       0.0637       0.0657       0.0981       0.0819        0.437      0.00455\n",
            "     39     7      0.00876      0.00876     3.18e-06       0.0554       0.0724       0.0479       0.0705       0.0592       0.0608       0.0913        0.076        0.121      0.00127\n",
            "     39     8       0.0104       0.0104     1.39e-05       0.0602       0.0789       0.0495       0.0816       0.0656       0.0625        0.104       0.0833        0.268      0.00279\n",
            "     39     9       0.0114       0.0113     9.05e-05       0.0625       0.0822       0.0528       0.0821       0.0674       0.0679        0.105       0.0865        0.705      0.00734\n",
            "     39    10       0.0104       0.0103     5.36e-05       0.0614       0.0785       0.0537       0.0768       0.0653       0.0678       0.0964       0.0821        0.543      0.00566\n",
            "     39    11       0.0104       0.0104     2.77e-05       0.0606       0.0787       0.0514        0.079       0.0652       0.0665       0.0988       0.0826        0.381      0.00397\n",
            "     39    12      0.00735      0.00735     1.99e-06       0.0501       0.0663        0.041       0.0683       0.0546       0.0522       0.0879       0.0701       0.0945     0.000985\n",
            "     39    13       0.0102       0.0101     3.77e-05       0.0588       0.0778       0.0495       0.0774       0.0635       0.0639          0.1       0.0819        0.445      0.00463\n",
            "     39    14         0.01         0.01      2.1e-05       0.0594       0.0775       0.0489       0.0805       0.0647       0.0626        0.101       0.0817        0.313      0.00326\n",
            "     39    15       0.0097      0.00961     9.49e-05       0.0572       0.0758       0.0453       0.0809       0.0631       0.0588        0.102       0.0802        0.723      0.00753\n",
            "     39    16      0.00783      0.00776     7.12e-05       0.0518       0.0681       0.0424       0.0707       0.0565       0.0544       0.0895       0.0719        0.619      0.00644\n",
            "     39    17       0.0106       0.0106     4.52e-05       0.0612       0.0796       0.0516       0.0802       0.0659        0.066        0.101       0.0837        0.489       0.0051\n",
            "     39    18         0.01      0.00991     0.000123       0.0577        0.077       0.0465       0.0799       0.0632       0.0602        0.103       0.0814        0.824      0.00858\n",
            "     39    19      0.00945      0.00943     2.45e-05        0.057       0.0751       0.0458       0.0794       0.0626       0.0596       0.0991       0.0794        0.348      0.00363\n",
            "     39    20      0.00987       0.0098     6.53e-05        0.058       0.0766       0.0472       0.0795       0.0634       0.0609        0.101       0.0809        0.594      0.00619\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     39     1       0.0104       0.0104     2.76e-06       0.0597       0.0788       0.0484       0.0823       0.0653       0.0629        0.103       0.0832        0.103      0.00107\n",
            "     39     2       0.0102       0.0102     9.67e-07       0.0583        0.078       0.0468       0.0814       0.0641       0.0613        0.104       0.0825       0.0591     0.000615\n",
            "     39     3      0.00924      0.00924     1.62e-06       0.0561       0.0744       0.0452       0.0778       0.0615       0.0583       0.0989       0.0786       0.0795     0.000828\n",
            "     39     4      0.00997      0.00997     1.21e-06       0.0578       0.0772       0.0469       0.0796       0.0632       0.0608        0.102       0.0816       0.0729     0.000759\n",
            "     39     5      0.00942      0.00941     1.71e-06       0.0565       0.0751       0.0463        0.077       0.0616       0.0607       0.0977       0.0792       0.0696     0.000725\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              39  151.328    0.005      0.00988     4.81e-05      0.00993       0.0585       0.0769       0.0483       0.0787       0.0635       0.0623       0.0999       0.0811        0.455      0.00474\n",
            "! Validation         39  151.328    0.005      0.00983     1.65e-06      0.00983       0.0577       0.0767       0.0467       0.0796       0.0632       0.0608        0.101        0.081       0.0768       0.0008\n",
            "Wall time: 151.3284268479997\n",
            "! Best model       39    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     40     1      0.00887      0.00881     5.89e-05       0.0558       0.0726       0.0456       0.0761       0.0608       0.0577       0.0957       0.0767        0.564      0.00588\n",
            "     40     2      0.00922      0.00921     1.04e-05       0.0558       0.0743       0.0454       0.0765       0.0609       0.0583       0.0987       0.0785        0.225      0.00235\n",
            "     40     3       0.0102       0.0101      2.6e-05       0.0582       0.0779       0.0464       0.0816        0.064       0.0605        0.104       0.0824        0.376      0.00392\n",
            "     40     4         0.01         0.01     1.03e-06       0.0607       0.0775       0.0528       0.0765       0.0646       0.0669       0.0953       0.0811        0.075     0.000781\n",
            "     40     5       0.0148       0.0148      1.8e-06       0.0752       0.0941        0.069       0.0878       0.0784       0.0857        0.109       0.0973       0.0877     0.000913\n",
            "     40     6       0.0148       0.0148      1.6e-05       0.0725       0.0941       0.0594       0.0988       0.0791       0.0768        0.121       0.0991         0.29      0.00302\n",
            "     40     7       0.0117       0.0116     9.54e-05       0.0635       0.0833       0.0522       0.0863       0.0692       0.0661         0.11        0.088         0.72       0.0075\n",
            "     40     8      0.00987      0.00987      4.3e-06       0.0584       0.0769       0.0488       0.0776       0.0632       0.0628       0.0992        0.081        0.126      0.00131\n",
            "     40     9       0.0104       0.0103     7.62e-05       0.0588       0.0786       0.0477       0.0809       0.0643       0.0623        0.104        0.083        0.646      0.00673\n",
            "     40    10      0.00979      0.00969     9.77e-05       0.0569       0.0761       0.0449       0.0809       0.0629       0.0591        0.102       0.0806        0.732      0.00762\n",
            "     40    11      0.00984      0.00984     6.26e-06       0.0574       0.0767       0.0456       0.0811       0.0633       0.0595        0.103       0.0812        0.179      0.00187\n",
            "     40    12      0.00998      0.00989     8.87e-05       0.0582       0.0769       0.0468       0.0811       0.0639       0.0605        0.102       0.0813        0.698      0.00727\n",
            "     40    13       0.0115       0.0115     4.29e-05       0.0655       0.0829       0.0607        0.075       0.0678       0.0762        0.095       0.0856         0.48        0.005\n",
            "     40    14       0.0111       0.0111     1.25e-05       0.0625       0.0815       0.0543       0.0789       0.0666       0.0696        0.101       0.0854        0.247      0.00257\n",
            "     40    15       0.0105       0.0104     3.75e-06       0.0593       0.0791       0.0467       0.0846       0.0656       0.0592        0.108       0.0838        0.139      0.00145\n",
            "     40    16      0.00892       0.0089     2.49e-05       0.0556        0.073       0.0438       0.0793       0.0615       0.0551       0.0995       0.0773        0.339      0.00354\n",
            "     40    17      0.00914      0.00914     3.35e-06       0.0563        0.074        0.046        0.077       0.0615       0.0592        0.097       0.0781       0.0926     0.000964\n",
            "     40    18      0.00933      0.00932     5.07e-06       0.0573       0.0747       0.0463       0.0793       0.0628       0.0594       0.0984       0.0789        0.145      0.00151\n",
            "     40    19       0.0095       0.0095     5.98e-06        0.058       0.0754         0.05        0.074        0.062       0.0627       0.0958       0.0793        0.177      0.00184\n",
            "     40    20      0.00943      0.00943     2.36e-06       0.0581       0.0751       0.0492       0.0759       0.0626       0.0627       0.0953        0.079        0.112      0.00116\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     40     1       0.0102       0.0102     2.66e-06       0.0593       0.0782       0.0479        0.082       0.0649       0.0624        0.103       0.0826        0.102      0.00106\n",
            "     40     2         0.01         0.01     9.43e-07       0.0579       0.0774       0.0463       0.0809       0.0636       0.0607        0.103       0.0819       0.0571     0.000595\n",
            "     40     3      0.00913      0.00913     1.64e-06       0.0557       0.0739       0.0449       0.0774       0.0611       0.0579       0.0985       0.0782       0.0791     0.000824\n",
            "     40     4      0.00984      0.00984     1.19e-06       0.0574       0.0767       0.0465       0.0792       0.0628       0.0602        0.102       0.0811       0.0729     0.000759\n",
            "     40     5      0.00928      0.00928      1.7e-06       0.0562       0.0745        0.046       0.0765       0.0612       0.0602       0.0971       0.0786       0.0685     0.000713\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              40  155.134    0.005       0.0104     2.92e-05       0.0104       0.0602        0.079       0.0501       0.0804       0.0653       0.0645        0.102       0.0832        0.323      0.00336\n",
            "! Validation         40  155.134    0.005       0.0097     1.63e-06       0.0097       0.0573       0.0762       0.0463       0.0792       0.0628       0.0603        0.101       0.0805       0.0759     0.000791\n",
            "Wall time: 155.13464983199992\n",
            "! Best model       40    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     41     1      0.00964      0.00961     3.03e-05       0.0577       0.0759       0.0462       0.0806       0.0634       0.0592        0.101       0.0802        0.406      0.00423\n",
            "     41     2      0.00915      0.00914     1.74e-05       0.0561       0.0739       0.0455       0.0772       0.0613       0.0586       0.0977       0.0781        0.292      0.00304\n",
            "     41     3      0.00833      0.00829     3.75e-05       0.0536       0.0705       0.0437       0.0735       0.0586       0.0553       0.0937       0.0745        0.452      0.00471\n",
            "     41     4      0.00947      0.00947     4.93e-07       0.0589       0.0753       0.0511       0.0745       0.0628       0.0641       0.0937       0.0789       0.0471      0.00049\n",
            "     41     5       0.0106       0.0106     1.78e-06       0.0619       0.0798       0.0532       0.0794       0.0663       0.0666        0.101       0.0838       0.0869     0.000905\n",
            "     41     6      0.00887      0.00885     1.93e-05       0.0546       0.0728       0.0435       0.0768       0.0602       0.0556       0.0986       0.0771        0.324      0.00338\n",
            "     41     7      0.00806      0.00806     2.36e-06       0.0532       0.0695       0.0429       0.0738       0.0584        0.055       0.0918       0.0734        0.104      0.00108\n",
            "     41     8      0.00844      0.00844     4.42e-06        0.054       0.0711       0.0453       0.0712       0.0583       0.0584       0.0913       0.0748        0.139      0.00145\n",
            "     41     9      0.00853      0.00853     1.17e-06        0.054       0.0715       0.0446       0.0728       0.0587       0.0567       0.0943       0.0755       0.0711     0.000741\n",
            "     41    10      0.00922      0.00922     6.01e-06       0.0569       0.0743       0.0481       0.0747       0.0614       0.0617       0.0946       0.0781         0.16      0.00167\n",
            "     41    11      0.00945      0.00944      1.1e-05       0.0552       0.0752       0.0424       0.0809       0.0616       0.0557        0.104       0.0797        0.236      0.00246\n",
            "     41    12         0.01         0.01     3.64e-05       0.0586       0.0774        0.049       0.0779       0.0634       0.0632       0.0998       0.0815        0.446      0.00464\n",
            "     41    13      0.00905      0.00905     1.02e-06       0.0557       0.0736       0.0457       0.0757       0.0607       0.0601        0.095       0.0776       0.0539     0.000562\n",
            "     41    14       0.0087      0.00869     1.41e-05       0.0541       0.0721       0.0419       0.0784       0.0602       0.0531       0.0998       0.0764        0.264      0.00275\n",
            "     41    15      0.00866      0.00866     7.42e-07        0.055        0.072       0.0447       0.0756       0.0602       0.0566       0.0956       0.0761        0.058     0.000604\n",
            "     41    16       0.0139       0.0139     1.79e-06       0.0692       0.0913       0.0581       0.0914       0.0747       0.0756        0.116        0.096       0.0885     0.000922\n",
            "     41    17       0.0145       0.0144     0.000122       0.0734       0.0929       0.0649       0.0903       0.0776       0.0817        0.112       0.0968        0.813      0.00847\n",
            "     41    18       0.0127       0.0127     2.22e-06       0.0693       0.0873       0.0617       0.0847       0.0732       0.0766        0.105        0.091       0.0869     0.000905\n",
            "     41    19         0.01      0.00997     5.04e-05       0.0593       0.0773       0.0478       0.0822        0.065       0.0602        0.103       0.0817        0.524      0.00546\n",
            "     41    20      0.00952       0.0094     0.000117       0.0562        0.075       0.0461       0.0763       0.0612       0.0597       0.0988       0.0792        0.803      0.00837\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     41     1       0.0101       0.0101     2.73e-06       0.0589       0.0777       0.0475       0.0816       0.0646       0.0618        0.102       0.0821        0.103      0.00107\n",
            "     41     2       0.0099       0.0099     9.01e-07       0.0575        0.077        0.046       0.0805       0.0632       0.0602        0.103       0.0814       0.0556     0.000579\n",
            "     41     3      0.00904      0.00904     1.58e-06       0.0554       0.0736       0.0446       0.0771       0.0608       0.0575       0.0981       0.0778       0.0781     0.000814\n",
            "     41     4      0.00972      0.00972     1.25e-06        0.057       0.0763       0.0461       0.0789       0.0625       0.0597        0.102       0.0806       0.0739      0.00077\n",
            "     41     5      0.00916      0.00916     1.71e-06       0.0558        0.074       0.0456       0.0761       0.0609       0.0597       0.0965       0.0781       0.0697     0.000726\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              41  158.932    0.005      0.00983     2.39e-05      0.00985       0.0583       0.0767       0.0483       0.0784       0.0634       0.0622       0.0996       0.0809        0.273      0.00284\n",
            "! Validation         41  158.932    0.005      0.00958     1.63e-06      0.00958       0.0569       0.0757        0.046       0.0788       0.0624       0.0598          0.1         0.08       0.0761     0.000792\n",
            "Wall time: 158.93352248899964\n",
            "! Best model       41    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     42     1       0.0105       0.0105     2.58e-05       0.0591       0.0792       0.0485       0.0803       0.0644       0.0641        0.103       0.0835        0.366      0.00381\n",
            "     42     2       0.0101       0.0098     0.000283       0.0576       0.0766       0.0463       0.0801       0.0632       0.0599        0.102        0.081         1.25        0.013\n",
            "     42     3        0.012        0.012     1.31e-05       0.0664       0.0846       0.0594       0.0805       0.0699        0.075        0.101       0.0881        0.257      0.00268\n",
            "     42     4       0.0123       0.0123     7.68e-05       0.0675       0.0856       0.0615       0.0795       0.0705       0.0762        0.102        0.089        0.649      0.00676\n",
            "     42     5       0.0112        0.011     0.000189       0.0619       0.0812       0.0504       0.0849       0.0677       0.0646        0.107       0.0858         1.02       0.0106\n",
            "     42     6      0.00962       0.0096     1.82e-05       0.0573       0.0758       0.0463       0.0791       0.0627         0.06          0.1       0.0801        0.314      0.00327\n",
            "     42     7      0.00826      0.00815     0.000107       0.0532       0.0699       0.0428       0.0738       0.0583       0.0542       0.0936       0.0739        0.768        0.008\n",
            "     42     8      0.00892      0.00892     7.06e-07       0.0545       0.0731       0.0429       0.0776       0.0603       0.0549       0.0999       0.0774       0.0543     0.000566\n",
            "     42     9      0.00886      0.00879     6.76e-05       0.0542       0.0725       0.0443        0.074       0.0591       0.0585       0.0946       0.0765        0.608      0.00633\n",
            "     42    10      0.00802      0.00799     3.01e-05       0.0525       0.0692       0.0419       0.0736       0.0578       0.0536       0.0928       0.0732        0.403      0.00419\n",
            "     42    11      0.00828      0.00825     2.95e-05       0.0537       0.0703       0.0444       0.0721       0.0583       0.0563        0.092       0.0742          0.4      0.00417\n",
            "     42    12      0.00902      0.00893     8.78e-05       0.0554       0.0731       0.0444       0.0776        0.061       0.0569       0.0978       0.0773         0.68      0.00708\n",
            "     42    13      0.00855      0.00854     2.07e-06       0.0542       0.0715       0.0436       0.0753       0.0595       0.0562       0.0951       0.0756       0.0998      0.00104\n",
            "     42    14      0.00853       0.0085     2.97e-05       0.0539       0.0713       0.0439       0.0739       0.0589       0.0562       0.0945       0.0754        0.396      0.00413\n",
            "     42    15      0.00886      0.00884     1.96e-05       0.0545       0.0727       0.0435       0.0764         0.06        0.057       0.0967       0.0769        0.322      0.00335\n",
            "     42    16      0.00855      0.00855     8.65e-07       0.0543       0.0716       0.0417       0.0794       0.0606       0.0527        0.099       0.0759       0.0627     0.000653\n",
            "     42    17       0.0102       0.0102     4.05e-05       0.0602        0.078       0.0505       0.0794        0.065       0.0641          0.1       0.0821        0.469      0.00489\n",
            "     42    18       0.0133       0.0132     1.89e-05       0.0696        0.089        0.061       0.0867       0.0738       0.0768        0.109       0.0931        0.312      0.00325\n",
            "     42    19       0.0131       0.0131     1.79e-06       0.0699       0.0884       0.0617       0.0862        0.074       0.0759        0.109       0.0925       0.0939     0.000979\n",
            "     42    20      0.00971      0.00971     5.33e-07       0.0581       0.0762       0.0478       0.0787       0.0633       0.0606          0.1       0.0805       0.0424     0.000441\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     42     1      0.00996      0.00996     2.82e-06       0.0585       0.0772       0.0472       0.0812       0.0642       0.0613        0.102       0.0816        0.104      0.00109\n",
            "     42     2      0.00978      0.00978     9.15e-07       0.0571       0.0765       0.0457         0.08       0.0628       0.0598        0.102       0.0809       0.0562     0.000585\n",
            "     42     3      0.00895      0.00895     1.64e-06       0.0551       0.0732       0.0443       0.0768       0.0605       0.0571       0.0977       0.0774       0.0805     0.000838\n",
            "     42     4       0.0096       0.0096     1.29e-06       0.0567       0.0758       0.0457       0.0786       0.0621       0.0592        0.101       0.0802        0.074     0.000771\n",
            "     42     5      0.00904      0.00904     1.66e-06       0.0555       0.0736       0.0453       0.0758       0.0606       0.0593       0.0959       0.0776       0.0688     0.000717\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              42  162.748    0.005      0.00984     5.21e-05      0.00989       0.0584       0.0767       0.0484       0.0785       0.0634       0.0622       0.0997       0.0809        0.428      0.00446\n",
            "! Validation         42  162.748    0.005      0.00947     1.66e-06      0.00947       0.0566       0.0753       0.0456       0.0785        0.062       0.0593       0.0998       0.0796       0.0768       0.0008\n",
            "Wall time: 162.74851267899976\n",
            "! Best model       42    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     43     1      0.00831      0.00831     2.15e-06       0.0524       0.0705       0.0418       0.0738       0.0578       0.0548       0.0944       0.0746        0.104      0.00108\n",
            "     43     2       0.0123       0.0123     1.33e-05        0.065       0.0858       0.0565       0.0821       0.0693       0.0738        0.106       0.0897        0.258      0.00268\n",
            "     43     3        0.015       0.0149     5.65e-05       0.0748       0.0945       0.0661       0.0921       0.0791       0.0818        0.116       0.0988        0.549      0.00572\n",
            "     43     4       0.0141       0.0141     3.69e-06       0.0723        0.092       0.0628       0.0912        0.077       0.0787        0.114       0.0963        0.137      0.00143\n",
            "     43     5      0.00989      0.00986     3.01e-05       0.0588       0.0768       0.0493       0.0777       0.0635        0.063       0.0988       0.0809        0.403       0.0042\n",
            "     43     6       0.0085      0.00849     5.93e-06       0.0532       0.0713       0.0417       0.0762       0.0589       0.0538       0.0973       0.0755        0.158      0.00165\n",
            "     43     7      0.00711      0.00706     4.84e-05       0.0491        0.065       0.0393       0.0688        0.054       0.0494       0.0884       0.0689        0.514      0.00536\n",
            "     43     8       0.0086      0.00859     9.54e-06       0.0542       0.0717       0.0442        0.074       0.0591       0.0574       0.0939       0.0757        0.207      0.00215\n",
            "     43     9      0.00877      0.00877     2.83e-06       0.0548       0.0724       0.0447       0.0752       0.0599       0.0579        0.095       0.0765       0.0879     0.000916\n",
            "     43    10      0.00804      0.00803     1.13e-05       0.0524       0.0693       0.0422        0.073       0.0576       0.0541       0.0925       0.0733        0.244      0.00254\n",
            "     43    11      0.00869      0.00868     1.92e-06        0.055       0.0721       0.0449       0.0751         0.06       0.0573        0.095       0.0762       0.0836     0.000871\n",
            "     43    12      0.00831      0.00829     2.15e-05       0.0528       0.0704       0.0429       0.0726       0.0577       0.0551       0.0939       0.0745        0.342      0.00356\n",
            "     43    13      0.00949      0.00949     4.86e-06       0.0567       0.0753       0.0463       0.0776        0.062       0.0601       0.0991       0.0796        0.153      0.00159\n",
            "     43    14      0.00825      0.00816     8.36e-05       0.0532       0.0699       0.0438       0.0721       0.0579       0.0554       0.0923       0.0739        0.672        0.007\n",
            "     43    15      0.00933      0.00926     6.23e-05       0.0562       0.0745       0.0456       0.0773       0.0615       0.0593        0.098       0.0786        0.582      0.00606\n",
            "     43    16      0.00807      0.00801     5.92e-05       0.0519       0.0692       0.0425       0.0708       0.0566       0.0545       0.0919       0.0732        0.569      0.00593\n",
            "     43    17      0.00878      0.00861      0.00017       0.0544       0.0718       0.0446       0.0741       0.0594       0.0568        0.095       0.0759        0.968       0.0101\n",
            "     43    18      0.00821      0.00821     4.87e-06       0.0529       0.0701       0.0419       0.0748       0.0584       0.0535        0.095       0.0742        0.155      0.00161\n",
            "     43    19      0.00892      0.00887     5.54e-05       0.0553       0.0728       0.0435       0.0789       0.0612        0.055       0.0993       0.0772        0.551      0.00574\n",
            "     43    20      0.00872      0.00867     5.48e-05       0.0548        0.072       0.0441       0.0763       0.0602       0.0563       0.0961       0.0762        0.547       0.0057\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     43     1      0.00984      0.00984     2.75e-06       0.0581       0.0767       0.0468       0.0808       0.0638       0.0608        0.101       0.0811        0.104      0.00108\n",
            "     43     2      0.00968      0.00967     9.14e-07       0.0568       0.0761       0.0453       0.0796       0.0625       0.0593        0.102       0.0805       0.0553     0.000576\n",
            "     43     3      0.00886      0.00886     1.61e-06       0.0548       0.0728        0.044       0.0765       0.0602       0.0567       0.0973        0.077       0.0787      0.00082\n",
            "     43     4      0.00949      0.00949     1.21e-06       0.0563       0.0754       0.0453       0.0782       0.0618       0.0587        0.101       0.0797       0.0728     0.000758\n",
            "     43     5      0.00893      0.00893     1.64e-06       0.0552       0.0731        0.045       0.0754       0.0602       0.0589       0.0954       0.0771       0.0675     0.000703\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              43  166.539    0.005      0.00934     3.51e-05      0.00937       0.0565       0.0747       0.0464       0.0767       0.0616         0.06       0.0978       0.0789        0.364      0.00379\n",
            "! Validation         43  166.539    0.005      0.00936     1.63e-06      0.00936       0.0562       0.0748       0.0453       0.0781       0.0617       0.0589       0.0993       0.0791       0.0756     0.000788\n",
            "Wall time: 166.53983182399998\n",
            "! Best model       43    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     44     1      0.00792      0.00791     5.02e-06       0.0521       0.0688        0.042       0.0723       0.0572        0.054       0.0915       0.0728        0.161      0.00167\n",
            "     44     2       0.0093      0.00927     2.28e-05       0.0558       0.0745       0.0456        0.076       0.0608         0.06       0.0972       0.0786        0.347      0.00361\n",
            "     44     3      0.00923      0.00923     1.06e-07       0.0571       0.0743       0.0472       0.0769        0.062         0.06       0.0968       0.0784       0.0201      0.00021\n",
            "     44     4      0.00878      0.00872     5.56e-05       0.0547       0.0722       0.0435        0.077       0.0603       0.0567       0.0961       0.0764        0.544      0.00566\n",
            "     44     5      0.00823      0.00822     3.02e-06       0.0531       0.0702       0.0425       0.0744       0.0585       0.0545       0.0939       0.0742        0.114      0.00119\n",
            "     44     6       0.0103       0.0103     8.07e-05       0.0598       0.0784       0.0501       0.0792       0.0646       0.0649          0.1       0.0825        0.662      0.00689\n",
            "     44     7      0.00921      0.00916     4.38e-05       0.0568       0.0741       0.0471       0.0762       0.0617         0.06       0.0961       0.0781        0.488      0.00508\n",
            "     44     8      0.00946      0.00946     2.51e-06       0.0583       0.0752         0.05       0.0747       0.0624       0.0628       0.0953       0.0791       0.0932      0.00097\n",
            "     44     9      0.00875      0.00865     0.000106        0.054       0.0719       0.0428       0.0765       0.0596       0.0545       0.0978       0.0762        0.757      0.00789\n",
            "     44    10      0.00892      0.00888     4.06e-05       0.0551       0.0729       0.0436        0.078       0.0608       0.0556       0.0987       0.0772        0.459      0.00479\n",
            "     44    11      0.00935      0.00934     1.82e-06       0.0556       0.0748        0.044       0.0788       0.0614       0.0573        0.101       0.0792       0.0941     0.000981\n",
            "     44    12       0.0114       0.0114     9.18e-06       0.0635       0.0827        0.051       0.0884       0.0697       0.0646         0.11       0.0875        0.218      0.00227\n",
            "     44    13       0.0117       0.0116     0.000117       0.0648       0.0834       0.0568       0.0806       0.0687       0.0712        0.103       0.0873        0.802      0.00835\n",
            "     44    14       0.0108       0.0108     1.22e-05       0.0622       0.0803       0.0544        0.078       0.0662         0.07       0.0978       0.0839        0.238      0.00248\n",
            "     44    15      0.00868      0.00863     4.28e-05       0.0543       0.0719       0.0436       0.0758       0.0597       0.0555       0.0966       0.0761        0.478      0.00498\n",
            "     44    16      0.00952      0.00952     5.23e-07       0.0575       0.0755       0.0488       0.0749       0.0618       0.0626       0.0962       0.0794       0.0475     0.000494\n",
            "     44    17       0.0103       0.0103     6.72e-07       0.0602       0.0786        0.052       0.0766       0.0643       0.0659       0.0992       0.0825       0.0533     0.000555\n",
            "     44    18       0.0089      0.00889     4.62e-06       0.0553        0.073       0.0463       0.0734       0.0598       0.0583       0.0957        0.077        0.154       0.0016\n",
            "     44    19      0.00855      0.00855     2.49e-06       0.0536       0.0715       0.0426       0.0756       0.0591       0.0543       0.0972       0.0758       0.0857     0.000893\n",
            "     44    20      0.00997      0.00996     6.35e-06       0.0578       0.0772       0.0468       0.0798       0.0633       0.0614        0.102       0.0816        0.162      0.00169\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     44     1      0.00972      0.00972     2.68e-06       0.0578       0.0763       0.0465       0.0804       0.0634       0.0604        0.101       0.0806        0.102      0.00106\n",
            "     44     2      0.00957      0.00957     8.95e-07       0.0564       0.0757        0.045       0.0793       0.0621       0.0589        0.101         0.08       0.0538     0.000561\n",
            "     44     3      0.00877      0.00877     1.57e-06       0.0545       0.0725       0.0437       0.0762       0.0599       0.0563        0.097       0.0767       0.0788     0.000821\n",
            "     44     4      0.00939      0.00939     1.22e-06        0.056        0.075        0.045       0.0779       0.0615       0.0582          0.1       0.0793        0.073     0.000761\n",
            "     44     5      0.00883      0.00883     1.62e-06       0.0549       0.0727       0.0448       0.0751       0.0599       0.0585        0.095       0.0767       0.0676     0.000704\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              44  170.349    0.005      0.00944     2.79e-05      0.00947       0.0571       0.0752        0.047       0.0772       0.0621       0.0604       0.0982       0.0793        0.299      0.00311\n",
            "! Validation         44  170.349    0.005      0.00926      1.6e-06      0.00926       0.0559       0.0744        0.045       0.0778       0.0614       0.0585       0.0989       0.0787        0.075     0.000782\n",
            "Wall time: 170.34987347800006\n",
            "! Best model       44    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     45     1      0.00941       0.0094     1.16e-05       0.0575        0.075       0.0478       0.0769       0.0624       0.0602        0.098       0.0791        0.235      0.00245\n",
            "     45     2       0.0103       0.0103     2.31e-06       0.0606       0.0787       0.0516       0.0786       0.0651       0.0659       0.0995       0.0827       0.0889     0.000926\n",
            "     45     3      0.00888      0.00888     1.31e-06       0.0554       0.0729       0.0462       0.0737       0.0599       0.0585       0.0954       0.0769       0.0682      0.00071\n",
            "     45     4       0.0104       0.0104     4.88e-06         0.06        0.079       0.0507       0.0785       0.0646       0.0654        0.101       0.0831        0.112      0.00116\n",
            "     45     5       0.0146       0.0146     1.98e-05       0.0741       0.0934       0.0669       0.0883       0.0776       0.0828        0.112       0.0972         0.32      0.00333\n",
            "     45     6        0.013       0.0129     1.25e-05       0.0679        0.088       0.0574       0.0888       0.0731       0.0722        0.113       0.0927        0.253      0.00263\n",
            "     45     7      0.00904      0.00904     6.42e-07       0.0564       0.0735        0.045       0.0791       0.0621       0.0571       0.0985       0.0778        0.057     0.000594\n",
            "     45     8      0.00731      0.00731     2.82e-06       0.0497       0.0661       0.0393       0.0704       0.0548       0.0499       0.0903       0.0701        0.115      0.00119\n",
            "     45     9      0.00948      0.00947      6.6e-06        0.057       0.0753       0.0463       0.0782       0.0623        0.059          0.1       0.0796        0.186      0.00193\n",
            "     45    10       0.0112       0.0111     1.19e-06       0.0633       0.0817       0.0534        0.083       0.0682       0.0674        0.105        0.086        0.067     0.000698\n",
            "     45    11       0.0118       0.0118     2.72e-05       0.0657       0.0841       0.0583       0.0804       0.0694       0.0743        0.101       0.0876        0.385      0.00401\n",
            "     45    12      0.00903      0.00903     4.02e-06       0.0558       0.0735       0.0436       0.0803        0.062       0.0556          0.1       0.0779         0.12      0.00125\n",
            "     45    13      0.00875      0.00871      3.1e-05       0.0544       0.0722       0.0436       0.0761       0.0598       0.0575       0.0951       0.0763          0.4      0.00416\n",
            "     45    14      0.00959      0.00957     2.13e-05        0.057       0.0757       0.0464       0.0782       0.0623       0.0594        0.101         0.08        0.335      0.00349\n",
            "     45    15      0.00932      0.00931     5.58e-06       0.0565       0.0747       0.0455       0.0785        0.062       0.0588       0.0991       0.0789        0.166      0.00173\n",
            "     45    16       0.0105       0.0104     8.42e-05       0.0595        0.079       0.0485       0.0816        0.065       0.0636        0.103       0.0834        0.678      0.00706\n",
            "     45    17      0.00821      0.00814     6.35e-05       0.0528       0.0698       0.0423        0.074       0.0581        0.053       0.0948       0.0739        0.591      0.00616\n",
            "     45    18       0.0103       0.0103     2.69e-05       0.0602       0.0784       0.0537       0.0732       0.0635       0.0687       0.0948       0.0817        0.382      0.00398\n",
            "     45    19      0.00921      0.00913     8.45e-05       0.0564       0.0739       0.0466        0.076       0.0613       0.0597       0.0963        0.078        0.673      0.00701\n",
            "     45    20      0.00996      0.00996     6.52e-06        0.059       0.0772       0.0482       0.0806       0.0644        0.062        0.101       0.0815        0.163       0.0017\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     45     1      0.00961      0.00961     2.73e-06       0.0574       0.0758       0.0461       0.0801       0.0631       0.0599          0.1       0.0801        0.103      0.00107\n",
            "     45     2      0.00948      0.00947     9.16e-07       0.0561       0.0753       0.0447       0.0789       0.0618       0.0585        0.101       0.0797        0.054     0.000563\n",
            "     45     3       0.0087       0.0087     1.56e-06       0.0543       0.0721       0.0434       0.0759       0.0597        0.056       0.0967       0.0763       0.0778     0.000811\n",
            "     45     4      0.00929      0.00929     1.26e-06       0.0557       0.0746       0.0447       0.0777       0.0612       0.0578          0.1       0.0789       0.0742     0.000773\n",
            "     45     5      0.00873      0.00872     1.59e-06       0.0546       0.0723       0.0444       0.0748       0.0596        0.058       0.0945       0.0763       0.0665     0.000693\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              45  174.155    0.005      0.00999     2.09e-05         0.01        0.059       0.0773       0.0491       0.0787       0.0639        0.063          0.1       0.0815         0.27      0.00281\n",
            "! Validation         45  174.155    0.005      0.00916     1.61e-06      0.00916       0.0556        0.074       0.0447       0.0775       0.0611       0.0581       0.0985       0.0783       0.0751     0.000782\n",
            "Wall time: 174.15537055599998\n",
            "! Best model       45    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     46     1      0.00805      0.00794     0.000107       0.0519       0.0689       0.0413       0.0731       0.0572       0.0534       0.0925       0.0729        0.766      0.00798\n",
            "     46     2       0.0114       0.0113     8.85e-05       0.0623       0.0824       0.0507       0.0854       0.0681        0.065        0.109       0.0871        0.694      0.00722\n",
            "     46     3       0.0109       0.0108     9.57e-05       0.0614       0.0802       0.0517       0.0807       0.0662       0.0668        0.102       0.0844         0.72       0.0075\n",
            "     46     4      0.00976      0.00968     8.33e-05       0.0596       0.0761       0.0533       0.0723       0.0628       0.0665       0.0923       0.0794        0.675      0.00703\n",
            "     46     5       0.0093       0.0093     3.89e-07       0.0571       0.0746       0.0456         0.08       0.0628        0.058       0.0998       0.0789       0.0387     0.000403\n",
            "     46     6      0.00816      0.00804     0.000115       0.0537       0.0694       0.0431       0.0749        0.059       0.0543       0.0925       0.0734        0.795      0.00828\n",
            "     46     7      0.00886      0.00885      9.2e-06       0.0552       0.0728        0.045       0.0755       0.0603       0.0581       0.0957       0.0769        0.219      0.00228\n",
            "     46     8      0.00994      0.00984     0.000105       0.0579       0.0767       0.0478       0.0783        0.063       0.0618          0.1       0.0809        0.761      0.00792\n",
            "     46     9         0.01         0.01     7.43e-06       0.0607       0.0775       0.0524       0.0774       0.0649        0.066       0.0964       0.0812         0.18      0.00188\n",
            "     46    10       0.0106       0.0106     2.45e-05       0.0607       0.0795       0.0512       0.0797       0.0654       0.0656        0.102       0.0837        0.365       0.0038\n",
            "     46    11      0.00785      0.00775     9.08e-05       0.0516       0.0681       0.0392       0.0765       0.0578       0.0499       0.0946       0.0722        0.701       0.0073\n",
            "     46    12      0.00723      0.00723     1.09e-06       0.0488       0.0658        0.039       0.0683       0.0537       0.0498       0.0896       0.0697       0.0689     0.000718\n",
            "     46    13         0.01      0.00993        7e-05       0.0577       0.0771       0.0458       0.0815       0.0637       0.0593        0.104       0.0816        0.616      0.00642\n",
            "     46    14      0.00728      0.00728     3.45e-06       0.0507        0.066       0.0412       0.0698       0.0555       0.0522       0.0873       0.0698        0.114      0.00119\n",
            "     46    15      0.00924      0.00923      6.3e-06       0.0566       0.0743       0.0474        0.075       0.0612       0.0617       0.0947       0.0782        0.158      0.00165\n",
            "     46    16      0.00874      0.00874     1.01e-06       0.0546       0.0723       0.0445       0.0747       0.0596       0.0563       0.0967       0.0765       0.0623     0.000649\n",
            "     46    17      0.00974      0.00973     4.83e-06       0.0584       0.0763       0.0486        0.078       0.0633        0.062        0.099       0.0805         0.16      0.00167\n",
            "     46    18       0.0101       0.0101     7.58e-06       0.0594       0.0776       0.0486        0.081       0.0648       0.0625        0.101       0.0819        0.171      0.00178\n",
            "     46    19      0.00961      0.00956      4.2e-05       0.0571       0.0757       0.0456       0.0803       0.0629       0.0585        0.102       0.0801        0.463      0.00482\n",
            "     46    20       0.0084      0.00839     9.91e-06       0.0536       0.0709        0.043       0.0749       0.0589       0.0557       0.0942       0.0749         0.22      0.00229\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     46     1      0.00951       0.0095     2.69e-06       0.0571       0.0754       0.0458       0.0797       0.0627       0.0595       0.0999       0.0797        0.102      0.00106\n",
            "     46     2      0.00939      0.00939     9.79e-07       0.0559        0.075       0.0445       0.0787       0.0616       0.0582          0.1       0.0793       0.0559     0.000582\n",
            "     46     3      0.00863      0.00863     1.52e-06        0.054       0.0719       0.0432       0.0757       0.0594       0.0557       0.0964        0.076       0.0765     0.000797\n",
            "     46     4       0.0092       0.0092     1.21e-06       0.0554       0.0742       0.0443       0.0774       0.0609       0.0574       0.0997       0.0785       0.0724     0.000754\n",
            "     46     5      0.00863      0.00863      1.6e-06       0.0542       0.0719       0.0441       0.0745       0.0593       0.0576       0.0941       0.0758       0.0668     0.000696\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              46  177.975    0.005      0.00921     4.37e-05      0.00926       0.0564       0.0743       0.0462       0.0769       0.0615       0.0594       0.0974       0.0784        0.397      0.00414\n",
            "! Validation         46  177.975    0.005      0.00907      1.6e-06      0.00907       0.0553       0.0737       0.0444       0.0772       0.0608       0.0577       0.0981       0.0779       0.0747     0.000778\n",
            "Wall time: 177.97563670499994\n",
            "! Best model       46    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     47     1       0.0102       0.0102     2.85e-06       0.0578       0.0781       0.0465       0.0804       0.0634       0.0611        0.104       0.0826         0.11      0.00115\n",
            "     47     2       0.0108       0.0108     1.09e-05       0.0621       0.0804       0.0534       0.0795       0.0665       0.0673        0.102       0.0845        0.213      0.00222\n",
            "     47     3      0.00865      0.00863     1.67e-05        0.055       0.0719        0.045        0.075         0.06       0.0576       0.0941       0.0759        0.302      0.00314\n",
            "     47     4       0.0087       0.0087     5.42e-06       0.0545       0.0722       0.0443        0.075       0.0596       0.0566       0.0959       0.0763         0.16      0.00167\n",
            "     47     5      0.00976      0.00976     1.81e-06       0.0582       0.0764       0.0455       0.0835       0.0645        0.057        0.105        0.081       0.0703     0.000732\n",
            "     47     6       0.0108       0.0108     1.25e-05       0.0607       0.0804       0.0488       0.0846       0.0667       0.0633        0.107        0.085        0.259       0.0027\n",
            "     47     7      0.00969      0.00965     3.65e-05       0.0595        0.076       0.0526       0.0734        0.063        0.066       0.0928       0.0794        0.442       0.0046\n",
            "     47     8      0.00748      0.00746     2.15e-05       0.0509       0.0668       0.0417       0.0693       0.0555       0.0532       0.0879       0.0706        0.338      0.00352\n",
            "     47     9      0.00913      0.00912     8.97e-06       0.0559       0.0739       0.0467       0.0742       0.0605       0.0588       0.0972        0.078         0.21      0.00219\n",
            "     47    10       0.0121       0.0121     1.62e-05       0.0659       0.0852       0.0563       0.0852       0.0707       0.0708        0.108       0.0896        0.294      0.00306\n",
            "     47    11       0.0106       0.0106     3.16e-05       0.0618       0.0797       0.0522        0.081       0.0666       0.0663        0.101       0.0838        0.399      0.00416\n",
            "     47    12      0.00879      0.00876     3.02e-05       0.0549       0.0724        0.043       0.0785       0.0608       0.0565       0.0966       0.0766        0.393      0.00409\n",
            "     47    13      0.00963      0.00956     6.96e-05        0.059       0.0757       0.0507       0.0754       0.0631       0.0647       0.0938       0.0793        0.615       0.0064\n",
            "     47    14       0.0118       0.0118     1.11e-05       0.0649       0.0839       0.0557       0.0832       0.0694       0.0706        0.106       0.0881        0.244      0.00255\n",
            "     47    15       0.0113       0.0113     3.34e-05        0.064       0.0821       0.0544       0.0833       0.0688        0.068        0.105       0.0864        0.423      0.00441\n",
            "     47    16      0.00918      0.00917     5.11e-06       0.0559       0.0741       0.0437       0.0804       0.0621       0.0558        0.101       0.0785        0.166      0.00173\n",
            "     47    17      0.00849      0.00847     2.28e-05        0.054       0.0712       0.0426       0.0769       0.0597       0.0555       0.0951       0.0753        0.341      0.00356\n",
            "     47    18       0.0124       0.0124     2.61e-05       0.0666       0.0861        0.058       0.0838       0.0709       0.0726        0.108       0.0904        0.378      0.00394\n",
            "     47    19       0.0138       0.0137      3.6e-05       0.0721       0.0907       0.0655       0.0853       0.0754       0.0802        0.109       0.0945        0.439      0.00457\n",
            "     47    20      0.00846      0.00844     1.68e-05       0.0553       0.0711       0.0473       0.0712       0.0593       0.0597       0.0896       0.0746        0.302      0.00315\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     47     1       0.0094       0.0094     2.62e-06       0.0568        0.075       0.0455       0.0794       0.0624        0.059       0.0995       0.0793        0.101      0.00105\n",
            "     47     2      0.00929      0.00929     9.62e-07       0.0556       0.0746       0.0442       0.0783       0.0612       0.0578          0.1       0.0789       0.0562     0.000585\n",
            "     47     3      0.00855      0.00854     1.47e-06       0.0537       0.0715       0.0429       0.0754       0.0592       0.0553       0.0961       0.0757       0.0737     0.000768\n",
            "     47     4      0.00911       0.0091     1.19e-06       0.0551       0.0738        0.044       0.0771       0.0606       0.0569       0.0993       0.0781       0.0721     0.000751\n",
            "     47     5      0.00855      0.00855     1.54e-06        0.054       0.0715       0.0439       0.0742       0.0591       0.0573       0.0937       0.0755       0.0643     0.000669\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              47  181.775    0.005       0.0101     2.08e-05       0.0101       0.0595       0.0776       0.0497       0.0789       0.0643       0.0635          0.1       0.0818        0.305      0.00318\n",
            "! Validation         47  181.775    0.005      0.00898     1.56e-06      0.00898        0.055       0.0733       0.0441       0.0769       0.0605       0.0573       0.0978       0.0775       0.0734     0.000765\n",
            "Wall time: 181.77527904\n",
            "! Best model       47    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     48     1      0.00929      0.00929     2.36e-06        0.057       0.0745       0.0473       0.0764       0.0619         0.06       0.0974       0.0787        0.108      0.00113\n",
            "     48     2       0.0151       0.0151      2.7e-06       0.0761       0.0951       0.0693       0.0898       0.0795       0.0856        0.112       0.0987       0.0969      0.00101\n",
            "     48     3       0.0134       0.0134     8.61e-06       0.0703       0.0897       0.0645       0.0818       0.0732       0.0805        0.106        0.093        0.197      0.00206\n",
            "     48     4      0.00944      0.00944     1.86e-06       0.0551       0.0752       0.0434       0.0785       0.0609       0.0582        0.101       0.0795       0.0871     0.000907\n",
            "     48     5      0.00934      0.00931     2.25e-05       0.0568       0.0747        0.047       0.0763       0.0616       0.0596        0.098       0.0788        0.333      0.00347\n",
            "     48     6       0.0116       0.0116     9.08e-05       0.0642       0.0831       0.0544       0.0837       0.0691       0.0689        0.106       0.0875        0.702      0.00732\n",
            "     48     7      0.00898      0.00896     2.21e-05       0.0553       0.0732        0.046        0.074         0.06        0.059       0.0955       0.0773        0.344      0.00358\n",
            "     48     8      0.00799      0.00787     0.000127       0.0519       0.0686       0.0411       0.0736       0.0574       0.0526       0.0927       0.0726        0.835       0.0087\n",
            "     48     9      0.00942       0.0094     2.33e-05       0.0577        0.075       0.0485       0.0759       0.0622       0.0617       0.0963        0.079        0.354      0.00369\n",
            "     48    10      0.00992      0.00992     1.23e-06       0.0598       0.0771       0.0515       0.0763       0.0639       0.0653       0.0964       0.0808       0.0527     0.000549\n",
            "     48    11       0.0082      0.00812     7.97e-05       0.0525       0.0697       0.0424       0.0727       0.0575       0.0549       0.0925       0.0737         0.66      0.00687\n",
            "     48    12       0.0088      0.00877     2.51e-05       0.0558       0.0725       0.0457        0.076       0.0609       0.0573       0.0959       0.0766         0.37      0.00385\n",
            "     48    13      0.00858      0.00858     1.41e-06       0.0548       0.0717       0.0437        0.077       0.0603       0.0558       0.0958       0.0758       0.0752     0.000783\n",
            "     48    14      0.00958      0.00958     3.33e-06       0.0567       0.0757       0.0442       0.0817       0.0629        0.059        0.101       0.0801        0.128      0.00133\n",
            "     48    15      0.00892      0.00892     9.93e-07       0.0554        0.073       0.0452       0.0756       0.0604        0.058       0.0963       0.0772       0.0678     0.000706\n",
            "     48    16      0.00874      0.00873     1.38e-06       0.0552       0.0723       0.0438        0.078       0.0609        0.056        0.097       0.0765       0.0691      0.00072\n",
            "     48    17      0.00804      0.00803     5.51e-06       0.0529       0.0693        0.043       0.0728       0.0579        0.055       0.0915       0.0732        0.151      0.00157\n",
            "     48    18      0.00815      0.00812     2.89e-05       0.0533       0.0697       0.0425        0.075       0.0587       0.0542       0.0933       0.0738        0.379      0.00394\n",
            "     48    19      0.00787      0.00786     5.03e-06       0.0514       0.0686       0.0412       0.0719       0.0565       0.0528       0.0925       0.0726        0.148      0.00154\n",
            "     48    20      0.00806      0.00806     2.11e-06       0.0515       0.0695       0.0409       0.0727       0.0568       0.0534       0.0936       0.0735       0.0941     0.000981\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     48     1      0.00932      0.00932     2.72e-06       0.0565       0.0747       0.0452       0.0791       0.0621       0.0587       0.0992       0.0789        0.104      0.00108\n",
            "     48     2      0.00922      0.00922     8.86e-07       0.0553       0.0743       0.0439        0.078        0.061       0.0575       0.0997       0.0786       0.0529     0.000551\n",
            "     48     3      0.00848      0.00848     1.45e-06       0.0535       0.0712       0.0426       0.0752       0.0589        0.055       0.0958       0.0754       0.0745     0.000776\n",
            "     48     4      0.00902      0.00902     1.27e-06       0.0548       0.0735       0.0438       0.0769       0.0603       0.0566        0.099       0.0778        0.074     0.000771\n",
            "     48     5      0.00845      0.00845     1.55e-06       0.0537       0.0711       0.0436       0.0739       0.0588       0.0569       0.0933       0.0751       0.0643     0.000669\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              48  185.551    0.005      0.00945     2.28e-05      0.00948       0.0572       0.0752       0.0473        0.077       0.0621        0.061       0.0976       0.0793        0.263      0.00274\n",
            "! Validation         48  185.551    0.005       0.0089     1.58e-06       0.0089       0.0548        0.073       0.0438       0.0766       0.0602       0.0569       0.0974       0.0772       0.0739      0.00077\n",
            "Wall time: 185.55215874299984\n",
            "! Best model       48    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     49     1      0.00795      0.00791     4.38e-05       0.0516       0.0688       0.0425         0.07       0.0562       0.0542       0.0913       0.0727        0.479      0.00499\n",
            "     49     2       0.0082      0.00818     1.85e-05       0.0525         0.07       0.0424       0.0727       0.0576        0.055       0.0929        0.074        0.317      0.00331\n",
            "     49     3      0.00815      0.00814     2.44e-06       0.0527       0.0698       0.0426       0.0728       0.0577       0.0549       0.0927       0.0738        0.106       0.0011\n",
            "     49     4      0.00861       0.0086     4.85e-06        0.054       0.0718       0.0436       0.0749       0.0593       0.0569       0.0947       0.0758        0.144       0.0015\n",
            "     49     5       0.0101      0.00999     5.97e-05       0.0597       0.0773       0.0503       0.0783       0.0643       0.0636       0.0992       0.0814        0.569      0.00592\n",
            "     49     6       0.0109       0.0109     9.73e-06       0.0622       0.0806       0.0531       0.0805       0.0668       0.0681        0.101       0.0846         0.22      0.00229\n",
            "     49     7      0.00808      0.00808     9.71e-07       0.0533       0.0695       0.0431       0.0736       0.0584       0.0548       0.0922       0.0735       0.0512     0.000533\n",
            "     49     8      0.00886      0.00886     3.25e-06       0.0555       0.0728       0.0453       0.0759       0.0606       0.0571       0.0969        0.077        0.123      0.00129\n",
            "     49     9      0.00927      0.00921     5.48e-05        0.059       0.0743       0.0531       0.0709        0.062       0.0651       0.0897       0.0774        0.545      0.00568\n",
            "     49    10      0.00792      0.00791     6.98e-06       0.0525       0.0688       0.0425       0.0724       0.0574       0.0542       0.0913       0.0727        0.178      0.00185\n",
            "     49    11        0.009      0.00899     2.33e-06        0.055       0.0734       0.0458       0.0735       0.0596       0.0593       0.0955       0.0774       0.0918     0.000956\n",
            "     49    12       0.0111       0.0111     1.25e-05       0.0636       0.0816       0.0546       0.0815        0.068       0.0682        0.103       0.0857        0.257      0.00267\n",
            "     49    13       0.0138       0.0137     6.06e-05       0.0699       0.0906       0.0572       0.0953       0.0763        0.072        0.119       0.0957        0.575      0.00599\n",
            "     49    14       0.0108       0.0108     9.84e-06       0.0617       0.0804       0.0501       0.0847       0.0674       0.0632        0.107        0.085        0.229      0.00238\n",
            "     49    15       0.0082       0.0082     5.92e-06       0.0543         0.07        0.045       0.0728       0.0589       0.0568        0.091       0.0739        0.162      0.00169\n",
            "     49    16      0.00981       0.0098     1.49e-05       0.0573       0.0766        0.048        0.076        0.062       0.0631       0.0981       0.0806        0.285      0.00297\n",
            "     49    17       0.0129       0.0129     1.38e-05       0.0695        0.088       0.0636       0.0813       0.0725       0.0797        0.103       0.0911         0.26      0.00271\n",
            "     49    18      0.00934      0.00932     1.62e-05       0.0574       0.0747       0.0476       0.0771       0.0624       0.0601       0.0975       0.0788         0.28      0.00292\n",
            "     49    19      0.00752      0.00749     2.29e-05       0.0504        0.067       0.0399       0.0714       0.0556       0.0497       0.0923        0.071        0.349      0.00363\n",
            "     49    20      0.00861       0.0086     9.19e-06       0.0544       0.0717       0.0434       0.0764       0.0599       0.0558        0.096       0.0759        0.221       0.0023\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     49     1      0.00923      0.00922     2.66e-06       0.0562       0.0743       0.0449       0.0788       0.0618       0.0583       0.0988       0.0786        0.103      0.00107\n",
            "     49     2      0.00913      0.00913     9.75e-07        0.055       0.0739       0.0437       0.0778       0.0607       0.0571       0.0994       0.0782       0.0558     0.000581\n",
            "     49     3      0.00842      0.00842     1.48e-06       0.0533        0.071       0.0424       0.0751       0.0587       0.0547       0.0956       0.0751       0.0738     0.000769\n",
            "     49     4      0.00894      0.00894      1.2e-06       0.0545       0.0732       0.0435       0.0766         0.06       0.0562       0.0987       0.0774       0.0728     0.000758\n",
            "     49     5      0.00837      0.00836     1.56e-06       0.0534       0.0708       0.0433       0.0736       0.0585       0.0565       0.0929       0.0747       0.0657     0.000685\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              49  189.348    0.005      0.00944     1.87e-05      0.00945       0.0573       0.0751       0.0477       0.0766       0.0621        0.061       0.0975       0.0792        0.272      0.00283\n",
            "! Validation         49  189.348    0.005      0.00882     1.58e-06      0.00882       0.0545       0.0726       0.0436       0.0764         0.06       0.0566       0.0971       0.0768       0.0741     0.000772\n",
            "Wall time: 189.34876862500005\n",
            "! Best model       49    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     50     1      0.00875      0.00869     5.99e-05       0.0541       0.0721       0.0433       0.0758       0.0596       0.0548        0.098       0.0764        0.572      0.00596\n",
            "     50     2      0.00815      0.00814     3.98e-06       0.0528       0.0698        0.043       0.0722       0.0576       0.0544       0.0933       0.0738        0.144       0.0015\n",
            "     50     3      0.00835      0.00834     1.11e-05        0.053       0.0707       0.0429       0.0732        0.058       0.0565       0.0927       0.0746        0.234      0.00243\n",
            "     50     4      0.00785      0.00783     1.47e-05       0.0521       0.0685       0.0412       0.0739       0.0576       0.0517       0.0934       0.0725        0.279       0.0029\n",
            "     50     5      0.00758      0.00758     1.14e-06       0.0513       0.0674       0.0419       0.0702        0.056       0.0529       0.0895       0.0712       0.0781     0.000814\n",
            "     50     6      0.00724      0.00724     2.04e-06       0.0503       0.0658       0.0401       0.0706       0.0554       0.0502       0.0893       0.0697       0.0936     0.000975\n",
            "     50     7      0.00875      0.00874     1.15e-05       0.0557       0.0723       0.0483       0.0705       0.0594       0.0612       0.0906       0.0759        0.239      0.00249\n",
            "     50     8      0.00945      0.00943     1.51e-05       0.0571       0.0751       0.0484       0.0746       0.0615       0.0628       0.0951        0.079        0.264      0.00275\n",
            "     50     9      0.00779      0.00779     2.01e-06       0.0522       0.0683       0.0421       0.0724       0.0572       0.0536       0.0908       0.0722       0.0961        0.001\n",
            "     50    10         0.01      0.00998     2.07e-05       0.0594       0.0773       0.0506        0.077       0.0638        0.065       0.0973       0.0812        0.336       0.0035\n",
            "     50    11       0.0106       0.0106     7.42e-07       0.0612       0.0796       0.0507       0.0822       0.0664       0.0638        0.104        0.084       0.0539     0.000562\n",
            "     50    12      0.00799      0.00799     7.87e-06       0.0517       0.0691       0.0408       0.0736       0.0572       0.0532       0.0932       0.0732        0.195      0.00203\n",
            "     50    13      0.00878      0.00877     1.18e-05       0.0547       0.0724       0.0432       0.0777       0.0604       0.0554       0.0979       0.0767        0.222      0.00232\n",
            "     50    14      0.00744      0.00742     2.32e-05       0.0511       0.0666       0.0414       0.0704       0.0559       0.0526       0.0882       0.0704        0.353      0.00368\n",
            "     50    15      0.00942      0.00942     4.63e-07       0.0559       0.0751       0.0434       0.0808       0.0621       0.0577        0.101       0.0795       0.0346      0.00036\n",
            "     50    16      0.00838      0.00838     6.57e-06       0.0531       0.0708       0.0417        0.076       0.0589       0.0546       0.0953       0.0749         0.15      0.00156\n",
            "     50    17      0.00986      0.00985     7.38e-07       0.0582       0.0768       0.0465       0.0815        0.064       0.0585        0.104       0.0813       0.0537     0.000559\n",
            "     50    18      0.00933      0.00933     1.15e-06        0.057       0.0747       0.0447       0.0818       0.0632       0.0568        0.102       0.0792       0.0648     0.000675\n",
            "     50    19      0.00745      0.00745      1.8e-06       0.0502       0.0668       0.0404       0.0699       0.0551        0.052       0.0893       0.0706       0.0967      0.00101\n",
            "     50    20      0.00969      0.00969     1.51e-06       0.0568       0.0761       0.0447       0.0811       0.0629       0.0584        0.103       0.0806       0.0617     0.000643\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     50     1      0.00913      0.00913     2.68e-06       0.0559       0.0739       0.0446       0.0784       0.0615       0.0579       0.0984       0.0782        0.102      0.00107\n",
            "     50     2      0.00906      0.00905     1.01e-06       0.0548       0.0736       0.0435       0.0775       0.0605       0.0568       0.0991       0.0779        0.056     0.000583\n",
            "     50     3      0.00836      0.00836     1.48e-06       0.0531       0.0707       0.0422       0.0748       0.0585       0.0544       0.0953       0.0749       0.0726     0.000756\n",
            "     50     4      0.00886      0.00886     1.17e-06       0.0542       0.0728       0.0432       0.0763       0.0598       0.0558       0.0983       0.0771       0.0716     0.000746\n",
            "     50     5      0.00829      0.00829     1.53e-06       0.0532       0.0704       0.0431       0.0734       0.0582       0.0562       0.0925       0.0744       0.0648     0.000675\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              50  193.141    0.005      0.00863      9.9e-06      0.00864       0.0544       0.0719        0.044       0.0753       0.0596       0.0565       0.0955        0.076        0.181      0.00189\n",
            "! Validation         50  193.141    0.005      0.00874     1.58e-06      0.00874       0.0542       0.0723       0.0433       0.0761       0.0597       0.0562       0.0968       0.0765       0.0735     0.000765\n",
            "Wall time: 193.14152890800005\n",
            "! Best model       50    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     51     1       0.0113       0.0113     4.34e-05       0.0644       0.0822       0.0556       0.0821       0.0688       0.0713        0.101       0.0859        0.484      0.00504\n",
            "     51     2      0.00958      0.00958     7.64e-07       0.0586       0.0757       0.0493       0.0773       0.0633       0.0613       0.0984       0.0799       0.0625     0.000651\n",
            "     51     3      0.00813      0.00813     9.56e-07       0.0519       0.0698       0.0403       0.0751       0.0577       0.0512       0.0967        0.074       0.0641     0.000667\n",
            "     51     4      0.00981      0.00979     1.41e-05        0.058       0.0766       0.0464       0.0811       0.0638       0.0595        0.103        0.081        0.267      0.00278\n",
            "     51     5       0.0137       0.0136     0.000104       0.0703       0.0902       0.0618       0.0872       0.0745       0.0769        0.112       0.0945        0.748      0.00779\n",
            "     51     6        0.012       0.0119      4.1e-05       0.0663       0.0845       0.0595       0.0798       0.0696        0.075        0.101        0.088        0.475      0.00494\n",
            "     51     7      0.00842       0.0084     2.31e-05       0.0537       0.0709       0.0438       0.0735       0.0587       0.0562       0.0936       0.0749        0.355      0.00369\n",
            "     51     8      0.00873      0.00873     5.58e-06       0.0553       0.0723       0.0452       0.0755       0.0603       0.0578       0.0948       0.0763        0.124      0.00129\n",
            "     51     9      0.00942      0.00942     5.05e-06       0.0587       0.0751       0.0503       0.0754       0.0629       0.0635        0.094       0.0788        0.162      0.00169\n",
            "     51    10      0.00983      0.00982     7.31e-06       0.0588       0.0767       0.0481       0.0802       0.0641       0.0612        0.101        0.081        0.189      0.00197\n",
            "     51    11      0.00952      0.00948     4.37e-05       0.0577       0.0753       0.0461        0.081       0.0636       0.0584        0.101       0.0797         0.49       0.0051\n",
            "     51    12      0.00842      0.00842     8.49e-06       0.0537        0.071       0.0433       0.0746       0.0589       0.0549       0.0953       0.0751        0.208      0.00216\n",
            "     51    13      0.00824      0.00815     8.82e-05       0.0526       0.0699       0.0423       0.0733       0.0578       0.0545       0.0933       0.0739        0.697      0.00726\n",
            "     51    14       0.0094      0.00938     1.81e-05       0.0571       0.0749       0.0477       0.0758       0.0617       0.0615       0.0964       0.0789        0.301      0.00314\n",
            "     51    15       0.0089      0.00885     5.77e-05        0.055       0.0728       0.0441       0.0766       0.0604       0.0563       0.0977        0.077        0.561      0.00584\n",
            "     51    16       0.0085      0.00842     8.08e-05       0.0536        0.071       0.0431       0.0745       0.0588       0.0545       0.0959       0.0752        0.663      0.00691\n",
            "     51    17       0.0073       0.0073     2.03e-06        0.052       0.0661       0.0453       0.0654       0.0554       0.0567       0.0816       0.0692       0.0799     0.000832\n",
            "     51    18      0.00832      0.00824     8.18e-05       0.0531       0.0702       0.0422       0.0748       0.0585       0.0538       0.0949       0.0744        0.671      0.00699\n",
            "     51    19       0.0116       0.0116     6.47e-06       0.0623       0.0832       0.0492       0.0885       0.0689       0.0641        0.112       0.0881        0.185      0.00193\n",
            "     51    20      0.00921      0.00921     1.84e-06       0.0576       0.0742       0.0495       0.0738       0.0617       0.0633       0.0923       0.0778       0.0809     0.000842\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     51     1      0.00905      0.00905     2.66e-06       0.0556       0.0736       0.0443       0.0782       0.0612       0.0576       0.0981       0.0778        0.102      0.00107\n",
            "     51     2      0.00898      0.00898     9.87e-07       0.0546       0.0733       0.0432       0.0773       0.0603       0.0564       0.0988       0.0776       0.0544     0.000567\n",
            "     51     3       0.0083       0.0083     1.43e-06       0.0529       0.0705        0.042       0.0746       0.0583       0.0542       0.0951       0.0746       0.0733     0.000764\n",
            "     51     4      0.00878      0.00878     1.22e-06        0.054       0.0725       0.0429       0.0761       0.0595       0.0555        0.098       0.0767       0.0729      0.00076\n",
            "     51     5      0.00821      0.00821     1.53e-06       0.0529       0.0701       0.0429       0.0731        0.058       0.0559       0.0922        0.074       0.0641     0.000667\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              51  196.919    0.005      0.00948     3.17e-05      0.00952       0.0575       0.0753       0.0476       0.0773       0.0625        0.061        0.098       0.0795        0.343      0.00358\n",
            "! Validation         51  196.919    0.005      0.00866     1.57e-06      0.00867        0.054        0.072       0.0431       0.0759       0.0595       0.0559       0.0965       0.0762       0.0734     0.000765\n",
            "Wall time: 196.91940838399978\n",
            "! Best model       51    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     52     1      0.00831      0.00831     7.32e-06       0.0528       0.0705       0.0414       0.0756       0.0585       0.0527       0.0968       0.0747        0.179      0.00186\n",
            "     52     2       0.0104       0.0104        1e-05       0.0622       0.0789       0.0559       0.0748       0.0653       0.0694       0.0951       0.0822        0.221       0.0023\n",
            "     52     3      0.00891      0.00891     1.41e-06       0.0559        0.073       0.0477       0.0721       0.0599       0.0614        0.092       0.0767       0.0797      0.00083\n",
            "     52     4      0.00804      0.00804     9.52e-07       0.0526       0.0694       0.0411       0.0755       0.0583       0.0532       0.0937       0.0734       0.0598     0.000623\n",
            "     52     5      0.00778      0.00776     2.14e-05        0.052       0.0682       0.0418       0.0723       0.0571       0.0536       0.0905       0.0721        0.324      0.00338\n",
            "     52     6      0.00916      0.00916     2.22e-06       0.0564        0.074       0.0453       0.0785       0.0619       0.0584       0.0981       0.0782        0.102      0.00107\n",
            "     52     7       0.0085      0.00848     1.71e-05       0.0545       0.0712       0.0432       0.0773       0.0602       0.0542       0.0967       0.0754        0.303      0.00316\n",
            "     52     8      0.00953      0.00953     3.66e-06       0.0567       0.0755       0.0459       0.0782        0.062       0.0598       0.0998       0.0798        0.093     0.000968\n",
            "     52     9      0.00786      0.00783     3.48e-05       0.0516       0.0685        0.042       0.0708       0.0564       0.0535       0.0913       0.0724        0.437      0.00455\n",
            "     52    10      0.00909      0.00908     1.15e-06       0.0567       0.0737        0.048       0.0743       0.0611       0.0607       0.0946       0.0776       0.0744     0.000775\n",
            "     52    11      0.00902      0.00896     6.37e-05       0.0565       0.0732       0.0487        0.072       0.0604       0.0625       0.0909       0.0767        0.588      0.00612\n",
            "     52    12       0.0076      0.00757      3.1e-05       0.0514       0.0673       0.0426       0.0689       0.0558       0.0536       0.0885       0.0711        0.409      0.00426\n",
            "     52    13      0.00824      0.00822     2.08e-05       0.0525       0.0701       0.0427       0.0721       0.0574       0.0554       0.0928       0.0741        0.337      0.00351\n",
            "     52    14      0.00938      0.00931     6.73e-05       0.0576       0.0747       0.0475       0.0778       0.0627         0.06       0.0976       0.0788        0.607      0.00633\n",
            "     52    15       0.0101       0.0101     4.73e-06       0.0584       0.0779       0.0454       0.0843       0.0649       0.0582        0.107       0.0826        0.135       0.0014\n",
            "     52    16      0.00734      0.00733     8.28e-06        0.051       0.0663       0.0421       0.0688       0.0554       0.0525       0.0875         0.07        0.199      0.00208\n",
            "     52    17      0.00879      0.00879     1.23e-06       0.0535       0.0725       0.0415       0.0774       0.0595       0.0542       0.0995       0.0769       0.0711     0.000741\n",
            "     52    18      0.00875      0.00874     4.03e-06        0.056       0.0723       0.0457       0.0768       0.0612       0.0577        0.095       0.0764        0.146      0.00152\n",
            "     52    19      0.00815      0.00815     6.95e-07       0.0521       0.0698        0.041       0.0743       0.0576       0.0532       0.0948        0.074       0.0607     0.000633\n",
            "     52    20       0.0094      0.00938      1.6e-05       0.0578       0.0749        0.051       0.0714       0.0612       0.0658       0.0904       0.0781        0.276      0.00288\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     52     1      0.00896      0.00896      2.7e-06       0.0553       0.0732       0.0441       0.0778        0.061       0.0572       0.0977       0.0775        0.103      0.00107\n",
            "     52     2       0.0089       0.0089     1.07e-06       0.0543        0.073        0.043        0.077         0.06       0.0561       0.0984       0.0773       0.0571     0.000595\n",
            "     52     3      0.00824      0.00824     1.44e-06       0.0527       0.0702       0.0418       0.0744       0.0581       0.0539       0.0949       0.0744       0.0719     0.000749\n",
            "     52     4       0.0087       0.0087     1.24e-06       0.0537       0.0721       0.0427       0.0758       0.0593       0.0551       0.0977       0.0764       0.0739      0.00077\n",
            "     52     5      0.00813      0.00813     1.49e-06       0.0527       0.0697       0.0426       0.0728       0.0577       0.0555       0.0918       0.0737       0.0635     0.000661\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              52  200.686    0.005       0.0087     1.59e-05      0.00872       0.0549       0.0722        0.045       0.0747       0.0598       0.0577       0.0947       0.0762        0.235      0.00245\n",
            "! Validation         52  200.686    0.005      0.00858     1.59e-06      0.00859       0.0538       0.0717       0.0428       0.0756       0.0592       0.0556       0.0961       0.0758       0.0739      0.00077\n",
            "Wall time: 200.68634128799977\n",
            "! Best model       52    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     53     1      0.00923       0.0092      2.6e-05       0.0575       0.0742       0.0474       0.0777       0.0626       0.0604       0.0961       0.0782        0.371      0.00387\n",
            "     53     2       0.0102       0.0102      1.2e-05       0.0604       0.0782       0.0476       0.0859       0.0668       0.0599        0.106       0.0828        0.245      0.00255\n",
            "     53     3      0.00793      0.00791     1.88e-05       0.0517       0.0688        0.041       0.0732       0.0571       0.0518        0.094       0.0729        0.313      0.00326\n",
            "     53     4      0.00806      0.00804     1.84e-05       0.0536       0.0694       0.0442       0.0723       0.0582       0.0557       0.0907       0.0732          0.3      0.00312\n",
            "     53     5      0.00844      0.00844     9.13e-07       0.0545       0.0711       0.0454       0.0726        0.059       0.0567       0.0934        0.075       0.0658     0.000686\n",
            "     53     6      0.00978      0.00973      4.8e-05       0.0565       0.0763       0.0447       0.0801       0.0624       0.0584        0.103       0.0808        0.511      0.00533\n",
            "     53     7      0.00742       0.0074     2.15e-05       0.0512       0.0665       0.0414       0.0709       0.0561       0.0519       0.0889       0.0704        0.339      0.00354\n",
            "     53     8      0.00977      0.00973     4.59e-05       0.0588       0.0763       0.0499       0.0765       0.0632       0.0638       0.0966       0.0802        0.495      0.00516\n",
            "     53     9      0.00706      0.00703     3.57e-05       0.0491       0.0649       0.0382       0.0711       0.0546       0.0484       0.0891       0.0687        0.439      0.00457\n",
            "     53    10       0.0088      0.00879     8.67e-06        0.057       0.0725       0.0509       0.0692         0.06       0.0638       0.0874       0.0756        0.208      0.00217\n",
            "     53    11      0.00823      0.00821     1.87e-05       0.0546       0.0701       0.0471       0.0696       0.0584       0.0586       0.0888       0.0737        0.313      0.00326\n",
            "     53    12      0.00786      0.00786     2.39e-06       0.0513       0.0686        0.039       0.0759       0.0574       0.0503       0.0951       0.0727          0.1      0.00104\n",
            "     53    13      0.00811      0.00804     6.85e-05       0.0519       0.0694       0.0395       0.0765        0.058       0.0502        0.097       0.0736        0.614      0.00639\n",
            "     53    14      0.00853      0.00849     3.46e-05       0.0538       0.0713       0.0423       0.0768       0.0595        0.054       0.0971       0.0755        0.434      0.00452\n",
            "     53    15      0.00729      0.00721     8.01e-05       0.0487       0.0657       0.0375       0.0712       0.0543       0.0491       0.0901       0.0696         0.66      0.00688\n",
            "     53    16      0.00821      0.00813     7.37e-05       0.0525       0.0698       0.0433        0.071       0.0571       0.0558       0.0916       0.0737        0.634       0.0066\n",
            "     53    17      0.00784      0.00783     3.25e-06       0.0519       0.0685       0.0411       0.0734       0.0572       0.0525       0.0924       0.0725        0.111      0.00116\n",
            "     53    18      0.00788      0.00783     5.44e-05       0.0519       0.0685       0.0424       0.0709       0.0567       0.0542       0.0904       0.0723        0.539      0.00562\n",
            "     53    19         0.01         0.01     1.42e-06       0.0584       0.0774       0.0465       0.0821       0.0643       0.0599        0.104       0.0819       0.0715     0.000745\n",
            "     53    20      0.00804      0.00804     1.13e-06        0.052       0.0694       0.0419        0.072        0.057       0.0539       0.0928       0.0734        0.065     0.000677\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     53     1      0.00888      0.00888     2.75e-06       0.0551       0.0729       0.0438       0.0775       0.0607       0.0569       0.0973       0.0771        0.103      0.00107\n",
            "     53     2      0.00882      0.00882     1.03e-06       0.0541       0.0727       0.0428       0.0768       0.0598       0.0558       0.0981       0.0769       0.0561     0.000584\n",
            "     53     3      0.00818      0.00818     1.48e-06       0.0525         0.07       0.0416       0.0742       0.0579       0.0536       0.0946       0.0741       0.0732     0.000763\n",
            "     53     4      0.00862      0.00862     1.29e-06       0.0535       0.0718       0.0424       0.0756        0.059       0.0548       0.0974       0.0761       0.0759      0.00079\n",
            "     53     5      0.00806      0.00806     1.46e-06       0.0525       0.0694       0.0424       0.0726       0.0575       0.0552       0.0915       0.0734       0.0638     0.000664\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              53  204.460    0.005      0.00841     2.87e-05      0.00844       0.0539       0.0709       0.0436       0.0744        0.059       0.0557       0.0943        0.075        0.341      0.00356\n",
            "! Validation         53  204.460    0.005      0.00851      1.6e-06      0.00851       0.0535       0.0714       0.0426       0.0753        0.059       0.0552       0.0958       0.0755       0.0744     0.000775\n",
            "Wall time: 204.46031877799987\n",
            "! Best model       53    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     54     1      0.00731      0.00731     1.17e-06       0.0502       0.0662       0.0409       0.0688       0.0549       0.0526       0.0872       0.0699       0.0754     0.000785\n",
            "     54     2      0.00891      0.00891      6.8e-06       0.0548        0.073       0.0426       0.0792       0.0609       0.0547          0.1       0.0774        0.188      0.00196\n",
            "     54     3      0.00783      0.00782     1.01e-05       0.0532       0.0684       0.0457        0.068       0.0569       0.0577       0.0859       0.0718        0.235      0.00244\n",
            "     54     4      0.00841      0.00838     2.79e-05       0.0539       0.0708       0.0433        0.075       0.0592       0.0546       0.0953        0.075        0.378      0.00393\n",
            "     54     5      0.00803      0.00796     6.77e-05       0.0526        0.069       0.0409       0.0759       0.0584       0.0518       0.0944       0.0731        0.609      0.00634\n",
            "     54     6      0.00706      0.00703     2.25e-05         0.05       0.0649       0.0407       0.0687       0.0547       0.0511       0.0861       0.0686        0.347      0.00361\n",
            "     54     7       0.0103       0.0103     1.49e-05       0.0603       0.0785       0.0476       0.0856       0.0666       0.0609        0.105       0.0831        0.271      0.00282\n",
            "     54     8       0.0118       0.0116     0.000183        0.064       0.0835       0.0528       0.0865       0.0696       0.0682        0.108        0.088            1       0.0104\n",
            "     54     9      0.00915      0.00915        3e-06       0.0574        0.074       0.0509       0.0703       0.0606       0.0644       0.0901       0.0773       0.0959     0.000999\n",
            "     54    10      0.00837      0.00834     2.75e-05       0.0528       0.0707       0.0415       0.0754       0.0584       0.0536       0.0961       0.0748        0.387      0.00403\n",
            "     54    11      0.00874      0.00873     4.27e-06       0.0552       0.0723       0.0461       0.0735       0.0598       0.0589       0.0935       0.0762        0.153      0.00159\n",
            "     54    12      0.00959      0.00959     3.74e-06       0.0589       0.0758       0.0493       0.0781       0.0637       0.0617        0.098       0.0798         0.14      0.00145\n",
            "     54    13      0.00895      0.00894     1.23e-06       0.0555       0.0732       0.0439       0.0785       0.0612        0.056       0.0989       0.0775       0.0617     0.000643\n",
            "     54    14      0.00793      0.00785     7.38e-05       0.0513       0.0686       0.0404       0.0731       0.0568       0.0526       0.0926       0.0726        0.633       0.0066\n",
            "     54    15      0.00773      0.00772     6.99e-06       0.0515        0.068       0.0418       0.0708       0.0563       0.0527       0.0911       0.0719        0.196      0.00204\n",
            "     54    16       0.0084      0.00834     6.16e-05       0.0528       0.0706       0.0423       0.0738        0.058       0.0548       0.0947       0.0747        0.576        0.006\n",
            "     54    17      0.00932      0.00925     7.26e-05        0.055       0.0744        0.042       0.0808       0.0614       0.0547        0.103       0.0789        0.629      0.00655\n",
            "     54    18      0.00791       0.0079     7.91e-06       0.0522       0.0688       0.0416       0.0735       0.0576       0.0533       0.0922       0.0728        0.197      0.00205\n",
            "     54    19      0.00916      0.00906     9.83e-05       0.0559       0.0736       0.0473       0.0733       0.0603       0.0604       0.0947       0.0775        0.736      0.00767\n",
            "     54    20       0.0084      0.00835     5.31e-05       0.0547       0.0707       0.0465       0.0711       0.0588       0.0583       0.0905       0.0744        0.537      0.00559\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     54     1      0.00879      0.00878     2.62e-06       0.0548       0.0725       0.0435       0.0772       0.0604       0.0564        0.097       0.0767        0.101      0.00105\n",
            "     54     2      0.00874      0.00874     1.12e-06       0.0539       0.0723       0.0425       0.0765       0.0595       0.0554       0.0977       0.0766       0.0581     0.000605\n",
            "     54     3      0.00812      0.00812     1.43e-06       0.0523       0.0697       0.0414        0.074       0.0577       0.0533       0.0943       0.0738       0.0697     0.000726\n",
            "     54     4      0.00854      0.00854     1.22e-06       0.0532       0.0715       0.0422       0.0753       0.0587       0.0544        0.097       0.0757       0.0735     0.000766\n",
            "     54     5      0.00798      0.00798     1.46e-06       0.0522       0.0691       0.0421       0.0724       0.0573       0.0549       0.0911        0.073       0.0617     0.000643\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              54  208.239    0.005      0.00863     3.74e-05      0.00867       0.0546       0.0719       0.0444        0.075       0.0597       0.0568        0.095       0.0759        0.372      0.00388\n",
            "! Validation         54  208.239    0.005      0.00843     1.57e-06      0.00843       0.0533        0.071       0.0424       0.0751       0.0587       0.0549       0.0955       0.0752       0.0728     0.000758\n",
            "Wall time: 208.23976354299975\n",
            "! Best model       54    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     55     1       0.0065       0.0065     1.44e-06       0.0476       0.0624       0.0381       0.0665       0.0523       0.0479       0.0841        0.066       0.0826     0.000861\n",
            "     55     2      0.00764      0.00759     4.64e-05       0.0508       0.0674       0.0404       0.0716        0.056       0.0511       0.0917       0.0714        0.504      0.00525\n",
            "     55     3      0.00911      0.00908     3.35e-05       0.0563       0.0737       0.0462       0.0765       0.0613       0.0587        0.097       0.0778        0.422      0.00439\n",
            "     55     4      0.00794      0.00792     1.93e-05       0.0517       0.0689       0.0412       0.0728        0.057       0.0527       0.0932       0.0729        0.324      0.00338\n",
            "     55     5       0.0077      0.00769     1.73e-06       0.0514       0.0679       0.0408       0.0725       0.0566       0.0513       0.0924       0.0719       0.0852     0.000887\n",
            "     55     6      0.00806      0.00805     5.13e-06       0.0514       0.0694       0.0401       0.0739        0.057       0.0533       0.0937       0.0735        0.161      0.00168\n",
            "     55     7      0.00879      0.00879     3.72e-06       0.0552       0.0725       0.0443        0.077       0.0606       0.0569       0.0965       0.0767        0.125      0.00131\n",
            "     55     8      0.00995      0.00995     7.22e-06       0.0593       0.0772       0.0497       0.0785       0.0641       0.0626          0.1       0.0813        0.194      0.00202\n",
            "     55     9       0.0132       0.0131     0.000114       0.0694       0.0886       0.0611        0.086       0.0735       0.0767        0.109       0.0927        0.793      0.00826\n",
            "     55    10       0.0123       0.0123     1.28e-05       0.0679       0.0859       0.0606       0.0825       0.0715       0.0754        0.104       0.0896        0.247      0.00258\n",
            "     55    11       0.0081      0.00805     4.69e-05       0.0535       0.0694       0.0436       0.0732       0.0584       0.0552       0.0915       0.0733        0.505      0.00526\n",
            "     55    12      0.00753       0.0075     3.05e-05       0.0508        0.067       0.0414       0.0695       0.0554       0.0529       0.0888       0.0708        0.405      0.00422\n",
            "     55    13      0.00972      0.00971     5.79e-06        0.058       0.0762       0.0486       0.0767       0.0626       0.0626       0.0979       0.0803        0.158      0.00164\n",
            "     55    14       0.0113       0.0112     5.26e-05       0.0626        0.082       0.0519       0.0839       0.0679       0.0666        0.106       0.0865        0.536      0.00558\n",
            "     55    15      0.00855      0.00855     2.13e-06        0.053       0.0715       0.0421       0.0748       0.0585       0.0567       0.0944       0.0756       0.0977      0.00102\n",
            "     55    16      0.00785      0.00784     2.61e-06       0.0527       0.0685       0.0418       0.0743       0.0581       0.0534       0.0916       0.0725       0.0926     0.000964\n",
            "     55    17         0.01         0.01     6.77e-06       0.0596       0.0773       0.0486       0.0815       0.0651        0.061        0.103       0.0817        0.173       0.0018\n",
            "     55    18      0.00971      0.00971      6.4e-06       0.0595       0.0762       0.0526       0.0733       0.0629       0.0664       0.0928       0.0796        0.171      0.00178\n",
            "     55    19      0.00874      0.00874     9.51e-07       0.0547       0.0723       0.0448       0.0745       0.0597       0.0564       0.0965       0.0765       0.0484     0.000505\n",
            "     55    20      0.00785      0.00779     5.69e-05       0.0516       0.0683       0.0409        0.073        0.057       0.0526       0.0919       0.0723         0.55      0.00573\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     55     1      0.00871      0.00871     2.81e-06       0.0545       0.0722       0.0433        0.077       0.0601       0.0561       0.0967       0.0764        0.105      0.00109\n",
            "     55     2      0.00867      0.00867     1.02e-06       0.0536       0.0721       0.0423       0.0763       0.0593       0.0551       0.0974       0.0763       0.0553     0.000576\n",
            "     55     3      0.00806      0.00806     1.44e-06        0.052       0.0694       0.0412       0.0737       0.0575        0.053       0.0941       0.0735       0.0739      0.00077\n",
            "     55     4      0.00847      0.00847     1.31e-06        0.053       0.0712       0.0419       0.0751       0.0585       0.0541       0.0968       0.0754       0.0757     0.000788\n",
            "     55     5      0.00792      0.00791     1.41e-06        0.052       0.0688       0.0419       0.0721        0.057       0.0546       0.0908       0.0727       0.0621     0.000647\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              55  212.021    0.005      0.00901     2.28e-05      0.00903       0.0558       0.0734       0.0459       0.0756       0.0608        0.059        0.096       0.0775        0.284      0.00296\n",
            "! Validation         55  212.021    0.005      0.00836      1.6e-06      0.00837        0.053       0.0708       0.0421       0.0748       0.0585       0.0546       0.0952       0.0749       0.0743     0.000774\n",
            "Wall time: 212.02152822900007\n",
            "! Best model       55    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     56     1      0.00833      0.00828     4.27e-05       0.0533       0.0704       0.0441       0.0716       0.0579       0.0571       0.0914       0.0742        0.479      0.00499\n",
            "     56     2      0.00896      0.00895     9.09e-06        0.055       0.0732       0.0438       0.0774       0.0606       0.0568       0.0981       0.0774        0.218      0.00227\n",
            "     56     3      0.00805      0.00802     2.06e-05       0.0534       0.0693       0.0439       0.0723       0.0581       0.0553       0.0911       0.0732         0.33      0.00344\n",
            "     56     4      0.00808      0.00807     1.04e-05       0.0535       0.0695       0.0428       0.0749       0.0589       0.0537       0.0934       0.0736        0.229      0.00239\n",
            "     56     5      0.00739      0.00739     5.71e-07         0.05       0.0665       0.0394        0.071       0.0552       0.0504       0.0905       0.0704       0.0432      0.00045\n",
            "     56     6      0.00788      0.00788     3.11e-06       0.0524       0.0687       0.0421       0.0731       0.0576       0.0531       0.0923       0.0727        0.126      0.00132\n",
            "     56     7      0.00913      0.00912     9.59e-06       0.0564       0.0739       0.0461       0.0771       0.0616        0.059        0.097        0.078        0.227      0.00236\n",
            "     56     8      0.00854      0.00854     6.08e-06       0.0542       0.0715       0.0441       0.0744       0.0593       0.0556       0.0956       0.0756        0.182      0.00189\n",
            "     56     9      0.00772      0.00772     5.39e-07       0.0511        0.068       0.0406        0.072       0.0563       0.0519       0.0921        0.072       0.0439     0.000458\n",
            "     56    10       0.0125       0.0125     9.97e-07       0.0654       0.0864       0.0527       0.0908       0.0717        0.068        0.115       0.0914       0.0652      0.00068\n",
            "     56    11       0.0113       0.0112     6.02e-05       0.0655        0.082       0.0588       0.0789       0.0689       0.0715       0.0997       0.0856        0.571      0.00594\n",
            "     56    12       0.0107       0.0107     4.15e-06       0.0622       0.0799       0.0535       0.0798       0.0666       0.0681       0.0995       0.0838        0.124      0.00129\n",
            "     56    13      0.00839      0.00835     4.15e-05       0.0529       0.0707        0.041       0.0765       0.0588       0.0532       0.0966       0.0749        0.473      0.00493\n",
            "     56    14      0.00802      0.00799     2.75e-05       0.0525       0.0692       0.0408       0.0757       0.0583       0.0521       0.0945       0.0733        0.384        0.004\n",
            "     56    15      0.00743      0.00743     4.37e-06       0.0501       0.0667       0.0394       0.0716       0.0555       0.0502       0.0912       0.0707         0.15      0.00156\n",
            "     56    16      0.00696      0.00694     2.36e-05       0.0481       0.0644       0.0377        0.069       0.0533       0.0476        0.089       0.0683        0.353      0.00367\n",
            "     56    17      0.00904      0.00902     1.15e-05       0.0558       0.0735       0.0443       0.0786       0.0615       0.0578       0.0976       0.0777        0.245      0.00255\n",
            "     56    18      0.00747      0.00747      2.3e-06       0.0508       0.0669       0.0415       0.0693       0.0554       0.0523       0.0891       0.0707       0.0984      0.00103\n",
            "     56    19      0.00707      0.00706     3.17e-06       0.0497        0.065       0.0414       0.0664       0.0539       0.0522        0.085       0.0686        0.107      0.00111\n",
            "     56    20      0.00758      0.00758     3.28e-06       0.0505       0.0674       0.0393        0.073       0.0561       0.0509       0.0918       0.0714        0.113      0.00118\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     56     1      0.00864      0.00863     2.69e-06       0.0543       0.0719        0.043       0.0767       0.0599       0.0558       0.0963       0.0761        0.101      0.00106\n",
            "     56     2      0.00861      0.00861      1.1e-06       0.0534       0.0718       0.0421        0.076       0.0591       0.0549       0.0971        0.076       0.0563     0.000587\n",
            "     56     3        0.008        0.008     1.39e-06       0.0518       0.0692        0.041       0.0736       0.0573       0.0527       0.0939       0.0733       0.0696     0.000725\n",
            "     56     4      0.00841       0.0084     1.25e-06       0.0528       0.0709       0.0417       0.0749       0.0583       0.0537       0.0965       0.0751        0.074     0.000771\n",
            "     56     5      0.00785      0.00785     1.39e-06       0.0517       0.0685       0.0417       0.0719       0.0568       0.0543       0.0905       0.0724       0.0608     0.000634\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              56  215.811    0.005      0.00851     1.43e-05      0.00852       0.0541       0.0714       0.0439       0.0747       0.0593       0.0562       0.0947       0.0754        0.228      0.00238\n",
            "! Validation         56  215.811    0.005       0.0083     1.56e-06       0.0083       0.0528       0.0705       0.0419       0.0746       0.0583       0.0543       0.0949       0.0746       0.0725     0.000755\n",
            "Wall time: 215.8118335209997\n",
            "! Best model       56    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     57     1      0.00818      0.00817     2.03e-06       0.0536       0.0699       0.0436       0.0736       0.0586       0.0551       0.0927       0.0739       0.0842     0.000877\n",
            "     57     2      0.00766      0.00765     5.36e-06       0.0514       0.0677       0.0418       0.0705       0.0562       0.0533       0.0898       0.0715        0.139      0.00144\n",
            "     57     3      0.00665      0.00665     2.83e-06       0.0477       0.0631       0.0379       0.0672       0.0526       0.0481       0.0855       0.0668        0.117      0.00122\n",
            "     57     4      0.00739      0.00739     3.09e-06       0.0497       0.0665       0.0396         0.07       0.0548       0.0512       0.0897       0.0704        0.105      0.00109\n",
            "     57     5      0.00781       0.0078     4.79e-06       0.0521       0.0683       0.0414       0.0734       0.0574       0.0526       0.0921       0.0723        0.154      0.00161\n",
            "     57     6      0.00935      0.00926     8.56e-05       0.0574       0.0745       0.0451       0.0819       0.0635       0.0565        0.101       0.0789        0.684      0.00713\n",
            "     57     7      0.00876      0.00876     5.73e-06        0.054       0.0724       0.0441        0.074        0.059       0.0574       0.0955       0.0765        0.154      0.00161\n",
            "     57     8      0.00832      0.00828     3.66e-05       0.0529       0.0704       0.0423       0.0741       0.0582       0.0554       0.0935       0.0744        0.446      0.00464\n",
            "     57     9      0.00743      0.00742     6.59e-06       0.0501       0.0667         0.04       0.0703       0.0551        0.052        0.089       0.0705        0.163       0.0017\n",
            "     57    10      0.00756      0.00755     5.21e-06       0.0513       0.0672       0.0417       0.0706       0.0562       0.0531        0.089       0.0711        0.154      0.00161\n",
            "     57    11      0.00731       0.0073     1.04e-05       0.0501       0.0661       0.0399       0.0706       0.0552       0.0512       0.0886       0.0699        0.233      0.00242\n",
            "     57    12      0.00738      0.00738     5.54e-06       0.0492       0.0665       0.0374       0.0728       0.0551       0.0476       0.0933       0.0705         0.17      0.00177\n",
            "     57    13      0.00706      0.00706     6.63e-07       0.0494        0.065       0.0399       0.0684       0.0541       0.0514       0.0859       0.0687       0.0525     0.000547\n",
            "     57    14      0.00764      0.00763     1.49e-05       0.0516       0.0676       0.0408       0.0734       0.0571       0.0512        0.092       0.0716        0.285      0.00296\n",
            "     57    15      0.00801      0.00801     2.78e-06       0.0525       0.0692       0.0436       0.0704        0.057       0.0553       0.0909       0.0731       0.0982      0.00102\n",
            "     57    16      0.00818      0.00818     6.57e-06       0.0531       0.0699       0.0447       0.0697       0.0572       0.0567       0.0908       0.0738        0.189      0.00197\n",
            "     57    17      0.00948      0.00946     2.41e-05       0.0565       0.0752       0.0445       0.0804       0.0625       0.0579        0.101       0.0796        0.362      0.00377\n",
            "     57    18      0.00811      0.00811     1.73e-06       0.0527       0.0697       0.0415       0.0752       0.0583       0.0531       0.0944       0.0738       0.0799     0.000832\n",
            "     57    19      0.00989      0.00987     2.16e-05       0.0577       0.0769       0.0469       0.0795       0.0632       0.0609        0.102       0.0812        0.345      0.00359\n",
            "     57    20       0.0103       0.0102     3.27e-05       0.0596       0.0783       0.0491       0.0807       0.0649       0.0619        0.103       0.0827        0.423      0.00441\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     57     1      0.00856      0.00856     2.68e-06        0.054       0.0716       0.0428       0.0764       0.0596       0.0555        0.096       0.0757        0.101      0.00105\n",
            "     57     2      0.00854      0.00854      1.2e-06       0.0532       0.0715       0.0419       0.0759       0.0589       0.0545       0.0969       0.0757       0.0609     0.000635\n",
            "     57     3      0.00795      0.00794     1.42e-06       0.0517        0.069       0.0408       0.0733       0.0571       0.0524       0.0937        0.073        0.069     0.000719\n",
            "     57     4      0.00833      0.00833     1.27e-06       0.0525       0.0706       0.0415       0.0747       0.0581       0.0534       0.0962       0.0748       0.0735     0.000766\n",
            "     57     5      0.00778      0.00778     1.42e-06       0.0515       0.0682       0.0414       0.0717       0.0565        0.054       0.0902       0.0721       0.0617     0.000643\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              57  219.593    0.005      0.00811     1.39e-05      0.00812       0.0526       0.0697       0.0423       0.0733       0.0578       0.0542       0.0932       0.0737        0.222      0.00231\n",
            "! Validation         57  219.593    0.005      0.00823      1.6e-06      0.00823       0.0526       0.0702       0.0417       0.0744        0.058        0.054       0.0946       0.0743       0.0733     0.000763\n",
            "Wall time: 219.59359670899994\n",
            "! Best model       57    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     58     1      0.00785      0.00785     3.72e-06       0.0526       0.0685       0.0414        0.075       0.0582       0.0521       0.0931       0.0726        0.138      0.00143\n",
            "     58     2      0.00659      0.00658      1.3e-05        0.047       0.0627       0.0371        0.067        0.052       0.0474       0.0856       0.0665        0.262      0.00273\n",
            "     58     3      0.00885      0.00881      4.2e-05       0.0544       0.0726       0.0437       0.0759       0.0598       0.0563       0.0973       0.0768        0.473      0.00493\n",
            "     58     4       0.0084      0.00838     1.74e-05        0.054       0.0708       0.0447       0.0727       0.0587       0.0568       0.0927       0.0747        0.302      0.00315\n",
            "     58     5      0.00868      0.00867     1.07e-05       0.0544        0.072       0.0434       0.0765       0.0599       0.0551       0.0974       0.0763        0.223      0.00232\n",
            "     58     6      0.00771      0.00769     1.77e-05       0.0516       0.0679       0.0414        0.072       0.0567       0.0533       0.0902       0.0717        0.305      0.00318\n",
            "     58     7      0.00666      0.00663     2.48e-05       0.0486        0.063       0.0398       0.0663       0.0531       0.0493       0.0839       0.0666        0.365      0.00381\n",
            "     58     8       0.0076       0.0076     2.81e-06       0.0511       0.0674       0.0414       0.0705        0.056       0.0538       0.0887       0.0712        0.109      0.00114\n",
            "     58     9      0.00787      0.00787      7.5e-06        0.052       0.0686       0.0408       0.0742       0.0575       0.0521       0.0933       0.0727        0.198      0.00206\n",
            "     58    10       0.0101       0.0101     3.78e-05       0.0594       0.0776       0.0484       0.0812       0.0648       0.0619        0.102        0.082        0.433      0.00451\n",
            "     58    11      0.00868      0.00867     5.07e-06       0.0548        0.072       0.0475       0.0694       0.0584       0.0601       0.0914       0.0757        0.157      0.00164\n",
            "     58    12      0.00748      0.00747     6.76e-06       0.0513       0.0669       0.0408       0.0724       0.0566       0.0518       0.0897       0.0708        0.172      0.00179\n",
            "     58    13      0.00837      0.00836     5.51e-06       0.0531       0.0707       0.0409       0.0775       0.0592       0.0529       0.0971        0.075        0.166      0.00173\n",
            "     58    14      0.00716      0.00715     1.82e-05       0.0499       0.0654       0.0399       0.0698       0.0549       0.0508       0.0876       0.0692        0.307       0.0032\n",
            "     58    15      0.00868      0.00867     1.21e-05       0.0543        0.072       0.0444        0.074       0.0592       0.0578       0.0943        0.076        0.256      0.00267\n",
            "     58    16      0.00782      0.00782     3.14e-06       0.0514       0.0684       0.0408       0.0725       0.0566        0.052       0.0929       0.0725        0.117      0.00122\n",
            "     58    17       0.0078      0.00777        3e-05       0.0515       0.0682       0.0402        0.074       0.0571       0.0508       0.0937       0.0723        0.404      0.00421\n",
            "     58    18      0.00769      0.00769     5.07e-07        0.051       0.0679       0.0409       0.0713       0.0561       0.0524       0.0913       0.0718       0.0461      0.00048\n",
            "     58    19      0.00701        0.007     9.62e-06       0.0485       0.0647       0.0389       0.0677       0.0533       0.0495       0.0875       0.0685        0.216      0.00225\n",
            "     58    20      0.00752      0.00752        1e-06       0.0504       0.0671       0.0395       0.0722       0.0558       0.0512       0.0909       0.0711       0.0654     0.000682\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     58     1       0.0085      0.00849     2.69e-06       0.0538       0.0713       0.0426       0.0762       0.0594       0.0552       0.0957       0.0755        0.102      0.00106\n",
            "     58     2      0.00848      0.00847     1.12e-06        0.053       0.0712       0.0417       0.0757       0.0587       0.0542       0.0966       0.0754       0.0575     0.000599\n",
            "     58     3      0.00789      0.00789     1.41e-06       0.0515       0.0687       0.0406       0.0732       0.0569       0.0521       0.0935       0.0728       0.0702     0.000731\n",
            "     58     4      0.00827      0.00826     1.27e-06       0.0523       0.0703       0.0412       0.0744       0.0578       0.0531       0.0959       0.0745       0.0744     0.000775\n",
            "     58     5      0.00772      0.00771     1.37e-06       0.0513       0.0679       0.0412       0.0714       0.0563       0.0537       0.0899       0.0718       0.0611     0.000637\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              58  223.378    0.005      0.00791     1.35e-05      0.00793       0.0521       0.0688       0.0418       0.0726       0.0572       0.0535       0.0921       0.0728        0.236      0.00246\n",
            "! Validation         58  223.378    0.005      0.00817     1.57e-06      0.00817       0.0524       0.0699       0.0415       0.0742       0.0578       0.0537       0.0944        0.074       0.0731     0.000761\n",
            "Wall time: 223.3792562059998\n",
            "! Best model       58    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     59     1      0.00664      0.00664     3.14e-06       0.0472        0.063       0.0378       0.0661       0.0519       0.0481       0.0854       0.0668        0.119      0.00124\n",
            "     59     2      0.00713      0.00713     8.97e-07       0.0491       0.0653        0.038       0.0713       0.0546       0.0481       0.0904       0.0692        0.058     0.000604\n",
            "     59     3      0.00893      0.00892     3.12e-06        0.056       0.0731       0.0459        0.076        0.061       0.0587       0.0956       0.0771        0.112      0.00117\n",
            "     59     4      0.00872       0.0087     1.77e-05       0.0553       0.0722       0.0467       0.0724       0.0596       0.0595       0.0924        0.076        0.306      0.00319\n",
            "     59     5      0.00832      0.00832     2.53e-06       0.0523       0.0706       0.0404       0.0759       0.0582       0.0529       0.0966       0.0748        0.101      0.00105\n",
            "     59     6      0.00755      0.00754     1.25e-05       0.0515       0.0672       0.0414       0.0716       0.0565       0.0525       0.0896       0.0711         0.26       0.0027\n",
            "     59     7      0.00741       0.0074     1.06e-05       0.0513       0.0665       0.0424       0.0693       0.0558       0.0535        0.087       0.0702         0.24       0.0025\n",
            "     59     8      0.00881       0.0088     1.28e-05       0.0542       0.0726       0.0412       0.0801       0.0607       0.0531        0.101       0.0769        0.264      0.00275\n",
            "     59     9      0.00736      0.00732     4.47e-05       0.0497       0.0662       0.0399       0.0694       0.0547       0.0515       0.0885         0.07        0.496      0.00517\n",
            "     59    10      0.00795      0.00789     5.88e-05       0.0524       0.0687       0.0426       0.0722       0.0574       0.0549       0.0902       0.0726        0.561      0.00584\n",
            "     59    11      0.00667      0.00667     3.71e-06       0.0481       0.0632       0.0385       0.0673       0.0529       0.0487        0.085       0.0669        0.136      0.00142\n",
            "     59    12      0.00867      0.00865     1.94e-05       0.0539        0.072       0.0433       0.0751       0.0592       0.0555       0.0968       0.0761        0.316       0.0033\n",
            "     59    13      0.00779      0.00777     1.84e-05       0.0508       0.0682       0.0404       0.0714       0.0559       0.0514       0.0931       0.0723        0.302      0.00314\n",
            "     59    14      0.00767      0.00766     1.33e-06       0.0515       0.0677       0.0402        0.074       0.0571       0.0512       0.0923       0.0718       0.0736     0.000767\n",
            "     59    15      0.00695      0.00695     1.68e-06       0.0484       0.0645       0.0386       0.0679       0.0533       0.0488       0.0879       0.0683       0.0793     0.000826\n",
            "     59    16      0.00771       0.0077     3.52e-06       0.0514       0.0679       0.0401       0.0738        0.057       0.0521       0.0917       0.0719        0.121      0.00126\n",
            "     59    17      0.00709      0.00708     5.28e-06         0.05       0.0651       0.0409       0.0682       0.0546       0.0513       0.0863       0.0688        0.163       0.0017\n",
            "     59    18      0.00825      0.00824     1.51e-05       0.0536       0.0702       0.0427       0.0755       0.0591       0.0541       0.0945       0.0743         0.27      0.00281\n",
            "     59    19        0.008      0.00798     1.71e-05       0.0521       0.0691       0.0417       0.0729       0.0573       0.0544       0.0917       0.0731        0.292      0.00304\n",
            "     59    20      0.00964      0.00962     1.78e-05       0.0589       0.0759       0.0501       0.0765       0.0633       0.0634        0.096       0.0797        0.311      0.00323\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     59     1      0.00842      0.00842     2.61e-06       0.0535        0.071       0.0423       0.0759       0.0591       0.0548       0.0954       0.0751       0.0995      0.00104\n",
            "     59     2       0.0084       0.0084     1.24e-06       0.0528       0.0709       0.0414       0.0754       0.0584       0.0539       0.0963       0.0751       0.0607     0.000633\n",
            "     59     3      0.00784      0.00784     1.43e-06       0.0513       0.0685       0.0404        0.073       0.0567       0.0518       0.0933       0.0726       0.0698     0.000727\n",
            "     59     4      0.00819      0.00819     1.25e-06       0.0521         0.07        0.041       0.0742       0.0576       0.0528       0.0956       0.0742       0.0729     0.000759\n",
            "     59     5      0.00764      0.00764     1.38e-06       0.0511       0.0676        0.041       0.0712       0.0561       0.0533       0.0896       0.0715       0.0606     0.000632\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              59  227.163    0.005      0.00785     1.35e-05      0.00786       0.0519       0.0685       0.0417       0.0724        0.057       0.0533       0.0917       0.0725        0.229      0.00239\n",
            "! Validation         59  227.163    0.005       0.0081     1.58e-06       0.0081       0.0521       0.0696       0.0412       0.0739       0.0576       0.0533       0.0941       0.0737       0.0727     0.000757\n",
            "Wall time: 227.1636381989997\n",
            "! Best model       59    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     60     1       0.0109       0.0109     1.41e-05       0.0632       0.0808       0.0533        0.083       0.0681       0.0678        0.102       0.0848        0.271      0.00282\n",
            "     60     2       0.0122       0.0122     3.27e-05       0.0658       0.0854       0.0539       0.0894       0.0717        0.068        0.112       0.0902        0.423      0.00441\n",
            "     60     3      0.00982      0.00981     6.85e-06       0.0586       0.0766       0.0477       0.0806       0.0641       0.0602        0.102        0.081        0.185      0.00193\n",
            "     60     4      0.00815      0.00814     6.37e-06       0.0522       0.0698       0.0416       0.0736       0.0576       0.0541       0.0937       0.0739        0.182      0.00189\n",
            "     60     5      0.00722      0.00717     5.21e-05       0.0493       0.0655       0.0407       0.0663       0.0535       0.0521       0.0862       0.0692        0.534      0.00557\n",
            "     60     6      0.00859      0.00858     9.16e-06       0.0557       0.0717       0.0479       0.0714       0.0596       0.0607       0.0896       0.0752        0.206      0.00215\n",
            "     60     7      0.00801      0.00797      3.8e-05       0.0517       0.0691       0.0409       0.0731        0.057       0.0533       0.0929       0.0731        0.456      0.00475\n",
            "     60     8      0.00772      0.00766     5.77e-05       0.0514       0.0677       0.0415       0.0712       0.0563       0.0519       0.0915       0.0717        0.558      0.00581\n",
            "     60     9        0.009      0.00899      1.5e-05       0.0568       0.0733       0.0489       0.0727       0.0608       0.0617       0.0923        0.077        0.276      0.00288\n",
            "     60    10      0.00804      0.00799     4.48e-05       0.0533       0.0692       0.0434       0.0732       0.0583       0.0542       0.0921       0.0731        0.496      0.00517\n",
            "     60    11      0.00859      0.00857     2.29e-05       0.0546       0.0716        0.042       0.0797       0.0609       0.0528       0.0991       0.0759        0.347      0.00362\n",
            "     60    12      0.00701      0.00701     3.56e-06       0.0493       0.0648       0.0387       0.0705       0.0546        0.049       0.0883       0.0686        0.113      0.00117\n",
            "     60    13      0.00764      0.00764     4.79e-06       0.0511       0.0676       0.0402       0.0728       0.0565       0.0511       0.0921       0.0716        0.138      0.00144\n",
            "     60    14       0.0082      0.00816     4.52e-05       0.0536       0.0699       0.0434       0.0739       0.0587       0.0539        0.094       0.0739        0.494      0.00514\n",
            "     60    15       0.0112       0.0111     1.03e-05       0.0629       0.0817       0.0541       0.0806       0.0674       0.0683        0.103       0.0858        0.212      0.00221\n",
            "     60    16       0.0122       0.0122     3.28e-06       0.0653       0.0856       0.0536       0.0887       0.0712       0.0693        0.111       0.0902        0.121      0.00126\n",
            "     60    17      0.00999      0.00999     5.23e-06       0.0591       0.0773       0.0488       0.0796       0.0642       0.0613        0.102       0.0817        0.163       0.0017\n",
            "     60    18      0.00859      0.00859     1.57e-06       0.0538       0.0717       0.0445       0.0725       0.0585       0.0576       0.0937       0.0757       0.0834     0.000869\n",
            "     60    19      0.00865      0.00864      2.2e-06       0.0537       0.0719       0.0418       0.0776       0.0597       0.0549       0.0975       0.0762        0.101      0.00105\n",
            "     60    20        0.011       0.0109     5.19e-05       0.0626       0.0808        0.055       0.0776       0.0663       0.0704       0.0984       0.0844         0.53      0.00552\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     60     1      0.00836      0.00835     2.74e-06       0.0533       0.0707       0.0421       0.0757       0.0589       0.0545       0.0952       0.0748        0.102      0.00107\n",
            "     60     2      0.00834      0.00833     1.08e-06       0.0526       0.0706       0.0412       0.0752       0.0582       0.0536        0.096       0.0748       0.0554     0.000577\n",
            "     60     3      0.00779      0.00778     1.41e-06       0.0511       0.0683       0.0402       0.0728       0.0565       0.0516       0.0931       0.0723       0.0707     0.000736\n",
            "     60     4      0.00814      0.00813     1.31e-06       0.0519       0.0698       0.0408       0.0741       0.0574       0.0524       0.0954       0.0739       0.0756     0.000787\n",
            "     60     5      0.00758      0.00758     1.32e-06       0.0508       0.0674       0.0408        0.071       0.0559        0.053       0.0894       0.0712       0.0581     0.000605\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              60  230.942    0.005      0.00911     2.14e-05      0.00914       0.0562       0.0739       0.0461       0.0764       0.0612        0.059        0.097        0.078        0.294      0.00307\n",
            "! Validation         60  230.942    0.005      0.00804     1.57e-06      0.00804       0.0519       0.0694        0.041       0.0738       0.0574        0.053       0.0938       0.0734       0.0724     0.000755\n",
            "Wall time: 230.94271897\n",
            "! Best model       60    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     61     1       0.0133       0.0133     3.14e-06       0.0701       0.0891       0.0625       0.0853       0.0739       0.0775        0.109       0.0931        0.129      0.00134\n",
            "     61     2       0.0093      0.00929     5.06e-06       0.0568       0.0746       0.0465       0.0775        0.062       0.0596       0.0979       0.0787        0.148      0.00155\n",
            "     61     3      0.00683       0.0068      3.6e-05        0.048       0.0638       0.0375       0.0692       0.0533       0.0478       0.0874       0.0676        0.444      0.00462\n",
            "     61     4      0.00761      0.00761     1.41e-06       0.0514       0.0675       0.0406       0.0728       0.0567       0.0511       0.0919       0.0715       0.0605     0.000631\n",
            "     61     5      0.00796      0.00789     7.23e-05       0.0528       0.0687       0.0446       0.0694        0.057       0.0553       0.0897       0.0725        0.628      0.00654\n",
            "     61     6      0.00675      0.00675     2.55e-07       0.0486       0.0636       0.0402       0.0653       0.0528       0.0508       0.0834       0.0671       0.0332     0.000346\n",
            "     61     7      0.00726      0.00724     2.68e-05       0.0497       0.0658       0.0393       0.0705       0.0549       0.0506       0.0887       0.0697        0.375      0.00391\n",
            "     61     8       0.0077       0.0077     2.67e-06       0.0521       0.0679       0.0426       0.0713       0.0569       0.0541       0.0893       0.0717        0.108      0.00113\n",
            "     61     9      0.00905      0.00904     1.72e-05       0.0567       0.0735       0.0479       0.0742       0.0611       0.0602       0.0948       0.0775        0.285      0.00297\n",
            "     61    10      0.00775      0.00774     1.22e-05       0.0516       0.0681       0.0406       0.0735        0.057       0.0514       0.0928       0.0721        0.255      0.00266\n",
            "     61    11      0.00748      0.00747     1.16e-05       0.0502       0.0669       0.0409       0.0686       0.0548        0.053       0.0883       0.0707        0.247      0.00258\n",
            "     61    12      0.00978      0.00974     4.42e-05       0.0577       0.0763       0.0465       0.0802       0.0634       0.0599        0.102       0.0807        0.489      0.00509\n",
            "     61    13      0.00976      0.00974     1.27e-05       0.0595       0.0764       0.0507       0.0769       0.0638       0.0631       0.0977       0.0804         0.26      0.00271\n",
            "     61    14      0.00946      0.00945     1.97e-05       0.0569       0.0752        0.047       0.0767       0.0619       0.0599       0.0989       0.0794        0.325      0.00339\n",
            "     61    15      0.00794      0.00794     3.38e-06        0.052       0.0689       0.0399       0.0763       0.0581       0.0514       0.0947        0.073        0.105      0.00109\n",
            "     61    16      0.00885      0.00885     6.98e-06       0.0562       0.0728        0.048       0.0724       0.0602        0.061       0.0919       0.0764        0.189      0.00197\n",
            "     61    17       0.0104       0.0104      3.1e-05       0.0611       0.0788        0.053       0.0775       0.0652       0.0667       0.0987       0.0827        0.403       0.0042\n",
            "     61    18      0.00889      0.00888     9.29e-06       0.0552       0.0729        0.044       0.0777       0.0608       0.0559       0.0985       0.0772        0.215      0.00224\n",
            "     61    19      0.00858      0.00857     5.59e-06       0.0541       0.0716       0.0415       0.0793       0.0604        0.053       0.0989       0.0759        0.152      0.00158\n",
            "     61    20       0.0072      0.00719      1.5e-05       0.0495       0.0656       0.0403       0.0681       0.0542       0.0517       0.0869       0.0693        0.284      0.00296\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     61     1      0.00829      0.00829     2.67e-06       0.0531       0.0704       0.0419       0.0755       0.0587       0.0542       0.0949       0.0746        0.101      0.00105\n",
            "     61     2      0.00827      0.00827      1.1e-06       0.0523       0.0704        0.041        0.075        0.058       0.0533       0.0957       0.0745       0.0562     0.000585\n",
            "     61     3      0.00773      0.00773     1.39e-06       0.0509        0.068         0.04       0.0726       0.0563       0.0513       0.0928       0.0721       0.0695     0.000724\n",
            "     61     4      0.00807      0.00807     1.35e-06       0.0516       0.0695       0.0405       0.0739       0.0572       0.0521       0.0952       0.0737       0.0764     0.000795\n",
            "     61     5      0.00752      0.00752     1.31e-06       0.0506       0.0671       0.0405       0.0708       0.0557       0.0528       0.0891       0.0709       0.0594     0.000618\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              61  234.728    0.005      0.00858     1.68e-05      0.00859       0.0545       0.0716       0.0447       0.0741       0.0594       0.0571       0.0942       0.0757        0.257      0.00267\n",
            "! Validation         61  234.728    0.005      0.00798     1.56e-06      0.00798       0.0517       0.0691       0.0408       0.0735       0.0572       0.0528       0.0936       0.0732       0.0725     0.000755\n",
            "Wall time: 234.72834258600005\n",
            "! Best model       61    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     62     1      0.00707      0.00707      1.9e-06       0.0499       0.0651       0.0393       0.0711       0.0552       0.0493       0.0885       0.0689       0.0791     0.000824\n",
            "     62     2      0.00769      0.00766     2.29e-05        0.051       0.0677        0.041       0.0709        0.056        0.053       0.0902       0.0716        0.352      0.00366\n",
            "     62     3      0.00726      0.00724     2.06e-05       0.0495       0.0658       0.0391       0.0705       0.0548       0.0507       0.0886       0.0697        0.326       0.0034\n",
            "     62     4       0.0066       0.0066     3.07e-06       0.0468       0.0628       0.0353       0.0699       0.0526        0.045       0.0883       0.0667        0.104      0.00108\n",
            "     62     5      0.00843      0.00842     1.19e-05       0.0536        0.071        0.043       0.0749       0.0589       0.0549       0.0953       0.0751        0.254      0.00264\n",
            "     62     6      0.00723      0.00723     3.01e-06       0.0496       0.0658       0.0388        0.071       0.0549       0.0492       0.0902       0.0697        0.106       0.0011\n",
            "     62     7      0.00705      0.00704     4.69e-06       0.0502       0.0649       0.0408       0.0691       0.0549       0.0508       0.0865       0.0687        0.151      0.00158\n",
            "     62     8      0.00726      0.00726     7.43e-07       0.0497       0.0659         0.04       0.0692       0.0546       0.0512       0.0883       0.0697       0.0498     0.000519\n",
            "     62     9      0.00806      0.00803     2.79e-05       0.0532       0.0693       0.0447       0.0702       0.0574       0.0558       0.0905       0.0731        0.388      0.00404\n",
            "     62    10      0.00982      0.00982     1.78e-06       0.0587       0.0767       0.0485       0.0792       0.0638       0.0621       0.0996       0.0808       0.0857     0.000893\n",
            "     62    11      0.00706        0.007     5.15e-05       0.0487       0.0647       0.0371       0.0718       0.0545       0.0465       0.0908       0.0687         0.53      0.00552\n",
            "     62    12      0.00825      0.00822      3.2e-05       0.0522       0.0701       0.0411       0.0746       0.0578       0.0534       0.0952       0.0743        0.414      0.00432\n",
            "     62    13      0.00812      0.00808     4.03e-05       0.0523       0.0696       0.0432       0.0703       0.0568        0.056       0.0907       0.0734        0.468      0.00487\n",
            "     62    14      0.00782      0.00774     8.24e-05       0.0502       0.0681       0.0391       0.0726       0.0558       0.0499       0.0944       0.0722        0.673      0.00701\n",
            "     62    15      0.00785      0.00784     3.07e-06       0.0511       0.0685       0.0401       0.0732       0.0566       0.0527       0.0924       0.0725        0.109      0.00114\n",
            "     62    16      0.00719      0.00717     2.02e-05       0.0493       0.0655       0.0385        0.071       0.0547       0.0491       0.0897       0.0694         0.33      0.00344\n",
            "     62    17      0.00795      0.00795     1.91e-06        0.052        0.069       0.0425       0.0712       0.0568       0.0546       0.0912       0.0729       0.0922      0.00096\n",
            "     62    18      0.00892      0.00891     4.88e-06       0.0556        0.073        0.047        0.073         0.06       0.0595       0.0944        0.077        0.145      0.00151\n",
            "     62    19      0.00685      0.00684     7.12e-06       0.0487        0.064       0.0391       0.0679       0.0535       0.0491       0.0864       0.0677        0.185      0.00193\n",
            "     62    20      0.00735      0.00735     6.25e-07       0.0512       0.0663       0.0421       0.0694       0.0558       0.0532       0.0869         0.07       0.0465     0.000484\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     62     1      0.00823      0.00822      2.7e-06       0.0529       0.0702       0.0417       0.0752       0.0584       0.0539       0.0946       0.0743        0.101      0.00105\n",
            "     62     2      0.00822      0.00822     1.12e-06       0.0521       0.0701       0.0408       0.0748       0.0578       0.0531       0.0955       0.0743       0.0564     0.000588\n",
            "     62     3      0.00769      0.00769     1.39e-06       0.0507       0.0678       0.0398       0.0725       0.0562       0.0511       0.0926       0.0719       0.0689     0.000718\n",
            "     62     4      0.00801      0.00801      1.3e-06       0.0514       0.0693       0.0403       0.0737        0.057       0.0519       0.0949       0.0734       0.0752     0.000783\n",
            "     62     5      0.00747      0.00747      1.3e-06       0.0504       0.0669       0.0404       0.0706       0.0555       0.0525       0.0889       0.0707       0.0592     0.000616\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              62  238.508    0.005      0.00767     1.71e-05      0.00769       0.0512       0.0678        0.041       0.0715       0.0563       0.0525        0.091       0.0717        0.244      0.00255\n",
            "! Validation         62  238.508    0.005      0.00792     1.56e-06      0.00792       0.0515       0.0689       0.0406       0.0733        0.057       0.0525       0.0933       0.0729       0.0722     0.000752\n",
            "Wall time: 238.5091182719998\n",
            "! Best model       62    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     63     1      0.00846      0.00844     1.15e-05       0.0532       0.0711       0.0427       0.0742       0.0584       0.0558       0.0945       0.0752        0.244      0.00255\n",
            "     63     2       0.0076      0.00757     2.78e-05       0.0513       0.0673       0.0399       0.0741        0.057       0.0498       0.0929       0.0714        0.389      0.00405\n",
            "     63     3      0.00935      0.00934     1.07e-05       0.0558       0.0748       0.0427        0.082       0.0623       0.0553        0.103       0.0793        0.235      0.00245\n",
            "     63     4      0.00825      0.00825     3.39e-06       0.0542       0.0703       0.0465       0.0698       0.0581       0.0592       0.0883       0.0738        0.113      0.00118\n",
            "     63     5      0.00979      0.00978     9.91e-06       0.0592       0.0765       0.0509       0.0756       0.0633       0.0645       0.0962       0.0803        0.216      0.00225\n",
            "     63     6      0.00781      0.00781     5.39e-06       0.0526       0.0684        0.043       0.0718       0.0574       0.0546       0.0898       0.0722         0.16      0.00167\n",
            "     63     7      0.00766      0.00765     3.77e-06       0.0507       0.0677       0.0401       0.0717       0.0559       0.0525       0.0908       0.0716        0.111      0.00115\n",
            "     63     8       0.0098       0.0098     3.21e-06        0.059       0.0766       0.0498       0.0773       0.0636       0.0627       0.0986       0.0807        0.124       0.0013\n",
            "     63     9       0.0124       0.0124     1.61e-05       0.0681       0.0862       0.0614       0.0814       0.0714       0.0755        0.104       0.0899        0.284      0.00295\n",
            "     63    10      0.00844      0.00843     8.55e-06       0.0543        0.071       0.0445       0.0738       0.0591       0.0559       0.0943       0.0751        0.215      0.00224\n",
            "     63    11      0.00706      0.00706     3.81e-06       0.0487        0.065       0.0384       0.0692       0.0538       0.0494       0.0883       0.0688         0.13      0.00136\n",
            "     63    12      0.00928      0.00928     4.38e-06       0.0572       0.0745       0.0464       0.0788       0.0626       0.0589       0.0986       0.0787        0.143      0.00149\n",
            "     63    13       0.0108       0.0108     2.33e-05       0.0635       0.0802       0.0551       0.0802       0.0677       0.0682          0.1       0.0841        0.355       0.0037\n",
            "     63    14       0.0106       0.0106     1.25e-06       0.0621       0.0797       0.0544       0.0776        0.066       0.0677       0.0995       0.0836       0.0699     0.000728\n",
            "     63    15      0.00766      0.00765      4.6e-06       0.0515       0.0677       0.0427       0.0693        0.056       0.0543       0.0885       0.0714         0.15      0.00157\n",
            "     63    16      0.00753      0.00752     1.29e-05       0.0506       0.0671       0.0397       0.0725       0.0561       0.0507       0.0914       0.0711         0.26      0.00271\n",
            "     63    17       0.0103       0.0102     6.22e-05       0.0604       0.0781       0.0521       0.0771       0.0646       0.0656       0.0985        0.082        0.582      0.00607\n",
            "     63    18       0.0127       0.0127     7.98e-06       0.0681       0.0871       0.0576       0.0891       0.0733       0.0723        0.111       0.0916        0.199      0.00207\n",
            "     63    19       0.0106       0.0106     1.74e-05       0.0606       0.0796       0.0517       0.0782        0.065       0.0655        0.102       0.0838        0.308      0.00321\n",
            "     63    20      0.00684      0.00682     1.76e-05       0.0484       0.0639        0.039        0.067        0.053       0.0486       0.0867       0.0677        0.307      0.00319\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     63     1      0.00817      0.00817     2.63e-06       0.0526       0.0699       0.0415        0.075       0.0582       0.0537       0.0944        0.074          0.1      0.00104\n",
            "     63     2      0.00816      0.00816     1.22e-06       0.0519       0.0699       0.0406       0.0746       0.0576       0.0528       0.0953        0.074       0.0581     0.000605\n",
            "     63     3      0.00764      0.00764     1.41e-06       0.0505       0.0676       0.0396       0.0723        0.056       0.0508       0.0925       0.0717        0.069     0.000719\n",
            "     63     4      0.00796      0.00796      1.3e-06       0.0513        0.069       0.0401       0.0735       0.0568       0.0516       0.0947       0.0731       0.0741     0.000772\n",
            "     63     5      0.00741      0.00741     1.28e-06       0.0502       0.0666       0.0402       0.0704       0.0553       0.0522       0.0886       0.0704       0.0579     0.000603\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              63  242.323    0.005      0.00913     1.28e-05      0.00914       0.0565       0.0739       0.0469       0.0755       0.0612       0.0599       0.0961        0.078         0.23      0.00239\n",
            "! Validation         63  242.323    0.005      0.00787     1.57e-06      0.00787       0.0513       0.0686       0.0404       0.0732       0.0568       0.0522       0.0931       0.0727       0.0719     0.000749\n",
            "Wall time: 242.32414018600002\n",
            "! Best model       63    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     64     1      0.00853      0.00853     1.46e-06       0.0536       0.0714       0.0432       0.0743       0.0587       0.0562       0.0949       0.0755       0.0727     0.000757\n",
            "     64     2      0.00956       0.0095     6.36e-05       0.0591       0.0754       0.0502        0.077       0.0636       0.0618        0.097       0.0794        0.592      0.00616\n",
            "     64     3      0.00859      0.00859     2.44e-06       0.0542       0.0717       0.0444       0.0738       0.0591        0.057       0.0944       0.0757        0.115       0.0012\n",
            "     64     4      0.00728      0.00728     2.85e-06       0.0499        0.066       0.0394       0.0708       0.0551       0.0496       0.0903       0.0699        0.111      0.00115\n",
            "     64     5      0.00803      0.00802     8.69e-06       0.0519       0.0693       0.0421       0.0715       0.0568       0.0539       0.0927       0.0733        0.209      0.00218\n",
            "     64     6      0.00771       0.0077     8.41e-06       0.0504       0.0679       0.0395       0.0721       0.0558       0.0507       0.0932        0.072          0.2      0.00209\n",
            "     64     7      0.00754      0.00754     1.05e-06       0.0503       0.0672       0.0395        0.072       0.0557       0.0518       0.0904       0.0711       0.0594     0.000618\n",
            "     64     8      0.00771       0.0077     1.23e-05        0.051       0.0679       0.0416       0.0699       0.0557       0.0538       0.0897       0.0717        0.225      0.00234\n",
            "     64     9      0.00713      0.00713     1.83e-06       0.0488       0.0653       0.0395       0.0673       0.0534        0.051       0.0872       0.0691       0.0838     0.000873\n",
            "     64    10      0.00809      0.00808     2.46e-06       0.0524       0.0696       0.0414       0.0745        0.058       0.0534       0.0938       0.0736        0.112      0.00117\n",
            "     64    11      0.00657      0.00656     2.46e-06       0.0474       0.0627       0.0382        0.066       0.0521       0.0482       0.0845       0.0663        0.107      0.00112\n",
            "     64    12      0.00808      0.00808     4.81e-06       0.0518       0.0695       0.0393       0.0768       0.0581       0.0498       0.0977       0.0737        0.152      0.00158\n",
            "     64    13      0.00837      0.00837     8.88e-07       0.0526       0.0708       0.0419       0.0738       0.0579       0.0532       0.0968        0.075        0.067     0.000698\n",
            "     64    14      0.00766      0.00761     5.06e-05       0.0519       0.0675       0.0437       0.0684       0.0561       0.0551       0.0871       0.0711        0.525      0.00547\n",
            "     64    15      0.00728      0.00728     1.05e-06       0.0496        0.066       0.0409        0.067        0.054       0.0524       0.0871       0.0697       0.0691      0.00072\n",
            "     64    16      0.00788      0.00784     3.56e-05       0.0523       0.0685       0.0411       0.0745       0.0578       0.0528       0.0922       0.0725        0.442       0.0046\n",
            "     64    17      0.00791      0.00788     2.66e-05       0.0518       0.0687        0.041       0.0735       0.0572       0.0533       0.0921       0.0727        0.377      0.00393\n",
            "     64    18      0.00773      0.00773     3.17e-06       0.0497        0.068       0.0376        0.074       0.0558       0.0502        0.094       0.0721        0.116      0.00121\n",
            "     64    19      0.00843      0.00839     3.33e-05       0.0532       0.0709       0.0396       0.0805         0.06       0.0507       0.0996       0.0752        0.412      0.00429\n",
            "     64    20      0.00715      0.00714     8.57e-06       0.0495       0.0654       0.0398       0.0688       0.0543       0.0509       0.0875       0.0692        0.212      0.00221\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     64     1      0.00811       0.0081     2.65e-06       0.0524       0.0696       0.0412       0.0748        0.058       0.0534       0.0941       0.0737       0.0995      0.00104\n",
            "     64     2      0.00811      0.00811     1.19e-06       0.0518       0.0697       0.0404       0.0744       0.0574       0.0526        0.095       0.0738        0.057     0.000594\n",
            "     64     3       0.0076       0.0076     1.41e-06       0.0504       0.0674       0.0395       0.0722       0.0558       0.0506       0.0923       0.0715        0.069     0.000719\n",
            "     64     4       0.0079       0.0079     1.32e-06        0.051       0.0688       0.0399       0.0733       0.0566       0.0513       0.0944       0.0729       0.0756     0.000787\n",
            "     64     5      0.00737      0.00737     1.23e-06       0.0501       0.0664         0.04       0.0702       0.0551        0.052       0.0885       0.0702       0.0561     0.000584\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              64  246.109    0.005      0.00785     1.36e-05      0.00786       0.0516       0.0685       0.0412       0.0723       0.0568       0.0529       0.0922       0.0725        0.213      0.00222\n",
            "! Validation         64  246.109    0.005      0.00782     1.56e-06      0.00782       0.0511       0.0684       0.0402        0.073       0.0566        0.052       0.0929       0.0724       0.0714     0.000744\n",
            "Wall time: 246.10934001899977\n",
            "! Best model       64    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     65     1      0.00775      0.00775     3.54e-07       0.0508       0.0681       0.0396       0.0732       0.0564       0.0515       0.0928       0.0721       0.0314     0.000328\n",
            "     65     2      0.00784      0.00783     5.12e-06       0.0512       0.0685        0.041       0.0716       0.0563       0.0523       0.0927       0.0725        0.163      0.00169\n",
            "     65     3      0.00855      0.00853     1.64e-05       0.0551       0.0715        0.047       0.0712       0.0591       0.0593        0.091       0.0752        0.282      0.00293\n",
            "     65     4      0.00766      0.00766     1.93e-06       0.0511       0.0677       0.0413       0.0706        0.056       0.0518       0.0916       0.0717       0.0891     0.000928\n",
            "     65     5       0.0105       0.0105      7.6e-06       0.0626       0.0793       0.0578       0.0721        0.065       0.0725       0.0913       0.0819        0.201      0.00209\n",
            "     65     6      0.00963      0.00963     6.63e-07       0.0598       0.0759       0.0561       0.0673       0.0617       0.0696       0.0873       0.0784        0.041     0.000427\n",
            "     65     7      0.00793      0.00793     3.23e-07       0.0517       0.0689       0.0397       0.0759       0.0578         0.05       0.0962       0.0731       0.0406     0.000423\n",
            "     65     8      0.00763      0.00761     2.04e-05       0.0523       0.0675       0.0445       0.0679       0.0562       0.0555       0.0866        0.071        0.333      0.00347\n",
            "     65     9      0.00998      0.00998     5.51e-07       0.0586       0.0773        0.047       0.0817       0.0644       0.0602        0.103       0.0817       0.0482     0.000503\n",
            "     65    10      0.00963      0.00951     0.000126       0.0562       0.0754        0.044       0.0805       0.0622       0.0565        0.103       0.0799         0.83      0.00865\n",
            "     65    11      0.00829      0.00829     3.44e-06        0.055       0.0704       0.0461       0.0726       0.0594       0.0581       0.0902       0.0741        0.114      0.00119\n",
            "     65    12      0.00812      0.00799     0.000132       0.0527       0.0692       0.0427       0.0727       0.0577       0.0542       0.0921       0.0731        0.853      0.00888\n",
            "     65    13      0.00832      0.00831     1.23e-05       0.0546       0.0705       0.0476       0.0685       0.0581       0.0594       0.0886        0.074        0.251      0.00262\n",
            "     65    14      0.00814      0.00809     5.51e-05       0.0532       0.0696       0.0428        0.074       0.0584       0.0552       0.0917       0.0735        0.546      0.00569\n",
            "     65    15      0.00812      0.00806     5.64e-05       0.0529       0.0695       0.0438       0.0709       0.0574       0.0565       0.0899       0.0732        0.547       0.0057\n",
            "     65    16      0.00849      0.00848     1.29e-05       0.0559       0.0712       0.0489       0.0699       0.0594       0.0611       0.0881       0.0746        0.251      0.00262\n",
            "     65    17      0.00799       0.0079     8.89e-05       0.0525       0.0688       0.0426       0.0723       0.0574       0.0535        0.092       0.0728        0.697      0.00726\n",
            "     65    18      0.00743      0.00742     5.49e-06       0.0502       0.0666       0.0403       0.0698       0.0551       0.0519       0.0891       0.0705        0.145      0.00151\n",
            "     65    19      0.00818      0.00816     2.73e-05       0.0524       0.0699       0.0405       0.0763       0.0584       0.0514       0.0967       0.0741        0.369      0.00385\n",
            "     65    20      0.00826      0.00825     1.36e-05       0.0534       0.0703       0.0426       0.0748       0.0587       0.0535       0.0953       0.0744        0.258      0.00269\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     65     1      0.00807      0.00806     2.63e-06       0.0522       0.0695       0.0411       0.0746       0.0578       0.0531        0.094       0.0736        0.101      0.00105\n",
            "     65     2      0.00806      0.00806     1.09e-06       0.0516       0.0694       0.0403       0.0743       0.0573       0.0523       0.0948       0.0736       0.0549     0.000572\n",
            "     65     3      0.00756      0.00756     1.33e-06       0.0502       0.0672       0.0393        0.072       0.0557       0.0504       0.0921       0.0713       0.0689     0.000718\n",
            "     65     4      0.00786      0.00785      1.4e-06       0.0509       0.0686       0.0397       0.0732       0.0565       0.0511       0.0943       0.0727       0.0782     0.000815\n",
            "     65     5      0.00731      0.00731     1.21e-06       0.0499       0.0662       0.0398         0.07       0.0549       0.0517       0.0882         0.07       0.0566      0.00059\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              65  249.885    0.005      0.00839     2.93e-05      0.00842       0.0541       0.0709       0.0448       0.0727       0.0587        0.057       0.0926       0.0748        0.305      0.00317\n",
            "! Validation         65  249.885    0.005      0.00777     1.53e-06      0.00777        0.051       0.0682         0.04       0.0728       0.0564       0.0517       0.0927       0.0722       0.0719     0.000748\n",
            "Wall time: 249.8861688439997\n",
            "! Best model       65    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     66     1      0.00826      0.00826     3.99e-06       0.0552       0.0703       0.0472       0.0712       0.0592       0.0595        0.088       0.0738        0.144       0.0015\n",
            "     66     2      0.00692      0.00692     1.21e-06       0.0485       0.0644       0.0388       0.0679       0.0534       0.0488       0.0876       0.0682       0.0711     0.000741\n",
            "     66     3      0.00833       0.0083     3.07e-05       0.0545       0.0705       0.0447        0.074       0.0594       0.0564       0.0925       0.0744        0.409      0.00426\n",
            "     66     4      0.00674      0.00673     1.69e-06       0.0479       0.0635        0.038       0.0678       0.0529       0.0493       0.0851       0.0672        0.083     0.000865\n",
            "     66     5      0.00757      0.00755     2.93e-05       0.0509       0.0672         0.04       0.0727       0.0564        0.051       0.0913       0.0712        0.391      0.00408\n",
            "     66     6      0.00767      0.00766     3.49e-06       0.0519       0.0677       0.0405       0.0745       0.0575       0.0519       0.0915       0.0717        0.106       0.0011\n",
            "     66     7      0.00796      0.00794     1.41e-05       0.0516       0.0689       0.0402       0.0744       0.0573       0.0513       0.0948       0.0731        0.277      0.00288\n",
            "     66     8      0.00763      0.00761     1.25e-05       0.0517       0.0675       0.0432       0.0688        0.056       0.0556       0.0865       0.0711        0.249       0.0026\n",
            "     66     9      0.00801        0.008     1.18e-05       0.0532       0.0692       0.0454        0.069       0.0572       0.0576        0.088       0.0728        0.253      0.00264\n",
            "     66    10       0.0068       0.0068     4.49e-06       0.0483       0.0638        0.038        0.069       0.0535       0.0481       0.0871       0.0676        0.127      0.00132\n",
            "     66    11      0.00823      0.00815     7.71e-05        0.053       0.0698       0.0414       0.0761       0.0588       0.0522       0.0959        0.074        0.643       0.0067\n",
            "     66    12      0.00744      0.00744     1.37e-06       0.0509       0.0667       0.0409        0.071       0.0559       0.0524       0.0887       0.0705        0.077     0.000802\n",
            "     66    13      0.00889      0.00883     5.92e-05       0.0564       0.0727       0.0471       0.0752       0.0611       0.0598       0.0933       0.0766        0.568      0.00591\n",
            "     66    14      0.00687      0.00686     1.39e-05       0.0482       0.0641       0.0381       0.0685       0.0533       0.0488       0.0869       0.0678        0.275      0.00287\n",
            "     66    15      0.00921      0.00918        3e-05       0.0558       0.0741       0.0458       0.0758       0.0608       0.0589       0.0977       0.0783        0.406      0.00423\n",
            "     66    16      0.00685      0.00685     1.93e-06       0.0482        0.064       0.0381       0.0684       0.0532       0.0485       0.0871       0.0678       0.0924     0.000962\n",
            "     66    17       0.0078      0.00779     1.23e-05       0.0516       0.0683        0.041        0.073        0.057       0.0526       0.0919       0.0723        0.256      0.00267\n",
            "     66    18      0.00723      0.00722     6.12e-06       0.0475       0.0657       0.0366       0.0691       0.0529       0.0473       0.0921       0.0697        0.176      0.00184\n",
            "     66    19      0.00799      0.00796     3.61e-05        0.051        0.069       0.0401       0.0729       0.0565       0.0509       0.0954       0.0732        0.442      0.00461\n",
            "     66    20      0.00774      0.00772     1.31e-05       0.0513        0.068       0.0426       0.0689       0.0557       0.0545       0.0891       0.0718        0.262      0.00273\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     66     1      0.00801      0.00801     2.66e-06        0.052       0.0692       0.0409       0.0744       0.0576       0.0529       0.0937       0.0733       0.0996      0.00104\n",
            "     66     2      0.00801        0.008     1.14e-06       0.0514       0.0692       0.0401       0.0741       0.0571       0.0521       0.0946       0.0733       0.0557      0.00058\n",
            "     66     3      0.00752      0.00752     1.39e-06       0.0501       0.0671       0.0392       0.0719       0.0555       0.0502        0.092       0.0711       0.0702     0.000731\n",
            "     66     4       0.0078       0.0078     1.36e-06       0.0507       0.0683       0.0396        0.073       0.0563       0.0508        0.094       0.0724       0.0766     0.000798\n",
            "     66     5      0.00726      0.00726     1.21e-06       0.0497       0.0659       0.0396       0.0698       0.0547       0.0514        0.088       0.0697       0.0569     0.000593\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              66  253.685    0.005      0.00769     1.82e-05      0.00771       0.0514       0.0678       0.0414       0.0714       0.0564       0.0529       0.0906       0.0718        0.265      0.00276\n",
            "! Validation         66  253.685    0.005      0.00772     1.55e-06      0.00772       0.0508        0.068       0.0398       0.0726       0.0562       0.0515       0.0925        0.072       0.0718     0.000748\n",
            "Wall time: 253.68594257799987\n",
            "! Best model       66    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     67     1      0.00779      0.00773     5.82e-05       0.0519        0.068       0.0427       0.0703       0.0565       0.0544       0.0892       0.0718        0.559      0.00582\n",
            "     67     2      0.00728      0.00728     1.56e-06       0.0487        0.066       0.0375       0.0711       0.0543       0.0485       0.0916         0.07       0.0795     0.000828\n",
            "     67     3      0.00713      0.00712     1.36e-05       0.0501       0.0653       0.0412        0.068       0.0546        0.051        0.087        0.069        0.267      0.00278\n",
            "     67     4      0.00781      0.00778     2.21e-05       0.0513       0.0683       0.0395        0.075       0.0572       0.0506        0.094       0.0723        0.344      0.00359\n",
            "     67     5       0.0093      0.00925     5.13e-05       0.0564       0.0744       0.0451        0.079       0.0621        0.057        0.101       0.0788         0.53      0.00552\n",
            "     67     6      0.00734      0.00733      7.4e-06       0.0496       0.0662       0.0383       0.0722       0.0553       0.0496       0.0908       0.0702        0.198      0.00207\n",
            "     67     7      0.00921      0.00914      6.4e-05       0.0551        0.074       0.0429       0.0796       0.0612       0.0552        0.102       0.0784        0.584      0.00609\n",
            "     67     8       0.0114       0.0114     2.74e-06       0.0639       0.0825       0.0571       0.0773       0.0672       0.0722          0.1       0.0861        0.113      0.00118\n",
            "     67     9       0.0133       0.0132     4.67e-05       0.0708        0.089       0.0655       0.0814       0.0734       0.0808        0.103       0.0921        0.499       0.0052\n",
            "     67    10        0.011       0.0109     6.62e-05       0.0629       0.0809       0.0528       0.0833        0.068       0.0658        0.105       0.0853        0.603      0.00628\n",
            "     67    11      0.00619      0.00611     8.41e-05       0.0456       0.0605       0.0354        0.066       0.0507       0.0443       0.0839       0.0641        0.679      0.00707\n",
            "     67    12      0.00907      0.00898     9.52e-05       0.0557       0.0733        0.044       0.0789       0.0615       0.0559       0.0994       0.0776        0.723      0.00753\n",
            "     67    13       0.0117       0.0117     9.55e-05       0.0646       0.0835       0.0531       0.0874       0.0703       0.0666         0.11       0.0882        0.723      0.00753\n",
            "     67    14      0.00853      0.00852     6.98e-06       0.0563       0.0714       0.0517       0.0655       0.0586        0.065       0.0829       0.0739        0.172      0.00179\n",
            "     67    15      0.00763      0.00761      1.9e-05       0.0512       0.0675       0.0396       0.0743       0.0569       0.0507       0.0924       0.0715        0.309      0.00322\n",
            "     67    16         0.01         0.01     9.68e-07       0.0617       0.0775       0.0572       0.0706       0.0639       0.0708       0.0895       0.0801        0.067     0.000698\n",
            "     67    17       0.0111       0.0111     2.46e-06       0.0654       0.0816       0.0597       0.0767       0.0682       0.0737       0.0955       0.0846        0.109      0.00114\n",
            "     67    18      0.00898      0.00896        2e-05       0.0549       0.0732       0.0423       0.0802       0.0612       0.0546        0.101       0.0776        0.329      0.00343\n",
            "     67    19      0.00783      0.00782     4.15e-06        0.052       0.0684       0.0431       0.0698       0.0565       0.0553       0.0891       0.0722        0.137      0.00143\n",
            "     67    20       0.0105       0.0105     1.71e-05       0.0595       0.0792       0.0454       0.0878       0.0666       0.0573        0.111        0.084          0.3      0.00313\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     67     1      0.00796      0.00796     2.65e-06       0.0519        0.069       0.0407       0.0742       0.0574       0.0526       0.0935       0.0731       0.0992      0.00103\n",
            "     67     2      0.00796      0.00796     1.18e-06       0.0513        0.069       0.0399        0.074       0.0569       0.0519       0.0944       0.0731        0.057     0.000594\n",
            "     67     3      0.00747      0.00747      1.4e-06       0.0499       0.0669        0.039       0.0717       0.0553         0.05       0.0918       0.0709       0.0693     0.000722\n",
            "     67     4      0.00775      0.00775     1.37e-06       0.0505       0.0681       0.0393       0.0728       0.0561       0.0505       0.0939       0.0722       0.0771     0.000803\n",
            "     67     5      0.00722      0.00721     1.21e-06       0.0495       0.0657       0.0394       0.0697       0.0545       0.0512       0.0878       0.0695       0.0556     0.000579\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              67  257.466    0.005      0.00912      3.4e-05      0.00916       0.0564       0.0739       0.0467       0.0757       0.0612       0.0597       0.0961       0.0779        0.366      0.00382\n",
            "! Validation         67  257.466    0.005      0.00767     1.56e-06      0.00767       0.0506       0.0678       0.0397       0.0725       0.0561       0.0512       0.0923       0.0718       0.0716     0.000746\n",
            "Wall time: 257.4670379109998\n",
            "! Best model       67    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     68     1       0.0106       0.0105     1.82e-05       0.0607       0.0794       0.0495       0.0831       0.0663       0.0627        0.105       0.0839         0.31      0.00323\n",
            "     68     2      0.00867      0.00867     8.34e-06       0.0554        0.072       0.0456        0.075       0.0603       0.0581       0.0938        0.076        0.194      0.00202\n",
            "     68     3      0.00794      0.00792     2.44e-05       0.0509       0.0688       0.0394        0.074       0.0567       0.0507       0.0953        0.073         0.35      0.00365\n",
            "     68     4       0.0135       0.0135      3.5e-05       0.0717       0.0899       0.0681       0.0788       0.0734       0.0838        0.101       0.0924        0.436      0.00454\n",
            "     68     5       0.0153       0.0153     2.54e-06       0.0766       0.0958       0.0673        0.095       0.0812       0.0822        0.118          0.1        0.105      0.00109\n",
            "     68     6       0.0101         0.01     3.18e-05       0.0594       0.0775        0.048       0.0824       0.0652       0.0597        0.104        0.082        0.414      0.00432\n",
            "     68     7      0.00616      0.00615     1.73e-05       0.0461       0.0607       0.0364       0.0655        0.051        0.046       0.0825       0.0643        0.302      0.00315\n",
            "     68     8      0.00953      0.00944     8.91e-05       0.0563       0.0752       0.0453       0.0781       0.0617       0.0571        0.102       0.0796        0.699      0.00728\n",
            "     68     9      0.00941       0.0094     1.72e-05       0.0574        0.075       0.0492       0.0737       0.0615       0.0624       0.0953       0.0789        0.304      0.00317\n",
            "     68    10      0.00729      0.00727     1.79e-05       0.0496        0.066       0.0404        0.068       0.0542       0.0521       0.0873       0.0697        0.304      0.00317\n",
            "     68    11      0.00773      0.00773     1.49e-06       0.0524        0.068       0.0428       0.0716       0.0572       0.0541       0.0896       0.0719       0.0846     0.000881\n",
            "     68    12      0.00839      0.00839     1.41e-06       0.0538       0.0709       0.0442       0.0731       0.0586       0.0566        0.093       0.0748        0.073     0.000761\n",
            "     68    13      0.00781      0.00781     2.45e-06       0.0522       0.0684        0.041       0.0745       0.0577       0.0519       0.0928       0.0724        0.104      0.00108\n",
            "     68    14      0.00794      0.00794     5.54e-06       0.0531       0.0689       0.0436       0.0722       0.0579       0.0556       0.0898       0.0727        0.148      0.00154\n",
            "     68    15       0.0099      0.00988     1.81e-05       0.0583       0.0769       0.0466       0.0816       0.0641       0.0594        0.103       0.0814        0.314      0.00327\n",
            "     68    16       0.0079       0.0079     3.12e-06       0.0519       0.0687       0.0411       0.0735       0.0573       0.0518       0.0939       0.0728        0.123      0.00128\n",
            "     68    17      0.00732       0.0073     1.88e-05       0.0495       0.0661        0.038       0.0725       0.0552       0.0488       0.0913       0.0701        0.303      0.00315\n",
            "     68    18      0.00769      0.00769     5.72e-07       0.0507       0.0678       0.0403       0.0714       0.0559       0.0517        0.092       0.0718       0.0457     0.000476\n",
            "     68    19      0.00747      0.00746     8.73e-06       0.0507       0.0668       0.0416       0.0687       0.0552       0.0536       0.0875       0.0705        0.177      0.00184\n",
            "     68    20      0.00642      0.00641     1.77e-05       0.0469       0.0619       0.0371       0.0667       0.0519       0.0471       0.0841       0.0656        0.306      0.00319\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     68     1      0.00791      0.00791     2.71e-06       0.0517       0.0688       0.0405       0.0741       0.0573       0.0524       0.0934       0.0729        0.101      0.00105\n",
            "     68     2      0.00792      0.00792     1.16e-06       0.0511       0.0688       0.0398       0.0738       0.0568       0.0517       0.0942        0.073       0.0558     0.000581\n",
            "     68     3      0.00744      0.00744     1.39e-06       0.0498       0.0667       0.0388       0.0716       0.0552       0.0498       0.0917       0.0707       0.0697     0.000726\n",
            "     68     4      0.00771      0.00771     1.39e-06       0.0503       0.0679       0.0391       0.0727       0.0559       0.0503       0.0937        0.072       0.0776     0.000809\n",
            "     68     5      0.00718      0.00718     1.14e-06       0.0493       0.0655       0.0392       0.0696       0.0544        0.051       0.0877       0.0693        0.055     0.000573\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              68  261.244    0.005      0.00884      1.7e-05      0.00885       0.0552       0.0727       0.0453        0.075       0.0601       0.0581       0.0955       0.0768        0.255      0.00265\n",
            "! Validation         68  261.244    0.005      0.00763     1.56e-06      0.00763       0.0504       0.0676       0.0395       0.0724       0.0559        0.051       0.0922       0.0716       0.0718     0.000748\n",
            "Wall time: 261.2445888889997\n",
            "! Best model       68    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     69     1      0.00689      0.00688     6.06e-06       0.0488       0.0642       0.0396       0.0672       0.0534       0.0508       0.0848       0.0678        0.174      0.00181\n",
            "     69     2      0.00686      0.00686     5.68e-06       0.0478       0.0641       0.0389       0.0657       0.0523       0.0497       0.0858       0.0678        0.171      0.00178\n",
            "     69     3      0.00689      0.00689     4.85e-06        0.048       0.0642       0.0374       0.0693       0.0534       0.0476       0.0886       0.0681        0.157      0.00163\n",
            "     69     4      0.00804      0.00803     8.15e-06       0.0519       0.0693       0.0397       0.0762        0.058       0.0506       0.0964       0.0735        0.207      0.00215\n",
            "     69     5      0.00708      0.00707     4.13e-06       0.0491       0.0651       0.0388       0.0696       0.0542       0.0503       0.0874       0.0688         0.13      0.00135\n",
            "     69     6      0.00808      0.00806     1.58e-05       0.0533       0.0695       0.0436       0.0727       0.0581       0.0563       0.0902       0.0733        0.286      0.00297\n",
            "     69     7      0.00799      0.00799     3.16e-06       0.0524       0.0692       0.0415       0.0742       0.0578       0.0533       0.0931       0.0732        0.117      0.00121\n",
            "     69     8      0.00609      0.00607     1.94e-05       0.0457       0.0603        0.036       0.0651       0.0506        0.045       0.0827       0.0639        0.324      0.00338\n",
            "     69     9      0.00748      0.00748     3.94e-07       0.0511       0.0669       0.0405       0.0724       0.0565       0.0508       0.0909       0.0709       0.0361     0.000376\n",
            "     69    10      0.00758      0.00756     1.64e-05        0.051       0.0673       0.0399       0.0732       0.0566       0.0499       0.0927       0.0713        0.297       0.0031\n",
            "     69    11      0.00759      0.00759     5.42e-06       0.0508       0.0674       0.0402       0.0721       0.0561        0.051       0.0918       0.0714        0.156      0.00162\n",
            "     69    12      0.00642      0.00641     1.08e-05       0.0469        0.062       0.0375       0.0656       0.0516       0.0471       0.0841       0.0656        0.241      0.00251\n",
            "     69    13      0.00836      0.00835     4.71e-06       0.0548       0.0707       0.0463       0.0719       0.0591       0.0579        0.091       0.0745         0.15      0.00156\n",
            "     69    14       0.0096      0.00959     7.44e-06       0.0587       0.0758       0.0512       0.0736       0.0624       0.0646       0.0942       0.0794        0.193      0.00201\n",
            "     69    15       0.0073      0.00728     1.87e-05       0.0506        0.066       0.0423       0.0671       0.0547       0.0534       0.0858       0.0696        0.318      0.00331\n",
            "     69    16      0.00725      0.00722     3.18e-05       0.0502       0.0657       0.0411       0.0686       0.0548       0.0519        0.087       0.0695        0.413      0.00431\n",
            "     69    17       0.0112       0.0112     4.38e-05       0.0632       0.0817       0.0523       0.0851       0.0687        0.066        0.106       0.0862        0.486      0.00506\n",
            "     69    18       0.0139       0.0139      2.1e-05       0.0714       0.0911       0.0601       0.0939        0.077       0.0756        0.116       0.0959        0.327      0.00341\n",
            "     69    19      0.00968      0.00968     1.27e-06       0.0583       0.0761       0.0489       0.0772        0.063        0.063       0.0972       0.0801       0.0699     0.000728\n",
            "     69    20      0.00743      0.00743     1.17e-06       0.0501       0.0667       0.0396       0.0712       0.0554       0.0501       0.0912       0.0707       0.0711     0.000741\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     69     1      0.00788      0.00787     2.64e-06       0.0515       0.0686       0.0403       0.0739       0.0571       0.0522       0.0932       0.0727       0.0992      0.00103\n",
            "     69     2      0.00788      0.00788     1.16e-06        0.051       0.0687       0.0396       0.0737       0.0567       0.0515       0.0941       0.0728        0.055     0.000573\n",
            "     69     3       0.0074       0.0074     1.43e-06       0.0496       0.0666       0.0387       0.0715       0.0551       0.0496       0.0915       0.0705       0.0711     0.000741\n",
            "     69     4      0.00767      0.00767     1.46e-06       0.0502       0.0677        0.039       0.0725       0.0558       0.0501       0.0935       0.0718       0.0784     0.000817\n",
            "     69     5      0.00714      0.00713     1.17e-06       0.0492       0.0653       0.0391       0.0694       0.0542       0.0507       0.0875       0.0691       0.0545     0.000568\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              69  265.020    0.005      0.00807     1.15e-05      0.00809       0.0527       0.0695       0.0428       0.0726       0.0577       0.0548       0.0922       0.0735        0.216      0.00225\n",
            "! Validation         69  265.020    0.005      0.00759     1.57e-06      0.00759       0.0503       0.0674       0.0393       0.0722       0.0558       0.0508        0.092       0.0714       0.0716     0.000746\n",
            "Wall time: 265.0204829109998\n",
            "! Best model       69    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     70     1       0.0103       0.0103     3.83e-05        0.062       0.0785       0.0567       0.0726       0.0647       0.0704       0.0925       0.0814        0.456      0.00475\n",
            "     70     2      0.00951      0.00951     2.78e-06       0.0585       0.0754       0.0507       0.0741       0.0624       0.0641       0.0941       0.0791        0.112      0.00116\n",
            "     70     3      0.00706      0.00704     1.91e-05       0.0489       0.0649       0.0408       0.0652        0.053       0.0527       0.0842       0.0684        0.314      0.00327\n",
            "     70     4      0.00875      0.00875     2.07e-06       0.0562       0.0724       0.0489       0.0708       0.0598       0.0616       0.0901       0.0759       0.0902      0.00094\n",
            "     70     5       0.0077       0.0077      1.5e-06       0.0504       0.0679       0.0391       0.0731       0.0561       0.0504       0.0935        0.072       0.0699     0.000728\n",
            "     70     6      0.00722      0.00719     3.39e-05       0.0504       0.0656       0.0415       0.0682       0.0548       0.0521       0.0865       0.0693        0.421      0.00439\n",
            "     70     7      0.00968      0.00968     1.66e-06       0.0575       0.0761       0.0457        0.081       0.0634        0.058        0.103       0.0806       0.0914     0.000952\n",
            "     70     8      0.00877      0.00868     8.16e-05       0.0539       0.0721       0.0426       0.0765       0.0595       0.0543       0.0985       0.0764        0.669      0.00697\n",
            "     70     9       0.0067       0.0067      7.9e-07       0.0479       0.0633       0.0376       0.0685       0.0531       0.0478       0.0864       0.0671       0.0568     0.000592\n",
            "     70    10      0.00927      0.00918     9.48e-05       0.0554       0.0741       0.0441       0.0781       0.0611       0.0566          0.1       0.0785        0.722      0.00752\n",
            "     70    11       0.0078      0.00779      6.9e-06       0.0523       0.0683       0.0431       0.0705       0.0568       0.0546       0.0895       0.0721        0.179      0.00187\n",
            "     70    12      0.00631      0.00628     3.17e-05       0.0472       0.0613        0.038       0.0657       0.0518        0.048       0.0816       0.0648        0.411      0.00428\n",
            "     70    13      0.00787      0.00785      2.6e-05       0.0521       0.0685       0.0421       0.0722       0.0572       0.0537       0.0913       0.0725        0.373      0.00389\n",
            "     70    14      0.00806      0.00806     8.83e-07       0.0534       0.0694       0.0445       0.0714       0.0579       0.0563       0.0901       0.0732       0.0549     0.000572\n",
            "     70    15      0.00687      0.00686     1.29e-05       0.0487       0.0641       0.0391       0.0681       0.0536       0.0497       0.0858       0.0678        0.253      0.00263\n",
            "     70    16      0.00889      0.00888     6.12e-06       0.0568       0.0729       0.0505       0.0695         0.06       0.0637       0.0885       0.0761        0.182       0.0019\n",
            "     70    17      0.00856      0.00856     8.39e-07       0.0541       0.0716       0.0439       0.0747       0.0593       0.0562       0.0951       0.0757       0.0584     0.000608\n",
            "     70    18      0.00794      0.00794     2.91e-06       0.0524       0.0689       0.0413       0.0746        0.058       0.0519       0.0942        0.073        0.084     0.000875\n",
            "     70    19       0.0069      0.00689     8.73e-06       0.0488       0.0642        0.039       0.0684       0.0537       0.0492       0.0868        0.068        0.203      0.00212\n",
            "     70    20      0.00771       0.0077     7.99e-06       0.0514       0.0679       0.0397       0.0746       0.0572         0.05        0.094        0.072          0.2      0.00209\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     70     1      0.00783      0.00783     2.58e-06       0.0514       0.0685       0.0402       0.0737        0.057        0.052        0.093       0.0725       0.0972      0.00101\n",
            "     70     2      0.00784      0.00784     1.22e-06       0.0509       0.0685       0.0395       0.0736       0.0565       0.0513       0.0939       0.0726       0.0577     0.000601\n",
            "     70     3      0.00737      0.00737      1.4e-06       0.0495       0.0664       0.0385       0.0714        0.055       0.0494       0.0914       0.0704       0.0694     0.000723\n",
            "     70     4      0.00763      0.00762      1.4e-06         0.05       0.0676       0.0389       0.0724       0.0556       0.0499       0.0933       0.0716       0.0765     0.000797\n",
            "     70     5      0.00709      0.00709     1.19e-06        0.049       0.0651       0.0389       0.0693       0.0541       0.0505       0.0873       0.0689       0.0552     0.000575\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              70  268.800    0.005      0.00808     1.91e-05      0.00809       0.0529       0.0695       0.0434       0.0719       0.0577       0.0554       0.0915       0.0734         0.25       0.0026\n",
            "! Validation         70  268.800    0.005      0.00755     1.56e-06      0.00755       0.0502       0.0672       0.0392       0.0721       0.0556       0.0506       0.0918       0.0712       0.0712     0.000742\n",
            "Wall time: 268.80093546199987\n",
            "! Best model       70    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     71     1       0.0081      0.00809     9.88e-06        0.052       0.0696       0.0405       0.0751       0.0578       0.0527       0.0947       0.0737        0.217      0.00226\n",
            "     71     2      0.00859      0.00859     5.34e-07       0.0553       0.0717       0.0468       0.0724       0.0596       0.0589       0.0921       0.0755       0.0422     0.000439\n",
            "     71     3      0.00752      0.00752     4.95e-07        0.051       0.0671       0.0404       0.0723       0.0563        0.051       0.0911        0.071       0.0473     0.000492\n",
            "     71     4       0.0086      0.00859      7.9e-06        0.055       0.0717       0.0433       0.0782       0.0608        0.054       0.0979        0.076        0.197      0.00206\n",
            "     71     5      0.00712       0.0071     1.66e-05       0.0485       0.0652       0.0386       0.0681       0.0534         0.05        0.088        0.069        0.299      0.00312\n",
            "     71     6      0.00918      0.00915     3.57e-05       0.0558        0.074       0.0446       0.0781       0.0614       0.0567       0.0999       0.0783        0.439      0.00457\n",
            "     71     7      0.00637      0.00634      2.7e-05       0.0459       0.0616       0.0361       0.0655       0.0508       0.0458       0.0848       0.0653        0.386      0.00402\n",
            "     71     8      0.00741      0.00734      6.5e-05       0.0508       0.0663       0.0407        0.071       0.0559       0.0518       0.0885       0.0701        0.596       0.0062\n",
            "     71     9      0.00742       0.0074     2.38e-05       0.0488       0.0666        0.037       0.0723       0.0546       0.0486       0.0925       0.0706        0.353      0.00367\n",
            "     71    10      0.00672      0.00664     7.64e-05        0.049        0.063       0.0405       0.0661       0.0533       0.0505       0.0826       0.0665        0.644       0.0067\n",
            "     71    11      0.00687      0.00687     5.48e-07       0.0482       0.0641       0.0375       0.0696       0.0536       0.0477       0.0883        0.068       0.0508     0.000529\n",
            "     71    12      0.00787      0.00776     0.000104       0.0518       0.0682       0.0416       0.0722       0.0569       0.0533       0.0908       0.0721        0.756      0.00788\n",
            "     71    13      0.00771      0.00767     3.68e-05       0.0502       0.0678       0.0393       0.0721       0.0557       0.0503       0.0933       0.0718        0.446      0.00465\n",
            "     71    14      0.00712      0.00702     9.43e-05       0.0489       0.0648        0.039       0.0687       0.0538       0.0502        0.087       0.0686        0.713      0.00743\n",
            "     71    15      0.00724       0.0072     4.68e-05       0.0495       0.0656       0.0382       0.0722       0.0552        0.048       0.0912       0.0696        0.505      0.00527\n",
            "     71    16      0.00726      0.00725     1.57e-05       0.0488       0.0659       0.0385       0.0696        0.054       0.0488       0.0908       0.0698         0.29      0.00302\n",
            "     71    17       0.0081      0.00807     2.42e-05       0.0538       0.0695       0.0457       0.0702       0.0579       0.0576       0.0887       0.0731        0.357      0.00372\n",
            "     71    18      0.00723      0.00723     3.79e-06       0.0505       0.0658       0.0419       0.0678       0.0548       0.0532       0.0856       0.0694        0.126      0.00131\n",
            "     71    19      0.00703      0.00701     1.22e-05       0.0482       0.0648       0.0378       0.0689       0.0534       0.0483        0.089       0.0687        0.247      0.00257\n",
            "     71    20      0.00659      0.00658      1.4e-05        0.048       0.0628       0.0393       0.0656       0.0524       0.0502       0.0823       0.0662        0.252      0.00262\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     71     1      0.00779      0.00779     2.68e-06       0.0512       0.0683       0.0401       0.0735       0.0568       0.0518       0.0928       0.0723       0.0996      0.00104\n",
            "     71     2       0.0078      0.00779     1.26e-06       0.0507       0.0683       0.0393       0.0735       0.0564       0.0511       0.0937       0.0724       0.0579     0.000603\n",
            "     71     3      0.00734      0.00734     1.45e-06       0.0494       0.0663       0.0384       0.0714       0.0549       0.0492       0.0913       0.0702       0.0707     0.000736\n",
            "     71     4      0.00758      0.00758     1.43e-06       0.0499       0.0674       0.0387       0.0722       0.0555       0.0497       0.0931       0.0714       0.0768       0.0008\n",
            "     71     5      0.00705      0.00705     1.15e-06       0.0489       0.0649       0.0387       0.0691       0.0539       0.0503       0.0871       0.0687       0.0551     0.000574\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              71  272.596    0.005      0.00747     3.08e-05       0.0075       0.0505       0.0669       0.0404       0.0708       0.0556       0.0515       0.0901       0.0708        0.348      0.00363\n",
            "! Validation         71  272.596    0.005      0.00751     1.59e-06      0.00751         0.05        0.067       0.0391       0.0719       0.0555       0.0504       0.0916        0.071        0.072      0.00075\n",
            "Wall time: 272.59684253399973\n",
            "! Best model       71    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     72     1      0.00672      0.00671     8.48e-06       0.0474       0.0634       0.0374       0.0674       0.0524       0.0485       0.0857       0.0671        0.197      0.00205\n",
            "     72     2      0.00656      0.00656     1.87e-06       0.0467       0.0627       0.0369       0.0662       0.0516       0.0475       0.0853       0.0664       0.0863     0.000899\n",
            "     72     3       0.0067      0.00669     6.34e-06        0.048       0.0633       0.0368       0.0704       0.0536       0.0459       0.0883       0.0671        0.161      0.00168\n",
            "     72     4      0.00734      0.00734     1.72e-06       0.0497       0.0663       0.0394       0.0702       0.0548       0.0513       0.0889       0.0701         0.09     0.000938\n",
            "     72     5      0.00672      0.00672     3.31e-06       0.0481       0.0634       0.0376       0.0689       0.0533       0.0476       0.0868       0.0672        0.116      0.00121\n",
            "     72     6      0.00834      0.00834     8.86e-07       0.0534       0.0707       0.0418       0.0766       0.0592       0.0534       0.0963       0.0749       0.0639     0.000665\n",
            "     72     7      0.00713      0.00711     1.27e-05       0.0498       0.0653       0.0399       0.0698       0.0548       0.0504       0.0878       0.0691        0.248      0.00258\n",
            "     72     8      0.00686      0.00685     1.16e-05        0.049        0.064       0.0401       0.0667       0.0534       0.0501       0.0853       0.0677         0.22      0.00229\n",
            "     72     9      0.00734      0.00734     4.88e-06       0.0483       0.0663       0.0363       0.0725       0.0544       0.0472       0.0934       0.0703        0.152      0.00158\n",
            "     72    10      0.00758      0.00758     8.87e-06       0.0509       0.0673       0.0402       0.0723       0.0563       0.0517       0.0909       0.0713        0.213      0.00222\n",
            "     72    11      0.00724      0.00724     6.07e-06       0.0491       0.0658       0.0384       0.0706       0.0545        0.049       0.0905       0.0697        0.179      0.00186\n",
            "     72    12      0.00701      0.00698     2.64e-05       0.0477       0.0647        0.037       0.0692       0.0531       0.0469       0.0902       0.0686         0.38      0.00396\n",
            "     72    13      0.00621      0.00621     2.89e-07       0.0465        0.061       0.0367       0.0659       0.0513       0.0462        0.083       0.0646       0.0348     0.000362\n",
            "     72    14      0.00681       0.0068     1.62e-05       0.0478       0.0638       0.0371       0.0693       0.0532       0.0474       0.0878       0.0676        0.295      0.00307\n",
            "     72    15      0.00814      0.00813     9.49e-06       0.0525       0.0697       0.0409       0.0758       0.0584        0.052       0.0958       0.0739        0.224      0.00233\n",
            "     72    16      0.00793       0.0079     3.68e-05        0.052       0.0688        0.042       0.0719        0.057       0.0538       0.0916       0.0727        0.446      0.00465\n",
            "     72    17      0.00722      0.00722     2.56e-06       0.0501       0.0657       0.0396        0.071       0.0553       0.0512       0.0879       0.0695          0.1      0.00104\n",
            "     72    18      0.00638      0.00637     5.03e-06       0.0465       0.0617       0.0363       0.0669       0.0516       0.0456       0.0853       0.0655        0.161      0.00167\n",
            "     72    19      0.00665      0.00664     2.12e-06       0.0478       0.0631       0.0384       0.0666       0.0525       0.0484       0.0851       0.0668        0.082     0.000854\n",
            "     72    20       0.0073       0.0073     7.57e-07       0.0495       0.0661       0.0386       0.0711       0.0549       0.0499       0.0902         0.07        0.059     0.000614\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     72     1      0.00774      0.00774     2.64e-06        0.051       0.0681       0.0399       0.0733       0.0566       0.0516       0.0925       0.0721       0.0974      0.00101\n",
            "     72     2      0.00774      0.00774     1.28e-06       0.0506       0.0681       0.0392       0.0733       0.0563       0.0508       0.0935       0.0721       0.0601     0.000626\n",
            "     72     3       0.0073       0.0073     1.44e-06       0.0492       0.0661       0.0382       0.0712       0.0547        0.049       0.0911       0.0701       0.0698     0.000727\n",
            "     72     4      0.00754      0.00754     1.39e-06       0.0498       0.0672       0.0386        0.072       0.0553       0.0495       0.0929       0.0712       0.0753     0.000784\n",
            "     72     5      0.00701      0.00701     1.17e-06       0.0487       0.0648       0.0386        0.069       0.0538       0.0501       0.0869       0.0685       0.0544     0.000567\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              72  276.375    0.005       0.0071     8.32e-06      0.00711        0.049       0.0652       0.0386         0.07       0.0543       0.0493       0.0889       0.0691        0.175      0.00183\n",
            "! Validation         72  276.375    0.005      0.00746     1.58e-06      0.00747       0.0499       0.0668       0.0389       0.0718       0.0553       0.0502       0.0914       0.0708       0.0714     0.000744\n",
            "Wall time: 276.37556366599983\n",
            "! Best model       72    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     73     1      0.00704      0.00704     3.73e-06       0.0485       0.0649       0.0381       0.0692       0.0537       0.0481       0.0895       0.0688        0.128      0.00133\n",
            "     73     2      0.00678      0.00678     1.37e-06       0.0482       0.0637       0.0389       0.0669       0.0529       0.0498       0.0849       0.0674       0.0789     0.000822\n",
            "     73     3      0.00835      0.00834     1.47e-05        0.053       0.0706       0.0417       0.0758       0.0587        0.053       0.0968       0.0749        0.275      0.00286\n",
            "     73     4      0.00877      0.00877     8.21e-06       0.0548       0.0724       0.0438        0.077       0.0604       0.0549       0.0985       0.0767        0.191      0.00199\n",
            "     73     5      0.00765      0.00765     2.22e-06       0.0507       0.0676       0.0413       0.0696       0.0555       0.0538       0.0891       0.0714       0.0969      0.00101\n",
            "     73     6      0.00787      0.00787     7.65e-06       0.0518       0.0686       0.0414       0.0727       0.0571       0.0528       0.0925       0.0726        0.186      0.00194\n",
            "     73     7      0.00924      0.00924     1.08e-06       0.0577       0.0744       0.0473       0.0785       0.0629       0.0593       0.0978       0.0785       0.0617     0.000643\n",
            "     73     8      0.00993      0.00993     3.04e-06       0.0596       0.0771       0.0506       0.0775       0.0641       0.0638       0.0984       0.0811        0.121      0.00126\n",
            "     73     9       0.0086      0.00857     3.59e-05       0.0536       0.0716       0.0429        0.075       0.0589       0.0553       0.0963       0.0758        0.443      0.00461\n",
            "     73    10      0.00662      0.00662     5.77e-07       0.0472        0.063       0.0369       0.0678       0.0523       0.0474        0.086       0.0667       0.0451      0.00047\n",
            "     73    11      0.00911      0.00908     3.16e-05       0.0569       0.0737       0.0456       0.0795       0.0626        0.056          0.1       0.0781        0.416      0.00433\n",
            "     73    12      0.00886      0.00886     1.13e-06        0.057       0.0728       0.0469       0.0772       0.0621       0.0588       0.0948       0.0768       0.0742     0.000773\n",
            "     73    13      0.00697      0.00695     1.69e-05       0.0487       0.0645       0.0403       0.0654       0.0528       0.0513        0.085       0.0681        0.284      0.00296\n",
            "     73    14      0.00741      0.00741     8.38e-07       0.0503       0.0666        0.039       0.0727       0.0559       0.0495       0.0917       0.0706       0.0594     0.000618\n",
            "     73    15      0.00913      0.00906      7.2e-05       0.0568       0.0736       0.0477       0.0749       0.0613       0.0605       0.0946       0.0775         0.63      0.00657\n",
            "     73    16      0.00956      0.00955     1.47e-06       0.0585       0.0756       0.0482       0.0793       0.0637       0.0603       0.0995       0.0799       0.0711     0.000741\n",
            "     73    17      0.00706      0.00706     8.13e-07       0.0486        0.065       0.0387       0.0685       0.0536       0.0494       0.0884       0.0689       0.0514     0.000535\n",
            "     73    18      0.00765      0.00763     2.07e-05       0.0514       0.0676       0.0389       0.0762       0.0576       0.0497       0.0936       0.0717        0.328      0.00342\n",
            "     73    19      0.00976      0.00974     1.98e-05       0.0589       0.0764       0.0488        0.079       0.0639       0.0613       0.0999       0.0806        0.324      0.00338\n",
            "     73    20       0.0109       0.0109     3.82e-05       0.0629       0.0807       0.0543       0.0801       0.0672       0.0676        0.102       0.0847        0.454      0.00472\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     73     1       0.0077      0.00769     2.63e-06       0.0509       0.0679       0.0397       0.0732       0.0565       0.0514       0.0924       0.0719       0.0972      0.00101\n",
            "     73     2       0.0077       0.0077     1.21e-06       0.0504       0.0679        0.039       0.0732       0.0561       0.0506       0.0933       0.0719       0.0567     0.000591\n",
            "     73     3      0.00726      0.00726      1.4e-06       0.0491       0.0659       0.0381       0.0711       0.0546       0.0488        0.091       0.0699         0.07     0.000729\n",
            "     73     4       0.0075      0.00749     1.44e-06       0.0496        0.067       0.0384       0.0719       0.0552       0.0493       0.0927        0.071       0.0774     0.000807\n",
            "     73     5      0.00696      0.00696     1.15e-06       0.0486       0.0646       0.0384       0.0688       0.0536       0.0499       0.0868       0.0683       0.0537     0.000559\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              73  280.150    0.005      0.00835     1.41e-05      0.00836       0.0538       0.0707       0.0436       0.0741       0.0589       0.0554       0.0941       0.0748        0.216      0.00225\n",
            "! Validation         73  280.150    0.005      0.00742     1.57e-06      0.00742       0.0497       0.0666       0.0387       0.0716       0.0552         0.05       0.0913       0.0706        0.071      0.00074\n",
            "Wall time: 280.15048875599996\n",
            "! Best model       73    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     74     1      0.00741       0.0074      9.2e-06       0.0499       0.0666       0.0393        0.071       0.0552       0.0508       0.0901       0.0705        0.204      0.00212\n",
            "     74     2      0.00726      0.00726     7.11e-06       0.0501       0.0659       0.0387       0.0728       0.0558       0.0491       0.0906       0.0698        0.186      0.00193\n",
            "     74     3      0.00816      0.00816     9.46e-07       0.0528       0.0699       0.0433       0.0717       0.0575       0.0552       0.0925       0.0739       0.0598     0.000623\n",
            "     74     4      0.00744      0.00742     1.07e-05         0.05       0.0667       0.0381        0.074        0.056       0.0483       0.0931       0.0707        0.238      0.00248\n",
            "     74     5      0.00749      0.00742     6.25e-05       0.0497       0.0667       0.0383       0.0726       0.0554       0.0495       0.0918       0.0707        0.583      0.00608\n",
            "     74     6       0.0075       0.0075     3.02e-06       0.0499        0.067       0.0389       0.0718       0.0554       0.0501       0.0919        0.071        0.104      0.00108\n",
            "     74     7       0.0085      0.00835     0.000154       0.0542       0.0707        0.044       0.0746       0.0593       0.0552       0.0943       0.0748        0.919      0.00957\n",
            "     74     8      0.00799      0.00796        3e-05       0.0516        0.069       0.0422       0.0705       0.0564       0.0552       0.0906       0.0729        0.403       0.0042\n",
            "     74     9      0.00723      0.00722     8.33e-06       0.0499       0.0658       0.0396       0.0706       0.0551       0.0502        0.089       0.0696        0.201       0.0021\n",
            "     74    10      0.00804        0.008     4.53e-05       0.0525       0.0692       0.0427       0.0719       0.0573       0.0554       0.0907        0.073        0.494      0.00515\n",
            "     74    11      0.00777      0.00776     4.96e-06       0.0521       0.0682       0.0434       0.0696       0.0565       0.0546       0.0893        0.072        0.145      0.00151\n",
            "     74    12       0.0073      0.00727     2.84e-05       0.0498        0.066       0.0393       0.0708       0.0551       0.0499       0.0899       0.0699        0.392      0.00409\n",
            "     74    13      0.00712      0.00711     2.23e-06       0.0496       0.0653       0.0399       0.0689       0.0544       0.0507       0.0873        0.069       0.0965      0.00101\n",
            "     74    14      0.00797       0.0079      6.8e-05       0.0515       0.0688       0.0387       0.0772        0.058       0.0498       0.0961       0.0729         0.61      0.00636\n",
            "     74    15        0.009      0.00899     1.18e-05       0.0563       0.0734       0.0459       0.0771       0.0615       0.0574       0.0977       0.0776        0.248      0.00258\n",
            "     74    16      0.00701      0.00695     5.76e-05       0.0495       0.0645       0.0411       0.0664       0.0537       0.0519       0.0842       0.0681        0.562      0.00585\n",
            "     74    17      0.00646      0.00644     1.78e-05       0.0461       0.0621       0.0362       0.0659       0.0511        0.047       0.0845       0.0658        0.307      0.00319\n",
            "     74    18      0.00693      0.00693     1.91e-06       0.0475       0.0644       0.0368       0.0688       0.0528       0.0484       0.0881       0.0682       0.0906     0.000944\n",
            "     74    19      0.00728      0.00726     2.42e-05       0.0497       0.0659       0.0393       0.0706       0.0549       0.0508       0.0887       0.0698        0.351      0.00366\n",
            "     74    20      0.00676      0.00671      4.6e-05       0.0488       0.0634        0.039       0.0684       0.0537       0.0488       0.0854       0.0671        0.495      0.00515\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     74     1      0.00765      0.00765     2.58e-06       0.0507       0.0677       0.0396        0.073       0.0563       0.0512       0.0922       0.0717       0.0956     0.000996\n",
            "     74     2      0.00766      0.00766     1.32e-06       0.0502       0.0677       0.0388        0.073       0.0559       0.0504       0.0931       0.0717       0.0599     0.000624\n",
            "     74     3      0.00722      0.00722     1.44e-06        0.049       0.0657       0.0379        0.071       0.0545       0.0486       0.0908       0.0697       0.0695     0.000724\n",
            "     74     4      0.00745      0.00745     1.47e-06       0.0494       0.0668       0.0383       0.0718        0.055        0.049       0.0925       0.0708       0.0771     0.000804\n",
            "     74     5      0.00692      0.00692     1.13e-06       0.0484       0.0644       0.0383       0.0687       0.0535       0.0496       0.0866       0.0681       0.0541     0.000564\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              74  283.931    0.005       0.0075     2.97e-05      0.00753       0.0506        0.067       0.0402       0.0713       0.0557       0.0515       0.0904       0.0709        0.334      0.00348\n",
            "! Validation         74  283.931    0.005      0.00738     1.59e-06      0.00738       0.0496       0.0665       0.0386       0.0715        0.055       0.0498       0.0911       0.0704       0.0713     0.000742\n",
            "Wall time: 283.9321672819997\n",
            "! Best model       74    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     75     1      0.00702      0.00702     5.51e-07       0.0504       0.0648       0.0411        0.069       0.0551       0.0512       0.0857       0.0685       0.0479     0.000498\n",
            "     75     2      0.00718      0.00715     3.48e-05       0.0488       0.0654       0.0394       0.0676       0.0535       0.0507       0.0876       0.0692        0.434      0.00452\n",
            "     75     3      0.00799      0.00799     2.77e-06       0.0509       0.0691       0.0398        0.073       0.0564       0.0526       0.0939       0.0732        0.114      0.00119\n",
            "     75     4      0.00864      0.00863     1.09e-05       0.0559       0.0719       0.0486       0.0705       0.0595        0.061       0.0897       0.0754        0.236      0.00246\n",
            "     75     5      0.00633      0.00633     3.56e-07       0.0472       0.0616       0.0391       0.0634       0.0513        0.049        0.081        0.065       0.0297     0.000309\n",
            "     75     6      0.00789      0.00787     2.21e-05       0.0518       0.0686       0.0427         0.07       0.0564        0.056       0.0886       0.0723         0.33      0.00344\n",
            "     75     7      0.00873      0.00872     1.17e-05       0.0569       0.0723       0.0489       0.0729       0.0609       0.0611       0.0905       0.0758        0.242      0.00252\n",
            "     75     8      0.00912       0.0091     1.82e-05       0.0557       0.0738       0.0454       0.0764       0.0609        0.057       0.0992       0.0781        0.315      0.00328\n",
            "     75     9      0.00733       0.0073     2.89e-05       0.0504       0.0661       0.0392       0.0727       0.0559       0.0493       0.0908       0.0701        0.394       0.0041\n",
            "     75    10      0.00717      0.00715     1.91e-05       0.0491       0.0654       0.0391       0.0691       0.0541       0.0493       0.0892       0.0693        0.319      0.00333\n",
            "     75    11      0.00923      0.00922     1.41e-06        0.056       0.0743       0.0455        0.077       0.0612        0.058       0.0992       0.0786       0.0654     0.000682\n",
            "     75    12      0.00911       0.0091     7.89e-06       0.0568       0.0738        0.048       0.0744       0.0612        0.061       0.0944       0.0777        0.191      0.00199\n",
            "     75    13      0.00713      0.00712     8.46e-06       0.0491       0.0653       0.0399       0.0675       0.0537       0.0513       0.0867        0.069        0.209      0.00218\n",
            "     75    14      0.00799      0.00797     1.55e-05       0.0524       0.0691       0.0411       0.0752       0.0581       0.0523        0.094       0.0732        0.282      0.00294\n",
            "     75    15      0.00914      0.00914     2.89e-07       0.0578        0.074       0.0494       0.0745       0.0619       0.0616        0.094       0.0778        0.035     0.000364\n",
            "     75    16      0.00914      0.00914     2.66e-06       0.0577       0.0739       0.0493       0.0743       0.0618       0.0608       0.0949       0.0779        0.101      0.00105\n",
            "     75    17       0.0073      0.00728     1.53e-05         0.05        0.066       0.0393       0.0715       0.0554       0.0495       0.0904         0.07        0.288        0.003\n",
            "     75    18      0.00695      0.00694     1.72e-05       0.0485       0.0644       0.0386       0.0685       0.0535       0.0497       0.0867       0.0682        0.274      0.00286\n",
            "     75    19      0.00963      0.00961      1.5e-05       0.0568       0.0758       0.0429       0.0846       0.0637       0.0552        0.106       0.0804        0.281      0.00293\n",
            "     75    20      0.00804      0.00804     3.62e-06       0.0526       0.0694       0.0426       0.0727       0.0577       0.0545       0.0922       0.0733        0.121      0.00126\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     75     1      0.00762      0.00762     2.62e-06       0.0506       0.0675       0.0395       0.0729       0.0562        0.051        0.092       0.0715       0.0977      0.00102\n",
            "     75     2      0.00762      0.00762     1.18e-06       0.0501       0.0675       0.0387       0.0729       0.0558       0.0502       0.0929       0.0716       0.0562     0.000585\n",
            "     75     3      0.00719      0.00719      1.4e-06       0.0488       0.0656       0.0378       0.0709       0.0544       0.0484       0.0907       0.0695       0.0695     0.000724\n",
            "     75     4      0.00742      0.00741     1.45e-06       0.0493       0.0666       0.0381       0.0717       0.0549       0.0488       0.0924       0.0706       0.0778     0.000811\n",
            "     75     5      0.00688      0.00688     1.15e-06       0.0483       0.0642       0.0381       0.0686       0.0533       0.0494       0.0864       0.0679       0.0538     0.000561\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              75  287.711    0.005      0.00804     1.18e-05      0.00805       0.0527       0.0694        0.043       0.0722       0.0576       0.0548       0.0919       0.0733        0.216      0.00225\n",
            "! Validation         75  287.711    0.005      0.00734     1.56e-06      0.00735       0.0494       0.0663       0.0384       0.0714       0.0549       0.0496       0.0909       0.0703        0.071      0.00074\n",
            "Wall time: 287.712104707\n",
            "! Best model       75    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     76     1      0.00663      0.00663        2e-06       0.0474        0.063       0.0376       0.0669       0.0523       0.0485       0.0848       0.0667       0.0877     0.000913\n",
            "     76     2      0.00704      0.00704     2.38e-06       0.0488       0.0649       0.0377        0.071       0.0544       0.0477       0.0899       0.0688        0.102      0.00107\n",
            "     76     3      0.00863      0.00863     7.89e-07       0.0552       0.0719       0.0462       0.0733       0.0598        0.057       0.0949       0.0759       0.0512     0.000533\n",
            "     76     4       0.0101       0.0101     2.19e-05       0.0598       0.0777       0.0481       0.0832       0.0656       0.0606        0.104       0.0822        0.335      0.00349\n",
            "     76     5      0.00828      0.00826     1.56e-05       0.0537       0.0703       0.0433       0.0744       0.0588       0.0551       0.0936       0.0744        0.286      0.00298\n",
            "     76     6      0.00652       0.0065     1.33e-05       0.0469       0.0624       0.0357       0.0695       0.0526       0.0453        0.087       0.0662        0.264      0.00275\n",
            "     76     7      0.00762      0.00761     3.74e-06       0.0509       0.0675         0.04       0.0726       0.0563       0.0508       0.0923       0.0715        0.137      0.00142\n",
            "     76     8      0.00682      0.00681      3.2e-06       0.0479       0.0639       0.0377       0.0682        0.053       0.0482       0.0871       0.0677        0.127      0.00132\n",
            "     76     9      0.00596      0.00595     4.67e-06       0.0449       0.0597       0.0329       0.0687       0.0508       0.0419       0.0847       0.0633        0.154      0.00161\n",
            "     76    10       0.0072       0.0072     5.23e-06       0.0493       0.0656       0.0382       0.0716       0.0549       0.0491         0.09       0.0695        0.155      0.00162\n",
            "     76    11      0.00718      0.00717     8.64e-06       0.0493       0.0655       0.0398       0.0683        0.054       0.0511       0.0875       0.0693        0.206      0.00215\n",
            "     76    12      0.00696      0.00696     2.41e-06       0.0494       0.0645       0.0388       0.0704       0.0546       0.0487        0.088       0.0684          0.1      0.00104\n",
            "     76    13      0.00703      0.00702        1e-05       0.0493       0.0648       0.0391       0.0697       0.0544       0.0489       0.0884       0.0686         0.23       0.0024\n",
            "     76    14      0.00743      0.00743     2.29e-06       0.0504       0.0667       0.0407       0.0699       0.0553       0.0514       0.0897       0.0706       0.0973      0.00101\n",
            "     76    15      0.00625      0.00624      1.2e-05       0.0449       0.0611       0.0345       0.0656       0.0501       0.0451       0.0845       0.0648        0.246      0.00256\n",
            "     76    16      0.00787      0.00787     2.04e-06        0.051       0.0686       0.0397       0.0737       0.0567        0.051       0.0945       0.0728       0.0691      0.00072\n",
            "     76    17       0.0076      0.00755     4.65e-05       0.0506       0.0672       0.0406       0.0706       0.0556       0.0521       0.0902       0.0712        0.502      0.00523\n",
            "     76    18        0.007      0.00697     2.53e-05       0.0489       0.0646       0.0391       0.0685       0.0538       0.0495       0.0873       0.0684        0.368      0.00383\n",
            "     76    19      0.00723      0.00721     1.37e-05       0.0487       0.0657       0.0386       0.0688       0.0537       0.0505       0.0886       0.0696        0.261      0.00272\n",
            "     76    20      0.00692      0.00691     7.51e-06       0.0486       0.0643        0.039       0.0677       0.0533       0.0496       0.0866       0.0681        0.193      0.00201\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     76     1      0.00758      0.00758     2.62e-06       0.0505       0.0673       0.0393       0.0727        0.056       0.0508       0.0919       0.0713        0.097      0.00101\n",
            "     76     2      0.00759      0.00759     1.22e-06         0.05       0.0674       0.0386       0.0728       0.0557       0.0501       0.0928       0.0714       0.0565     0.000589\n",
            "     76     3      0.00716      0.00716     1.43e-06       0.0487       0.0654       0.0376       0.0708       0.0542       0.0482       0.0906       0.0694       0.0708     0.000738\n",
            "     76     4      0.00738      0.00737     1.45e-06       0.0492       0.0664        0.038       0.0715       0.0548       0.0486       0.0922       0.0704       0.0774     0.000807\n",
            "     76     5      0.00684      0.00684     1.12e-06       0.0481        0.064        0.038       0.0684       0.0532       0.0492       0.0863       0.0677       0.0525     0.000547\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              76  291.484    0.005       0.0073     1.02e-05      0.00731       0.0498       0.0661       0.0394       0.0706        0.055       0.0503       0.0898         0.07        0.199      0.00207\n",
            "! Validation         76  291.484    0.005      0.00731     1.57e-06      0.00731       0.0493       0.0661       0.0383       0.0713       0.0548       0.0494       0.0908       0.0701       0.0709     0.000738\n",
            "Wall time: 291.48431510599994\n",
            "! Best model       76    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     77     1       0.0063       0.0063     1.42e-06       0.0463       0.0614       0.0354       0.0681       0.0517       0.0443        0.086       0.0651       0.0719     0.000749\n",
            "     77     2      0.00776      0.00776     3.33e-06        0.051       0.0681       0.0401       0.0729       0.0565       0.0511       0.0934       0.0722        0.124      0.00129\n",
            "     77     3      0.00644      0.00644     5.43e-06       0.0468       0.0621       0.0384       0.0637       0.0511       0.0486       0.0827       0.0656        0.155      0.00162\n",
            "     77     4      0.00615      0.00615     7.71e-07        0.045       0.0607       0.0349       0.0653       0.0501       0.0443       0.0844       0.0643       0.0629     0.000655\n",
            "     77     5      0.00755      0.00753        2e-05         0.05       0.0671       0.0403       0.0693       0.0548       0.0516       0.0905        0.071        0.322      0.00335\n",
            "     77     6      0.00751      0.00749     1.23e-05       0.0502        0.067        0.038       0.0744       0.0562       0.0488       0.0932        0.071        0.256      0.00267\n",
            "     77     7      0.00633      0.00632     1.05e-05       0.0461       0.0615       0.0358       0.0667       0.0513       0.0456       0.0847       0.0652        0.235      0.00245\n",
            "     77     8      0.00695      0.00694     3.36e-06       0.0489       0.0645        0.038       0.0705       0.0543       0.0485       0.0882       0.0683        0.134       0.0014\n",
            "     77     9      0.00783      0.00782     5.46e-06       0.0521       0.0684        0.041       0.0742       0.0576       0.0518       0.0932       0.0725         0.16      0.00166\n",
            "     77    10      0.00828      0.00827     5.39e-06       0.0527       0.0704       0.0404       0.0773       0.0589       0.0512        0.098       0.0746        0.165      0.00172\n",
            "     77    11      0.00766      0.00765     4.03e-06       0.0512       0.0677       0.0418         0.07       0.0559       0.0538       0.0891       0.0715        0.134       0.0014\n",
            "     77    12      0.00705      0.00705     4.87e-06       0.0496       0.0649       0.0394         0.07       0.0547       0.0498       0.0877       0.0688        0.145      0.00151\n",
            "     77    13      0.00631      0.00629     2.05e-05       0.0461       0.0613        0.036       0.0664       0.0512       0.0468       0.0831        0.065        0.319      0.00332\n",
            "     77    14      0.00626      0.00625     1.35e-05       0.0465       0.0611        0.037       0.0656       0.0513        0.047       0.0825       0.0647        0.247      0.00257\n",
            "     77    15      0.00747      0.00746     9.68e-06       0.0502       0.0668       0.0404       0.0697        0.055       0.0522       0.0892       0.0707        0.207      0.00216\n",
            "     77    16      0.00728      0.00728     2.09e-06       0.0507        0.066       0.0419       0.0684       0.0551       0.0527       0.0867       0.0697       0.0904     0.000942\n",
            "     77    17      0.00642      0.00642     6.55e-06       0.0479        0.062       0.0383        0.067       0.0527       0.0481       0.0831       0.0656        0.174      0.00181\n",
            "     77    18      0.00613      0.00613     1.61e-06       0.0461       0.0606       0.0375       0.0632       0.0504       0.0467       0.0815       0.0641       0.0887     0.000924\n",
            "     77    19      0.00807      0.00806     3.28e-06       0.0528       0.0695       0.0431       0.0722       0.0577       0.0554       0.0914       0.0734        0.118      0.00122\n",
            "     77    20       0.0072       0.0072     3.12e-06       0.0491       0.0656       0.0365       0.0745       0.0555       0.0462        0.093       0.0696        0.127      0.00132\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     77     1      0.00754      0.00753      2.6e-06       0.0503       0.0671       0.0392       0.0725       0.0559       0.0506       0.0916       0.0711       0.0976      0.00102\n",
            "     77     2      0.00754      0.00754     1.24e-06       0.0499       0.0672       0.0384       0.0727       0.0556       0.0498       0.0926       0.0712       0.0571     0.000595\n",
            "     77     3      0.00713      0.00712     1.39e-06       0.0486       0.0653       0.0375       0.0707       0.0541        0.048       0.0905       0.0692       0.0692     0.000721\n",
            "     77     4      0.00734      0.00734     1.49e-06        0.049       0.0663       0.0379       0.0714       0.0546       0.0485       0.0921       0.0703        0.078     0.000813\n",
            "     77     5      0.00681       0.0068     1.11e-06        0.048       0.0638       0.0379       0.0683       0.0531        0.049       0.0861       0.0676        0.053     0.000552\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              77  295.267    0.005      0.00704     6.86e-06      0.00705        0.049       0.0649       0.0387       0.0695       0.0541       0.0493       0.0882       0.0688        0.167      0.00174\n",
            "! Validation         77  295.267    0.005      0.00727     1.57e-06      0.00727       0.0492       0.0659       0.0382       0.0711       0.0546       0.0492       0.0906       0.0699        0.071      0.00074\n",
            "Wall time: 295.2677337079999\n",
            "! Best model       77    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     78     1      0.00657      0.00656     5.27e-06       0.0474       0.0627       0.0378       0.0667       0.0523       0.0475       0.0852       0.0664        0.123      0.00128\n",
            "     78     2       0.0083      0.00829     1.41e-05       0.0541       0.0704       0.0436       0.0753       0.0594       0.0541        0.095       0.0745        0.274      0.00285\n",
            "     78     3      0.00854      0.00853     1.37e-05       0.0546       0.0714        0.044       0.0759       0.0599       0.0557       0.0954       0.0756        0.275      0.00286\n",
            "     78     4      0.00744      0.00743     3.61e-07        0.051       0.0667       0.0424       0.0683       0.0554        0.054       0.0867       0.0704       0.0385     0.000401\n",
            "     78     5      0.00683      0.00682     9.55e-06       0.0479       0.0639       0.0373       0.0691       0.0532       0.0477       0.0878       0.0677        0.224      0.00233\n",
            "     78     6      0.00856      0.00856     5.26e-07       0.0552       0.0716       0.0475       0.0708       0.0591       0.0595       0.0911       0.0753       0.0367     0.000382\n",
            "     78     7      0.00764      0.00764     2.29e-06       0.0517       0.0676       0.0418       0.0716       0.0567       0.0532       0.0898       0.0715        0.107      0.00111\n",
            "     78     8      0.00803      0.00803     5.22e-07       0.0524       0.0693       0.0423       0.0724       0.0574       0.0543       0.0923       0.0733       0.0461      0.00048\n",
            "     78     9      0.00683      0.00682     1.41e-06       0.0487       0.0639       0.0403       0.0656       0.0529       0.0512       0.0838       0.0675       0.0531     0.000553\n",
            "     78    10      0.00668      0.00668     7.33e-07       0.0478       0.0632       0.0373       0.0686        0.053       0.0473       0.0866        0.067        0.058     0.000604\n",
            "     78    11      0.00616      0.00616     3.56e-06       0.0448       0.0607       0.0341       0.0661       0.0501       0.0428       0.0859       0.0644        0.131      0.00136\n",
            "     78    12      0.00773      0.00772      2.5e-06       0.0507        0.068       0.0389       0.0742       0.0566         0.05       0.0942       0.0721       0.0855     0.000891\n",
            "     78    13      0.00804        0.008     3.88e-05       0.0522       0.0692       0.0421       0.0724       0.0572       0.0544       0.0919       0.0731        0.456      0.00475\n",
            "     78    14      0.00716      0.00715     9.92e-06       0.0494       0.0654       0.0397       0.0689       0.0543       0.0502       0.0884       0.0693        0.224      0.00234\n",
            "     78    15      0.00699      0.00691     7.39e-05       0.0482       0.0643       0.0369       0.0706       0.0538       0.0463       0.0901       0.0682        0.636      0.00662\n",
            "     78    16      0.00886      0.00885     1.17e-05       0.0552       0.0728       0.0483        0.069       0.0587       0.0621       0.0904       0.0763        0.227      0.00236\n",
            "     78    17      0.00751      0.00748     3.76e-05       0.0507       0.0669       0.0406        0.071       0.0558       0.0524        0.089       0.0707        0.449      0.00468\n",
            "     78    18      0.00663      0.00661     1.53e-05       0.0479       0.0629       0.0385       0.0666       0.0526       0.0484       0.0848       0.0666        0.279      0.00291\n",
            "     78    19      0.00718      0.00714     4.03e-05       0.0509       0.0654       0.0417       0.0693       0.0555       0.0526       0.0854        0.069        0.466      0.00486\n",
            "     78    20       0.0092      0.00915     4.71e-05       0.0556        0.074       0.0435       0.0798       0.0616       0.0551        0.102       0.0784        0.502      0.00523\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     78     1      0.00749      0.00749     2.66e-06       0.0502        0.067       0.0391       0.0724       0.0557       0.0504       0.0915       0.0709       0.0974      0.00101\n",
            "     78     2      0.00749      0.00749     1.22e-06       0.0497        0.067       0.0383       0.0726       0.0554       0.0496       0.0924        0.071       0.0563     0.000587\n",
            "     78     3      0.00709      0.00709     1.45e-06       0.0484       0.0651       0.0373       0.0706        0.054       0.0478       0.0903       0.0691       0.0707     0.000736\n",
            "     78     4       0.0073       0.0073     1.49e-06       0.0489       0.0661       0.0377       0.0712       0.0545       0.0483       0.0919       0.0701       0.0784     0.000817\n",
            "     78     5      0.00677      0.00677     1.07e-06       0.0479       0.0637       0.0377       0.0682       0.0529       0.0488       0.0859       0.0674       0.0521     0.000543\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              78  299.052    0.005      0.00753     1.65e-05      0.00754       0.0508       0.0671       0.0409       0.0706       0.0558       0.0521       0.0899        0.071        0.235      0.00244\n",
            "! Validation         78  299.052    0.005      0.00723     1.58e-06      0.00723        0.049       0.0658        0.038        0.071       0.0545        0.049       0.0904       0.0697        0.071      0.00074\n",
            "Wall time: 299.052273538\n",
            "! Best model       78    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     79     1      0.00878      0.00871     6.53e-05       0.0543       0.0722       0.0447       0.0737       0.0592       0.0565       0.0962       0.0764        0.599      0.00624\n",
            "     79     2      0.00735      0.00733     1.87e-05       0.0498       0.0662       0.0389       0.0717       0.0553       0.0492       0.0913       0.0702        0.317      0.00331\n",
            "     79     3      0.00768      0.00764     4.24e-05       0.0503       0.0676       0.0381       0.0748       0.0564       0.0489       0.0945       0.0717        0.476      0.00496\n",
            "     79     4      0.00821      0.00821     2.44e-06       0.0543       0.0701       0.0454       0.0719       0.0587       0.0572       0.0906       0.0739       0.0955     0.000995\n",
            "     79     5      0.00746      0.00744     2.14e-05       0.0517       0.0667       0.0427       0.0696       0.0562       0.0539        0.087       0.0704        0.329      0.00343\n",
            "     79     6      0.00751       0.0075     9.74e-06       0.0504        0.067       0.0393       0.0725       0.0559       0.0495       0.0925        0.071        0.227      0.00236\n",
            "     79     7      0.00709      0.00705     3.71e-05       0.0494        0.065       0.0407        0.067       0.0538       0.0525       0.0845       0.0685        0.441      0.00459\n",
            "     79     8      0.00718      0.00715     3.23e-05       0.0484       0.0654       0.0373       0.0706        0.054       0.0477        0.091       0.0694        0.415      0.00433\n",
            "     79     9      0.00734      0.00729     5.28e-05       0.0488       0.0661       0.0372       0.0718       0.0545       0.0487       0.0913         0.07        0.537      0.00559\n",
            "     79    10      0.00673      0.00667     6.25e-05       0.0476       0.0632       0.0371       0.0686       0.0529        0.047        0.087        0.067        0.583      0.00608\n",
            "     79    11      0.00767      0.00763      3.8e-05       0.0508       0.0676       0.0397       0.0731       0.0564       0.0511       0.0921       0.0716        0.451       0.0047\n",
            "     79    12      0.00854      0.00852     2.34e-05       0.0561       0.0714       0.0499       0.0684       0.0592        0.062       0.0872       0.0746        0.356      0.00371\n",
            "     79    13      0.00774      0.00774      1.5e-06       0.0531       0.0681       0.0467       0.0661       0.0564       0.0583       0.0843       0.0713       0.0674     0.000702\n",
            "     79    14      0.00679      0.00677     2.07e-05        0.048       0.0637       0.0366       0.0707       0.0537       0.0467       0.0883       0.0675        0.329      0.00343\n",
            "     79    15      0.00852      0.00852     4.13e-07       0.0551       0.0714       0.0467       0.0719       0.0593       0.0589       0.0915       0.0752       0.0381     0.000397\n",
            "     79    16      0.00801      0.00797      4.1e-05       0.0518       0.0691       0.0419       0.0716       0.0567       0.0528       0.0935       0.0731        0.472      0.00492\n",
            "     79    17      0.00683      0.00683     1.11e-06       0.0472       0.0639       0.0362       0.0692       0.0527       0.0471       0.0885       0.0678        0.076     0.000791\n",
            "     79    18       0.0084      0.00834     6.36e-05       0.0544       0.0706       0.0436        0.076       0.0598       0.0553       0.0941       0.0747        0.587      0.00612\n",
            "     79    19      0.00912      0.00912     2.35e-06       0.0569       0.0739       0.0475       0.0758       0.0616       0.0591       0.0969        0.078        0.102      0.00106\n",
            "     79    20      0.00719      0.00718     6.16e-06       0.0501       0.0656       0.0411       0.0681       0.0546       0.0519       0.0867       0.0693        0.169      0.00176\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     79     1      0.00745      0.00745     2.58e-06         0.05       0.0668       0.0389       0.0722       0.0556       0.0502       0.0913       0.0708       0.0953     0.000993\n",
            "     79     2      0.00745      0.00745     1.23e-06       0.0496       0.0668       0.0382       0.0724       0.0553       0.0494       0.0922       0.0708       0.0571     0.000595\n",
            "     79     3      0.00706      0.00706     1.41e-06       0.0483        0.065       0.0372       0.0705       0.0539       0.0476       0.0902       0.0689       0.0703     0.000732\n",
            "     79     4      0.00727      0.00727     1.51e-06       0.0488       0.0659       0.0376       0.0712       0.0544       0.0481       0.0918       0.0699       0.0778     0.000811\n",
            "     79     5      0.00673      0.00673     1.05e-06       0.0477       0.0635       0.0376       0.0681       0.0528       0.0486       0.0858       0.0672       0.0516     0.000537\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              79  302.834    0.005      0.00768     2.71e-05      0.00771       0.0514       0.0678       0.0416       0.0712       0.0564       0.0529       0.0905       0.0717        0.333      0.00347\n",
            "! Validation         79  302.834    0.005      0.00719     1.56e-06      0.00719       0.0489       0.0656       0.0379       0.0709       0.0544       0.0488       0.0903       0.0695       0.0704     0.000734\n",
            "Wall time: 302.8345795179998\n",
            "! Best model       79    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     80     1      0.00652       0.0065     2.02e-05       0.0471       0.0624       0.0368       0.0675       0.0522       0.0463       0.0859       0.0661        0.329      0.00343\n",
            "     80     2      0.00776      0.00775     8.73e-06       0.0516       0.0681       0.0404       0.0741       0.0572       0.0512       0.0932       0.0722        0.195      0.00203\n",
            "     80     3      0.00845      0.00843     1.93e-05       0.0543        0.071        0.045       0.0729        0.059       0.0568       0.0932        0.075        0.315      0.00328\n",
            "     80     4      0.00673      0.00673     3.18e-07       0.0486       0.0634       0.0393       0.0674       0.0533       0.0496       0.0846       0.0671       0.0346      0.00036\n",
            "     80     5      0.00669      0.00668     2.27e-06       0.0479       0.0633       0.0384       0.0669       0.0527       0.0483       0.0857        0.067        0.105      0.00109\n",
            "     80     6      0.00577      0.00577     2.19e-06       0.0438       0.0588       0.0336       0.0642       0.0489       0.0434       0.0812       0.0623        0.104      0.00108\n",
            "     80     7      0.00646      0.00646     2.48e-06       0.0473       0.0622       0.0371       0.0677       0.0524       0.0468        0.085       0.0659        0.107      0.00111\n",
            "     80     8      0.00664      0.00664     9.82e-07       0.0467        0.063       0.0361       0.0679        0.052       0.0473       0.0863       0.0668       0.0674     0.000702\n",
            "     80     9      0.00669      0.00669     1.37e-06       0.0472       0.0633       0.0355       0.0706        0.053        0.046       0.0882       0.0671       0.0566      0.00059\n",
            "     80    10      0.00662      0.00662      1.2e-06       0.0472       0.0629       0.0359       0.0699       0.0529       0.0455        0.088       0.0667       0.0789     0.000822\n",
            "     80    11      0.00619      0.00619     4.06e-06       0.0451       0.0609       0.0335       0.0681       0.0508        0.042       0.0871       0.0645        0.137      0.00143\n",
            "     80    12      0.00711      0.00711     3.65e-06       0.0485       0.0652       0.0375       0.0705        0.054       0.0486       0.0897       0.0691        0.116      0.00121\n",
            "     80    13      0.00643      0.00642     3.74e-06       0.0466        0.062       0.0367       0.0664       0.0516       0.0474       0.0839       0.0657        0.108      0.00113\n",
            "     80    14      0.00745      0.00745     3.24e-07       0.0481       0.0668       0.0361       0.0721       0.0541        0.048       0.0936       0.0708        0.034     0.000354\n",
            "     80    15      0.00778      0.00778     2.03e-06       0.0508       0.0682        0.038       0.0763       0.0572       0.0495       0.0952       0.0724       0.0895     0.000932\n",
            "     80    16      0.00705      0.00704     6.58e-06       0.0496       0.0649       0.0388       0.0712        0.055       0.0486        0.089       0.0688         0.17      0.00177\n",
            "     80    17      0.00639      0.00638     1.07e-05       0.0472       0.0618        0.039       0.0634       0.0512       0.0499       0.0805       0.0652        0.236      0.00246\n",
            "     80    18      0.00755      0.00754     7.81e-06       0.0517       0.0672       0.0437       0.0677       0.0557       0.0556       0.0857       0.0707        0.195      0.00203\n",
            "     80    19      0.00781      0.00781     4.57e-06       0.0498       0.0684       0.0365       0.0764       0.0565       0.0468       0.0982       0.0725        0.155      0.00162\n",
            "     80    20      0.00887      0.00883     4.05e-05        0.055       0.0727       0.0434       0.0784       0.0609       0.0551        0.099        0.077        0.467      0.00487\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     80     1      0.00741      0.00741     2.63e-06       0.0499       0.0666       0.0388        0.072       0.0554         0.05       0.0911       0.0706       0.0974      0.00101\n",
            "     80     2      0.00742      0.00742     1.18e-06       0.0495       0.0666        0.038       0.0723       0.0552       0.0492        0.092       0.0706       0.0557      0.00058\n",
            "     80     3      0.00703      0.00703     1.42e-06       0.0482       0.0649       0.0371       0.0704       0.0538       0.0475       0.0901       0.0688       0.0705     0.000734\n",
            "     80     4      0.00723      0.00723     1.54e-06       0.0486       0.0658       0.0375        0.071       0.0542       0.0479       0.0916       0.0698       0.0791     0.000824\n",
            "     80     5       0.0067       0.0067     1.05e-06       0.0476       0.0633       0.0374        0.068       0.0527       0.0484       0.0857       0.0671       0.0508     0.000529\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              80  306.616    0.005      0.00704     7.15e-06      0.00705       0.0487       0.0649       0.0381         0.07        0.054       0.0488       0.0888       0.0688        0.155      0.00161\n",
            "! Validation         80  306.616    0.005      0.00716     1.56e-06      0.00716       0.0488       0.0654       0.0378       0.0707       0.0543       0.0486       0.0901       0.0694       0.0707     0.000736\n",
            "Wall time: 306.6170847919998\n",
            "! Best model       80    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     81     1      0.00722      0.00722     6.04e-07       0.0503       0.0657       0.0407       0.0695       0.0551       0.0517       0.0873       0.0695       0.0559     0.000582\n",
            "     81     2      0.00644      0.00644     3.74e-06       0.0476       0.0621       0.0377       0.0674       0.0526       0.0473       0.0841       0.0657        0.129      0.00135\n",
            "     81     3      0.00631      0.00631     2.04e-06       0.0468       0.0614        0.038       0.0644       0.0512       0.0481       0.0818        0.065       0.0793     0.000826\n",
            "     81     4      0.00631      0.00631     3.72e-06       0.0474       0.0614       0.0393       0.0636       0.0515       0.0494       0.0802       0.0648         0.12      0.00125\n",
            "     81     5      0.00669      0.00668     4.85e-06       0.0474       0.0632       0.0356       0.0711       0.0533       0.0448       0.0894       0.0671        0.148      0.00155\n",
            "     81     6      0.00699      0.00699     3.91e-07       0.0497       0.0647       0.0405       0.0681       0.0543       0.0503       0.0866       0.0684       0.0461      0.00048\n",
            "     81     7      0.00729      0.00728     1.06e-05       0.0492        0.066       0.0389       0.0699       0.0544       0.0503       0.0896       0.0699        0.238      0.00248\n",
            "     81     8      0.00634      0.00633     9.04e-06       0.0466       0.0615       0.0354       0.0691       0.0522       0.0452       0.0853       0.0653        0.218      0.00227\n",
            "     81     9      0.00588      0.00586     1.51e-05       0.0445       0.0592       0.0351       0.0633       0.0492       0.0444       0.0811       0.0628        0.284      0.00295\n",
            "     81    10      0.00803      0.00803      3.8e-06       0.0515       0.0693       0.0403        0.074       0.0571       0.0524       0.0945       0.0734        0.142      0.00148\n",
            "     81    11      0.00837      0.00837      1.7e-06       0.0535       0.0708       0.0435       0.0736       0.0586       0.0562       0.0933       0.0748       0.0824     0.000859\n",
            "     81    12      0.00666      0.00666     1.74e-06       0.0473       0.0631       0.0377       0.0665       0.0521       0.0478       0.0859       0.0669       0.0752     0.000783\n",
            "     81    13      0.00668      0.00668     3.18e-06       0.0481       0.0632       0.0396       0.0651       0.0523       0.0498       0.0838       0.0668        0.118      0.00122\n",
            "     81    14      0.00732      0.00731     1.84e-05       0.0512       0.0661       0.0412       0.0711       0.0561       0.0512       0.0888         0.07        0.313      0.00327\n",
            "     81    15      0.00871      0.00871     1.19e-06       0.0549       0.0722       0.0428       0.0791        0.061       0.0544       0.0986       0.0765       0.0744     0.000775\n",
            "     81    16      0.00933      0.00926     7.69e-05       0.0571       0.0744       0.0461        0.079       0.0626       0.0575          0.1       0.0788        0.646      0.00673\n",
            "     81    17      0.00835      0.00835      5.9e-06       0.0543       0.0707       0.0443       0.0745       0.0594        0.056       0.0933       0.0747        0.168      0.00175\n",
            "     81    18      0.00715      0.00712      2.7e-05       0.0492       0.0653       0.0392       0.0693       0.0542       0.0499       0.0883       0.0691        0.379      0.00395\n",
            "     81    19      0.00811      0.00811     2.58e-06       0.0528       0.0697       0.0426       0.0732       0.0579       0.0544       0.0929       0.0737        0.104      0.00108\n",
            "     81    20      0.00875      0.00873     2.23e-05       0.0558       0.0723        0.046       0.0754       0.0607        0.058       0.0946       0.0763        0.338      0.00352\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     81     1      0.00738      0.00738     2.63e-06       0.0497       0.0664       0.0387       0.0719       0.0553       0.0498        0.091       0.0704       0.0967      0.00101\n",
            "     81     2      0.00737      0.00737     1.15e-06       0.0493       0.0664       0.0379       0.0722        0.055        0.049       0.0918       0.0704       0.0535     0.000557\n",
            "     81     3        0.007        0.007     1.37e-06       0.0481       0.0647        0.037       0.0703       0.0536       0.0473       0.0899       0.0686       0.0697     0.000726\n",
            "     81     4      0.00719      0.00719     1.52e-06       0.0485       0.0656       0.0373       0.0709       0.0541       0.0477       0.0915       0.0696       0.0792     0.000825\n",
            "     81     5      0.00667      0.00667     9.98e-07       0.0475       0.0632       0.0373       0.0678       0.0526       0.0482       0.0856       0.0669       0.0498     0.000519\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              81  310.405    0.005      0.00734     1.07e-05      0.00735       0.0503       0.0663       0.0402       0.0704       0.0553       0.0511       0.0891       0.0701        0.188      0.00196\n",
            "! Validation         81  310.405    0.005      0.00712     1.53e-06      0.00712       0.0486       0.0653       0.0376       0.0706       0.0541       0.0484         0.09       0.0692       0.0698     0.000727\n",
            "Wall time: 310.4055352739997\n",
            "! Best model       81    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     82     1      0.00939      0.00937     2.05e-05       0.0578       0.0749       0.0483       0.0768       0.0626       0.0606       0.0973        0.079        0.332      0.00345\n",
            "     82     2      0.00878      0.00871     7.32e-05       0.0531       0.0722       0.0413       0.0766       0.0589       0.0535       0.0996       0.0765        0.635      0.00662\n",
            "     82     3      0.00619      0.00619      3.1e-06        0.046       0.0608       0.0353       0.0675       0.0514       0.0447       0.0843       0.0645        0.109      0.00114\n",
            "     82     4      0.00697      0.00695     2.28e-05       0.0485       0.0645       0.0378       0.0699       0.0539       0.0482       0.0885       0.0684        0.348      0.00363\n",
            "     82     5      0.00713      0.00713     8.12e-07       0.0492       0.0653       0.0403       0.0669       0.0536       0.0523       0.0856       0.0689       0.0623     0.000649\n",
            "     82     6      0.00661      0.00661     1.51e-06       0.0477       0.0629       0.0373       0.0686       0.0529        0.047       0.0864       0.0667       0.0809     0.000842\n",
            "     82     7      0.00629      0.00629     8.87e-07       0.0455       0.0614       0.0355       0.0657       0.0506       0.0462       0.0838        0.065       0.0604     0.000629\n",
            "     82     8      0.00699      0.00698     2.78e-06       0.0484       0.0646       0.0387       0.0677       0.0532         0.05       0.0868       0.0684       0.0914     0.000952\n",
            "     82     9      0.00798      0.00798     3.72e-07       0.0508       0.0691       0.0397        0.073       0.0563       0.0507       0.0958       0.0733       0.0402     0.000419\n",
            "     82    10      0.00646      0.00644     2.33e-05       0.0465       0.0621       0.0351       0.0692       0.0522       0.0446       0.0871       0.0658        0.348      0.00362\n",
            "     82    11      0.00648      0.00648     2.03e-06       0.0464       0.0623       0.0343       0.0706       0.0524       0.0434       0.0887       0.0661       0.0951     0.000991\n",
            "     82    12      0.00788      0.00783     5.32e-05       0.0512       0.0684       0.0398       0.0742        0.057       0.0506       0.0945       0.0726        0.534      0.00556\n",
            "     82    13      0.00683      0.00683     2.97e-06       0.0495       0.0639       0.0412       0.0661       0.0537       0.0519       0.0829       0.0674        0.123      0.00129\n",
            "     82    14      0.00678      0.00676     2.21e-05       0.0487       0.0636       0.0379       0.0701        0.054       0.0471       0.0877       0.0674        0.342      0.00356\n",
            "     82    15      0.00608      0.00608     2.61e-07       0.0452       0.0603        0.035       0.0656       0.0503       0.0447       0.0832       0.0639       0.0279     0.000291\n",
            "     82    16      0.00678      0.00677     1.35e-05       0.0486       0.0636       0.0391       0.0678       0.0534       0.0497        0.085       0.0673         0.26      0.00271\n",
            "     82    17      0.00622      0.00621     7.39e-06       0.0463        0.061       0.0367       0.0654       0.0511       0.0471        0.082       0.0645        0.173       0.0018\n",
            "     82    18      0.00655      0.00654     1.26e-05       0.0479       0.0626       0.0388        0.066       0.0524       0.0486       0.0838       0.0662        0.261      0.00272\n",
            "     82    19      0.00803      0.00801     1.45e-05       0.0521       0.0692        0.042       0.0722       0.0571       0.0534       0.0932       0.0733        0.254      0.00265\n",
            "     82    20      0.00872       0.0087     1.36e-05       0.0544       0.0722        0.042       0.0791       0.0605       0.0528          0.1       0.0765        0.273      0.00284\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     82     1      0.00734      0.00734     2.52e-06       0.0496       0.0663       0.0385       0.0718       0.0552       0.0497       0.0908       0.0702       0.0949     0.000989\n",
            "     82     2      0.00734      0.00734     1.16e-06       0.0492       0.0663       0.0378       0.0721       0.0549       0.0489       0.0917       0.0703       0.0543     0.000566\n",
            "     82     3      0.00697      0.00697     1.37e-06        0.048       0.0646       0.0368       0.0702       0.0535       0.0471       0.0898       0.0685       0.0694     0.000723\n",
            "     82     4      0.00716      0.00716     1.54e-06       0.0484       0.0654       0.0372       0.0708        0.054       0.0475       0.0913       0.0694       0.0792     0.000825\n",
            "     82     5      0.00664      0.00664     9.96e-07       0.0473        0.063       0.0372       0.0677       0.0524        0.048       0.0854       0.0667       0.0503     0.000524\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              82  314.193    0.005      0.00714     1.46e-05      0.00716       0.0492       0.0654       0.0388         0.07       0.0544       0.0495        0.089       0.0693        0.223      0.00232\n",
            "! Validation         82  314.193    0.005      0.00709     1.52e-06      0.00709       0.0485       0.0651       0.0375       0.0705        0.054       0.0482       0.0898        0.069       0.0696     0.000725\n",
            "Wall time: 314.19339243600007\n",
            "! Best model       82    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     83     1        0.007        0.007     3.83e-07       0.0479       0.0647       0.0376       0.0684        0.053       0.0479       0.0893       0.0686       0.0432      0.00045\n",
            "     83     2      0.00695      0.00695     9.13e-06       0.0491       0.0645        0.038       0.0714       0.0547       0.0473       0.0894       0.0684        0.217      0.00226\n",
            "     83     3       0.0083      0.00826     3.32e-05       0.0552       0.0703       0.0465       0.0726       0.0596       0.0582       0.0898        0.074        0.423      0.00441\n",
            "     83     4       0.0102       0.0102     4.67e-06       0.0626       0.0781       0.0585       0.0708       0.0647       0.0725       0.0882       0.0804        0.135      0.00141\n",
            "     83     5        0.008        0.008      1.2e-06       0.0534       0.0692       0.0462        0.068       0.0571       0.0584       0.0869       0.0726       0.0619     0.000645\n",
            "     83     6      0.00795      0.00789     6.09e-05       0.0518       0.0687       0.0405       0.0745       0.0575        0.052       0.0936       0.0728        0.578      0.00602\n",
            "     83     7      0.00699      0.00698     5.22e-06       0.0487       0.0646       0.0376        0.071       0.0543       0.0482       0.0888       0.0685        0.152      0.00158\n",
            "     83     8      0.00801        0.008     1.47e-05       0.0523       0.0692       0.0403       0.0764       0.0584       0.0503       0.0964       0.0734        0.282      0.00294\n",
            "     83     9      0.00788      0.00787      5.2e-06       0.0518       0.0687        0.042       0.0714       0.0567       0.0534       0.0918       0.0726        0.148      0.00154\n",
            "     83    10      0.00759      0.00758     5.75e-06       0.0511       0.0674       0.0413       0.0707        0.056       0.0519       0.0907       0.0713        0.173       0.0018\n",
            "     83    11      0.00706      0.00703     2.63e-05       0.0496       0.0649       0.0392       0.0704       0.0548       0.0486       0.0888       0.0687        0.375       0.0039\n",
            "     83    12      0.00976      0.00976     1.31e-06       0.0593       0.0764       0.0514       0.0751       0.0632       0.0639       0.0967       0.0803        0.077     0.000802\n",
            "     83    13       0.0103       0.0103      1.5e-06       0.0608       0.0786       0.0515       0.0795       0.0655       0.0647        0.101       0.0828       0.0699     0.000728\n",
            "     83    14       0.0087      0.00867     3.39e-05       0.0538        0.072       0.0417        0.078       0.0598       0.0544       0.0983       0.0763        0.429      0.00447\n",
            "     83    15      0.00696      0.00695     9.65e-06       0.0481       0.0645       0.0379       0.0685       0.0532       0.0478       0.0889       0.0683         0.23      0.00239\n",
            "     83    16      0.00674      0.00674     1.48e-06       0.0485       0.0635       0.0388       0.0679       0.0533       0.0484       0.0861       0.0673       0.0803     0.000836\n",
            "     83    17      0.00804      0.00804     1.06e-06       0.0529       0.0694       0.0429        0.073       0.0579       0.0548       0.0918       0.0733       0.0592     0.000616\n",
            "     83    18       0.0071      0.00705     4.88e-05       0.0508        0.065       0.0441       0.0642       0.0542        0.055       0.0813       0.0682        0.512      0.00533\n",
            "     83    19      0.00651      0.00649     1.86e-05       0.0468       0.0623       0.0361        0.068       0.0521       0.0466       0.0855        0.066        0.312      0.00325\n",
            "     83    20      0.00789      0.00785     3.63e-05       0.0526       0.0685        0.042       0.0737       0.0578       0.0536       0.0913       0.0725        0.444      0.00463\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     83     1      0.00731      0.00731     2.54e-06       0.0495       0.0661       0.0384       0.0717       0.0551       0.0495       0.0907       0.0701       0.0941     0.000981\n",
            "     83     2      0.00731      0.00731     1.22e-06       0.0491       0.0661       0.0377       0.0719       0.0548       0.0487       0.0915       0.0701       0.0562     0.000586\n",
            "     83     3      0.00694      0.00694      1.4e-06       0.0478       0.0644       0.0367       0.0701       0.0534        0.047       0.0897       0.0683       0.0704     0.000733\n",
            "     83     4      0.00713      0.00713     1.55e-06       0.0483       0.0653        0.037       0.0707       0.0539       0.0473       0.0912       0.0693       0.0788     0.000821\n",
            "     83     5      0.00661       0.0066     1.04e-06       0.0472       0.0629        0.037       0.0676       0.0523       0.0478       0.0853       0.0666       0.0521     0.000542\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              83  317.971    0.005      0.00788      1.6e-05       0.0079       0.0524       0.0687       0.0427       0.0717       0.0572       0.0543       0.0908       0.0726         0.24       0.0025\n",
            "! Validation         83  317.971    0.005      0.00706     1.55e-06      0.00706       0.0484        0.065       0.0374       0.0704       0.0539       0.0481       0.0897       0.0689       0.0703     0.000733\n",
            "Wall time: 317.97188583499974\n",
            "! Best model       83    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     84     1      0.00744      0.00741     2.58e-05       0.0502       0.0666       0.0407       0.0693        0.055       0.0517       0.0892       0.0705        0.371      0.00386\n",
            "     84     2      0.00705      0.00705     1.56e-06       0.0482        0.065       0.0375       0.0696       0.0535       0.0482       0.0895       0.0688       0.0791     0.000824\n",
            "     84     3      0.00681      0.00679     2.48e-05       0.0486       0.0637       0.0398       0.0662        0.053       0.0503       0.0845       0.0674        0.365       0.0038\n",
            "     84     4      0.00717      0.00717     2.56e-07       0.0496       0.0655       0.0392       0.0702       0.0547       0.0498        0.089       0.0694       0.0303     0.000315\n",
            "     84     5      0.00854      0.00848     5.72e-05       0.0527       0.0713       0.0395       0.0791       0.0593       0.0512          0.1       0.0756        0.549      0.00571\n",
            "     84     6      0.00757      0.00755     1.84e-05       0.0522       0.0672        0.044       0.0685       0.0562       0.0555        0.086       0.0707        0.313      0.00326\n",
            "     84     7      0.00706      0.00702     3.86e-05       0.0486       0.0648       0.0378       0.0702        0.054       0.0482       0.0892       0.0687        0.454      0.00473\n",
            "     84     8       0.0061      0.00609     7.91e-06       0.0448       0.0604       0.0346       0.0651       0.0499       0.0444       0.0836        0.064        0.186      0.00194\n",
            "     84     9      0.00693       0.0069     3.51e-05       0.0495       0.0643       0.0419       0.0646       0.0533       0.0521       0.0834       0.0678        0.438      0.00456\n",
            "     84    10      0.00739      0.00739     1.67e-06       0.0504       0.0665       0.0394       0.0724       0.0559       0.0499        0.091       0.0705       0.0824     0.000859\n",
            "     84    11      0.00724      0.00718     5.96e-05       0.0496       0.0656       0.0388       0.0711        0.055       0.0492       0.0898       0.0695        0.572      0.00596\n",
            "     84    12       0.0057      0.00569     4.39e-06       0.0445       0.0584       0.0354       0.0626        0.049       0.0447       0.0789       0.0618        0.133      0.00138\n",
            "     84    13      0.00812      0.00807     4.47e-05       0.0525       0.0695       0.0414       0.0749       0.0581       0.0519       0.0954       0.0737        0.483      0.00503\n",
            "     84    14      0.00854      0.00853     5.44e-06       0.0561       0.0715       0.0471        0.074       0.0605       0.0586        0.092       0.0753        0.159      0.00166\n",
            "     84    15      0.00737      0.00737     2.47e-06       0.0516       0.0664       0.0428       0.0693       0.0561       0.0539       0.0861         0.07        0.106      0.00111\n",
            "     84    16      0.00654      0.00651     3.04e-05       0.0467       0.0624       0.0361        0.068       0.0521       0.0467       0.0856       0.0661        0.401      0.00418\n",
            "     84    17      0.00592      0.00592     2.87e-06       0.0448       0.0595       0.0347        0.065       0.0499       0.0435       0.0827       0.0631        0.116      0.00121\n",
            "     84    18      0.00741      0.00738     2.82e-05       0.0498       0.0664        0.039       0.0713       0.0552       0.0501       0.0907       0.0704        0.383      0.00399\n",
            "     84    19      0.00739      0.00738      1.3e-05       0.0494       0.0665       0.0399       0.0686       0.0542        0.051       0.0898       0.0704        0.264      0.00275\n",
            "     84    20      0.00719      0.00719     3.43e-06        0.049       0.0656       0.0384       0.0703       0.0544       0.0492       0.0898       0.0695        0.131      0.00137\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     84     1      0.00728      0.00728     2.55e-06       0.0494        0.066       0.0383       0.0716       0.0549       0.0493       0.0905       0.0699       0.0941     0.000981\n",
            "     84     2      0.00727      0.00727     1.16e-06        0.049        0.066       0.0375       0.0718       0.0547       0.0485       0.0913       0.0699       0.0551     0.000574\n",
            "     84     3      0.00691      0.00691     1.33e-06       0.0477       0.0643       0.0366         0.07       0.0533       0.0468       0.0896       0.0682       0.0688     0.000717\n",
            "     84     4       0.0071       0.0071     1.55e-06       0.0482       0.0652       0.0369       0.0706       0.0538       0.0472       0.0911       0.0691       0.0798     0.000831\n",
            "     84     5      0.00657      0.00657     9.93e-07       0.0471       0.0627       0.0369       0.0675       0.0522       0.0477       0.0852       0.0664       0.0491     0.000512\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              84  321.771    0.005      0.00715     2.03e-05      0.00717       0.0494       0.0654       0.0394       0.0695       0.0545       0.0501       0.0884       0.0693        0.281      0.00293\n",
            "! Validation         84  321.771    0.005      0.00703     1.52e-06      0.00703       0.0483       0.0648       0.0372       0.0703       0.0538       0.0479       0.0896       0.0687       0.0694     0.000723\n",
            "Wall time: 321.7718210529997\n",
            "! Best model       84    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     85     1      0.00635      0.00635     1.03e-06       0.0462       0.0617       0.0356       0.0675       0.0515        0.046       0.0847       0.0654       0.0701      0.00073\n",
            "     85     2      0.00691      0.00689      1.6e-05       0.0481       0.0642       0.0381       0.0682       0.0532       0.0489       0.0872        0.068        0.278       0.0029\n",
            "     85     3      0.00841      0.00839     1.54e-05       0.0533       0.0709       0.0423       0.0754       0.0588       0.0543       0.0958        0.075         0.28      0.00292\n",
            "     85     4      0.00663      0.00656     6.77e-05       0.0473       0.0627       0.0369       0.0679       0.0524       0.0469       0.0859       0.0664        0.609      0.00635\n",
            "     85     5      0.00581      0.00581     3.73e-06       0.0438        0.059       0.0336       0.0641       0.0488       0.0424       0.0827       0.0625        0.137      0.00142\n",
            "     85     6      0.00663      0.00662     1.29e-05       0.0467       0.0629        0.036       0.0682       0.0521       0.0452       0.0882       0.0667        0.262      0.00273\n",
            "     85     7      0.00723      0.00722     5.07e-06       0.0494       0.0658       0.0376       0.0729       0.0553       0.0471       0.0924       0.0697        0.165      0.00172\n",
            "     85     8      0.00652      0.00652     2.24e-06       0.0469       0.0625       0.0364       0.0677       0.0521       0.0464        0.086       0.0662        0.102      0.00106\n",
            "     85     9      0.00592      0.00591     1.32e-05       0.0445       0.0595       0.0351       0.0634       0.0492       0.0455       0.0805        0.063        0.266      0.00277\n",
            "     85    10      0.00634      0.00633     1.47e-05       0.0469       0.0615       0.0371       0.0663       0.0517       0.0473        0.083       0.0652        0.265      0.00276\n",
            "     85    11      0.00729      0.00728     1.39e-05       0.0499        0.066         0.04       0.0696       0.0548       0.0506       0.0891       0.0699        0.273      0.00284\n",
            "     85    12      0.00601        0.006     1.19e-05       0.0454       0.0599       0.0354       0.0653       0.0504       0.0444       0.0826       0.0635        0.241      0.00251\n",
            "     85    13      0.00677      0.00677     6.61e-07       0.0471       0.0636       0.0367        0.068       0.0523       0.0473       0.0876       0.0675       0.0449     0.000468\n",
            "     85    14      0.00675      0.00674     1.22e-05       0.0473       0.0635       0.0365       0.0687       0.0526       0.0478       0.0868       0.0673        0.253      0.00263\n",
            "     85    15      0.00669      0.00667     1.39e-05       0.0482       0.0632       0.0386       0.0675        0.053       0.0485       0.0853       0.0669         0.26       0.0027\n",
            "     85    16      0.00645      0.00643      2.1e-05       0.0469        0.062       0.0364       0.0678       0.0521        0.046       0.0855       0.0658        0.337      0.00351\n",
            "     85    17      0.00684      0.00683     1.51e-05       0.0481       0.0639       0.0379       0.0686       0.0533       0.0482       0.0872       0.0677        0.282      0.00294\n",
            "     85    18      0.00704      0.00704     5.68e-07       0.0487       0.0649       0.0373       0.0717       0.0545       0.0477         0.09       0.0688       0.0477     0.000496\n",
            "     85    19      0.00666      0.00663     2.63e-05       0.0476        0.063       0.0372       0.0684       0.0528        0.048       0.0855       0.0667        0.374       0.0039\n",
            "     85    20      0.00709      0.00706     2.85e-05       0.0491        0.065       0.0374       0.0725       0.0549       0.0477       0.0901       0.0689        0.385      0.00401\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     85     1      0.00724      0.00724     2.46e-06       0.0493       0.0658       0.0382       0.0715       0.0548       0.0492       0.0904       0.0698       0.0928     0.000966\n",
            "     85     2      0.00724      0.00724     1.15e-06       0.0488       0.0658       0.0374       0.0717       0.0546       0.0484       0.0912       0.0698       0.0542     0.000565\n",
            "     85     3      0.00688      0.00688      1.4e-06       0.0476       0.0642       0.0365         0.07       0.0532       0.0466       0.0894        0.068       0.0704     0.000733\n",
            "     85     4      0.00707      0.00706     1.52e-06        0.048        0.065       0.0368       0.0705       0.0536        0.047       0.0909        0.069       0.0787      0.00082\n",
            "     85     5      0.00655      0.00655        1e-06        0.047       0.0626       0.0368       0.0674       0.0521       0.0475       0.0851       0.0663       0.0497     0.000518\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              85  325.554    0.005       0.0067     1.48e-05      0.00672       0.0476       0.0633       0.0371       0.0685       0.0528       0.0474       0.0869       0.0671        0.247      0.00257\n",
            "! Validation         85  325.554    0.005      0.00699     1.51e-06        0.007       0.0481       0.0647       0.0371       0.0702       0.0537       0.0477       0.0894       0.0686       0.0692      0.00072\n",
            "Wall time: 325.5542331639999\n",
            "! Best model       85    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     86     1      0.00612      0.00608     3.41e-05       0.0454       0.0603       0.0348       0.0668       0.0508       0.0436       0.0844        0.064        0.431      0.00449\n",
            "     86     2      0.00653      0.00646      6.9e-05       0.0461       0.0622       0.0346        0.069       0.0518        0.044       0.0879       0.0659        0.613      0.00638\n",
            "     86     3      0.00747      0.00746     1.64e-05         0.05       0.0668       0.0398       0.0706       0.0552       0.0513       0.0901       0.0707        0.299      0.00311\n",
            "     86     4      0.00699      0.00695     4.73e-05       0.0488       0.0645       0.0389       0.0685       0.0537       0.0493       0.0872       0.0683        0.505      0.00526\n",
            "     86     5       0.0071       0.0071     1.86e-06       0.0495       0.0652       0.0385       0.0715        0.055       0.0488       0.0894       0.0691        0.076     0.000791\n",
            "     86     6      0.00738      0.00734     3.58e-05       0.0488       0.0663       0.0374       0.0716       0.0545       0.0482       0.0924       0.0703        0.441      0.00459\n",
            "     86     7      0.00677      0.00676     7.71e-06       0.0476       0.0636       0.0375        0.068       0.0527       0.0485       0.0862       0.0674        0.193      0.00201\n",
            "     86     8      0.00567      0.00564     2.98e-05       0.0439       0.0581        0.033       0.0656       0.0493       0.0413        0.082       0.0616        0.399      0.00415\n",
            "     86     9      0.00662      0.00659     2.83e-05       0.0478       0.0628       0.0379       0.0675       0.0527        0.048        0.085       0.0665        0.386      0.00402\n",
            "     86    10      0.00637      0.00632     5.68e-05       0.0462       0.0615        0.036       0.0667       0.0513       0.0462       0.0841       0.0651        0.558      0.00581\n",
            "     86    11        0.006      0.00599     4.65e-06       0.0458       0.0599       0.0359       0.0656       0.0507       0.0452       0.0817       0.0634        0.144       0.0015\n",
            "     86    12      0.00668      0.00665     2.66e-05       0.0479       0.0631       0.0385       0.0665       0.0525       0.0491       0.0844       0.0668        0.379      0.00395\n",
            "     86    13      0.00633      0.00629     3.32e-05       0.0463       0.0614       0.0364       0.0659       0.0512        0.046       0.0841        0.065        0.426      0.00443\n",
            "     86    14      0.00652      0.00652     1.41e-06       0.0475       0.0625       0.0385       0.0655        0.052        0.048       0.0842       0.0661       0.0713     0.000743\n",
            "     86    15      0.00687      0.00685     2.39e-05        0.048        0.064       0.0365        0.071       0.0537       0.0469       0.0888       0.0679        0.359      0.00374\n",
            "     86    16      0.00769      0.00769     1.07e-06       0.0513       0.0679       0.0399       0.0741        0.057       0.0509       0.0929       0.0719       0.0701      0.00073\n",
            "     86    17      0.00823      0.00822     9.76e-06       0.0528       0.0701       0.0416       0.0752       0.0584       0.0538       0.0947       0.0742        0.221       0.0023\n",
            "     86    18      0.00841      0.00841     1.84e-06       0.0549        0.071       0.0489       0.0668       0.0579       0.0611       0.0874       0.0742       0.0912      0.00095\n",
            "     86    19      0.00777      0.00777     8.63e-07       0.0516       0.0682       0.0418       0.0711       0.0565       0.0541         0.09        0.072       0.0518     0.000539\n",
            "     86    20      0.00764      0.00762     2.64e-05       0.0508       0.0675       0.0402       0.0721       0.0562       0.0502        0.093       0.0716        0.366      0.00381\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     86     1      0.00721      0.00721     2.55e-06       0.0491       0.0657        0.038       0.0713       0.0547        0.049       0.0902       0.0696       0.0945     0.000985\n",
            "     86     2      0.00721      0.00721     1.11e-06       0.0487       0.0657       0.0373       0.0716       0.0544       0.0482       0.0911       0.0696       0.0521     0.000542\n",
            "     86     3      0.00685      0.00685     1.37e-06       0.0475        0.064       0.0363       0.0699       0.0531       0.0465       0.0894       0.0679       0.0706     0.000735\n",
            "     86     4      0.00703      0.00703     1.53e-06       0.0479       0.0649       0.0367       0.0704       0.0535       0.0468       0.0908       0.0688       0.0786     0.000819\n",
            "     86     5      0.00652      0.00652     9.44e-07       0.0469       0.0625       0.0367       0.0673        0.052       0.0473        0.085       0.0662       0.0475     0.000494\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              86  329.329    0.005      0.00694     2.28e-05      0.00696       0.0485       0.0644       0.0383        0.069       0.0537       0.0489       0.0876       0.0682        0.304      0.00317\n",
            "! Validation         86  329.329    0.005      0.00696      1.5e-06      0.00697        0.048       0.0646        0.037       0.0701       0.0535       0.0476       0.0893       0.0684       0.0687     0.000715\n",
            "Wall time: 329.32923474499967\n",
            "! Best model       86    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     87     1      0.00604      0.00604      6.1e-07       0.0455       0.0601       0.0366       0.0634         0.05       0.0469       0.0803       0.0636       0.0535     0.000557\n",
            "     87     2       0.0077      0.00769     5.76e-06         0.05       0.0678        0.038       0.0739       0.0559       0.0481       0.0958        0.072        0.174      0.00181\n",
            "     87     3      0.00743      0.00741     1.44e-05       0.0499       0.0666       0.0399       0.0698       0.0548       0.0513       0.0897       0.0705         0.28      0.00292\n",
            "     87     4      0.00908      0.00907     3.47e-06       0.0571       0.0737       0.0511       0.0693       0.0602        0.065       0.0885       0.0768        0.129      0.00135\n",
            "     87     5      0.00732      0.00726     5.14e-05        0.051       0.0659       0.0423       0.0684       0.0554       0.0525       0.0867       0.0696        0.529      0.00552\n",
            "     87     6      0.00672      0.00672     4.79e-06       0.0482       0.0634       0.0377       0.0692       0.0535       0.0469       0.0875       0.0672        0.148      0.00154\n",
            "     87     7      0.00676      0.00667     8.21e-05       0.0483       0.0632       0.0374       0.0701       0.0538       0.0466       0.0874        0.067        0.664      0.00691\n",
            "     87     8      0.00744      0.00742     1.84e-05       0.0513       0.0667       0.0402       0.0735       0.0568       0.0513       0.0898       0.0706        0.304      0.00317\n",
            "     87     9      0.00756      0.00752     4.02e-05       0.0503       0.0671       0.0385       0.0738       0.0562       0.0489       0.0934       0.0711        0.471      0.00491\n",
            "     87    10      0.00725      0.00724     1.12e-05       0.0498       0.0658       0.0393       0.0708        0.055       0.0501       0.0894       0.0697        0.237      0.00247\n",
            "     87    11      0.00792      0.00792      1.6e-06       0.0526       0.0689       0.0447       0.0685       0.0566       0.0567       0.0883       0.0725       0.0854     0.000889\n",
            "     87    12      0.00741       0.0074     1.58e-05       0.0503       0.0665       0.0387       0.0735       0.0561       0.0489       0.0921       0.0705        0.294      0.00306\n",
            "     87    13      0.00882      0.00881     9.61e-06       0.0542       0.0726         0.04       0.0827       0.0613       0.0507        0.103        0.077        0.225      0.00234\n",
            "     87    14      0.00703        0.007     2.69e-05       0.0482       0.0647       0.0385       0.0676        0.053       0.0495       0.0876       0.0685        0.375      0.00391\n",
            "     87    15      0.00739      0.00737     1.56e-05        0.051       0.0664       0.0408       0.0713       0.0561       0.0509       0.0898       0.0703        0.269       0.0028\n",
            "     87    16      0.00603        0.006     3.09e-05       0.0449       0.0599        0.034       0.0665       0.0503       0.0432       0.0839       0.0635        0.407      0.00424\n",
            "     87    17      0.00679      0.00677     1.38e-05       0.0482       0.0637       0.0393       0.0659       0.0526       0.0511       0.0833       0.0672        0.269       0.0028\n",
            "     87    18      0.00749      0.00749     3.06e-06         0.05        0.067       0.0395       0.0711       0.0553       0.0518       0.0899       0.0709          0.1      0.00105\n",
            "     87    19      0.00654       0.0065     4.37e-05       0.0463       0.0624       0.0361       0.0666       0.0513       0.0471       0.0851       0.0661        0.481      0.00501\n",
            "     87    20      0.00623      0.00623     1.15e-06       0.0458       0.0611       0.0358       0.0658       0.0508       0.0459       0.0836       0.0647       0.0656     0.000684\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     87     1      0.00717      0.00717      2.5e-06        0.049       0.0655       0.0379       0.0712       0.0546       0.0488       0.0901       0.0694       0.0931     0.000969\n",
            "     87     2      0.00718      0.00717      1.2e-06       0.0486       0.0655       0.0372       0.0715       0.0543        0.048       0.0909       0.0695       0.0551     0.000574\n",
            "     87     3      0.00683      0.00683      1.4e-06       0.0474       0.0639       0.0362       0.0698        0.053       0.0463       0.0893       0.0678       0.0708     0.000738\n",
            "     87     4        0.007        0.007     1.57e-06       0.0478       0.0647       0.0366       0.0702       0.0534       0.0466       0.0907       0.0686       0.0791     0.000824\n",
            "     87     5      0.00648      0.00648     9.62e-07       0.0467       0.0623       0.0365       0.0672       0.0518       0.0471       0.0848        0.066       0.0481     0.000502\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              87  333.137    0.005      0.00723     1.97e-05      0.00725       0.0496       0.0658       0.0394       0.0701       0.0547       0.0504       0.0889       0.0696        0.278       0.0029\n",
            "! Validation         87  333.137    0.005      0.00693     1.53e-06      0.00693       0.0479       0.0644       0.0369         0.07       0.0534       0.0474       0.0892       0.0683       0.0692     0.000721\n",
            "Wall time: 333.1372009689999\n",
            "! Best model       87    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     88     1      0.00728      0.00724     3.96e-05       0.0496       0.0658       0.0389        0.071        0.055       0.0499       0.0896       0.0697        0.467      0.00486\n",
            "     88     2      0.00762      0.00762     1.21e-06       0.0511       0.0675        0.041       0.0713       0.0561       0.0515       0.0916       0.0715       0.0688     0.000716\n",
            "     88     3       0.0064      0.00639     8.99e-06       0.0474       0.0618       0.0381       0.0659        0.052       0.0485       0.0823       0.0654        0.214      0.00223\n",
            "     88     4      0.00578      0.00577     1.28e-05        0.044       0.0588       0.0346       0.0628       0.0487       0.0441       0.0804       0.0623        0.263      0.00274\n",
            "     88     5      0.00811      0.00811     4.17e-06       0.0537       0.0697       0.0452       0.0706       0.0579       0.0575       0.0891       0.0733        0.131      0.00137\n",
            "     88     6      0.00816      0.00811     4.44e-05       0.0532       0.0697        0.044       0.0715       0.0577       0.0559       0.0912       0.0735        0.492      0.00512\n",
            "     88     7      0.00663      0.00662     2.68e-06       0.0469        0.063       0.0348       0.0712        0.053       0.0437       0.0899       0.0668        0.108      0.00112\n",
            "     88     8      0.00615      0.00612     2.77e-05       0.0456       0.0605       0.0367       0.0634         0.05       0.0464       0.0818       0.0641        0.387      0.00403\n",
            "     88     9      0.00813      0.00813     1.32e-06       0.0527       0.0697       0.0407       0.0768       0.0587       0.0517       0.0962       0.0739       0.0688     0.000716\n",
            "     88    10      0.00857      0.00857     1.19e-06       0.0554       0.0716       0.0439       0.0784       0.0612       0.0538        0.098       0.0759        0.076     0.000791\n",
            "     88    11      0.00849      0.00848     1.32e-05       0.0556       0.0712       0.0476       0.0717       0.0597       0.0595       0.0902       0.0749        0.246      0.00256\n",
            "     88    12      0.00655      0.00655     1.82e-06       0.0471       0.0626       0.0388       0.0637       0.0513       0.0501       0.0821       0.0661       0.0846     0.000881\n",
            "     88    13      0.00681       0.0068     8.49e-06       0.0476       0.0638       0.0358       0.0713       0.0535       0.0458       0.0895       0.0677        0.215      0.00224\n",
            "     88    14      0.00701      0.00701     1.57e-06       0.0491       0.0648       0.0382       0.0711       0.0546       0.0483        0.089       0.0686       0.0766     0.000798\n",
            "     88    15      0.00923      0.00923     7.57e-07       0.0569       0.0743       0.0467       0.0772       0.0619       0.0583       0.0989       0.0786       0.0477     0.000496\n",
            "     88    16      0.00967      0.00965      2.8e-05       0.0571        0.076       0.0462       0.0788       0.0625       0.0596        0.101       0.0803        0.383      0.00399\n",
            "     88    17      0.00702      0.00698     4.74e-05       0.0499       0.0646       0.0423       0.0653       0.0538       0.0529       0.0832       0.0681        0.508      0.00529\n",
            "     88    18      0.00604      0.00604     6.03e-06       0.0454       0.0601       0.0351       0.0659       0.0505       0.0444       0.0831       0.0637        0.175      0.00182\n",
            "     88    19      0.00683      0.00681     2.54e-05        0.048       0.0638       0.0388       0.0665       0.0527       0.0503       0.0847       0.0675        0.372      0.00388\n",
            "     88    20      0.00863      0.00863     2.02e-06       0.0558       0.0719       0.0475       0.0725         0.06       0.0598       0.0914       0.0756       0.0881     0.000918\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     88     1      0.00714      0.00714     2.46e-06       0.0489       0.0654       0.0378       0.0711       0.0544       0.0486       0.0899       0.0693       0.0921     0.000959\n",
            "     88     2      0.00714      0.00714     1.13e-06       0.0485       0.0654        0.037       0.0714       0.0542       0.0479       0.0908       0.0693       0.0544     0.000567\n",
            "     88     3      0.00679      0.00679     1.33e-06       0.0473       0.0638        0.036       0.0697       0.0529       0.0461       0.0891       0.0676       0.0684     0.000712\n",
            "     88     4      0.00697      0.00697     1.56e-06       0.0477       0.0646       0.0364       0.0702       0.0533       0.0464       0.0906       0.0685        0.079     0.000823\n",
            "     88     5      0.00645      0.00645     9.51e-07       0.0466       0.0621       0.0364       0.0671       0.0517       0.0469       0.0847       0.0658       0.0479     0.000499\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              88  336.984    0.005      0.00744     1.39e-05      0.00746       0.0506       0.0667       0.0407       0.0703       0.0555       0.0519       0.0894       0.0706        0.224      0.00233\n",
            "! Validation         88  336.984    0.005       0.0069     1.49e-06       0.0069       0.0478       0.0643       0.0367       0.0699       0.0533       0.0472       0.0891       0.0681       0.0684     0.000712\n",
            "Wall time: 336.9846814319999\n",
            "! Best model       88    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     89     1      0.00879      0.00879     8.91e-07       0.0561       0.0725       0.0477       0.0729       0.0603       0.0593       0.0936       0.0764       0.0617     0.000643\n",
            "     89     2      0.00654      0.00654     3.96e-06       0.0469       0.0626       0.0369       0.0667       0.0518       0.0466        0.086       0.0663        0.144       0.0015\n",
            "     89     3       0.0068       0.0068     8.45e-07       0.0479       0.0638       0.0377       0.0683        0.053       0.0483       0.0869       0.0676        0.048       0.0005\n",
            "     89     4      0.00537      0.00536     2.64e-06        0.043       0.0567       0.0336       0.0617       0.0477       0.0421        0.078       0.0601          0.1      0.00104\n",
            "     89     5      0.00615      0.00614     4.96e-06       0.0461       0.0606       0.0359       0.0664       0.0511       0.0452       0.0833       0.0643        0.149      0.00155\n",
            "     89     6      0.00642      0.00642     3.61e-06       0.0461        0.062       0.0353       0.0676       0.0514       0.0452       0.0863       0.0657        0.133      0.00139\n",
            "     89     7      0.00692      0.00691     1.01e-05       0.0488       0.0643       0.0383         0.07       0.0541       0.0483       0.0879       0.0681         0.21      0.00219\n",
            "     89     8      0.00758      0.00758      8.8e-07       0.0499       0.0673       0.0402       0.0693       0.0547       0.0519       0.0907       0.0713       0.0611     0.000637\n",
            "     89     9      0.00656      0.00656     1.69e-06        0.048       0.0626       0.0392       0.0655       0.0524       0.0496       0.0827       0.0662       0.0867     0.000903\n",
            "     89    10      0.00732      0.00731     1.86e-05       0.0497       0.0661       0.0392       0.0706       0.0549       0.0498       0.0903       0.0701         0.32      0.00333\n",
            "     89    11      0.00686      0.00686     1.28e-06       0.0483       0.0641       0.0368       0.0713        0.054       0.0468       0.0891       0.0679       0.0775     0.000808\n",
            "     89    12       0.0082      0.00818     2.63e-05       0.0526         0.07       0.0425       0.0726       0.0576       0.0537       0.0944       0.0741        0.375      0.00391\n",
            "     89    13      0.00715      0.00715     5.89e-07       0.0505       0.0654        0.042       0.0673       0.0547       0.0526       0.0855        0.069       0.0473     0.000492\n",
            "     89    14        0.007        0.007      2.6e-06       0.0483       0.0647       0.0362       0.0725       0.0544       0.0466       0.0907       0.0686        0.101      0.00105\n",
            "     89    15      0.00674      0.00672      1.8e-05       0.0479       0.0634       0.0379       0.0678       0.0529        0.049       0.0853       0.0671        0.302      0.00315\n",
            "     89    16      0.00718      0.00717     3.26e-06       0.0494       0.0655       0.0398       0.0687       0.0542       0.0511       0.0876       0.0693        0.123      0.00128\n",
            "     89    17      0.00713       0.0071      2.4e-05        0.048       0.0652       0.0351        0.074       0.0545       0.0445       0.0937       0.0691        0.361      0.00376\n",
            "     89    18      0.00669      0.00669      6.8e-07       0.0476       0.0633       0.0365       0.0696       0.0531       0.0461       0.0881       0.0671       0.0518     0.000539\n",
            "     89    19      0.00597      0.00596     4.18e-06       0.0445       0.0597        0.034       0.0655       0.0497       0.0436       0.0831       0.0634        0.115      0.00119\n",
            "     89    20      0.00637      0.00637     5.27e-06       0.0463       0.0617       0.0358       0.0673       0.0516        0.046       0.0849       0.0654        0.152      0.00158\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     89     1      0.00711      0.00711     2.43e-06       0.0488       0.0652       0.0377        0.071       0.0543       0.0485       0.0898       0.0691       0.0911     0.000949\n",
            "     89     2      0.00711      0.00711     1.11e-06       0.0484       0.0652       0.0369       0.0713       0.0541       0.0477       0.0906       0.0692       0.0532     0.000554\n",
            "     89     3      0.00677      0.00677     1.38e-06       0.0472       0.0636       0.0359       0.0697       0.0528        0.046        0.089       0.0675       0.0703     0.000732\n",
            "     89     4      0.00694      0.00694     1.55e-06       0.0476       0.0644       0.0363       0.0701       0.0532       0.0463       0.0904       0.0684       0.0788     0.000821\n",
            "     89     5      0.00642      0.00642     9.42e-07       0.0465        0.062       0.0363        0.067       0.0516       0.0468       0.0846       0.0657        0.048       0.0005\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              89  340.765    0.005      0.00688     6.72e-06      0.00689       0.0483       0.0642        0.038       0.0688       0.0534       0.0485       0.0875        0.068        0.151      0.00157\n",
            "! Validation         89  340.765    0.005      0.00687     1.48e-06      0.00687       0.0477       0.0641       0.0366       0.0698       0.0532       0.0471       0.0889        0.068       0.0683     0.000711\n",
            "Wall time: 340.7661060219998\n",
            "! Best model       89    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     90     1      0.00706      0.00705     6.98e-06       0.0477        0.065       0.0363       0.0705       0.0534       0.0475       0.0903       0.0689         0.19      0.00198\n",
            "     90     2      0.00669      0.00669     1.12e-06       0.0483       0.0633       0.0374       0.0702       0.0538       0.0472       0.0869       0.0671       0.0674     0.000702\n",
            "     90     3       0.0072       0.0072     4.67e-06       0.0491       0.0656       0.0387       0.0699       0.0543        0.049       0.0901       0.0696        0.111      0.00115\n",
            "     90     4      0.00706      0.00706     2.03e-06       0.0487        0.065         0.04       0.0662       0.0531       0.0511       0.0863       0.0687        0.104      0.00108\n",
            "     90     5       0.0066      0.00659     1.28e-05       0.0459       0.0628       0.0355       0.0668       0.0511       0.0462       0.0869       0.0666        0.263      0.00274\n",
            "     90     6      0.00604      0.00604      1.4e-06       0.0457       0.0601       0.0346       0.0679       0.0513       0.0436       0.0839       0.0638       0.0617     0.000643\n",
            "     90     7      0.00628      0.00628     1.78e-06       0.0462       0.0613       0.0364       0.0659       0.0512       0.0463       0.0836        0.065       0.0957     0.000997\n",
            "     90     8      0.00655      0.00655     5.26e-07       0.0477       0.0626       0.0367       0.0696       0.0532       0.0461       0.0866       0.0664       0.0516     0.000537\n",
            "     90     9      0.00603      0.00603     2.08e-06       0.0453       0.0601       0.0348       0.0664       0.0506        0.044       0.0834       0.0637        0.101      0.00105\n",
            "     90    10      0.00628      0.00628     4.46e-06       0.0458       0.0613       0.0352       0.0671       0.0512       0.0448       0.0852        0.065        0.144       0.0015\n",
            "     90    11      0.00754      0.00754     4.02e-06       0.0498       0.0672       0.0378       0.0738       0.0558       0.0484        0.094       0.0712        0.142      0.00148\n",
            "     90    12      0.00751      0.00747     3.84e-05        0.051       0.0669       0.0419       0.0693       0.0556       0.0522       0.0892       0.0707        0.456      0.00475\n",
            "     90    13      0.00748      0.00747     2.53e-06       0.0511       0.0669       0.0422       0.0689       0.0556       0.0534       0.0878       0.0706       0.0799     0.000832\n",
            "     90    14      0.00677      0.00676     6.64e-06       0.0478       0.0636       0.0366         0.07       0.0533       0.0464       0.0885       0.0674        0.183      0.00191\n",
            "     90    15       0.0069      0.00688      1.9e-05       0.0475       0.0642       0.0371       0.0682       0.0526       0.0482       0.0878        0.068        0.321      0.00335\n",
            "     90    16      0.00574      0.00574        1e-06       0.0448       0.0586       0.0354       0.0634       0.0494        0.044       0.0802       0.0621       0.0615     0.000641\n",
            "     90    17      0.00711      0.00706     4.92e-05        0.049        0.065       0.0389       0.0691        0.054       0.0501       0.0875       0.0688        0.514      0.00536\n",
            "     90    18      0.00606      0.00606     1.08e-06       0.0445       0.0602       0.0338       0.0659       0.0498       0.0431       0.0847       0.0639       0.0668     0.000696\n",
            "     90    19      0.00695      0.00691     3.62e-05       0.0491       0.0643       0.0391       0.0692       0.0542        0.049       0.0872       0.0681        0.446      0.00464\n",
            "     90    20      0.00605      0.00605     4.85e-06       0.0447       0.0602       0.0336       0.0668       0.0502       0.0436        0.084       0.0638        0.156      0.00162\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     90     1      0.00708      0.00708     2.45e-06       0.0487       0.0651       0.0376       0.0709       0.0542       0.0483       0.0896        0.069       0.0916     0.000954\n",
            "     90     2      0.00708      0.00707     1.09e-06       0.0483       0.0651       0.0368       0.0712        0.054       0.0475       0.0905        0.069       0.0528      0.00055\n",
            "     90     3      0.00674      0.00674     1.34e-06       0.0471       0.0635       0.0358       0.0696       0.0527       0.0458       0.0889       0.0674       0.0706     0.000735\n",
            "     90     4      0.00691      0.00691     1.55e-06       0.0475       0.0643       0.0362       0.0699       0.0531       0.0461       0.0903       0.0682       0.0786     0.000819\n",
            "     90     5       0.0064       0.0064     9.57e-07       0.0464       0.0619       0.0361       0.0669       0.0515       0.0466       0.0845       0.0656       0.0475     0.000494\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              90  344.545    0.005      0.00669        1e-05       0.0067       0.0475       0.0633       0.0371       0.0682       0.0527       0.0473       0.0868        0.067        0.181      0.00188\n",
            "! Validation         90  344.545    0.005      0.00684     1.48e-06      0.00684       0.0476        0.064       0.0365       0.0697       0.0531       0.0469       0.0888       0.0678       0.0682     0.000711\n",
            "Wall time: 344.5461854129999\n",
            "! Best model       90    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     91     1      0.00793       0.0079     3.22e-05       0.0524       0.0688        0.044       0.0693       0.0567       0.0564       0.0884       0.0724        0.409      0.00426\n",
            "     91     2      0.00643      0.00643      3.4e-06       0.0464        0.062       0.0356       0.0681       0.0518       0.0446        0.087       0.0658        0.123      0.00129\n",
            "     91     3      0.00932      0.00928     3.85e-05        0.057       0.0745       0.0444       0.0823       0.0633       0.0553        0.103        0.079        0.451       0.0047\n",
            "     91     4       0.0088      0.00879     1.21e-05        0.056       0.0725       0.0467       0.0747       0.0607       0.0592       0.0936       0.0764        0.248      0.00258\n",
            "     91     5      0.00752      0.00752     2.28e-06       0.0525       0.0671       0.0464       0.0648       0.0556       0.0574       0.0831       0.0703       0.0895     0.000932\n",
            "     91     6      0.00619      0.00619     1.04e-06       0.0457       0.0609       0.0353       0.0666       0.0509       0.0451       0.0839       0.0645       0.0738     0.000769\n",
            "     91     7      0.00626      0.00624     1.46e-05       0.0462       0.0611       0.0366       0.0654        0.051       0.0464       0.0831       0.0647        0.265      0.00276\n",
            "     91     8      0.00617      0.00617     6.52e-07       0.0455       0.0608       0.0335       0.0695       0.0515       0.0426       0.0863       0.0645       0.0385     0.000401\n",
            "     91     9      0.00652      0.00651     9.83e-06       0.0468       0.0624        0.037       0.0663       0.0517       0.0469       0.0854       0.0661        0.228      0.00237\n",
            "     91    10      0.00646      0.00645      5.3e-06       0.0465       0.0621       0.0357       0.0681       0.0519       0.0449       0.0869       0.0659         0.17      0.00177\n",
            "     91    11      0.00759      0.00759     7.29e-06       0.0498       0.0674       0.0371       0.0752       0.0561       0.0479       0.0951       0.0715        0.198      0.00206\n",
            "     91    12      0.00641      0.00641     3.02e-06       0.0465       0.0619       0.0368       0.0659       0.0513        0.046       0.0853       0.0656        0.112      0.00117\n",
            "     91    13      0.00765      0.00765      2.4e-06        0.052       0.0677       0.0444       0.0673       0.0558        0.056       0.0864       0.0712         0.11      0.00114\n",
            "     91    14      0.00623      0.00622     7.15e-06       0.0464        0.061        0.037       0.0652       0.0511       0.0473       0.0818       0.0646        0.188      0.00196\n",
            "     91    15      0.00606      0.00606     1.23e-06        0.045       0.0602       0.0341       0.0667       0.0504       0.0432       0.0845       0.0639       0.0693     0.000722\n",
            "     91    16       0.0068      0.00677     2.43e-05       0.0478       0.0637        0.038       0.0673       0.0527        0.048       0.0869       0.0675        0.362      0.00377\n",
            "     91    17      0.00779      0.00777      1.6e-05       0.0504       0.0682       0.0387       0.0737       0.0562        0.051       0.0936       0.0723        0.294      0.00306\n",
            "     91    18      0.00638      0.00637     1.52e-05       0.0444       0.0617       0.0335       0.0662       0.0498       0.0435       0.0875       0.0655        0.287      0.00299\n",
            "     91    19      0.00674      0.00673     1.01e-05       0.0476       0.0635       0.0357       0.0714       0.0535       0.0449       0.0897       0.0673        0.221       0.0023\n",
            "     91    20       0.0086      0.00854     5.74e-05       0.0552       0.0715       0.0457       0.0743         0.06       0.0576       0.0933       0.0754         0.56      0.00583\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     91     1      0.00705      0.00705      2.5e-06       0.0486        0.065       0.0374       0.0708       0.0541       0.0482       0.0895       0.0689       0.0926     0.000964\n",
            "     91     2      0.00706      0.00706      1.1e-06       0.0482        0.065       0.0367       0.0711       0.0539       0.0474       0.0904       0.0689       0.0521     0.000543\n",
            "     91     3      0.00672      0.00672     1.37e-06        0.047       0.0634       0.0357       0.0695       0.0526       0.0457       0.0888       0.0672       0.0707     0.000736\n",
            "     91     4      0.00688      0.00688     1.51e-06       0.0473       0.0642       0.0361       0.0698        0.053        0.046       0.0901       0.0681       0.0793     0.000826\n",
            "     91     5      0.00637      0.00637     9.37e-07       0.0463       0.0617        0.036       0.0668       0.0514       0.0464       0.0844       0.0654       0.0466     0.000485\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              91  348.325    0.005      0.00708     1.32e-05      0.00709        0.049       0.0651       0.0388       0.0694       0.0541       0.0495       0.0884       0.0689        0.225      0.00234\n",
            "! Validation         91  348.325    0.005      0.00681     1.48e-06      0.00682       0.0475       0.0639       0.0364       0.0696        0.053       0.0468       0.0887       0.0677       0.0683     0.000711\n",
            "Wall time: 348.32533711099995\n",
            "! Best model       91    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     92     1      0.00935      0.00935     3.99e-06       0.0587       0.0748       0.0529       0.0702       0.0616       0.0652        0.091       0.0781        0.134       0.0014\n",
            "     92     2      0.00671      0.00666     4.67e-05       0.0478       0.0631       0.0387        0.066       0.0523       0.0485       0.0852       0.0668        0.504      0.00525\n",
            "     92     3      0.00622      0.00621     1.11e-05       0.0466        0.061        0.038       0.0638       0.0509       0.0475       0.0815       0.0645         0.23       0.0024\n",
            "     92     4      0.00851      0.00847      4.2e-05       0.0549       0.0712       0.0454        0.074       0.0597       0.0573       0.0929       0.0751        0.479      0.00499\n",
            "     92     5      0.00964      0.00949     0.000145       0.0579       0.0754       0.0469       0.0799       0.0634       0.0588        0.101       0.0797        0.893       0.0093\n",
            "     92     6      0.00868      0.00863     5.91e-05       0.0544       0.0719       0.0438       0.0755       0.0597       0.0547       0.0975       0.0761        0.569      0.00592\n",
            "     92     7      0.00644      0.00634     0.000101       0.0466       0.0616       0.0373       0.0652       0.0513       0.0475       0.0828       0.0652        0.745      0.00776\n",
            "     92     8      0.00691       0.0069     2.85e-06       0.0482       0.0643       0.0376       0.0694       0.0535       0.0471       0.0893       0.0682        0.122      0.00127\n",
            "     92     9      0.00649      0.00647      1.8e-05       0.0479       0.0622       0.0401       0.0636       0.0518       0.0506       0.0805       0.0656        0.311      0.00324\n",
            "     92    10      0.00748      0.00746     2.01e-05       0.0509       0.0668       0.0434        0.066       0.0547       0.0549       0.0858       0.0703        0.332      0.00346\n",
            "     92    11      0.00629      0.00628     1.08e-05       0.0458       0.0613       0.0365       0.0642       0.0504       0.0461       0.0838        0.065        0.234      0.00244\n",
            "     92    12      0.00657      0.00656     7.99e-06       0.0482       0.0627       0.0388        0.067       0.0529       0.0491       0.0834       0.0663        0.183      0.00191\n",
            "     92    13       0.0069       0.0069        6e-07       0.0477       0.0643       0.0371       0.0688        0.053       0.0483       0.0879       0.0681         0.05     0.000521\n",
            "     92    14      0.00724      0.00724     3.97e-06       0.0496       0.0658       0.0389        0.071        0.055       0.0494       0.0901       0.0697        0.145      0.00151\n",
            "     92    15      0.00587      0.00587     1.13e-06       0.0446       0.0593       0.0337       0.0664       0.0501       0.0428        0.083       0.0629       0.0635     0.000661\n",
            "     92    16      0.00742      0.00742     1.43e-06       0.0508       0.0667       0.0393       0.0737       0.0565       0.0492       0.0921       0.0707       0.0703     0.000732\n",
            "     92    17      0.00683      0.00682     1.69e-05       0.0478       0.0639       0.0355       0.0725        0.054       0.0453       0.0902       0.0677          0.3      0.00313\n",
            "     92    18      0.00782      0.00781     3.15e-06       0.0527       0.0684       0.0432       0.0716       0.0574       0.0548       0.0896       0.0722        0.111      0.00116\n",
            "     92    19      0.00669      0.00667      1.6e-05       0.0475       0.0632       0.0371       0.0683       0.0527       0.0467       0.0873        0.067        0.288        0.003\n",
            "     92    20      0.00708      0.00708     4.75e-06       0.0498       0.0651       0.0397       0.0702       0.0549       0.0497       0.0881       0.0689        0.153       0.0016\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     92     1      0.00703      0.00703     2.47e-06       0.0485       0.0649       0.0373       0.0707        0.054       0.0481       0.0894       0.0688       0.0916     0.000954\n",
            "     92     2      0.00703      0.00703     1.08e-06       0.0481       0.0648       0.0366        0.071       0.0538       0.0473       0.0903       0.0688       0.0517     0.000538\n",
            "     92     3      0.00669      0.00669     1.36e-06       0.0469       0.0633       0.0356       0.0695       0.0525       0.0455       0.0887       0.0671       0.0704     0.000733\n",
            "     92     4      0.00685      0.00685     1.58e-06       0.0473        0.064        0.036       0.0698       0.0529       0.0458         0.09       0.0679       0.0808     0.000841\n",
            "     92     5      0.00634      0.00634     9.26e-07       0.0462       0.0616       0.0359       0.0667       0.0513       0.0463       0.0843       0.0653       0.0474     0.000493\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              92  352.109    0.005      0.00723     2.59e-05      0.00726       0.0499       0.0658       0.0402       0.0694       0.0548       0.0509       0.0883       0.0696        0.296      0.00308\n",
            "! Validation         92  352.109    0.005      0.00679     1.48e-06      0.00679       0.0474       0.0637       0.0363       0.0695       0.0529       0.0466       0.0886       0.0676       0.0684     0.000712\n",
            "Wall time: 352.1094722349999\n",
            "! Best model       92    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     93     1      0.00661       0.0066     1.22e-05       0.0469       0.0628       0.0361       0.0684       0.0523       0.0464       0.0869       0.0666        0.247      0.00257\n",
            "     93     2      0.00794      0.00794     7.74e-07        0.055       0.0689       0.0498       0.0654       0.0576        0.061       0.0825       0.0718        0.052     0.000541\n",
            "     93     3      0.00818      0.00817     5.61e-06       0.0536       0.0699       0.0444       0.0718       0.0581        0.057       0.0904       0.0737        0.173      0.00181\n",
            "     93     4      0.00754      0.00754     3.68e-06       0.0504       0.0672       0.0389       0.0732       0.0561       0.0489       0.0936       0.0712         0.14      0.00146\n",
            "     93     5      0.00645      0.00642     2.89e-05       0.0459        0.062        0.034       0.0697       0.0519       0.0436       0.0879       0.0658        0.392      0.00408\n",
            "     93     6      0.00696      0.00693     3.22e-05       0.0481       0.0644       0.0367        0.071       0.0539       0.0465         0.09       0.0683        0.418      0.00435\n",
            "     93     7      0.00753      0.00744     9.52e-05       0.0509       0.0667       0.0421       0.0685       0.0553       0.0538        0.087       0.0704        0.722      0.00752\n",
            "     93     8      0.00715      0.00713     2.03e-05       0.0487       0.0653       0.0396       0.0669       0.0532       0.0504       0.0879       0.0692        0.325      0.00339\n",
            "     93     9      0.00676      0.00673     2.77e-05       0.0478       0.0635       0.0363        0.071       0.0536       0.0461       0.0886       0.0673        0.377      0.00393\n",
            "     93    10      0.00718      0.00714     4.87e-05       0.0491       0.0654       0.0407       0.0658       0.0533       0.0527       0.0852        0.069        0.514      0.00535\n",
            "     93    11      0.00746      0.00746     1.52e-06       0.0506       0.0668       0.0394       0.0729       0.0562       0.0498       0.0918       0.0708       0.0703     0.000732\n",
            "     93    12      0.00746      0.00733     0.000134       0.0499       0.0662       0.0386       0.0724       0.0555        0.048       0.0925       0.0702        0.858      0.00894\n",
            "     93    13      0.00574      0.00574     7.17e-06       0.0443       0.0586       0.0344       0.0641       0.0493       0.0438       0.0804       0.0621        0.189      0.00197\n",
            "     93    14      0.00761      0.00755     6.21e-05       0.0505       0.0672       0.0394       0.0728       0.0561       0.0504       0.0921       0.0712        0.583      0.00608\n",
            "     93    15      0.00726      0.00726     2.77e-06       0.0491       0.0659       0.0385       0.0704       0.0545       0.0493       0.0904       0.0698        0.118      0.00123\n",
            "     93    16      0.00685      0.00684     8.54e-06       0.0486        0.064         0.04       0.0657       0.0528       0.0505       0.0847       0.0676        0.209      0.00218\n",
            "     93    17      0.00544      0.00544     5.43e-06        0.043        0.057       0.0344       0.0602       0.0473       0.0443       0.0764       0.0603        0.131      0.00136\n",
            "     93    18      0.00741      0.00741     5.45e-06       0.0515       0.0666       0.0411       0.0723       0.0567       0.0512       0.0897       0.0705         0.16      0.00166\n",
            "     93    19       0.0104       0.0103     2.03e-05       0.0597       0.0787       0.0484       0.0823       0.0654       0.0615        0.105       0.0832        0.329      0.00343\n",
            "     93    20      0.00948      0.00936     0.000112        0.058       0.0749       0.0481       0.0779        0.063         0.06        0.098        0.079        0.784      0.00817\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     93     1        0.007        0.007     2.45e-06       0.0484       0.0647       0.0372       0.0706       0.0539       0.0479       0.0893       0.0686       0.0943     0.000983\n",
            "     93     2        0.007        0.007     9.57e-07        0.048       0.0647       0.0365        0.071       0.0537       0.0471       0.0902       0.0686       0.0463     0.000482\n",
            "     93     3      0.00667      0.00667     1.37e-06       0.0468       0.0632       0.0355       0.0694       0.0524       0.0454       0.0886        0.067       0.0719     0.000749\n",
            "     93     4      0.00683      0.00682     1.62e-06       0.0471       0.0639       0.0359       0.0697       0.0528       0.0457       0.0899       0.0678       0.0822     0.000857\n",
            "     93     5      0.00633      0.00632     8.97e-07       0.0461       0.0615       0.0358       0.0666       0.0512       0.0461       0.0843       0.0652       0.0464     0.000483\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              93  355.892    0.005      0.00734     3.18e-05      0.00737       0.0501       0.0663       0.0401       0.0701       0.0551        0.051       0.0892       0.0701         0.34      0.00354\n",
            "! Validation         93  355.892    0.005      0.00676     1.46e-06      0.00676       0.0473       0.0636       0.0362       0.0695       0.0528       0.0465       0.0885       0.0675       0.0682     0.000711\n",
            "Wall time: 355.8924238259997\n",
            "! Best model       93    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     94     1        0.007      0.00698     2.29e-05       0.0488       0.0646       0.0389       0.0685       0.0537       0.0496       0.0872       0.0684        0.348      0.00363\n",
            "     94     2      0.00692      0.00688     4.16e-05       0.0478       0.0642       0.0364       0.0706       0.0535       0.0471        0.089        0.068        0.477      0.00497\n",
            "     94     3      0.00743      0.00738     4.41e-05        0.051       0.0665        0.041       0.0708       0.0559       0.0512       0.0895       0.0704        0.491      0.00511\n",
            "     94     4      0.00824      0.00823     2.51e-06       0.0556       0.0702       0.0482       0.0705       0.0593       0.0591       0.0883       0.0737       0.0869     0.000905\n",
            "     94     5      0.00853      0.00847     5.94e-05       0.0548       0.0712       0.0449       0.0746       0.0598       0.0564        0.094       0.0752        0.567      0.00591\n",
            "     94     6      0.00649      0.00649     4.55e-07        0.046       0.0623       0.0359        0.066        0.051        0.046       0.0861       0.0661       0.0443     0.000462\n",
            "     94     7      0.00658      0.00657     1.33e-05       0.0471       0.0627       0.0365       0.0683       0.0524       0.0473       0.0855       0.0664        0.262      0.00273\n",
            "     94     8      0.00833      0.00832      5.3e-06       0.0539       0.0706       0.0426       0.0764       0.0595       0.0529       0.0967       0.0748        0.164      0.00171\n",
            "     94     9      0.00884      0.00883     1.24e-05       0.0558       0.0727       0.0462       0.0749       0.0606       0.0582       0.0953       0.0768        0.246      0.00256\n",
            "     94    10      0.00694      0.00694     7.17e-06       0.0485       0.0644       0.0394       0.0668       0.0531       0.0508       0.0854       0.0681        0.196      0.00204\n",
            "     94    11      0.00647      0.00647     1.22e-06        0.047       0.0622       0.0354         0.07       0.0527       0.0447       0.0873        0.066       0.0533     0.000555\n",
            "     94    12      0.00758      0.00757     1.72e-05       0.0512       0.0673       0.0434       0.0667        0.055       0.0554       0.0863       0.0708        0.284      0.00296\n",
            "     94    13      0.00766      0.00765     7.51e-06       0.0518       0.0677       0.0417       0.0721       0.0569       0.0527       0.0904       0.0716        0.169      0.00176\n",
            "     94    14       0.0057      0.00569     7.35e-06       0.0433       0.0584        0.033        0.064       0.0485       0.0419       0.0819       0.0619        0.186      0.00193\n",
            "     94    15      0.00668      0.00668     1.01e-06       0.0484       0.0632       0.0394       0.0665       0.0529       0.0495       0.0842       0.0669       0.0697     0.000726\n",
            "     94    16      0.00795      0.00795        8e-07       0.0521        0.069       0.0423       0.0717        0.057       0.0538       0.0922        0.073       0.0547      0.00057\n",
            "     94    17      0.00754      0.00751     2.96e-05       0.0494        0.067       0.0374       0.0734       0.0554       0.0483       0.0938       0.0711        0.401      0.00417\n",
            "     94    18      0.00615      0.00614     1.25e-06       0.0456       0.0606       0.0348       0.0672        0.051       0.0438       0.0849       0.0643       0.0641     0.000667\n",
            "     94    19      0.00587      0.00586        1e-05       0.0451       0.0592        0.036       0.0633       0.0497       0.0451       0.0803       0.0627         0.23       0.0024\n",
            "     94    20      0.00618      0.00618      1.4e-06       0.0456       0.0608       0.0354       0.0659       0.0507       0.0454       0.0834       0.0644       0.0803     0.000836\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     94     1      0.00698      0.00698     2.42e-06       0.0483       0.0646       0.0371       0.0705       0.0538       0.0478       0.0892       0.0685        0.091     0.000948\n",
            "     94     2      0.00698      0.00698     1.07e-06       0.0479       0.0646       0.0364       0.0709       0.0536        0.047       0.0901       0.0685       0.0512     0.000533\n",
            "     94     3      0.00665      0.00665     1.34e-06       0.0467       0.0631       0.0354       0.0693       0.0524       0.0453       0.0885       0.0669       0.0696     0.000725\n",
            "     94     4       0.0068       0.0068     1.57e-06        0.047       0.0638       0.0358       0.0696       0.0527       0.0456       0.0898       0.0677       0.0797      0.00083\n",
            "     94     5       0.0063       0.0063     9.23e-07        0.046       0.0614       0.0357       0.0665       0.0511        0.046       0.0842       0.0651       0.0457     0.000476\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              94  359.669    0.005      0.00714     1.43e-05      0.00715       0.0494       0.0654       0.0394       0.0694       0.0544       0.0502       0.0882       0.0692        0.224      0.00233\n",
            "! Validation         94  359.669    0.005      0.00674     1.47e-06      0.00674       0.0472       0.0635       0.0361       0.0694       0.0527       0.0463       0.0884       0.0674       0.0674     0.000703\n",
            "Wall time: 359.66920795199985\n",
            "! Best model       94    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     95     1       0.0064       0.0064     7.38e-07       0.0464       0.0619       0.0357       0.0678       0.0517       0.0456       0.0857       0.0656        0.051     0.000531\n",
            "     95     2      0.00669      0.00668     1.15e-05       0.0487       0.0632       0.0392       0.0676       0.0534       0.0491       0.0848       0.0669        0.216      0.00225\n",
            "     95     3      0.00605      0.00605     4.01e-07       0.0458       0.0602       0.0372       0.0628         0.05       0.0467       0.0806       0.0636        0.043     0.000448\n",
            "     95     4      0.00691      0.00689     2.15e-05       0.0475       0.0642       0.0363       0.0699       0.0531       0.0473       0.0888       0.0681        0.344      0.00358\n",
            "     95     5      0.00608      0.00604     3.44e-05       0.0451       0.0601       0.0348       0.0657       0.0503       0.0439       0.0836       0.0638        0.433      0.00451\n",
            "     95     6      0.00739      0.00739     1.36e-06        0.049       0.0665       0.0363       0.0745       0.0554       0.0468       0.0943       0.0705       0.0727     0.000757\n",
            "     95     7      0.00721      0.00705     0.000166       0.0497       0.0649       0.0408       0.0677       0.0542       0.0515       0.0857       0.0686        0.953      0.00993\n",
            "     95     8      0.00657      0.00656     6.43e-06       0.0477       0.0627       0.0373       0.0685       0.0529       0.0473       0.0855       0.0664        0.186      0.00194\n",
            "     95     9      0.00672      0.00662      9.3e-05       0.0467        0.063       0.0352       0.0696       0.0524       0.0445        0.089       0.0668        0.715      0.00745\n",
            "     95    10      0.00832       0.0083     2.13e-05       0.0546       0.0705       0.0481       0.0678       0.0579       0.0618       0.0853       0.0735        0.319      0.00332\n",
            "     95    11      0.00795      0.00794     8.88e-06       0.0528       0.0689        0.042       0.0744       0.0582       0.0529        0.093        0.073        0.218      0.00227\n",
            "     95    12      0.00583       0.0058        3e-05       0.0443       0.0589       0.0334       0.0662       0.0498       0.0421        0.083       0.0625        0.396      0.00412\n",
            "     95    13      0.00613      0.00611     2.05e-05       0.0452       0.0605       0.0365       0.0628       0.0496       0.0468       0.0812        0.064        0.331      0.00345\n",
            "     95    14      0.00782      0.00776      5.8e-05       0.0505       0.0681       0.0378       0.0758       0.0568       0.0488       0.0958       0.0723        0.563      0.00586\n",
            "     95    15      0.00695       0.0069     5.57e-05       0.0487       0.0643        0.038       0.0699        0.054       0.0485       0.0877       0.0681        0.553      0.00576\n",
            "     95    16      0.00765      0.00756     8.71e-05       0.0507       0.0673         0.04       0.0721       0.0561       0.0513       0.0912       0.0713        0.691       0.0072\n",
            "     95    17      0.00608      0.00607     7.45e-06       0.0447       0.0603       0.0345       0.0653       0.0499       0.0445       0.0832       0.0639        0.198      0.00207\n",
            "     95    18      0.00704      0.00695     8.64e-05        0.049       0.0645       0.0397       0.0678       0.0537       0.0505        0.086       0.0682        0.686      0.00714\n",
            "     95    19      0.00706      0.00704     2.56e-05       0.0473       0.0649       0.0361       0.0696       0.0528       0.0466       0.0911       0.0688        0.371      0.00387\n",
            "     95    20      0.00626       0.0062     5.93e-05       0.0457       0.0609       0.0353       0.0666       0.0509       0.0452        0.084       0.0646        0.568      0.00592\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     95     1      0.00696      0.00695     2.47e-06       0.0482       0.0645       0.0371       0.0704       0.0537       0.0477       0.0891       0.0684       0.0925     0.000963\n",
            "     95     2      0.00696      0.00695     9.81e-07       0.0478       0.0645       0.0363       0.0708       0.0536       0.0469       0.0899       0.0684       0.0484     0.000505\n",
            "     95     3      0.00662      0.00662     1.31e-06       0.0466        0.063       0.0353       0.0693       0.0523       0.0451       0.0884       0.0668       0.0696     0.000725\n",
            "     95     4      0.00678      0.00678     1.56e-06       0.0469       0.0637       0.0357       0.0695       0.0526       0.0454       0.0897       0.0676       0.0805     0.000838\n",
            "     95     5      0.00628      0.00628     9.13e-07       0.0459       0.0613       0.0356       0.0665        0.051       0.0458       0.0841        0.065       0.0463     0.000482\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              95  363.463    0.005      0.00682     3.98e-05      0.00685        0.048       0.0639       0.0377       0.0686       0.0532       0.0483       0.0871       0.0677        0.395      0.00412\n",
            "! Validation         95  363.463    0.005      0.00672     1.45e-06      0.00672       0.0471       0.0634        0.036       0.0693       0.0526       0.0462       0.0883       0.0672       0.0675     0.000703\n",
            "Wall time: 363.46329155600006\n",
            "! Best model       95    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     96     1      0.00695      0.00688     7.18e-05       0.0489       0.0642       0.0385       0.0695        0.054        0.048        0.088        0.068        0.627      0.00653\n",
            "     96     2      0.00869      0.00865     4.45e-05       0.0536       0.0719       0.0432       0.0744       0.0588       0.0547       0.0977       0.0762        0.492      0.00513\n",
            "     96     3      0.00839      0.00839     3.63e-06       0.0552       0.0708       0.0453       0.0748       0.0601       0.0567       0.0928       0.0748        0.139      0.00145\n",
            "     96     4      0.00668      0.00668      5.7e-07       0.0473       0.0632       0.0373       0.0673       0.0523       0.0474       0.0866        0.067        0.049     0.000511\n",
            "     96     5      0.00642      0.00642     1.72e-06       0.0457        0.062       0.0354       0.0661       0.0508        0.046       0.0854       0.0657       0.0869     0.000905\n",
            "     96     6      0.00746      0.00745        7e-06       0.0512       0.0668        0.042       0.0695       0.0557       0.0542       0.0867       0.0704        0.187      0.00194\n",
            "     96     7       0.0082      0.00818     2.15e-05       0.0525         0.07        0.042       0.0734       0.0577       0.0538       0.0944       0.0741        0.342      0.00356\n",
            "     96     8      0.00762      0.00762     1.64e-06       0.0508       0.0675       0.0415       0.0692       0.0554       0.0526       0.0903       0.0714       0.0891     0.000928\n",
            "     96     9      0.00662      0.00662     2.03e-06       0.0474       0.0629       0.0386        0.065       0.0518       0.0494       0.0837       0.0665       0.0949     0.000989\n",
            "     96    10      0.00628      0.00628     3.42e-06       0.0449       0.0613       0.0333       0.0682       0.0508       0.0428       0.0872        0.065        0.126      0.00132\n",
            "     96    11      0.00683      0.00683     6.09e-06       0.0483       0.0639       0.0382       0.0684       0.0533       0.0489       0.0864       0.0677        0.169      0.00176\n",
            "     96    12      0.00544      0.00544     5.55e-06        0.043        0.057       0.0334       0.0622       0.0478       0.0422       0.0787       0.0605        0.151      0.00157\n",
            "     96    13      0.00599      0.00598     9.78e-06       0.0451       0.0598       0.0353       0.0648       0.0501       0.0441       0.0828       0.0634         0.21      0.00219\n",
            "     96    14      0.00587      0.00586     1.54e-05       0.0435       0.0592       0.0321       0.0663       0.0492       0.0407       0.0849       0.0628         0.29      0.00302\n",
            "     96    15      0.00606      0.00605     9.83e-06       0.0452       0.0602       0.0342       0.0672       0.0507       0.0433       0.0844       0.0638        0.219      0.00228\n",
            "     96    16      0.00526      0.00521     4.99e-05       0.0418       0.0559        0.033       0.0594       0.0462        0.042       0.0764       0.0592        0.522      0.00544\n",
            "     96    17      0.00639      0.00639     1.56e-06       0.0466       0.0618       0.0371       0.0657       0.0514       0.0477       0.0832       0.0654       0.0844     0.000879\n",
            "     96    18      0.00582      0.00575     6.26e-05       0.0444       0.0587       0.0339       0.0654       0.0496       0.0428       0.0817       0.0622        0.586       0.0061\n",
            "     96    19      0.00573      0.00572     1.21e-05       0.0439       0.0585       0.0337       0.0644        0.049       0.0437       0.0803        0.062        0.254      0.00264\n",
            "     96    20       0.0064      0.00636      3.9e-05       0.0463       0.0617       0.0346       0.0697       0.0522       0.0436       0.0873       0.0655        0.455      0.00474\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     96     1      0.00693      0.00693      2.5e-06       0.0481       0.0644        0.037       0.0703       0.0536       0.0476        0.089       0.0683       0.0937     0.000976\n",
            "     96     2      0.00693      0.00693        1e-06       0.0477       0.0644       0.0362       0.0707       0.0535       0.0468       0.0898       0.0683       0.0481     0.000502\n",
            "     96     3       0.0066       0.0066     1.39e-06       0.0465       0.0629       0.0352       0.0692       0.0522        0.045       0.0883       0.0667       0.0724     0.000754\n",
            "     96     4      0.00676      0.00675     1.55e-06       0.0469       0.0636       0.0356       0.0694       0.0525       0.0453       0.0896       0.0674       0.0803     0.000836\n",
            "     96     5      0.00626      0.00626     8.73e-07       0.0458       0.0612       0.0355       0.0664        0.051       0.0457        0.084       0.0649       0.0465     0.000484\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              96  367.252    0.005      0.00664     1.85e-05      0.00666       0.0473        0.063       0.0371       0.0676       0.0523       0.0475       0.0861       0.0668        0.259      0.00269\n",
            "! Validation         96  367.252    0.005      0.00669     1.47e-06       0.0067        0.047       0.0633       0.0359       0.0692       0.0526       0.0461       0.0882       0.0671       0.0682      0.00071\n",
            "Wall time: 367.2530293059999\n",
            "! Best model       96    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     97     1      0.00652       0.0065     1.69e-05       0.0463       0.0624       0.0354       0.0682       0.0518       0.0454       0.0869       0.0661          0.3      0.00313\n",
            "     97     2       0.0075      0.00748     2.31e-05       0.0509       0.0669       0.0415       0.0697       0.0556       0.0527       0.0887       0.0707        0.345      0.00359\n",
            "     97     3      0.00628      0.00627     5.47e-06       0.0464       0.0613       0.0372       0.0649       0.0511       0.0471       0.0826       0.0649        0.169      0.00176\n",
            "     97     4      0.00636      0.00635     1.18e-05       0.0449       0.0617       0.0339       0.0668       0.0504       0.0434       0.0874       0.0654        0.252      0.00262\n",
            "     97     5       0.0065      0.00647     3.17e-05       0.0466       0.0622       0.0365        0.067       0.0517       0.0473       0.0846       0.0659        0.412      0.00429\n",
            "     97     6      0.00587      0.00585     1.89e-05        0.045       0.0592       0.0351        0.065         0.05       0.0438       0.0817       0.0627        0.312      0.00325\n",
            "     97     7      0.00649      0.00639     9.48e-05        0.045       0.0619       0.0342       0.0666       0.0504       0.0443       0.0869       0.0656        0.719      0.00748\n",
            "     97     8      0.00609      0.00608     1.17e-05       0.0453       0.0603       0.0354       0.0652       0.0503       0.0453       0.0825       0.0639        0.251      0.00262\n",
            "     97     9      0.00645      0.00633     0.000123       0.0461       0.0616       0.0366       0.0653       0.0509       0.0466       0.0838       0.0652        0.822      0.00856\n",
            "     97    10      0.00687      0.00686     2.26e-06       0.0479       0.0641       0.0357       0.0723        0.054        0.046         0.09        0.068       0.0914     0.000952\n",
            "     97    11      0.00775      0.00772     3.62e-05       0.0522        0.068       0.0431       0.0705       0.0568       0.0549       0.0885       0.0717        0.438      0.00457\n",
            "     97    12      0.00757      0.00749     7.88e-05        0.052        0.067       0.0437       0.0687       0.0562        0.055       0.0861       0.0705        0.656      0.00684\n",
            "     97    13      0.00708      0.00707      1.5e-05       0.0483        0.065       0.0388       0.0673        0.053       0.0506        0.087       0.0688        0.282      0.00294\n",
            "     97    14      0.00552      0.00551      1.3e-05       0.0426       0.0574        0.033       0.0617       0.0474       0.0419       0.0799       0.0609        0.264      0.00275\n",
            "     97    15      0.00745      0.00744     1.12e-06       0.0484       0.0668       0.0362       0.0727       0.0544       0.0475       0.0941       0.0708       0.0609     0.000635\n",
            "     97    16       0.0078       0.0078     7.61e-06       0.0525       0.0683       0.0416       0.0742       0.0579       0.0516       0.0931       0.0724        0.198      0.00207\n",
            "     97    17      0.00846      0.00845     8.11e-06       0.0547       0.0711       0.0463       0.0714       0.0589       0.0584       0.0914       0.0749        0.192        0.002\n",
            "     97    18      0.00746      0.00745     1.53e-05       0.0498       0.0668       0.0387       0.0722       0.0554       0.0501       0.0914       0.0707        0.288        0.003\n",
            "     97    19      0.00597      0.00597     1.36e-06       0.0456       0.0598       0.0365       0.0638       0.0501       0.0463       0.0802       0.0632       0.0818     0.000852\n",
            "     97    20      0.00631      0.00631     2.76e-06       0.0461       0.0614       0.0367       0.0649       0.0508       0.0466       0.0835       0.0651        0.114      0.00118\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     97     1      0.00691      0.00691     2.47e-06        0.048       0.0643       0.0369       0.0702       0.0535       0.0474       0.0889       0.0682       0.0931     0.000969\n",
            "     97     2      0.00691       0.0069     1.01e-06       0.0476       0.0643       0.0361       0.0707       0.0534       0.0466       0.0897       0.0682       0.0488     0.000509\n",
            "     97     3      0.00658      0.00658     1.33e-06       0.0465       0.0628       0.0351       0.0692       0.0521       0.0449       0.0882       0.0666       0.0706     0.000735\n",
            "     97     4      0.00674      0.00673      1.6e-06       0.0468       0.0635       0.0355       0.0694       0.0524       0.0452       0.0895       0.0673        0.081     0.000843\n",
            "     97     5      0.00624      0.00624     8.88e-07       0.0457       0.0611       0.0354       0.0663       0.0509       0.0456       0.0839       0.0648       0.0455     0.000474\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              97  371.046    0.005      0.00679      2.6e-05      0.00682       0.0478       0.0637       0.0378       0.0679       0.0529       0.0484       0.0866       0.0675        0.312      0.00325\n",
            "! Validation         97  371.046    0.005      0.00667     1.46e-06      0.00667       0.0469       0.0632       0.0358       0.0692       0.0525        0.046       0.0881        0.067       0.0678     0.000706\n",
            "Wall time: 371.0464381009997\n",
            "! Best model       97    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     98     1        0.007      0.00697     2.87e-05        0.049       0.0646       0.0384       0.0701       0.0542       0.0484       0.0885       0.0684        0.389      0.00406\n",
            "     98     2        0.007      0.00699     4.08e-06       0.0492       0.0647       0.0379       0.0719       0.0549        0.048       0.0891       0.0686        0.112      0.00117\n",
            "     98     3      0.00582      0.00581     1.45e-05       0.0432        0.059       0.0319       0.0657       0.0488       0.0402       0.0848       0.0625        0.279       0.0029\n",
            "     98     4      0.00726      0.00726     4.78e-07       0.0483       0.0659       0.0356       0.0736       0.0546       0.0461       0.0937       0.0699       0.0436     0.000454\n",
            "     98     5      0.00698      0.00696     2.36e-05       0.0475       0.0645        0.036       0.0705       0.0532       0.0461       0.0908       0.0684         0.36      0.00375\n",
            "     98     6      0.00578      0.00578     3.69e-06       0.0442       0.0588       0.0327       0.0671       0.0499       0.0415       0.0833       0.0624        0.124      0.00129\n",
            "     98     7      0.00624      0.00622     1.64e-05       0.0455        0.061       0.0348       0.0668       0.0508       0.0448       0.0847       0.0647        0.298       0.0031\n",
            "     98     8      0.00577      0.00577     2.85e-06       0.0443       0.0588       0.0349        0.063        0.049       0.0445         0.08       0.0622        0.113      0.00117\n",
            "     98     9      0.00686      0.00686     2.74e-06       0.0484       0.0641       0.0386       0.0681       0.0534       0.0487       0.0869       0.0678         0.11      0.00115\n",
            "     98    10      0.00717      0.00713     3.73e-05       0.0491       0.0653       0.0387         0.07       0.0543       0.0499       0.0885       0.0692         0.45      0.00469\n",
            "     98    11      0.00718      0.00717     6.91e-06       0.0491       0.0655       0.0375       0.0724        0.055       0.0481       0.0909       0.0695        0.189      0.00197\n",
            "     98    12      0.00634      0.00634     6.06e-06       0.0467       0.0616       0.0376       0.0649       0.0512       0.0473       0.0831       0.0652        0.178      0.00185\n",
            "     98    13      0.00581      0.00581     4.06e-06       0.0445        0.059       0.0338       0.0658       0.0498       0.0428       0.0823       0.0625        0.135      0.00141\n",
            "     98    14       0.0062       0.0062     5.19e-06       0.0452       0.0609       0.0353        0.065       0.0501       0.0445       0.0847       0.0646         0.16      0.00166\n",
            "     98    15      0.00583      0.00583     9.47e-07        0.044        0.059        0.034       0.0639        0.049       0.0429       0.0823       0.0626       0.0693     0.000722\n",
            "     98    16      0.00496      0.00496     1.53e-06       0.0416       0.0545       0.0332       0.0584       0.0458       0.0418       0.0736       0.0577       0.0756     0.000787\n",
            "     98    17      0.00721      0.00721     3.13e-07       0.0484       0.0657       0.0369       0.0713       0.0541       0.0477       0.0916       0.0697       0.0361     0.000376\n",
            "     98    18      0.00642       0.0064     1.94e-05       0.0471       0.0619       0.0365       0.0682       0.0523       0.0454       0.0858       0.0656        0.326      0.00339\n",
            "     98    19       0.0063       0.0063     1.12e-06       0.0463       0.0614       0.0367       0.0654        0.051       0.0474       0.0826        0.065       0.0662      0.00069\n",
            "     98    20      0.00573      0.00572     1.02e-05       0.0434       0.0585       0.0334       0.0633       0.0484       0.0438       0.0802        0.062        0.235      0.00244\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     98     1      0.00688      0.00688     2.42e-06       0.0479       0.0642       0.0368       0.0701       0.0535       0.0473       0.0888        0.068       0.0904     0.000942\n",
            "     98     2      0.00688      0.00688     1.05e-06       0.0476       0.0642        0.036       0.0706       0.0533       0.0465       0.0896        0.068       0.0509      0.00053\n",
            "     98     3      0.00656      0.00656     1.39e-06       0.0464       0.0627        0.035       0.0691        0.052       0.0448       0.0881       0.0664       0.0716     0.000746\n",
            "     98     4      0.00671      0.00671     1.55e-06       0.0467       0.0634       0.0354       0.0693       0.0523        0.045       0.0894       0.0672       0.0787      0.00082\n",
            "     98     5      0.00622      0.00622     8.62e-07       0.0456        0.061       0.0353       0.0663       0.0508       0.0455       0.0839       0.0647       0.0454     0.000473\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              98  374.825    0.005      0.00638      9.5e-06      0.00639       0.0462       0.0618       0.0357       0.0673       0.0515       0.0456       0.0855       0.0655        0.187      0.00195\n",
            "! Validation         98  374.825    0.005      0.00665     1.45e-06      0.00665       0.0468       0.0631       0.0357       0.0691       0.0524       0.0458        0.088       0.0669       0.0674     0.000702\n",
            "Wall time: 374.82536158999983\n",
            "! Best model       98    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     99     1      0.00634      0.00633     1.32e-05        0.046       0.0615       0.0356       0.0669       0.0512       0.0453       0.0852       0.0653        0.259       0.0027\n",
            "     99     2      0.00599      0.00598     1.07e-05       0.0442       0.0598       0.0349       0.0629       0.0489       0.0454       0.0813       0.0633        0.216      0.00225\n",
            "     99     3      0.00641       0.0064     5.52e-06        0.046       0.0619       0.0342       0.0697        0.052       0.0442        0.087       0.0656        0.167      0.00174\n",
            "     99     4      0.00864      0.00859      4.8e-05       0.0546       0.0717       0.0449       0.0741       0.0595       0.0568       0.0948       0.0758        0.513      0.00534\n",
            "     99     5      0.00912      0.00911     2.68e-06       0.0566       0.0739       0.0459       0.0779       0.0619       0.0574       0.0988       0.0781        0.105      0.00109\n",
            "     99     6      0.00862      0.00862     6.39e-07       0.0564       0.0718       0.0462       0.0769       0.0615       0.0563       0.0955       0.0759       0.0465     0.000484\n",
            "     99     7      0.00806      0.00803     3.14e-05       0.0535       0.0693       0.0444       0.0718       0.0581       0.0569       0.0891        0.073        0.412       0.0043\n",
            "     99     8      0.00605      0.00605     2.63e-06       0.0445       0.0602       0.0349       0.0636       0.0493       0.0442       0.0834       0.0638        0.112      0.00117\n",
            "     99     9       0.0066       0.0066     2.62e-06       0.0468       0.0629       0.0354       0.0696       0.0525       0.0452       0.0881       0.0667        0.102      0.00106\n",
            "     99    10        0.007      0.00699     6.19e-06       0.0484       0.0647       0.0392       0.0669        0.053       0.0508        0.086       0.0684        0.181      0.00189\n",
            "     99    11      0.00712       0.0071      1.7e-05       0.0498       0.0652       0.0421       0.0651       0.0536       0.0537       0.0836       0.0686        0.296      0.00309\n",
            "     99    12      0.00568      0.00568     4.24e-06       0.0435       0.0583       0.0338       0.0628       0.0483       0.0429       0.0807       0.0618        0.145      0.00151\n",
            "     99    13      0.00585      0.00585     9.44e-07        0.044       0.0592       0.0324       0.0672       0.0498       0.0413       0.0842       0.0628       0.0541     0.000564\n",
            "     99    14      0.00694      0.00694     9.98e-07       0.0467       0.0645       0.0354       0.0692       0.0523       0.0467         0.09       0.0684       0.0641     0.000667\n",
            "     99    15       0.0064      0.00638     2.05e-05       0.0462       0.0618       0.0355       0.0675       0.0515       0.0449       0.0862       0.0655        0.326      0.00339\n",
            "     99    16      0.00619      0.00618     6.58e-06       0.0452       0.0608       0.0332       0.0694       0.0513       0.0422       0.0868       0.0645        0.188      0.00196\n",
            "     99    17      0.00657      0.00654     2.58e-05       0.0477       0.0626       0.0384       0.0664       0.0524       0.0482       0.0842       0.0662        0.372      0.00388\n",
            "     99    18      0.00755      0.00755     9.19e-07       0.0526       0.0672       0.0443       0.0693       0.0568       0.0553       0.0862       0.0708       0.0541     0.000564\n",
            "     99    19      0.00822      0.00822     2.46e-06        0.053       0.0701       0.0427       0.0736       0.0581        0.055       0.0933       0.0741       0.0943     0.000983\n",
            "     99    20      0.00629      0.00628     2.49e-06       0.0456       0.0613       0.0348       0.0672        0.051       0.0445       0.0856        0.065        0.104      0.00108\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     99     1      0.00686      0.00685     2.43e-06       0.0478        0.064       0.0367         0.07       0.0534       0.0472       0.0886       0.0679       0.0917     0.000955\n",
            "     99     2      0.00686      0.00686     9.97e-07       0.0475       0.0641       0.0359       0.0706       0.0532       0.0464       0.0895       0.0679       0.0487     0.000508\n",
            "     99     3      0.00654      0.00654     1.36e-06       0.0463       0.0626       0.0349       0.0691        0.052       0.0446       0.0881       0.0664       0.0713     0.000743\n",
            "     99     4      0.00669      0.00669     1.58e-06       0.0466       0.0633       0.0353       0.0693       0.0523       0.0449       0.0893       0.0671       0.0802     0.000835\n",
            "     99     5      0.00619      0.00619     8.55e-07       0.0455       0.0609       0.0352       0.0662       0.0507       0.0453       0.0837       0.0645        0.046     0.000479\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              99  378.611    0.005      0.00697     1.03e-05      0.00698       0.0486       0.0646       0.0384       0.0689       0.0536       0.0492       0.0876       0.0684        0.191      0.00199\n",
            "! Validation         99  378.611    0.005      0.00663     1.44e-06      0.00663       0.0467        0.063       0.0356        0.069       0.0523       0.0457       0.0879       0.0668       0.0676     0.000704\n",
            "Wall time: 378.61130683400006\n",
            "! Best model       99    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    100     1      0.00575      0.00575     2.86e-06       0.0442       0.0587       0.0339       0.0647       0.0493        0.042       0.0824       0.0622       0.0924     0.000962\n",
            "    100     2      0.00585      0.00584     5.37e-06       0.0445       0.0591       0.0345       0.0645       0.0495       0.0444       0.0809       0.0627        0.158      0.00164\n",
            "    100     3      0.00614      0.00613     6.38e-06        0.045       0.0606        0.034       0.0668       0.0504       0.0442       0.0843       0.0642        0.174      0.00182\n",
            "    100     4      0.00541      0.00539     1.32e-05       0.0431       0.0568       0.0341        0.061       0.0476       0.0427       0.0777       0.0602        0.262      0.00273\n",
            "    100     5      0.00612      0.00611     6.74e-06       0.0445       0.0605       0.0334       0.0666         0.05       0.0422        0.086       0.0641        0.185      0.00193\n",
            "    100     6      0.00636      0.00636     5.75e-07       0.0444       0.0617       0.0333       0.0666         0.05        0.043       0.0879       0.0654       0.0432      0.00045\n",
            "    100     7      0.00645      0.00645     1.08e-06       0.0466       0.0621       0.0369       0.0659       0.0514       0.0476        0.084       0.0658       0.0617     0.000643\n",
            "    100     8      0.00663      0.00662      1.2e-05       0.0483        0.063       0.0403       0.0643       0.0523       0.0512       0.0815       0.0664        0.247      0.00257\n",
            "    100     9      0.00644      0.00644     1.26e-06       0.0475       0.0621       0.0384       0.0657       0.0521       0.0481       0.0832       0.0657       0.0754     0.000785\n",
            "    100    10      0.00577      0.00577     1.43e-06       0.0442       0.0588       0.0333       0.0661       0.0497       0.0423       0.0824       0.0623       0.0719     0.000749\n",
            "    100    11      0.00703      0.00702     2.97e-06       0.0488       0.0648       0.0394       0.0676       0.0535       0.0501       0.0872       0.0686         0.11      0.00114\n",
            "    100    12      0.00658      0.00657     1.02e-06        0.047       0.0627       0.0363       0.0685       0.0524       0.0455       0.0876       0.0665       0.0637     0.000663\n",
            "    100    13      0.00745      0.00743     2.53e-05       0.0494       0.0667       0.0372       0.0738       0.0555       0.0477       0.0938       0.0707        0.354      0.00369\n",
            "    100    14      0.00614      0.00614     6.64e-07       0.0455       0.0606       0.0343       0.0679       0.0511       0.0434       0.0852       0.0643       0.0512     0.000533\n",
            "    100    15      0.00553      0.00552     1.54e-05       0.0437       0.0575       0.0341       0.0628       0.0484       0.0435       0.0783       0.0609         0.28      0.00292\n",
            "    100    16      0.00693      0.00693     1.64e-06       0.0483       0.0644       0.0383       0.0682       0.0533       0.0498       0.0865       0.0682       0.0895     0.000932\n",
            "    100    17      0.00652      0.00651     9.22e-06       0.0486       0.0624       0.0388        0.068       0.0534       0.0485       0.0836        0.066        0.218      0.00227\n",
            "    100    18      0.00654      0.00654        2e-06       0.0468       0.0626       0.0351       0.0703       0.0527        0.045       0.0877       0.0663       0.0729     0.000759\n",
            "    100    19      0.00617      0.00616     5.72e-06       0.0449       0.0607        0.035       0.0647       0.0499       0.0446       0.0842       0.0644        0.173      0.00181\n",
            "    100    20      0.00662      0.00661     3.46e-06       0.0464       0.0629       0.0338       0.0716       0.0527       0.0437       0.0898       0.0667        0.124      0.00129\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    100     1      0.00683      0.00683     2.42e-06       0.0477       0.0639       0.0366       0.0699       0.0533        0.047       0.0885       0.0678       0.0915     0.000953\n",
            "    100     2      0.00683      0.00683     9.89e-07       0.0474       0.0639       0.0358       0.0705       0.0532       0.0462       0.0894       0.0678       0.0495     0.000516\n",
            "    100     3      0.00652      0.00652     1.37e-06       0.0462       0.0625       0.0348        0.069       0.0519       0.0445        0.088       0.0663        0.071      0.00074\n",
            "    100     4      0.00666      0.00666     1.58e-06       0.0465       0.0631       0.0352       0.0692       0.0522       0.0448       0.0892        0.067       0.0802     0.000835\n",
            "    100     5      0.00617      0.00617     8.57e-07       0.0454       0.0608       0.0351       0.0661       0.0506       0.0452       0.0836       0.0644       0.0452     0.000471\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             100  382.402    0.005      0.00631     5.92e-06      0.00632       0.0461       0.0615       0.0357       0.0668       0.0512       0.0456       0.0848       0.0652        0.145      0.00151\n",
            "! Validation        100  382.402    0.005       0.0066     1.44e-06       0.0066       0.0466       0.0629       0.0355       0.0689       0.0522       0.0456       0.0878       0.0667       0.0675     0.000703\n",
            "Wall time: 382.40250896399994\n",
            "! Best model      100    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    101     1      0.00603      0.00603     4.27e-07       0.0451       0.0601       0.0343       0.0666       0.0505       0.0431       0.0844       0.0637       0.0363     0.000378\n",
            "    101     2      0.00635      0.00634     7.42e-06       0.0456       0.0616       0.0332       0.0703       0.0517       0.0424       0.0882       0.0653        0.187      0.00194\n",
            "    101     3      0.00515      0.00514     2.73e-06       0.0422       0.0555       0.0319       0.0626       0.0473         0.04       0.0777       0.0588        0.076     0.000791\n",
            "    101     4      0.00612      0.00611     9.61e-06       0.0457       0.0605       0.0345       0.0683       0.0514       0.0436       0.0846       0.0641        0.225      0.00234\n",
            "    101     5      0.00725      0.00725     9.93e-07       0.0505       0.0659       0.0427       0.0661       0.0544       0.0537       0.0851       0.0694        0.068     0.000708\n",
            "    101     6      0.00793      0.00793     2.16e-06       0.0531       0.0689       0.0451       0.0689        0.057       0.0568       0.0882       0.0725        0.099      0.00103\n",
            "    101     7      0.00667      0.00665      2.5e-05       0.0476       0.0631       0.0367       0.0694       0.0531        0.046       0.0878       0.0669        0.367      0.00382\n",
            "    101     8       0.0059       0.0059     6.27e-07       0.0436       0.0594       0.0334       0.0641       0.0487        0.043        0.083        0.063       0.0572     0.000596\n",
            "    101     9      0.00676      0.00673      3.2e-05       0.0483       0.0634        0.038        0.069       0.0535        0.048       0.0864       0.0672        0.417      0.00434\n",
            "    101    10      0.00729      0.00728     5.32e-06       0.0501        0.066       0.0413       0.0677       0.0545       0.0529       0.0864       0.0697        0.167      0.00174\n",
            "    101    11      0.00568      0.00568     1.41e-06       0.0441       0.0583        0.034       0.0644       0.0492       0.0431       0.0805       0.0618       0.0807      0.00084\n",
            "    101    12      0.00625      0.00625     1.06e-06       0.0457       0.0612       0.0342       0.0686       0.0514        0.043       0.0868       0.0649       0.0662      0.00069\n",
            "    101    13      0.00578      0.00577     9.93e-06       0.0442       0.0588       0.0344       0.0638       0.0491       0.0443       0.0803       0.0623        0.231       0.0024\n",
            "    101    14      0.00731      0.00731     6.37e-07       0.0492       0.0662       0.0377       0.0722       0.0549        0.049       0.0913       0.0701       0.0385     0.000401\n",
            "    101    15      0.00592      0.00591     1.27e-05       0.0453       0.0595       0.0342       0.0676       0.0509       0.0426       0.0835       0.0631        0.255      0.00266\n",
            "    101    16      0.00608      0.00608     1.05e-06       0.0452       0.0603        0.035       0.0654       0.0502       0.0452       0.0826       0.0639       0.0672       0.0007\n",
            "    101    17      0.00592      0.00592     3.71e-06       0.0439       0.0595       0.0328       0.0663       0.0495       0.0419       0.0844       0.0631        0.129      0.00135\n",
            "    101    18      0.00645      0.00645     9.16e-07       0.0462       0.0621       0.0343       0.0702       0.0522       0.0441       0.0877       0.0659       0.0643     0.000669\n",
            "    101    19      0.00609      0.00609     2.98e-06       0.0448       0.0604       0.0356       0.0631       0.0494       0.0456       0.0823        0.064       0.0961        0.001\n",
            "    101    20      0.00717      0.00715     1.67e-05        0.049       0.0654        0.039        0.069        0.054       0.0496        0.089       0.0693        0.287      0.00299\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    101     1       0.0068       0.0068     2.41e-06       0.0476       0.0638       0.0365       0.0698       0.0532       0.0469       0.0884       0.0677       0.0917     0.000955\n",
            "    101     2      0.00681      0.00681     1.03e-06       0.0473       0.0638       0.0357       0.0704       0.0531       0.0461       0.0893       0.0677       0.0479     0.000498\n",
            "    101     3       0.0065       0.0065     1.28e-06       0.0461       0.0624       0.0347       0.0689       0.0518       0.0444       0.0879       0.0662       0.0698     0.000727\n",
            "    101     4      0.00664      0.00664     1.59e-06       0.0464        0.063       0.0351       0.0691       0.0521       0.0446       0.0891       0.0669       0.0806     0.000839\n",
            "    101     5      0.00615      0.00615      8.6e-07       0.0454       0.0607        0.035        0.066       0.0505        0.045       0.0836       0.0643       0.0454     0.000473\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             101  386.174    0.005       0.0064     6.87e-06      0.00641       0.0465       0.0619       0.0361       0.0672       0.0516       0.0461       0.0851       0.0656        0.151      0.00157\n",
            "! Validation        101  386.174    0.005      0.00658     1.43e-06      0.00658       0.0466       0.0628       0.0354       0.0689       0.0521       0.0454       0.0877       0.0666       0.0671     0.000699\n",
            "Wall time: 386.17495529200005\n",
            "! Best model      101    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    102     1      0.00582      0.00581     2.35e-06       0.0443        0.059       0.0331       0.0669         0.05       0.0423       0.0828       0.0626        0.101      0.00105\n",
            "    102     2      0.00694      0.00693     9.23e-06        0.049       0.0644       0.0395       0.0678       0.0537       0.0508       0.0853       0.0681        0.225      0.00234\n",
            "    102     3      0.00761      0.00761     2.07e-06       0.0513       0.0675       0.0412       0.0714       0.0563        0.052       0.0908       0.0714        0.098      0.00102\n",
            "    102     4        0.008        0.008     1.91e-06       0.0524       0.0692       0.0413       0.0748        0.058       0.0526        0.094       0.0733       0.0785     0.000818\n",
            "    102     5      0.00773      0.00773     7.94e-06       0.0522        0.068       0.0415       0.0735       0.0575       0.0518       0.0922        0.072        0.199      0.00207\n",
            "    102     6      0.00646      0.00645     9.86e-06       0.0472       0.0621       0.0393       0.0629       0.0511       0.0497       0.0815       0.0656        0.217      0.00226\n",
            "    102     7      0.00671      0.00671     4.43e-06       0.0483       0.0634       0.0386       0.0678       0.0532       0.0491        0.085       0.0671        0.148      0.00155\n",
            "    102     8      0.00587      0.00584      2.1e-05       0.0445       0.0591       0.0349       0.0639       0.0494       0.0438       0.0816       0.0627        0.333      0.00347\n",
            "    102     9      0.00834      0.00834     1.67e-06       0.0546       0.0707       0.0453       0.0732       0.0592       0.0568       0.0923       0.0746       0.0729     0.000759\n",
            "    102    10      0.00848      0.00847     8.08e-06       0.0545       0.0712       0.0428       0.0781       0.0604       0.0538        0.097       0.0754        0.204      0.00212\n",
            "    102    11      0.00785      0.00783     2.48e-05       0.0524       0.0685        0.042       0.0731       0.0575       0.0535       0.0913       0.0724        0.361      0.00376\n",
            "    102    12      0.00638      0.00633     4.17e-05        0.047       0.0616       0.0387       0.0635       0.0511       0.0485       0.0817       0.0651        0.474      0.00494\n",
            "    102    13      0.00572      0.00569     2.41e-05       0.0443       0.0584       0.0354       0.0621       0.0487       0.0449       0.0787       0.0618        0.361      0.00376\n",
            "    102    14      0.00665      0.00659     5.84e-05       0.0475       0.0628       0.0364       0.0696        0.053       0.0466       0.0866       0.0666         0.56      0.00583\n",
            "    102    15      0.00742      0.00737     4.55e-05       0.0513       0.0664       0.0422       0.0694       0.0558       0.0524        0.088       0.0702        0.496      0.00517\n",
            "    102    16       0.0079       0.0079     8.95e-07       0.0518       0.0688       0.0406       0.0742       0.0574       0.0513       0.0945       0.0729       0.0576       0.0006\n",
            "    102    17      0.00654      0.00635     0.000181       0.0467       0.0617       0.0378       0.0645       0.0512       0.0476       0.0829       0.0653            1       0.0104\n",
            "    102    18      0.00594      0.00593     1.37e-05       0.0436       0.0596       0.0331       0.0647       0.0489       0.0422       0.0841       0.0632        0.265      0.00276\n",
            "    102    19      0.00757      0.00746     0.000108         0.05       0.0668       0.0383       0.0734       0.0558       0.0486       0.0931       0.0709        0.771      0.00804\n",
            "    102    20       0.0092       0.0092      2.1e-06       0.0575       0.0742       0.0498       0.0731       0.0614       0.0633       0.0922       0.0778       0.0957     0.000997\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    102     1      0.00678      0.00678     2.36e-06       0.0475       0.0637       0.0364       0.0697       0.0531       0.0468       0.0883       0.0675        0.089     0.000927\n",
            "    102     2      0.00678      0.00678     1.02e-06       0.0472       0.0637       0.0356       0.0703        0.053        0.046       0.0891       0.0675        0.049     0.000511\n",
            "    102     3      0.00648      0.00648     1.32e-06        0.046       0.0623       0.0346       0.0688       0.0517       0.0442       0.0878        0.066       0.0697     0.000726\n",
            "    102     4      0.00662      0.00662     1.57e-06       0.0463       0.0629        0.035        0.069        0.052       0.0445        0.089       0.0668       0.0799     0.000832\n",
            "    102     5      0.00612      0.00612     8.68e-07       0.0452       0.0605       0.0349        0.066       0.0504       0.0449       0.0835       0.0642       0.0461      0.00048\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             102  389.951    0.005      0.00713     2.85e-05      0.00716       0.0495       0.0653       0.0396       0.0694       0.0545       0.0503       0.0879       0.0691        0.306      0.00319\n",
            "! Validation        102  389.951    0.005      0.00655     1.43e-06      0.00656       0.0465       0.0626       0.0353       0.0688        0.052       0.0453       0.0876       0.0664       0.0667     0.000695\n",
            "Wall time: 389.9521317849999\n",
            "! Best model      102    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    103     1      0.00817      0.00816     5.04e-06       0.0546       0.0699        0.047       0.0699       0.0584       0.0583       0.0886       0.0735        0.141      0.00146\n",
            "    103     2      0.00655      0.00655     3.56e-06       0.0468       0.0626       0.0348       0.0708       0.0528       0.0441       0.0888       0.0664        0.131      0.00136\n",
            "    103     3      0.00691      0.00689     2.46e-05       0.0481       0.0642       0.0367       0.0707       0.0537       0.0468       0.0894       0.0681        0.363      0.00378\n",
            "    103     4      0.00642      0.00641     5.06e-06       0.0465       0.0619        0.036       0.0674       0.0517       0.0457       0.0856       0.0657        0.142      0.00148\n",
            "    103     5      0.00543      0.00536     6.92e-05       0.0435       0.0567       0.0347       0.0612       0.0479       0.0434       0.0766         0.06         0.61      0.00635\n",
            "    103     6      0.00593      0.00592     5.77e-06       0.0443       0.0595       0.0335       0.0659       0.0497       0.0427       0.0835       0.0631        0.165      0.00172\n",
            "    103     7      0.00753      0.00747     5.42e-05       0.0504       0.0669        0.038       0.0753       0.0566       0.0479        0.094       0.0709        0.544      0.00567\n",
            "    103     8      0.00775      0.00775     3.04e-07       0.0519       0.0681       0.0413        0.073       0.0571       0.0522        0.092       0.0721       0.0322     0.000336\n",
            "    103     9      0.00814      0.00812     1.81e-05       0.0551       0.0697       0.0506        0.064       0.0573       0.0622       0.0827       0.0724        0.315      0.00328\n",
            "    103    10      0.00692      0.00691     8.83e-06       0.0482       0.0643        0.039       0.0667       0.0528       0.0501       0.0859        0.068        0.219      0.00228\n",
            "    103    11       0.0068      0.00679     1.06e-05       0.0484       0.0637       0.0364       0.0723       0.0543       0.0456       0.0896       0.0676        0.233      0.00243\n",
            "    103    12      0.00605      0.00605     2.86e-06       0.0457       0.0602       0.0369       0.0635       0.0502       0.0475       0.0797       0.0636        0.109      0.00114\n",
            "    103    13      0.00811      0.00809      2.2e-05        0.052       0.0696        0.039       0.0782       0.0586       0.0489       0.0987       0.0738        0.341      0.00355\n",
            "    103    14       0.0067       0.0067     7.72e-06       0.0468       0.0633       0.0363       0.0679       0.0521       0.0469       0.0874       0.0671        0.189      0.00197\n",
            "    103    15      0.00626      0.00626     7.63e-06       0.0471       0.0612       0.0381       0.0651       0.0516       0.0471       0.0825       0.0648        0.192        0.002\n",
            "    103    16      0.00687      0.00687     4.15e-06       0.0486       0.0641       0.0376       0.0706       0.0541       0.0468       0.0891        0.068        0.145      0.00151\n",
            "    103    17       0.0086       0.0086     4.72e-07       0.0574       0.0717        0.052       0.0682       0.0601       0.0636       0.0858       0.0747       0.0436     0.000454\n",
            "    103    18      0.00919      0.00918     4.57e-06       0.0587       0.0741        0.053       0.0701       0.0615       0.0654        0.089       0.0772        0.144       0.0015\n",
            "    103    19      0.00954      0.00953     1.15e-05       0.0573       0.0755       0.0442       0.0835       0.0639        0.056        0.104         0.08        0.241      0.00251\n",
            "    103    20       0.0073      0.00729      1.1e-05       0.0501       0.0661       0.0389       0.0725       0.0557       0.0483       0.0918         0.07        0.245      0.00255\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    103     1      0.00676      0.00675     2.34e-06       0.0474       0.0636       0.0363       0.0696        0.053       0.0466       0.0882       0.0674       0.0898     0.000936\n",
            "    103     2      0.00676      0.00675     9.32e-07       0.0471       0.0636       0.0356       0.0702       0.0529       0.0459        0.089       0.0674       0.0468     0.000487\n",
            "    103     3      0.00646      0.00646     1.34e-06       0.0459       0.0622       0.0345       0.0687       0.0516       0.0441       0.0877       0.0659       0.0704     0.000733\n",
            "    103     4       0.0066       0.0066     1.57e-06       0.0462       0.0628       0.0349        0.069       0.0519       0.0444       0.0889       0.0666       0.0807      0.00084\n",
            "    103     5      0.00611      0.00611     7.95e-07       0.0452       0.0605       0.0348       0.0659       0.0504       0.0448       0.0834       0.0641       0.0442     0.000461\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             103  393.750    0.005      0.00724     1.39e-05      0.00726       0.0501       0.0658       0.0402       0.0698        0.055       0.0509       0.0885       0.0697        0.227      0.00237\n",
            "! Validation        103  393.750    0.005      0.00653      1.4e-06      0.00654       0.0464       0.0625       0.0352       0.0687        0.052       0.0452       0.0875       0.0663       0.0664     0.000692\n",
            "Wall time: 393.75070172000005\n",
            "! Best model      103    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    104     1      0.00525      0.00525        3e-06       0.0423        0.056       0.0315        0.064       0.0478       0.0398       0.0791       0.0594        0.119      0.00123\n",
            "    104     2      0.00726      0.00724     2.02e-05       0.0485       0.0658       0.0357       0.0742       0.0549       0.0459       0.0938       0.0698        0.329      0.00343\n",
            "    104     3      0.00773      0.00772     1.03e-05       0.0537        0.068       0.0475       0.0662       0.0568       0.0593       0.0826        0.071        0.223      0.00233\n",
            "    104     4      0.00919      0.00919     2.18e-06       0.0574       0.0742       0.0497       0.0727       0.0612       0.0632       0.0923       0.0777       0.0756     0.000787\n",
            "    104     5      0.00781      0.00779     1.64e-05       0.0516       0.0683       0.0417       0.0714       0.0566       0.0528       0.0918       0.0723        0.286      0.00298\n",
            "    104     6      0.00558      0.00556     1.88e-05       0.0429       0.0577       0.0323        0.064       0.0482       0.0408       0.0816       0.0612        0.317       0.0033\n",
            "    104     7      0.00563      0.00562     7.62e-06       0.0433        0.058       0.0336       0.0626       0.0481       0.0423       0.0807       0.0615        0.203      0.00212\n",
            "    104     8      0.00679      0.00677     1.91e-05       0.0479       0.0636       0.0384        0.067       0.0527       0.0481       0.0867       0.0674        0.323      0.00336\n",
            "    104     9      0.00732      0.00732     2.74e-07       0.0507       0.0662       0.0419       0.0681        0.055       0.0529       0.0868       0.0699       0.0346      0.00036\n",
            "    104    10      0.00636      0.00636      4.4e-06       0.0475       0.0617       0.0377       0.0669       0.0523       0.0469       0.0838       0.0653        0.144       0.0015\n",
            "    104    11      0.00675      0.00674     6.22e-06       0.0486       0.0635        0.038       0.0697       0.0539       0.0488       0.0857       0.0673        0.163       0.0017\n",
            "    104    12      0.00731       0.0073     3.57e-06       0.0513       0.0661       0.0442       0.0655       0.0549       0.0555       0.0833       0.0694        0.137      0.00143\n",
            "    104    13      0.00742      0.00742      1.8e-06       0.0501       0.0666       0.0392       0.0718       0.0555       0.0499       0.0913       0.0706       0.0768       0.0008\n",
            "    104    14      0.00666      0.00663     3.46e-05       0.0469        0.063       0.0352       0.0701       0.0527       0.0447       0.0888       0.0668        0.433      0.00451\n",
            "    104    15      0.00603      0.00603     3.66e-06       0.0441       0.0601       0.0347        0.063       0.0489       0.0451       0.0822       0.0636         0.11      0.00114\n",
            "    104    16      0.00607      0.00605     2.22e-05       0.0455       0.0602       0.0362       0.0641       0.0501        0.046       0.0815       0.0637        0.338      0.00352\n",
            "    104    17      0.00772      0.00772     1.12e-06       0.0526        0.068       0.0435       0.0708       0.0571       0.0547       0.0887       0.0717       0.0633     0.000659\n",
            "    104    18      0.00724      0.00724     2.46e-07       0.0513       0.0658       0.0429        0.068       0.0555       0.0534       0.0854       0.0694       0.0293     0.000305\n",
            "    104    19      0.00716      0.00716     1.31e-06        0.048       0.0654        0.035       0.0742       0.0546       0.0445       0.0943       0.0694        0.068     0.000708\n",
            "    104    20      0.00778      0.00778     8.27e-07       0.0528       0.0682       0.0459       0.0667       0.0563       0.0574       0.0858       0.0716       0.0619     0.000645\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    104     1      0.00674      0.00673     2.35e-06       0.0473       0.0635       0.0362       0.0696       0.0529       0.0465       0.0881       0.0673       0.0907     0.000945\n",
            "    104     2      0.00674      0.00674     9.54e-07        0.047       0.0635       0.0355       0.0702       0.0528       0.0458       0.0889       0.0673        0.046     0.000479\n",
            "    104     3      0.00644      0.00644     1.34e-06       0.0459       0.0621       0.0345       0.0687       0.0516       0.0441       0.0877       0.0659        0.071      0.00074\n",
            "    104     4      0.00658      0.00658     1.61e-06       0.0462       0.0628       0.0348       0.0689       0.0518       0.0443       0.0888       0.0666       0.0817     0.000851\n",
            "    104     5      0.00609      0.00609     8.37e-07       0.0451       0.0604       0.0347       0.0659       0.0503       0.0446       0.0834        0.064       0.0457     0.000476\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             104  397.534    0.005      0.00694     8.89e-06      0.00695       0.0488       0.0645       0.0392       0.0681       0.0537         0.05       0.0864       0.0682        0.177      0.00184\n",
            "! Validation        104  397.534    0.005      0.00652     1.42e-06      0.00652       0.0463       0.0625       0.0351       0.0686       0.0519       0.0451       0.0874       0.0662        0.067     0.000698\n",
            "Wall time: 397.53499456200007\n",
            "! Best model      104    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    105     1      0.00718      0.00718     1.29e-06         0.05       0.0655       0.0417       0.0667       0.0542       0.0533       0.0849       0.0691       0.0781     0.000814\n",
            "    105     2      0.00697      0.00696     1.28e-05       0.0475       0.0645       0.0344       0.0737       0.0541       0.0445       0.0924       0.0684        0.265      0.00276\n",
            "    105     3      0.00696      0.00696     3.54e-07       0.0485       0.0646       0.0386       0.0683       0.0534       0.0499       0.0868       0.0683       0.0307     0.000319\n",
            "    105     4       0.0063      0.00627     2.87e-05        0.047       0.0613       0.0379       0.0652       0.0516        0.047       0.0827       0.0648        0.395      0.00412\n",
            "    105     5      0.00617      0.00617     1.51e-06       0.0458       0.0608       0.0359       0.0654       0.0507       0.0464       0.0823       0.0644       0.0771     0.000804\n",
            "    105     6      0.00607      0.00606     1.51e-05       0.0449       0.0602       0.0344       0.0657       0.0501       0.0431       0.0846       0.0639        0.282      0.00294\n",
            "    105     7      0.00634      0.00634      7.1e-07       0.0461       0.0616        0.035       0.0685       0.0517       0.0444       0.0863       0.0653        0.052     0.000541\n",
            "    105     8        0.007        0.007     4.25e-06       0.0496       0.0647       0.0413       0.0661       0.0537       0.0521       0.0845       0.0683        0.133      0.00139\n",
            "    105     9      0.00608      0.00607     2.66e-06       0.0443       0.0603        0.032       0.0689       0.0505        0.041       0.0869       0.0639        0.117      0.00122\n",
            "    105    10      0.00681       0.0068     8.08e-06       0.0502       0.0638       0.0421       0.0663       0.0542       0.0526       0.0817       0.0672        0.204      0.00213\n",
            "    105    11      0.00832       0.0083     1.92e-05       0.0547       0.0705       0.0458       0.0726       0.0592       0.0577       0.0908       0.0742        0.316      0.00329\n",
            "    105    12      0.00849      0.00847     2.42e-05       0.0545       0.0712       0.0435       0.0765         0.06       0.0537       0.0971       0.0754        0.362      0.00377\n",
            "    105    13      0.00645      0.00639     5.15e-05       0.0458       0.0619       0.0357       0.0658       0.0508       0.0457       0.0855       0.0656        0.527      0.00549\n",
            "    105    14      0.00654      0.00648     6.02e-05       0.0461       0.0623       0.0343       0.0697        0.052       0.0434       0.0887       0.0661        0.572      0.00595\n",
            "    105    15      0.00771      0.00766     5.07e-05        0.051       0.0677       0.0419       0.0692       0.0556       0.0539       0.0891       0.0715        0.523      0.00545\n",
            "    105    16      0.00828      0.00828      3.7e-07       0.0553       0.0704       0.0499        0.066       0.0579       0.0619       0.0848       0.0734       0.0318     0.000332\n",
            "    105    17      0.00778      0.00778     2.84e-06       0.0512       0.0682       0.0398       0.0741       0.0569       0.0503       0.0944       0.0723        0.119      0.00123\n",
            "    105    18      0.00668      0.00667     1.01e-06       0.0476       0.0632       0.0375       0.0677       0.0526       0.0468       0.0872        0.067       0.0666     0.000694\n",
            "    105    19      0.00578      0.00576     2.71e-05       0.0437       0.0587        0.034       0.0631       0.0486       0.0434        0.081       0.0622        0.379      0.00395\n",
            "    105    20      0.00799      0.00796     2.72e-05       0.0522        0.069       0.0401       0.0764       0.0582       0.0508       0.0957       0.0732        0.373      0.00389\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    105     1      0.00672      0.00672      2.3e-06       0.0473       0.0634       0.0362       0.0695       0.0528       0.0464        0.088       0.0672       0.0888     0.000925\n",
            "    105     2      0.00672      0.00672     1.02e-06        0.047       0.0634       0.0354       0.0701       0.0528       0.0457       0.0889       0.0673       0.0486     0.000507\n",
            "    105     3      0.00643      0.00643     1.37e-06       0.0458        0.062       0.0344       0.0687       0.0516        0.044       0.0876       0.0658       0.0711     0.000741\n",
            "    105     4      0.00657      0.00657     1.58e-06       0.0461       0.0627       0.0347       0.0689       0.0518       0.0442       0.0888       0.0665       0.0802     0.000835\n",
            "    105     5      0.00607      0.00607     8.38e-07        0.045       0.0603       0.0346       0.0658       0.0502       0.0445       0.0833       0.0639       0.0453     0.000472\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             105  401.316    0.005      0.00698      1.7e-05        0.007       0.0488       0.0646       0.0388       0.0688       0.0538       0.0494       0.0875       0.0684        0.245      0.00255\n",
            "! Validation        105  401.316    0.005       0.0065     1.42e-06       0.0065       0.0462       0.0624       0.0351       0.0686       0.0518        0.045       0.0873       0.0662       0.0668     0.000696\n",
            "Wall time: 401.31640305799965\n",
            "! Best model      105    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    106     1      0.00682      0.00669     0.000136       0.0479       0.0633        0.038       0.0677       0.0529       0.0482       0.0858        0.067        0.861      0.00897\n",
            "    106     2       0.0073      0.00725     5.06e-05       0.0513       0.0659       0.0442       0.0656       0.0549        0.055       0.0835       0.0692        0.526      0.00548\n",
            "    106     3      0.00627      0.00624     2.72e-05       0.0462       0.0611       0.0353       0.0678       0.0516       0.0457       0.0838       0.0648        0.374       0.0039\n",
            "    106     4       0.0065      0.00635     0.000154       0.0462       0.0616       0.0373       0.0642       0.0507       0.0472       0.0834       0.0653        0.919      0.00957\n",
            "    106     5      0.00607      0.00607      4.9e-07       0.0445       0.0603       0.0337       0.0661       0.0499       0.0433       0.0845       0.0639        0.043     0.000448\n",
            "    106     6      0.00717      0.00704     0.000124       0.0482       0.0649       0.0364       0.0717        0.054       0.0473       0.0904       0.0688        0.823      0.00858\n",
            "    106     7      0.00715      0.00714     3.15e-06       0.0495       0.0654       0.0398       0.0688       0.0543       0.0501       0.0884       0.0692        0.111      0.00115\n",
            "    106     8      0.00639      0.00638     1.27e-05       0.0473       0.0618       0.0384       0.0651       0.0517       0.0482       0.0825       0.0654        0.254      0.00265\n",
            "    106     9      0.00552      0.00552     6.24e-07        0.043       0.0575       0.0325       0.0639       0.0482       0.0405       0.0814       0.0609       0.0527     0.000549\n",
            "    106    10      0.00648      0.00647     5.82e-06       0.0472       0.0622       0.0384       0.0647       0.0516       0.0484       0.0833       0.0659        0.157      0.00164\n",
            "    106    11      0.00502      0.00502     3.51e-07       0.0409       0.0548       0.0309        0.061        0.046       0.0394       0.0768       0.0581       0.0342     0.000356\n",
            "    106    12      0.00711      0.00708     3.21e-05       0.0499       0.0651       0.0402       0.0691       0.0547       0.0509       0.0868       0.0688         0.42      0.00437\n",
            "    106    13      0.00655      0.00655      2.4e-06       0.0474       0.0626       0.0396        0.063       0.0513       0.0499       0.0823       0.0661        0.114      0.00119\n",
            "    106    14      0.00771      0.00764     7.41e-05       0.0511       0.0676       0.0399       0.0735       0.0567       0.0501       0.0933       0.0717        0.637      0.00664\n",
            "    106    15      0.00643      0.00642     3.95e-06       0.0461        0.062       0.0342       0.0699       0.0521       0.0433       0.0883       0.0658        0.138      0.00144\n",
            "    106    16      0.00655      0.00653      2.7e-05       0.0469       0.0625       0.0368       0.0673        0.052       0.0469       0.0855       0.0662        0.382      0.00398\n",
            "    106    17      0.00648      0.00647     7.38e-06       0.0452       0.0623       0.0327       0.0702       0.0514       0.0418       0.0902        0.066        0.199      0.00208\n",
            "    106    18      0.00638      0.00637     9.33e-06       0.0459       0.0618       0.0356       0.0665        0.051       0.0463       0.0847       0.0655        0.196      0.00204\n",
            "    106    19      0.00584      0.00583     9.71e-07       0.0448       0.0591        0.035       0.0642       0.0496       0.0443        0.081       0.0626       0.0672       0.0007\n",
            "    106    20      0.00664      0.00664     7.52e-07       0.0474       0.0631        0.036         0.07        0.053       0.0456       0.0882       0.0669       0.0516     0.000537\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    106     1       0.0067       0.0067     2.28e-06       0.0472       0.0633       0.0361       0.0694       0.0528       0.0464       0.0879       0.0671       0.0895     0.000932\n",
            "    106     2       0.0067       0.0067     9.63e-07       0.0469       0.0633       0.0354       0.0701       0.0527       0.0456       0.0888       0.0672       0.0468     0.000487\n",
            "    106     3      0.00642      0.00642     1.36e-06       0.0458        0.062       0.0344       0.0687       0.0515       0.0439       0.0876       0.0657       0.0717     0.000747\n",
            "    106     4      0.00655      0.00655     1.56e-06       0.0461       0.0626       0.0347       0.0688       0.0518       0.0441       0.0887       0.0664       0.0798     0.000831\n",
            "    106     5      0.00606      0.00606     7.75e-07       0.0449       0.0602       0.0345       0.0657       0.0501       0.0444       0.0832       0.0638       0.0435     0.000453\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             106  405.096    0.005      0.00649     3.36e-05      0.00652       0.0468       0.0623       0.0367        0.067       0.0519       0.0467       0.0853        0.066        0.318      0.00331\n",
            "! Validation        106  405.096    0.005      0.00649     1.39e-06      0.00649       0.0462       0.0623        0.035       0.0686       0.0518       0.0449       0.0873       0.0661       0.0662      0.00069\n",
            "Wall time: 405.09664915799976\n",
            "! Best model      106    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    107     1      0.00575      0.00574     4.91e-06       0.0441       0.0586       0.0346       0.0633       0.0489       0.0438       0.0804       0.0621        0.154       0.0016\n",
            "    107     2       0.0058       0.0058     3.88e-06        0.045       0.0589       0.0358       0.0634       0.0496       0.0452       0.0795       0.0624        0.131      0.00136\n",
            "    107     3      0.00648      0.00646     1.41e-05       0.0458       0.0622       0.0354       0.0668       0.0511       0.0461       0.0858       0.0659        0.272      0.00283\n",
            "    107     4      0.00623      0.00622     3.25e-06       0.0451        0.061       0.0346       0.0662       0.0504       0.0443       0.0852       0.0647        0.117      0.00122\n",
            "    107     5      0.00598      0.00597     9.93e-06       0.0433       0.0598       0.0323       0.0652       0.0488       0.0417        0.085       0.0634        0.226      0.00235\n",
            "    107     6      0.00617      0.00615     1.72e-05       0.0454       0.0607       0.0347       0.0666       0.0507       0.0442       0.0844       0.0643        0.301      0.00314\n",
            "    107     7      0.00618      0.00617     3.24e-06        0.046       0.0608       0.0351       0.0677       0.0514       0.0441       0.0848       0.0645        0.127      0.00132\n",
            "    107     8      0.00695      0.00689     5.58e-05       0.0483       0.0642       0.0378       0.0692       0.0535       0.0486       0.0875        0.068        0.552      0.00575\n",
            "    107     9      0.00591      0.00591     3.93e-06       0.0445       0.0595       0.0333        0.067       0.0501       0.0425       0.0836       0.0631        0.115      0.00119\n",
            "    107    10      0.00608      0.00606     2.84e-05       0.0451       0.0602       0.0335       0.0683       0.0509       0.0426       0.0851       0.0639        0.388      0.00404\n",
            "    107    11      0.00608      0.00605     3.23e-05       0.0455       0.0602       0.0353       0.0659       0.0506       0.0454        0.082       0.0637        0.419      0.00436\n",
            "    107    12      0.00637      0.00636     2.43e-06       0.0459       0.0617       0.0347       0.0683       0.0515       0.0444       0.0865       0.0655        0.104      0.00108\n",
            "    107    13      0.00611      0.00607     4.24e-05       0.0447       0.0603       0.0336       0.0669       0.0503       0.0423       0.0856       0.0639        0.477      0.00497\n",
            "    107    14      0.00578      0.00574     4.16e-05       0.0442       0.0586       0.0334       0.0657       0.0495       0.0428       0.0815       0.0622        0.477      0.00497\n",
            "    107    15      0.00585      0.00583     1.36e-05       0.0446       0.0591       0.0355       0.0627       0.0491       0.0449       0.0803       0.0626         0.27      0.00281\n",
            "    107    16      0.00618      0.00617     5.57e-06       0.0451       0.0608       0.0342        0.067       0.0506       0.0443       0.0846       0.0644        0.166      0.00173\n",
            "    107    17      0.00589      0.00587     1.59e-05       0.0442       0.0593       0.0345       0.0635        0.049       0.0436       0.0821       0.0628        0.295      0.00307\n",
            "    107    18      0.00591      0.00591     2.93e-06       0.0438       0.0595       0.0332        0.065       0.0491       0.0426       0.0836       0.0631        0.104      0.00108\n",
            "    107    19      0.00694      0.00691     2.73e-05       0.0485       0.0643       0.0385       0.0686       0.0535        0.049       0.0872       0.0681        0.386      0.00402\n",
            "    107    20      0.00711      0.00711     6.23e-07       0.0506       0.0652       0.0411       0.0695       0.0553       0.0515       0.0864       0.0689       0.0449     0.000468\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "    107     1      0.00668      0.00668     2.24e-06       0.0471       0.0632        0.036       0.0694       0.0527       0.0462       0.0878        0.067       0.0875     0.000911\n",
            "    107     2      0.00668      0.00668     9.72e-07       0.0468       0.0632       0.0353         0.07       0.0526       0.0454       0.0887       0.0671       0.0468     0.000487\n",
            "    107     3       0.0064       0.0064     1.34e-06       0.0457       0.0619       0.0343       0.0686       0.0514       0.0438       0.0875       0.0656       0.0705     0.000734\n",
            "    107     4      0.00653      0.00653     1.57e-06        0.046       0.0625       0.0346       0.0687       0.0517        0.044       0.0886       0.0663       0.0793     0.000826\n",
            "    107     5      0.00604      0.00604     8.25e-07       0.0449       0.0601       0.0344       0.0657       0.0501       0.0443       0.0832       0.0637       0.0445     0.000464\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train             107  408.876    0.005      0.00617     1.65e-05      0.00619       0.0455       0.0608       0.0351       0.0663       0.0507       0.0448       0.0841       0.0644        0.256      0.00267\n",
            "! Validation        107  408.876    0.005      0.00647     1.39e-06      0.00647       0.0461       0.0622       0.0349       0.0685       0.0517       0.0448       0.0872        0.066       0.0657     0.000685\n",
            "Wall time: 408.8766826179999\n",
            "! Best model      107    0.006\n",
            "! Stop training: Early stopping: validation_loss has not reduced for 50 epochs\n",
            "Wall time: 408.90752218299986\n",
            "Cumulative wall time: 408.90752218299986\n"
          ]
        }
      ],
      "source": [
        "!rm -rf ./results\n",
        "!nequip-train /content/nequip/configs/water-example.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJitSZgLYNNF"
      },
      "source": [
        "### Deploy the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo_kIpYV00as"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/deploy.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VoeGtlA02KQ"
      },
      "source": [
        "We now convert the model to a potential file. This makes it independent of NequIP and we can use it any downstream application, such as LAMMPS. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLlagzVhrVGz",
        "outputId": "2c4ae5d3-27b9-4bdb-9806-75dc2bb2b498"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 5.3M\n",
            "-rw------- 1 root root 227K Jul 11 14:43 config.yaml\n",
            "-rw-r--r-- 1 root root  489 Jul 11 14:43 metrics_initialization.csv\n",
            "-rw-r--r-- 1 root root 517K Jul 11 14:50 metrics_batch_train.csv\n",
            "-rw-r--r-- 1 root root 131K Jul 11 14:50 metrics_batch_val.csv\n",
            "-rw-r--r-- 1 root root  45K Jul 11 14:50 metrics_epoch.csv\n",
            "-rw------- 1 root root 657K Jul 11 14:50 best_model.pth\n",
            "-rw-r--r-- 1 root root 594K Jul 11 14:50 log\n",
            "-rw------- 1 root root 2.5M Jul 11 14:50 trainer.pth\n",
            "-rw------- 1 root root 657K Jul 11 14:50 last_model.pth\n"
          ]
        }
      ],
      "source": [
        "! ls -lrth results/water/example-run-water"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJmFAbBzez3P",
        "outputId": "744aea66-83cc-47a1-cbbe-2e132b0efb2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: nequip-deploy [-h] [--verbose VERBOSE] {info,build} ...\n",
            "\n",
            "Create and view information about deployed NequIP potentials.\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help         show this help message and exit\n",
            "  --verbose VERBOSE  log level\n",
            "\n",
            "commands:\n",
            "  {info,build}\n",
            "    info             Get information from a deployed model file\n",
            "    build            Build a deployment model\n"
          ]
        }
      ],
      "source": [
        "! nequip-deploy -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3NJJgtDIDNc",
        "outputId": "fb22e03a-0566-4890-e1ff-7b49417418d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:root:Loading best_model from training session...\n",
            "INFO:root:Compiled & optimized model.\n"
          ]
        }
      ],
      "source": [
        "!nequip-deploy build --train-dir /content/results/water/example-run-water water-deploy.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXpcE3oP0LyD"
      },
      "source": [
        "## Evaluate Test Error on all remaining frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wRKKCZ2PRl3"
      },
      "source": [
        "Before running inference, we'd like to know how well the model is doing on a hold-out test set. We run the nequip-evaluate command to compute the test error on all data that we didn't use for training or validation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB54WSrN0PaS",
        "outputId": "db5b087a-deaf-4eb9-b89e-91f5f0c3e845"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "WARNING: please note that models running on CUDA are usually nondeterministc and that this manifests in the final test errors; for a _more_ deterministic result, please use `--device cpu`\n",
            "Loading model... \n",
            "loaded model from training session\n",
            "Loading original dataset...\n",
            "Loaded dataset specified in config.yaml.\n",
            "Using origial training dataset (10000 frames) minus training (100 frames) and validation frames (50 frames), yielding a test set size of 9850 frames.\n",
            "Starting...\n",
            "  0% 0/9850 [00:00<?, ?it/s]\n",
            "\u001b[A\n",
            "  1% 50/9850 [00:00<01:41, 97.03it/s]\n",
            "  1% 100/9850 [00:01<02:30, 64.80it/s]\n",
            "  2% 150/9850 [00:02<03:26, 46.94it/s]\n",
            "  2% 200/9850 [00:04<03:50, 41.84it/s]\n",
            "  3% 250/9850 [00:04<02:50, 56.46it/s]\n",
            "  3% 300/9850 [00:04<02:13, 71.60it/s]\n",
            "  4% 350/9850 [00:05<01:50, 86.07it/s]\n",
            "  4% 400/9850 [00:05<01:35, 99.08it/s]\n",
            "  5% 450/9850 [00:05<01:25, 110.50it/s]\n",
            "  5% 500/9850 [00:06<01:18, 119.81it/s]\n",
            "  6% 550/9850 [00:06<01:13, 126.59it/s]\n",
            "  6% 600/9850 [00:06<01:10, 131.96it/s]\n",
            "  7% 650/9850 [00:07<01:07, 135.63it/s]\n",
            "  7% 700/9850 [00:07<01:06, 138.43it/s]\n",
            "  8% 750/9850 [00:08<01:04, 140.54it/s]\n",
            "  8% 800/9850 [00:08<01:03, 142.20it/s]\n",
            "  9% 850/9850 [00:08<01:02, 143.84it/s]\n",
            "  9% 900/9850 [00:09<01:01, 145.30it/s]\n",
            " 10% 950/9850 [00:09<01:00, 146.18it/s]\n",
            " 10% 1000/9850 [00:09<01:00, 146.62it/s]\n",
            " 11% 1050/9850 [00:10<00:59, 147.12it/s]\n",
            " 11% 1100/9850 [00:10<00:59, 147.23it/s]\n",
            " 12% 1150/9850 [00:10<00:58, 147.83it/s]\n",
            " 12% 1200/9850 [00:11<00:58, 148.47it/s]\n",
            " 13% 1250/9850 [00:11<00:57, 149.20it/s]\n",
            " 13% 1300/9850 [00:11<00:57, 149.58it/s]\n",
            " 14% 1350/9850 [00:12<00:56, 149.69it/s]\n",
            " 14% 1400/9850 [00:12<00:56, 149.28it/s]\n",
            " 15% 1450/9850 [00:12<00:56, 148.89it/s]\n",
            " 15% 1500/9850 [00:13<00:56, 149.10it/s]\n",
            " 16% 1550/9850 [00:13<00:55, 149.60it/s]\n",
            " 16% 1600/9850 [00:13<00:54, 150.51it/s]\n",
            " 17% 1650/9850 [00:14<00:54, 149.83it/s]\n",
            " 17% 1700/9850 [00:14<00:54, 149.64it/s]\n",
            " 18% 1750/9850 [00:14<00:54, 148.96it/s]\n",
            " 18% 1800/9850 [00:15<00:54, 148.75it/s]\n",
            " 19% 1850/9850 [00:15<00:53, 148.54it/s]\n",
            " 19% 1900/9850 [00:15<00:53, 148.63it/s]\n",
            " 20% 1950/9850 [00:16<00:53, 148.78it/s]\n",
            " 20% 2000/9850 [00:16<00:52, 148.71it/s]\n",
            " 21% 2050/9850 [00:16<00:52, 148.49it/s]\n",
            " 21% 2100/9850 [00:17<00:52, 148.38it/s]\n",
            " 22% 2150/9850 [00:17<00:51, 148.45it/s]\n",
            " 22% 2200/9850 [00:17<00:51, 148.69it/s]\n",
            " 23% 2250/9850 [00:18<00:51, 148.65it/s]\n",
            " 23% 2300/9850 [00:18<00:50, 148.99it/s]\n",
            " 24% 2350/9850 [00:18<00:50, 148.61it/s]\n",
            " 24% 2400/9850 [00:19<00:49, 149.15it/s]\n",
            " 25% 2450/9850 [00:19<00:49, 148.82it/s]\n",
            " 25% 2500/9850 [00:19<00:49, 149.12it/s]\n",
            " 26% 2550/9850 [00:20<00:49, 148.72it/s]\n",
            " 26% 2600/9850 [00:20<00:48, 148.16it/s]\n",
            " 27% 2650/9850 [00:20<00:48, 148.43it/s]\n",
            " 27% 2700/9850 [00:21<00:48, 148.23it/s]\n",
            " 28% 2750/9850 [00:21<00:47, 148.16it/s]\n",
            " 28% 2800/9850 [00:21<00:47, 148.87it/s]\n",
            " 29% 2850/9850 [00:22<00:47, 148.45it/s]\n",
            " 29% 2900/9850 [00:22<00:46, 148.00it/s]\n",
            " 30% 2950/9850 [00:22<00:46, 148.50it/s]\n",
            " 30% 3000/9850 [00:23<00:45, 148.96it/s]\n",
            " 31% 3050/9850 [00:23<00:45, 149.19it/s]\n",
            " 31% 3100/9850 [00:23<00:44, 150.04it/s]\n",
            " 32% 3150/9850 [00:24<00:44, 150.27it/s]\n",
            " 32% 3200/9850 [00:24<00:44, 149.86it/s]\n",
            " 33% 3250/9850 [00:24<00:43, 150.36it/s]\n",
            " 34% 3300/9850 [00:25<00:43, 149.63it/s]\n",
            " 34% 3350/9850 [00:25<00:43, 149.21it/s]\n",
            " 35% 3400/9850 [00:25<00:43, 149.43it/s]\n",
            " 35% 3450/9850 [00:26<00:42, 149.62it/s]\n",
            " 36% 3500/9850 [00:26<00:42, 149.44it/s]\n",
            " 36% 3550/9850 [00:26<00:42, 149.23it/s]\n",
            " 37% 3600/9850 [00:27<00:41, 149.11it/s]\n",
            " 37% 3650/9850 [00:27<00:41, 147.93it/s]\n",
            " 38% 3700/9850 [00:27<00:41, 147.32it/s]\n",
            " 38% 3750/9850 [00:28<00:41, 147.14it/s]\n",
            " 39% 3800/9850 [00:28<00:41, 146.89it/s]\n",
            " 39% 3850/9850 [00:28<00:40, 147.32it/s]\n",
            " 40% 3900/9850 [00:29<00:40, 147.02it/s]\n",
            " 40% 3950/9850 [00:29<00:40, 146.69it/s]\n",
            " 41% 4000/9850 [00:29<00:39, 146.57it/s]\n",
            " 41% 4050/9850 [00:30<00:39, 146.46it/s]\n",
            " 42% 4100/9850 [00:30<00:39, 146.63it/s]\n",
            " 42% 4150/9850 [00:30<00:38, 146.48it/s]\n",
            " 43% 4200/9850 [00:31<00:38, 146.27it/s]\n",
            " 43% 4250/9850 [00:31<00:38, 145.32it/s]\n",
            " 44% 4300/9850 [00:31<00:38, 145.71it/s]\n",
            " 44% 4350/9850 [00:32<00:37, 145.97it/s]\n",
            " 45% 4400/9850 [00:32<00:37, 145.70it/s]\n",
            " 45% 4450/9850 [00:32<00:37, 145.27it/s]\n",
            " 46% 4500/9850 [00:33<00:36, 145.70it/s]\n",
            " 46% 4550/9850 [00:33<00:36, 145.84it/s]\n",
            " 47% 4600/9850 [00:33<00:35, 146.53it/s]\n",
            " 47% 4650/9850 [00:34<00:35, 146.33it/s]\n",
            " 48% 4700/9850 [00:34<00:35, 146.22it/s]\n",
            " 48% 4750/9850 [00:35<00:34, 146.09it/s]\n",
            " 49% 4800/9850 [00:35<00:34, 145.45it/s]\n",
            " 49% 4850/9850 [00:35<00:34, 145.18it/s]\n",
            " 50% 4900/9850 [00:36<00:34, 145.11it/s]\n",
            " 50% 4950/9850 [00:36<00:33, 144.93it/s]\n",
            " 51% 5000/9850 [00:36<00:33, 145.06it/s]\n",
            " 51% 5050/9850 [00:37<00:33, 145.07it/s]\n",
            " 52% 5100/9850 [00:37<00:32, 145.06it/s]\n",
            " 52% 5150/9850 [00:37<00:32, 145.14it/s]\n",
            " 53% 5200/9850 [00:38<00:32, 145.00it/s]\n",
            " 53% 5250/9850 [00:38<00:31, 144.99it/s]\n",
            " 54% 5300/9850 [00:38<00:31, 144.95it/s]\n",
            " 54% 5350/9850 [00:39<00:31, 144.89it/s]\n",
            " 55% 5400/9850 [00:39<00:30, 144.32it/s]\n",
            " 55% 5450/9850 [00:39<00:30, 144.23it/s]\n",
            " 56% 5500/9850 [00:40<00:30, 144.44it/s]\n",
            " 56% 5550/9850 [00:40<00:29, 143.80it/s]\n",
            " 57% 5600/9850 [00:40<00:29, 143.61it/s]\n",
            " 57% 5650/9850 [00:41<00:29, 143.64it/s]\n",
            " 58% 5700/9850 [00:41<00:28, 143.59it/s]\n",
            " 58% 5750/9850 [00:41<00:28, 143.56it/s]\n",
            " 59% 5800/9850 [00:42<00:28, 143.61it/s]\n",
            " 59% 5850/9850 [00:42<00:27, 143.26it/s]\n",
            " 60% 5900/9850 [00:43<00:27, 143.87it/s]\n",
            " 60% 5950/9850 [00:43<00:27, 144.05it/s]\n",
            " 61% 6000/9850 [00:43<00:26, 143.95it/s]\n",
            " 61% 6050/9850 [00:44<00:26, 144.18it/s]\n",
            " 62% 6100/9850 [00:44<00:26, 144.13it/s]\n",
            " 62% 6150/9850 [00:44<00:25, 143.82it/s]\n",
            " 63% 6200/9850 [00:45<00:25, 143.79it/s]\n",
            " 63% 6250/9850 [00:45<00:25, 143.73it/s]\n",
            " 64% 6300/9850 [00:45<00:24, 144.01it/s]\n",
            " 64% 6350/9850 [00:46<00:24, 144.04it/s]\n",
            " 65% 6400/9850 [00:46<00:23, 144.52it/s]\n",
            " 65% 6450/9850 [00:46<00:23, 144.49it/s]\n",
            " 66% 6500/9850 [00:47<00:23, 144.93it/s]\n",
            " 66% 6550/9850 [00:47<00:22, 144.61it/s]\n",
            " 67% 6600/9850 [00:47<00:22, 144.53it/s]\n",
            " 68% 6650/9850 [00:48<00:22, 144.64it/s]\n",
            " 68% 6700/9850 [00:48<00:21, 144.71it/s]\n",
            " 69% 6750/9850 [00:48<00:21, 144.89it/s]\n",
            " 69% 6800/9850 [00:49<00:21, 144.97it/s]\n",
            " 70% 6850/9850 [00:49<00:20, 145.14it/s]\n",
            " 70% 6900/9850 [00:49<00:20, 145.20it/s]\n",
            " 71% 6950/9850 [00:50<00:19, 145.63it/s]\n",
            " 71% 7000/9850 [00:50<00:19, 145.62it/s]\n",
            " 72% 7050/9850 [00:50<00:19, 145.50it/s]\n",
            " 72% 7100/9850 [00:51<00:18, 145.02it/s]\n",
            " 73% 7150/9850 [00:51<00:18, 144.82it/s]\n",
            " 73% 7200/9850 [00:51<00:18, 144.57it/s]\n",
            " 74% 7250/9850 [00:52<00:17, 144.70it/s]\n",
            " 74% 7300/9850 [00:52<00:17, 144.90it/s]\n",
            " 75% 7350/9850 [00:53<00:17, 144.95it/s]\n",
            " 75% 7400/9850 [00:53<00:16, 145.09it/s]\n",
            " 76% 7450/9850 [00:53<00:16, 145.46it/s]\n",
            " 76% 7500/9850 [00:54<00:16, 145.05it/s]\n",
            " 77% 7550/9850 [00:54<00:15, 144.84it/s]\n",
            " 77% 7600/9850 [00:54<00:15, 145.18it/s]\n",
            " 78% 7650/9850 [00:55<00:15, 145.34it/s]\n",
            " 78% 7700/9850 [00:55<00:14, 145.20it/s]\n",
            " 79% 7750/9850 [00:55<00:14, 144.92it/s]\n",
            " 79% 7800/9850 [00:56<00:14, 144.71it/s]\n",
            " 80% 7850/9850 [00:56<00:13, 145.01it/s]\n",
            " 80% 7900/9850 [00:56<00:13, 145.17it/s]\n",
            " 81% 7950/9850 [00:57<00:13, 145.28it/s]\n",
            " 81% 8000/9850 [00:57<00:12, 145.06it/s]\n",
            " 82% 8050/9850 [00:57<00:12, 144.98it/s]\n",
            " 82% 8100/9850 [00:58<00:12, 145.39it/s]\n",
            " 83% 8150/9850 [00:58<00:11, 145.74it/s]\n",
            " 83% 8200/9850 [00:58<00:11, 146.22it/s]\n",
            " 84% 8250/9850 [00:59<00:10, 146.23it/s]\n",
            " 84% 8300/9850 [00:59<00:10, 146.09it/s]\n",
            " 85% 8350/9850 [00:59<00:10, 146.17it/s]\n",
            " 85% 8400/9850 [01:00<00:09, 146.35it/s]\n",
            " 86% 8450/9850 [01:00<00:09, 146.21it/s]\n",
            " 86% 8500/9850 [01:00<00:09, 145.79it/s]\n",
            " 87% 8550/9850 [01:01<00:08, 145.87it/s]\n",
            " 87% 8600/9850 [01:01<00:08, 145.77it/s]\n",
            " 88% 8650/9850 [01:01<00:08, 145.34it/s]\n",
            " 88% 8700/9850 [01:02<00:07, 145.33it/s]\n",
            " 89% 8750/9850 [01:02<00:07, 145.10it/s]\n",
            " 89% 8800/9850 [01:02<00:07, 144.93it/s]\n",
            " 90% 8850/9850 [01:03<00:06, 144.81it/s]\n",
            " 90% 8900/9850 [01:03<00:06, 144.79it/s]\n",
            " 91% 8950/9850 [01:04<00:06, 144.79it/s]\n",
            " 91% 9000/9850 [01:04<00:05, 144.69it/s]\n",
            " 92% 9050/9850 [01:04<00:05, 144.70it/s]\n",
            " 92% 9100/9850 [01:05<00:05, 144.75it/s]\n",
            " 93% 9150/9850 [01:05<00:04, 144.47it/s]\n",
            " 93% 9200/9850 [01:05<00:04, 144.43it/s]\n",
            " 94% 9250/9850 [01:06<00:04, 144.64it/s]\n",
            " 94% 9300/9850 [01:06<00:03, 145.02it/s]\n",
            " 95% 9350/9850 [01:06<00:03, 145.34it/s]\n",
            " 95% 9400/9850 [01:07<00:03, 145.52it/s]\n",
            " 96% 9450/9850 [01:07<00:02, 145.06it/s]\n",
            " 96% 9500/9850 [01:07<00:02, 144.16it/s]\n",
            " 97% 9550/9850 [01:08<00:02, 144.00it/s]\n",
            " 97% 9600/9850 [01:08<00:01, 144.22it/s]\n",
            " 98% 9650/9850 [01:08<00:01, 144.06it/s]\n",
            " 98% 9700/9850 [01:09<00:01, 143.77it/s]\n",
            " 99% 9750/9850 [01:09<00:00, 144.12it/s]\n",
            " 99% 9800/9850 [01:09<00:00, 143.50it/s]\n",
            "100% 9850/9850 [01:10<00:00, 140.16it/s]\n",
            "\n",
            "\n",
            "--- Final result: ---\n",
            "               f_mae =  0.046357           \n",
            "              f_rmse =  0.062583           \n",
            "             H_f_mae =  0.035020           \n",
            "             O_f_mae =  0.069030           \n",
            "         psavg_f_mae =  0.052025           \n",
            "            H_f_rmse =  0.045198           \n",
            "            O_f_rmse =  0.087545           \n",
            "        psavg_f_rmse =  0.066372           \n",
            "               e_mae =  0.059487           \n",
            "             e/N_mae =  0.000620           \n",
            "               f_mae =  0.046357           \n",
            "              f_rmse =  0.062583           \n",
            "             H_f_mae =  0.035020           \n",
            "             O_f_mae =  0.069030           \n",
            "         psavg_f_mae =  0.052025           \n",
            "            H_f_rmse =  0.045198           \n",
            "            O_f_rmse =  0.087545           \n",
            "        psavg_f_rmse =  0.066372           \n",
            "               e_mae =  0.059487           \n",
            "             e/N_mae =  0.000620           \n"
          ]
        }
      ],
      "source": [
        "!nequip-evaluate --train-dir results/water/example-run-water --batch-size 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQHrMMnsPaJO"
      },
      "source": [
        "Again, energy errors of < 1meV/atom (converted from kcal/mol to eV), and force errors of ~45 meV/A 🎉"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4r5FBXaum9n"
      },
      "source": [
        "# LAMMPS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qIYIYyr1B4O"
      },
      "source": [
        "We are now in a position to run MD with our potential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UirNBTlJ1BNZ"
      },
      "source": [
        "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/run.png?raw=true\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQs0ijPhvAGb"
      },
      "source": [
        "Set up a simple LAMMPS input file\n",
        "\n",
        "CAUTION: the reference data here are in eV for the energies and eV/A for the forces. The NequIP model will therefore also be predicting outputs in these units. We are therefore using `units metal` in LAMMPS (see [docs](https://docs.lammps.org/units.html)). Time units are also in`ps`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AXv_y_JD1TJ"
      },
      "source": [
        "### We now run MD in the NVT ensemble for 10 ps (20,000 steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W090KfMsd2Do"
      },
      "outputs": [],
      "source": [
        "lammps_input_md = \"\"\"\n",
        "units           metal\n",
        "boundary        p p p\n",
        "atom_style      atomic\n",
        "thermo 1\n",
        "newton off\n",
        "read_data structure.data\n",
        "\n",
        "neighbor        1.0 bin\n",
        "neigh_modify    every 10 delay 0 check no\n",
        "\n",
        "pair_style\tnequip\n",
        "pair_coeff\t* * ../water-deploy.pth H O\n",
        "mass            1 1.00794\n",
        "mass            2 15.9994\n",
        "\n",
        "velocity        all create 300.0 23456789\n",
        "timestep        0.0005\n",
        "fix             1 all nvt temp 300.0 300.0 $(100.0*dt)\n",
        "\n",
        "#print log every X steps\n",
        "thermo          100\n",
        "thermo_style    custom step pe ke etotal temp press vol\n",
        "\n",
        "#print trajectory in xyz every X time units\n",
        "dump              1 all xyz 20 water.xyz \n",
        "dump_modify       1 element H O\n",
        "\n",
        "# dump            2 all custom 1 dump_frc.lamppstrj id type element fx fy fz\n",
        "# dump_modify     2 element O H\n",
        "# dump            3 all custom 1 dump.lammpstrj id type element x y z\n",
        "# dump_modify     3 element O H\n",
        "\n",
        "run             20000\n",
        "\"\"\"\n",
        "\n",
        "!mkdir lammps_run\n",
        "with open(\"lammps_run/water_md.in\", \"w\") as f:\n",
        "    f.write(lammps_input_md)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mu4kbiXOt1pI"
      },
      "outputs": [],
      "source": [
        "! cp /content/water-deploy.pth /content/lammps_run/."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvWZCw1zvjRc"
      },
      "source": [
        "We specify the initial water configuration by reading the extxyz file and writing to structure.data file easily parsed by lammps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXeHb2ZPvIbU"
      },
      "outputs": [],
      "source": [
        "wat_pos_frc_trj = read(\"/content/nequip/data/AIMD_data/wat_pos_frc-10k.extxyz\")\n",
        "write(\"/content/lammps_run/structure.data\", wat_pos_frc_trj, format=\"lammps-data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDuyueY11YBF"
      },
      "source": [
        "### Run the LAMMPS command: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG1LE98LukSO",
        "outputId": "ec225291-2d19-4501-97d2-f78907ae62a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "units           metal\n",
            "boundary        p p p\n",
            "atom_style      atomic\n",
            "thermo 1\n",
            "newton off\n",
            "read_data structure.data\n",
            "\n",
            "neighbor        1.0 bin\n",
            "neigh_modify    every 10 delay 0 check no\n",
            "\n",
            "pair_style\tnequip\n",
            "pair_coeff\t* * ../water-deploy.pth H O\n",
            "mass            1 1.00794\n",
            "mass            2 15.9994\n",
            "\n",
            "velocity        all create 300.0 23456789\n",
            "timestep        0.0005\n",
            "fix             1 all nvt temp 300.0 300.0 $(100.0*dt)\n",
            "\n",
            "#print log every X steps\n",
            "thermo          100\n",
            "thermo_style    custom step pe ke etotal temp press vol\n",
            "\n",
            "#print trajectory in xyz every X time units\n",
            "dump              1 all xyz 20 water.xyz \n",
            "dump_modify       1 element H O\n",
            "\n",
            "# dump            2 all custom 1 dump_frc.lamppstrj id type element fx fy fz\n",
            "# dump_modify     2 element O H\n",
            "# dump            3 all custom 1 dump.lammpstrj id type element x y z\n",
            "# dump_modify     3 element O H\n",
            "\n",
            "run             20000\n",
            "/content/lammps_run/structure.data (written by ASE) \n",
            "\n",
            "96 \t atoms \n",
            "2  atom types\n",
            "0.0      9.8499999999999996  xlo xhi\n",
            "0.0      9.8499999999999996  ylo yhi\n",
            "0.0      9.8499999999999996  zlo zhi\n",
            "\n",
            "\n",
            "Atoms \n",
            "\n",
            "     1   1      42.886169670000001   -0.055681660000000001      38.329161120000002\n",
            "     2   1      34.202588720000001              -0.6185484      37.365568080000003\n",
            "     3   1      30.080392589999999     -2.0124176500000002      36.480796079999998\n",
            "     4   1      28.705791179999999     -2.6880392799999999      36.602098329999997\n",
            "     5   1             36.24794267    -0.51634849000000005      34.492359610000001\n",
            "     6   1      37.696472460000003   -0.041087279999999997      35.014073510000003\n",
            "     7   1      27.760669979999999      7.4854206000000003      33.927691950000003\n",
            "     8   1      28.816099959999999              6.49857774             34.21636084\n",
            "     9   1      37.157637200000003      9.0188280800000005      31.926581209999998\n",
            "    10   1      38.606381650000003      9.5820079600000003      32.343597279999997\n",
            "    11   1      34.303195930000001      2.2195014400000002      45.988045190000001\n",
            "    12   1      33.244413940000001              1.30253325      46.469842720000003\n",
            "    13   1      38.728617479999997     -5.0541897699999998      26.074396839999999\n",
            "    14   1      38.348392150000002     -6.2832846900000003      26.986725310000001\n",
            "    15   1      32.864252090000001              3.20606327               30.897116\n",
            "    16   1      31.290408809999999      3.0871834699999998             30.62739775\n",
            "    17   1      33.751986969999997             -3.13832624      39.672760769999996\n",
            "    18   1      34.664297990000001             -3.66438591      38.646602719999997\n",
            "    19   1      42.717321439999999      5.1246883700000003      32.588340119999998\n",
            "    20   1             41.56274552      5.5893544000000004      33.417490280000003\n",
            "    21   1      32.428380089999997      9.1182520999999994             30.54776786\n",
            "    22   1              32.6432407             10.77068308             30.48427787\n",
            "    23   1      31.484867080000001      4.6777144699999997      37.395719499999998\n",
            "    24   1      32.317188299999998     -6.2287496200000003      36.467186439999999\n",
            "    25   1      26.662134099999999      3.1708123800000001      35.682014649999999\n",
            "    26   1             26.52713675              1.60390403      35.488348299999998\n",
            "    27   1      32.023823659999998      16.918208029999999      31.688356989999999\n",
            "    28   1      31.400657970000001      7.0315610800000004      30.239455469999999\n",
            "    29   1             33.52642531     -3.5594808100000002             34.26368308\n",
            "    30   1      34.640485550000001             -3.26538336      35.497148240000001\n",
            "    31   1      40.056437510000002    -0.30543864999999998      29.831207429999999\n",
            "    32   1      39.478446419999997             -1.09483146      38.310114040000002\n",
            "    33   1      39.704076149999999      1.9584631400000001      33.390237599999999\n",
            "    34   1      38.333857080000001              2.69671781             42.92619457\n",
            "    35   1      40.182045549999998     -7.2199289499999999      27.658039049999999\n",
            "    36   1      39.320443179999998     -8.4564252700000004             28.13196589\n",
            "    37   1             36.38769637      8.8117085900000003      38.354536240000002\n",
            "    38   1      36.320563759999999      9.0063075799999996      36.752600139999998\n",
            "    39   1      29.999158309999999     -5.5637817500000004      33.929505059999997\n",
            "    40   1      30.772854550000002     -5.0385870199999996      35.199806719999998\n",
            "    41   1      40.059251779999997      6.3305279499999996      28.257946189999998\n",
            "    42   1      40.239836089999997      5.1745923999999999             29.29629568\n",
            "    43   1             26.33209111      2.4393638599999998      33.565386850000003\n",
            "    44   1      26.960697119999999      1.2711078899999999      32.592388440000001\n",
            "    45   1      34.837269769999999    -0.47227084000000003      30.382436200000001\n",
            "    46   1      35.396881370000003             -1.92684834      30.308183759999999\n",
            "    47   1      32.121760719999997    -0.73334299000000003      36.510438299999997\n",
            "    48   1      32.218084349999998      7.8454304099999996      35.667196779999998\n",
            "    49   1             36.37809987     -4.3048878799999999              36.4539793\n",
            "    50   1      35.811927560000001             -3.00139282      27.034893759999999\n",
            "    51   1      29.645249140000001      1.0652123600000001      35.714365399999998\n",
            "    52   1      30.379465499999998   -0.066814650000000003      34.988246869999998\n",
            "    53   1             34.21493366             -1.65591205      33.887643709999999\n",
            "    54   1      34.784243549999999     -1.0252141100000001      32.503483260000003\n",
            "    55   1      40.464995450000004              1.14678254      31.307350320000001\n",
            "    56   1      41.326246990000001     0.65508034999999998      32.455588290000001\n",
            "    57   1             29.02108599      3.5038194900000001             39.90877029\n",
            "    58   1      29.494542689999999      3.7276637300000002      41.376613800000001\n",
            "    59   1      34.135966400000001     -6.7533422300000003      32.356841029999998\n",
            "    60   1      34.954657009999998     -5.7704242399999996             31.45710669\n",
            "    61   1      33.253235689999997      1.5268048299999999      44.056217189999998\n",
            "    62   1      33.793166939999999     0.50146325999999997      43.059759010000001\n",
            "    63   1      36.820540960000002      2.6214681999999998      40.683400659999997\n",
            "    64   1      37.555270669999999              1.56498329             39.76489351\n",
            "    65   2      43.209908720000001   -0.062845650000000003             47.25931559\n",
            "    66   2      29.394058350000002     -2.3133019500000001      37.140788360000002\n",
            "    67   2      36.741570840000001   -0.083871000000000001      35.259178339999998\n",
            "    68   2             27.94247769      6.7622961500000001      34.564838440000003\n",
            "    69   2      37.681265600000003      9.4216399800000001      32.647864349999999\n",
            "    70   2      33.317129080000001      2.0951401700000001      45.872226509999997\n",
            "    71   2      37.995135589999997      4.3611431200000004             26.55718199\n",
            "    72   2      32.182467090000003      2.6611503399999998      30.457724819999999\n",
            "    73   2      34.653801209999997     -3.4374573100000001      39.588924570000003\n",
            "    74   2      42.292983370000002      5.9471069500000002      32.846099549999998\n",
            "    75   2      32.960469009999997      9.9050313200000009      30.158730670000001\n",
            "    76   2             31.42818866     -5.8338304000000001      36.673874359999999\n",
            "    77   2             26.05637308      2.4973869199999998      35.348687040000002\n",
            "    78   2      32.033492709999997      17.325228939999999      30.811601379999999\n",
            "    79   2      33.825218239999998     -2.9520949600000002      35.022046070000002\n",
            "    80   2      39.456998159999998    -0.30727594000000003      38.934782900000002\n",
            "    81   2             29.48467089      2.8692561099999998      43.006186839999998\n",
            "    82   2      39.286418410000003             -7.62061031      27.627114779999999\n",
            "    83   2      35.879750280000003      8.6515870800000005      37.522173430000002\n",
            "    84   2      30.358254389999999     -4.7607656800000004      34.335564570000003\n",
            "    85   2      40.709895690000003      5.8331250199999998      28.755837589999999\n",
            "    86   2      26.717908300000001      2.2415138300000002      32.657729799999998\n",
            "    87   2      35.658925619999998    -0.99689035000000004      30.574953059999999\n",
            "    88   2      31.585160200000001             -1.31218042      35.901110940000002\n",
            "    89   2      35.548938669999998     -3.9056138900000001      26.821449000000001\n",
            "    90   2      29.565661609999999     0.46817945999999999      34.967071199999999\n",
            "    91   2             34.76151282    -0.95696802000000003      33.489136729999998\n",
            "    92   2      40.485340669999999     0.40236209000000001             31.94254162\n",
            "    93   2      29.672828979999998      4.0134825300000001               40.450578\n",
            "    94   2      34.127228619999997     -5.8796882899999998      31.892542299999999\n",
            "    95   2      33.116888420000002      1.2338084899999999      43.112771199999997\n",
            "    96   2      37.199699350000003      2.5049007099999998      39.791712660000002\n"
          ]
        }
      ],
      "source": [
        "! cat /content/lammps_run/water_md.in\n",
        "! cat /content/lammps_run/structure.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plER-wpaFCCT"
      },
      "source": [
        "## Run MD!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gurLjNK5upvq",
        "outputId": "b4571bbd-5a1c-450c-c628-9560266690ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LAMMPS (29 Sep 2021 - Update 2)\n",
            "OMP_NUM_THREADS environment is not set. Defaulting to 1 thread. (src/comm.cpp:98)\n",
            "  using 1 OpenMP thread(s) per MPI task\n",
            "Reading data file ...\n",
            "  orthogonal box = (0.0000000 0.0000000 0.0000000) to (9.8500000 9.8500000 9.8500000)\n",
            "  1 by 1 by 1 MPI processor grid\n",
            "  reading atoms ...\n",
            "  96 atoms\n",
            "  read_data CPU = 0.001 seconds\n",
            "NEQUIP is using device cuda\n",
            "NequIP Coeff: type 1 is element H\n",
            "NequIP Coeff: type 2 is element O\n",
            "Loading model from ../water-deploy.pth\n",
            "Freezing TorchScript model...\n",
            "Neighbor list info ...\n",
            "  update every 10 steps, delay 0 steps, check no\n",
            "  max neighbors/atom: 2000, page size: 100000\n",
            "  master list distance cutoff = 5\n",
            "  ghost atom cutoff = 5\n",
            "  binsize = 2.5, bins = 4 4 4\n",
            "  1 neighbor lists, perpetual/occasional/extra = 1 0 0\n",
            "  (1) pair nequip, perpetual\n",
            "      attributes: full, newton off\n",
            "      pair build: full/bin/atomonly\n",
            "      stencil: full/bin/3d\n",
            "      bin: standard\n",
            "Setting up Verlet run ...\n",
            "  Unit style    : metal\n",
            "  Current step  : 0\n",
            "  Time step     : 0.0005\n",
            "Per MPI rank memory allocation (min/avg/max) = 3.586 | 3.586 | 3.586 Mbytes\n",
            "Step PotEng KinEng TotEng Temp Press Volume \n",
            "       0   -14985.618    3.6839141   -14981.934          300    4117.3701    955.67162 \n",
            "     100   -14985.151    3.6123106   -14981.539    294.16896    4037.3416    955.67162 \n",
            "     200   -14984.611    3.5535801   -14981.058    289.38624    3971.7008    955.67162 \n",
            "     300   -14984.508    3.7110432   -14980.797    302.20926    4147.6912    955.67162 \n",
            "     400   -14984.445    3.9257864    -14980.52     319.6969    4387.7015    955.67162 \n",
            "     500   -14984.365    4.1371156   -14980.228    336.90651     4623.896    955.67162 \n",
            "     600   -14983.877    3.9036258   -14979.973    317.89225    4362.9334    955.67162 \n",
            "     700    -14983.34     4.096895   -14979.243    333.63115     4578.943    955.67162 \n",
            "     800   -14983.479    4.3721838   -14979.106    356.04932    4886.6228    955.67162 \n",
            "     900   -14983.755    3.7052239    -14980.05    301.73536    4141.1872    955.67162 \n",
            "    1000   -14983.901    3.6019323   -14980.299     293.3238    4025.7422    955.67162 \n",
            "    1100   -14984.225    3.6738373   -14980.551    299.17939    4106.1076    955.67162 \n",
            "    1200   -14984.442    3.7880747   -14980.654    308.48233    4233.7864    955.67162 \n",
            "    1300   -14984.693    3.6200107   -14981.073    294.79601    4045.9476    955.67162 \n",
            "    1400   -14984.766     3.126188   -14981.639     254.5815    3494.0209    955.67162 \n",
            "    1500   -14985.058    3.5207939   -14981.537    286.71628    3935.0568    955.67162 \n",
            "    1600   -14984.859    3.7217579   -14981.138    303.08182    4159.6667    955.67162 \n",
            "    1700   -14984.491    3.4494693   -14981.042    280.90796    3855.3401    955.67162 \n",
            "    1800   -14985.077    3.2763723   -14981.801    266.81178    3661.8761    955.67162 \n",
            "    1900   -14985.245    3.1466423   -14982.098    256.24721    3516.8819    955.67162 \n",
            "    2000   -14985.201    3.6105757   -14981.591    294.02768    4035.4025    955.67162 \n",
            "    2100   -14984.441    3.3856246   -14981.056    275.70876    3783.9833    955.67162 \n",
            "    2200   -14984.773    4.0076486   -14980.766    326.36336    4479.1957    955.67162 \n",
            "    2300    -14984.79    4.0068637   -14980.783    326.29944    4478.3185    955.67162 \n",
            "    2400   -14984.748    3.4734636   -14981.275    282.86194    3882.1576    955.67162 \n",
            "    2500   -14984.849    3.2176037   -14981.631    262.02595    3596.1927    955.67162 \n",
            "    2600   -14984.524    3.4759491   -14981.048    283.06434    3884.9356    955.67162 \n",
            "    2700   -14984.942    4.1598211   -14980.783    338.75554    4649.2731    955.67162 \n",
            "    2800   -14984.374    3.2277883   -14981.146    262.85533    3607.5756    955.67162 \n",
            "    2900   -14984.801    3.3986996   -14981.402    276.77352    3798.5967    955.67162 \n",
            "    3000   -14984.835    3.7436241   -14981.091    304.86249    4184.1056    955.67162 \n",
            "    3100    -14983.94    3.6624904   -14980.278    298.25536    4093.4257    955.67162 \n",
            "    3200   -14984.037    3.9728386   -14980.064     323.5286      4440.29    955.67162 \n",
            "    3300   -14984.453    4.0740636   -14980.379    331.77187    4553.4252    955.67162 \n",
            "    3400   -14984.795     3.730877   -14981.064    303.82443    4169.8587    955.67162 \n",
            "    3500    -14984.76    3.5523698   -14981.207    289.28767     3970.348    955.67162 \n",
            "    3600   -14984.745    3.6238159   -14981.121    295.10589    4050.2006    955.67162 \n",
            "    3700   -14984.856    4.3507074   -14980.506    354.30039    4862.6194    955.67162 \n",
            "    3800   -14984.089    4.1389228    -14979.95    337.05368    4625.9159    955.67162 \n",
            "    3900    -14984.26    3.6528936   -14980.607    297.47384    4082.6997    955.67162 \n",
            "    4000   -14984.498    3.4428976   -14981.055    280.37279    3847.9951    955.67162 \n",
            "    4100   -14984.476    3.6437269   -14980.832    296.72735    4072.4544    955.67162 \n",
            "    4200   -14984.105    3.6933962   -14980.412    300.77217    4127.9678    955.67162 \n",
            "    4300    -14984.55    4.3843853   -14980.165    357.04296      4900.26    955.67162 \n",
            "    4400   -14983.876    2.9250781   -14980.951     238.2041    3269.2481    955.67162 \n",
            "    4500   -14985.088    3.7019429   -14981.386    301.46818    4137.5202    955.67162 \n",
            "    4600    -14984.74    3.5210258   -14981.219    286.73517     3935.316    955.67162 \n",
            "    4700   -14984.476    3.7908816   -14980.685    308.71091    4236.9236    955.67162 \n",
            "    4800   -14984.167    3.8749942   -14980.292    315.56063    4330.9329    955.67162 \n",
            "    4900   -14984.064    3.7941703    -14980.27    308.97872    4240.5991    955.67162 \n",
            "    5000   -14983.939    3.5711142   -14980.368    290.81413     3991.298    955.67162 \n",
            "    5100   -14984.608    4.0680219    -14980.54    331.27987    4546.6727    955.67162 \n",
            "    5200   -14984.487    3.8734635   -14980.614    315.43597    4329.2221    955.67162 \n",
            "    5300   -14983.903    3.2591831   -14980.644    265.41198    3642.6644    955.67162 \n",
            "    5400   -14984.293    4.0295942   -14980.263     328.1505    4503.7235    955.67162 \n",
            "    5500   -14983.937    4.1462568    -14979.79    337.65093    4634.1128    955.67162 \n",
            "    5600   -14984.063    4.1246772   -14979.939    335.89359    4609.9941    955.67162 \n",
            "    5700   -14984.361    3.6479021   -14980.713    297.06736    4077.1208    955.67162 \n",
            "    5800   -14984.575    3.5160229   -14981.059    286.32776    3929.7245    955.67162 \n",
            "    5900     -14984.4    3.4965392   -14980.904     284.7411    3907.9482    955.67162 \n",
            "    6000   -14984.905    4.1865199   -14980.719    340.92977    4679.1134    955.67162 \n",
            "    6100   -14984.776    4.0263091    -14980.75    327.88298    4500.0519    955.67162 \n",
            "    6200   -14984.618    3.3176528   -14981.301    270.17347    3708.0138    955.67162 \n",
            "    6300   -14984.605    3.2474039   -14981.358    264.45273    3629.4993    955.67162 \n",
            "    6400   -14984.439    3.7581473   -14980.681    306.04519    4200.3377    955.67162 \n",
            "    6500   -14983.938    3.4546562   -14980.483    281.33035    3861.1372    955.67162 \n",
            "    6600   -14984.075    3.5848968    -14980.49    291.93651    4006.7022    955.67162 \n",
            "    6700   -14984.146    3.9519328   -14980.195    321.82614    4416.9244    955.67162 \n",
            "    6800   -14984.149    4.3618097   -14979.788    355.20451    4875.0281    955.67162 \n",
            "    6900   -14984.054     4.043296    -14980.01     329.2663    4519.0374    955.67162 \n",
            "    7000   -14983.929    3.1878685   -14980.741    259.60446    3562.9588    955.67162 \n",
            "    7100   -14984.193     3.526064   -14980.667    287.14546    3940.9471    955.67162 \n",
            "    7200   -14983.732    3.5778325   -14980.155    291.36123    3998.8067    955.67162 \n",
            "    7300   -14983.611     3.939267   -14979.672    320.79469    4402.7682    955.67162 \n",
            "    7400    -14983.25    3.3876334   -14979.862    275.87235    3786.2285    955.67162 \n",
            "    7500   -14984.083    3.7822533   -14980.301    308.00826      4227.28    955.67162 \n",
            "    7600   -14984.343    3.7397486   -14980.603    304.54689    4179.7741    955.67162 \n",
            "    7700   -14984.462    3.8293449   -14980.633    311.84317    4279.9125    955.67162 \n",
            "    7800   -14984.234    3.7065634   -14980.528    301.84445    4142.6843    955.67162 \n",
            "    7900   -14984.705    4.2473233   -14980.458     345.8813     4747.071    955.67162 \n",
            "    8000   -14983.805    3.4972461   -14980.307    284.79866    3908.7383    955.67162 \n",
            "    8100   -14983.761    3.9248009   -14979.836    319.61664       4386.6    955.67162 \n",
            "    8200   -14983.794    4.3742195    -14979.42     356.2151     4888.898    955.67162 \n",
            "    8300   -14984.128    4.1587071   -14979.969    338.66482    4648.0281    955.67162 \n",
            "    8400    -14983.61    3.0240463   -14980.586    246.26359    3379.8611    955.67162 \n",
            "    8500   -14983.716    3.3647184   -14980.351    274.00626    3760.6173    955.67162 \n",
            "    8600   -14983.766    3.9268369   -14979.839    319.78244    4388.8755    955.67162 \n",
            "    8700   -14983.777    3.9829162   -14979.794    324.34927    4451.5533    955.67162 \n",
            "    8800   -14984.444    3.9998311   -14980.445    325.72674    4470.4584    955.67162 \n",
            "    8900   -14984.528    3.5784839    -14980.95    291.41427    3999.5347    955.67162 \n",
            "    9000   -14984.938    4.0614453   -14980.877    330.74429    4539.3222    955.67162 \n",
            "    9100   -14984.538    3.7019433   -14980.836    301.46821    4137.5206    955.67162 \n",
            "    9200   -14984.214    3.8067723   -14980.407    310.00497     4254.684    955.67162 \n",
            "    9300   -14983.783    3.8929485    -14979.89    317.02274    4350.9999    955.67162 \n",
            "    9400   -14983.818    3.7937046   -14980.025     308.9408    4240.0787    955.67162 \n",
            "    9500   -14983.962    3.2453296   -14980.717    264.28382    3627.1809    955.67162 \n",
            "    9600   -14984.527    3.6192835   -14980.908     294.7368    4045.1349    955.67162 \n",
            "    9700   -14984.178    3.2173201    -14980.96    262.00286    3595.8758    955.67162 \n",
            "    9800   -14984.335    3.9580626   -14980.377    322.32531    4423.7753    955.67162 \n",
            "    9900   -14983.777    3.9075625    -14979.87    318.21283    4367.3334    955.67162 \n",
            "   10000    -14984.19    4.3420518   -14979.848    353.59552    4852.9454    955.67162 \n",
            "   10100   -14984.221    3.6802056    -14980.54    299.69799    4113.2252    955.67162 \n",
            "   10200   -14984.404     3.617982   -14980.786    294.63081    4043.6802    955.67162 \n",
            "   10300   -14984.094    3.4679127   -14980.626     282.4099    3875.9536    955.67162 \n",
            "   10400   -14984.083    3.6335769   -14980.449    295.90078    4061.1101    955.67162 \n",
            "   10500   -14983.769    3.4816904   -14980.287    283.53189    3891.3524    955.67162 \n",
            "   10600   -14983.863    3.8215389   -14980.042    311.20749     4271.188    955.67162 \n",
            "   10700   -14983.705    4.0807634   -14979.624    332.31747    4560.9134    955.67162 \n",
            "   10800    -14984.31     4.349726    -14979.96    354.22047    4861.5225    955.67162 \n",
            "   10900   -14984.252    3.5970385   -14980.655    292.92527    4020.2726    955.67162 \n",
            "   11000   -14984.796    3.8757744    -14980.92    315.62416     4331.805    955.67162 \n",
            "   11100   -14984.197    3.3850633   -14980.812    275.66305     3783.356    955.67162 \n",
            "   11200   -14983.656    3.5196862   -14980.137    286.62608    3933.8188    955.67162 \n",
            "   11300   -14983.613    3.9609671   -14979.652    322.56184    4427.0216    955.67162 \n",
            "   11400   -14983.681    3.9003196    -14979.78    317.62301    4359.2382    955.67162 \n",
            "   11500   -14983.821    3.5760633   -14980.245    291.21715    3996.8293    955.67162 \n",
            "   11600   -14983.903    3.3847633   -14980.519    275.63861    3783.0206    955.67162 \n",
            "   11700   -14983.897    3.6356918   -14980.262    296.07301    4063.4739    955.67162 \n",
            "   11800   -14983.391    4.0415016   -14979.349    329.12018    4517.0319    955.67162 \n",
            "   11900   -14983.468    3.7651063   -14979.703    306.61189    4208.1154    955.67162 \n",
            "   12000   -14984.084    3.7975005   -14980.286    309.24992    4244.3212    955.67162 \n",
            "   12100   -14984.018    3.3858465   -14980.632    275.72683    3784.2313    955.67162 \n",
            "   12200   -14984.445    3.6672435   -14980.778    298.64242    4098.7379    955.67162 \n",
            "   12300   -14984.219    3.5210717   -14980.698     286.7389    3935.3673    955.67162 \n",
            "   12400   -14984.161    4.0508001    -14980.11     329.8774    4527.4245    955.67162 \n",
            "   12500     -14983.7    4.0583482   -14979.642    330.49208    4535.8607    955.67162 \n",
            "   12600   -14984.015    3.9294108   -14980.085    319.99205    4391.7523    955.67162 \n",
            "   12700    -14984.05    3.4538751   -14980.596    281.26675    3860.2643    955.67162 \n",
            "   12800   -14983.771    3.2010706   -14980.569    260.67958    3577.7143    955.67162 \n",
            "   12900    -14983.39    3.2420278   -14980.148    264.01493    3623.4906    955.67162 \n",
            "   13000    -14983.26    3.5525934   -14979.707    289.30588     3970.598    955.67162 \n",
            "   13100   -14983.353     3.756935   -14979.596    305.94646    4198.9827    955.67162 \n",
            "   13200   -14983.404    3.7101034   -14979.694    302.13273    4146.6409    955.67162 \n",
            "   13300   -14983.564     3.478078   -14980.086    283.23771    3887.3149    955.67162 \n",
            "   13400   -14983.967    3.6192803   -14980.348    294.73654    4045.1314    955.67162 \n",
            "   13500   -14983.804    3.4684244   -14980.335    282.45157    3876.5254    955.67162 \n",
            "   13600    -14983.38    3.6453879   -14979.734    296.86261    4074.3108    955.67162 \n",
            "   13700   -14983.336    4.1644152   -14979.172    339.12966    4654.4077    955.67162 \n",
            "   13800   -14982.979    3.7232112   -14979.255    303.20016     4161.291    955.67162 \n",
            "   13900   -14982.959    3.2281244   -14979.731     262.8827    3607.9513    955.67162 \n",
            "   14000   -14983.876    3.7751172   -14980.101    307.42714    4219.3043    955.67162 \n",
            "   14100   -14983.779    3.5643549   -14980.215    290.26368    3983.7433    955.67162 \n",
            "   14200   -14983.877     3.717411    -14980.16    302.72782    4154.8082    955.67162 \n",
            "   14300    -14983.79    3.5585671   -14980.231    289.79235    3977.2745    955.67162 \n",
            "   14400     -14983.7    3.6474541   -14980.053    297.03087    4076.6201    955.67162 \n",
            "   14500   -14983.053    3.6121492   -14979.441    294.15582    4037.1612    955.67162 \n",
            "   14600   -14983.163    3.9775183   -14979.186     323.9097    4445.5203    955.67162 \n",
            "   14700   -14983.847    4.0432482   -14979.803    329.26242    4518.9841    955.67162 \n",
            "   14800   -14983.799    3.5748376   -14980.224    291.11734    3995.4594    955.67162 \n",
            "   14900    -14983.52    3.4818338   -14980.038    283.54356    3891.5126    955.67162 \n",
            "   15000   -14983.573    4.0626397   -14979.511    330.84156    4540.6572    955.67162 \n",
            "   15100   -14982.962    3.7466931   -14979.215    305.11242    4187.5358    955.67162 \n",
            "   15200   -14983.619    4.2771407   -14979.342    348.30948    4780.3967    955.67162 \n",
            "   15300   -14983.586    3.4316783   -14980.154    279.45914    3835.4557    955.67162 \n",
            "   15400   -14983.537    3.3536187   -14980.183    273.10235    3748.2115    955.67162 \n",
            "   15500   -14983.915    4.1688935   -14979.746    339.49435     4659.413    955.67162 \n",
            "   15600   -14983.747    4.0145598   -14979.733    326.92617    4486.9202    955.67162 \n",
            "   15700   -14983.666    3.6463497    -14980.02    296.94094    4075.3858    955.67162 \n",
            "   15800    -14983.66    3.6727467   -14979.987    299.09058    4104.8887    955.67162 \n",
            "   15900   -14983.644    3.6922701   -14979.951    300.68047    4126.7092    955.67162 \n",
            "   16000   -14983.906    3.8615156   -14980.045      314.463    4315.8684    955.67162 \n",
            "   16100    -14983.87     3.691034   -14980.179    300.57981    4125.3277    955.67162 \n",
            "   16200   -14983.897    3.5406676   -14980.357     288.3347    3957.2689    955.67162 \n",
            "   16300   -14984.265    3.8159226   -14980.449    310.75013    4264.9109    955.67162 \n",
            "   16400   -14984.133    3.5401644   -14980.593    288.29373    3956.7066    955.67162 \n",
            "   16500   -14984.354    3.6063613   -14980.748    293.68448    4030.6923    955.67162 \n",
            "   16600   -14984.705    4.3156834   -14980.389    351.44821    4823.4744    955.67162 \n",
            "   16700   -14983.904     3.827114   -14980.077     311.6615    4277.4192    955.67162 \n",
            "   16800   -14983.965    4.0241598   -14979.941    327.70795    4497.6497    955.67162 \n",
            "   16900   -14983.931      3.38525   -14980.545    275.67825    3783.5646    955.67162 \n",
            "   17000   -14983.829    3.0756121   -14980.753    250.46285    3437.4942    955.67162 \n",
            "   17100   -14983.971    3.6949455   -14980.276    300.89834    4129.6994    955.67162 \n",
            "   17200   -14983.509    3.7060856   -14979.803    301.80553    4142.1503    955.67162 \n",
            "   17300   -14983.333    3.5952445   -14979.738    292.77918    4018.2674    955.67162 \n",
            "   17400   -14983.796    3.9026386   -14979.893    317.81185    4361.8301    955.67162 \n",
            "   17500   -14983.998    3.6541303   -14980.344    297.57455    4084.0819    955.67162 \n",
            "   17600   -14984.128    3.5376795    -14980.59    288.09137    3953.9292    955.67162 \n",
            "   17700    -14984.23    3.8931573   -14980.337    317.03974    4351.2332    955.67162 \n",
            "   17800   -14983.454    3.7853705   -14979.669    308.26211     4230.764    955.67162 \n",
            "   17900   -14983.418     3.867538    -14979.55    314.95343    4322.5995    955.67162 \n",
            "   18000   -14983.656    3.9432289   -14979.713    321.11733    4407.1963    955.67162 \n",
            "   18100   -14983.688    3.3379455   -14980.351      271.826    3730.6942    955.67162 \n",
            "   18200     -14983.9    3.3083982   -14980.592    269.41981    3697.6702    955.67162 \n",
            "   18300   -14983.871    3.6217171   -14980.249    294.93498    4047.8549    955.67162 \n",
            "   18400   -14983.274    3.4200182   -14979.854     278.5096    3822.4237    955.67162 \n",
            "   18500    -14983.37    3.9366827   -14979.433    320.58424    4399.8799    955.67162 \n",
            "   18600   -14982.896    3.5455063   -14979.351    288.72874    3962.6769    955.67162 \n",
            "   18700   -14982.932    3.8547114   -14979.077     313.9089    4308.2637    955.67162 \n",
            "   18800   -14983.172    4.2189004   -14978.953    343.56667    4715.3038    955.67162 \n",
            "   18900   -14983.894    4.1367641   -14979.757    336.87789    4623.5032    955.67162 \n",
            "   19000   -14983.324    3.1943499    -14980.13    260.13228    3570.2029    955.67162 \n",
            "   19100   -14983.448    3.5913774   -14979.857    292.46426    4013.9453    955.67162 \n",
            "   19200   -14983.139    3.6756875   -14979.463    299.33006    4108.1755    955.67162 \n",
            "   19300   -14983.045    3.7048768    -14979.34     301.7071    4140.7993    955.67162 \n",
            "   19400   -14983.393    3.3326774    -14980.06      271.397    3724.8062    955.67162 \n",
            "   19500   -14983.714    3.4999442   -14980.214    285.01838    3911.7539    955.67162 \n",
            "   19600   -14983.838    3.8327471   -14980.005    312.12023     4283.715    955.67162 \n",
            "   19700   -14983.923    4.3226217     -14979.6    352.01323    4831.2291    955.67162 \n",
            "   19800    -14983.18    3.8673411   -14979.312    314.93739    4322.3794    955.67162 \n",
            "   19900   -14983.477    3.7527653   -14979.724    305.60691    4194.3225    955.67162 \n",
            "   20000   -14983.829    3.5744479   -14980.255     291.0856    3995.0238    955.67162 \n",
            "Loop time of 549.116 on 1 procs for 20000 steps with 96 atoms\n",
            "\n",
            "Performance: 1.573 ns/day, 15.253 hours/ns, 36.422 timesteps/s\n",
            "99.8% CPU use with 1 MPI tasks x 1 OpenMP threads\n",
            "\n",
            "MPI task timing breakdown:\n",
            "Section |  min time  |  avg time  |  max time  |%varavg| %total\n",
            "---------------------------------------------------------------\n",
            "Pair    | 548.14     | 548.14     | 548.14     |   0.0 | 99.82\n",
            "Neigh   | 0.62405    | 0.62405    | 0.62405    |   0.0 |  0.11\n",
            "Comm    | 0.074847   | 0.074847   | 0.074847   |   0.0 |  0.01\n",
            "Output  | 0.11503    | 0.11503    | 0.11503    |   0.0 |  0.02\n",
            "Modify  | 0.13235    | 0.13235    | 0.13235    |   0.0 |  0.02\n",
            "Other   |            | 0.03104    |            |       |  0.01\n",
            "\n",
            "Nlocal:        96.0000 ave          96 max          96 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "Nghost:        688.000 ave         688 max         688 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "Neighs:         0.00000 ave           0 max           0 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "FullNghs:      4982.00 ave        4982 max        4982 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "\n",
            "Total # of neighbors = 4982\n",
            "Ave neighs/atom = 51.895833\n",
            "Neighbor list builds = 2000\n",
            "Dangerous builds not checked\n",
            "Total wall time: 0:09:13\n",
            "[8d4ca33493df:03235] *** Process received signal ***\n",
            "[8d4ca33493df:03235] Signal: Segmentation fault (11)\n",
            "[8d4ca33493df:03235] Signal code: Address not mapped (1)\n",
            "[8d4ca33493df:03235] Failing at address: 0x7f6255fac20d\n",
            "[8d4ca33493df:03235] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f6258e59980]\n",
            "[8d4ca33493df:03235] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7f6258a98775]\n",
            "[8d4ca33493df:03235] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7f6259303e44]\n",
            "[8d4ca33493df:03235] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7f6258a99605]\n",
            "[8d4ca33493df:03235] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7f6259301cb3]\n",
            "[8d4ca33493df:03235] *** End of error message ***\n"
          ]
        }
      ],
      "source": [
        "!cd /content/lammps_run/ && ../lammps/build/lmp -in water_md.in"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETDEkUHc39u4"
      },
      "source": [
        "### Visualize the trajectory with ase and ngl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGEqNBu2fmKQ"
      },
      "outputs": [],
      "source": [
        "from ase.visualize import view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYxvyvyK2ctM"
      },
      "outputs": [],
      "source": [
        "wat_traj = read(\"/content/lammps_run/water.xyz\", index=\"::10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G_nFd7dgWhc"
      },
      "outputs": [],
      "source": [
        "for i in range(len(wat_traj)):\n",
        "    wat_traj[i].cell = cell_vec_abc\n",
        "    wat_traj[i].pbc = np.array([True, True, True])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZLtkZ7bdszJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517,
          "referenced_widgets": [
            "bbe09d800dff4befad106bb6f379c3ca",
            "0b20111fd465408498d5e18cede15336",
            "218966dd7fa6471fbeea95790d5a1d20",
            "587e46469c1b40b7a7fd0dddab12fd40",
            "e9101a0a83624dd5ad3f01efd501b676",
            "5593bd983e06477ebe6f61d308d0de00",
            "dcd4125d82544a94a7e18e126febbfea",
            "aad778fab43b45f4a1b2b0acc869b718",
            "0ff7f5767d3d4cd981e0f3d0fd1a176c",
            "0c7f34bbc26e454ab7c0a4e6b00aedc6",
            "9c74398308864fd595e5d1ef3c305bef",
            "781a4063245448bb9408db98b2adaaee",
            "d699f01053394ddd919c442a5205e9ec",
            "6cd6a128f3eb48d98f2ad3239224fc07",
            "2c7c333da3f347cca7446932479e9a76",
            "29bccb94ca334c09a588ca903054f167",
            "ec349da3a00a4505b896b6b7a2d6e380",
            "8e38d82c9a364c5f9af95d37d46469ff",
            "85df14fcd2fe45fdbf076f0566563fc6",
            "773d52906773488f98bc32dff9484206",
            "e5b9207e29bd4528b192e362bb6a5189",
            "b98edcf4c077434b8802166872507b5d",
            "b9fdf07bb0e14da3bfc8c0ec0ec257e3",
            "8538ff68402b4a70a5d4269193a03080",
            "670cdc83f2f54137a2fff88e1be17fcf",
            "38bd9de63edf488bb62178a622b59e7f",
            "9471a2080aad4e08b846958314bdde39",
            "4d2ea2c966e24183b0879e416592a7c0",
            "123c8f3742b94d2f88f5e9de681159aa"
          ]
        },
        "id": "-P5s2IbvfocL",
        "outputId": "4d7865cf-d23d-4f3a-ffc3-2615f660a4fd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbe09d800dff4befad106bb6f379c3ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(NGLWidget(max_frame=100), VBox(children=(Dropdown(description='Show', options=('All', 'O', 'H')…"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/a8874ba6619b6106/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "view(wat_traj, viewer=\"ngl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CMwyeFr4BAZ"
      },
      "source": [
        "### Calculate radial distribution function with MDAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOo4fyaB8jAi",
        "outputId": "1c4cc150-bcea-4db0-b499-f1077545fe75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting MDAnalysis\n",
            "  Downloading MDAnalysis-2.1.0.tar.gz (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 8.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx>=1.0 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (2.6.3)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (1.4.1)\n",
            "Collecting biopython>=1.71\n",
            "  Downloading biopython-1.79-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 61.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (1.1.0)\n",
            "Collecting gsd>=1.4.0\n",
            "  Downloading gsd-2.5.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 90.8 MB/s \n",
            "\u001b[?25hCollecting mmtf-python>=1.0.0\n",
            "  Downloading mmtf_python-1.1.3-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (3.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (21.3)\n",
            "Collecting GridDataFormats>=0.4.0\n",
            "  Downloading GridDataFormats-0.7.0-py2.py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 81.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.43.0 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (4.64.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GridDataFormats>=0.4.0->MDAnalysis) (1.15.0)\n",
            "Collecting mrcfile\n",
            "  Downloading mrcfile-1.4.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (1.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.1->MDAnalysis) (4.1.1)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from mmtf-python>=1.0.0->MDAnalysis) (1.0.4)\n",
            "Building wheels for collected packages: MDAnalysis\n",
            "  Building wheel for MDAnalysis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for MDAnalysis: filename=MDAnalysis-2.1.0-cp37-cp37m-linux_x86_64.whl size=4652192 sha256=52b981f75160e92e46154d707c2a218ae7212dc621c194574b20fdf199784eb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/dd/6b/9d51e7216a401b71949467a123e3b2dffba11256346f7f7bda\n",
            "Successfully built MDAnalysis\n",
            "Installing collected packages: mrcfile, mmtf-python, gsd, GridDataFormats, biopython, MDAnalysis\n",
            "Successfully installed GridDataFormats-0.7.0 MDAnalysis-2.1.0 biopython-1.79 gsd-2.5.3 mmtf-python-1.1.3 mrcfile-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install MDAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV4F1NHf9WUo"
      },
      "outputs": [],
      "source": [
        "import MDAnalysis as mda\n",
        "from MDAnalysis.analysis import rdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtqT9im4AC15"
      },
      "outputs": [],
      "source": [
        "reader = mda.coordinates.XYZ.XYZReader(\"/content/lammps_run/water.xyz\")\n",
        "topology = mda.topology.XYZParser.XYZParser(\"/content/lammps_run/water.xyz\")\n",
        "\n",
        "u = mda.Universe(\"/content/lammps_run/water.xyz\")\n",
        "\n",
        "u.dimensions = [cell_vec_abc[0], cell_vec_abc[1], cell_vec_abc[2], 90.0, 90.0, 90.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkoMQR8ZA0Sc",
        "outputId": "ffb4469d-f631-4903-d8da-e4c3b54a42f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/MDAnalysis/coordinates/base.py:892: UserWarning: Reader has no dt information, set to 1.0 ps\n",
            "  warnings.warn(\"Reader has no dt information, set to 1.0 ps\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<MDAnalysis.analysis.rdf.InterRDF at 0x7f723aa697d0>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "O_at = u.select_atoms(\"name O\")\n",
        "H_at = u.select_atoms(\"name H\")\n",
        "\n",
        "Ordf = rdf.InterRDF(\n",
        "    O_at,\n",
        "    O_at,\n",
        "    nbins=75,  # default\n",
        "    range=(0.00001, 4.9),  # distance in angstroms\n",
        ")\n",
        "Ordf.run()\n",
        "\n",
        "OHrdf = rdf.InterRDF(\n",
        "    O_at,\n",
        "    H_at,\n",
        "    nbins=75,  # default\n",
        "    range=(0.00001, 4.9),  # distance in angstroms\n",
        ")\n",
        "OHrdf.run()\n",
        "\n",
        "HHrdf = rdf.InterRDF(\n",
        "    H_at,\n",
        "    H_at,\n",
        "    nbins=75,  # default\n",
        "    range=(0.00001, 4.9),  # distance in angstroms\n",
        ")\n",
        "HHrdf.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "Og07Lts5C1q_",
        "outputId": "5a28727d-cd29-4d87-f44a-46ec58ff3d81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:417: DeprecationWarning: The `bins` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.bins` instead\n",
            "  warnings.warn(wmsg, DeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:425: DeprecationWarning: The `rdf` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.rdf` instead\n",
            "  warnings.warn(wmsg, DeprecationWarning)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Radial distribution')"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(Ordf.bins, Ordf.rdf)\n",
        "plt.xlabel(\"Radius (angstrom)\")\n",
        "plt.ylabel(\"Radial distribution\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "w8dxSzw_L_3z",
        "outputId": "b2d8842a-4ccc-47e4-faba-264a709f7f17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:417: DeprecationWarning: The `bins` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.bins` instead\n",
            "  warnings.warn(wmsg, DeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:425: DeprecationWarning: The `rdf` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.rdf` instead\n",
            "  warnings.warn(wmsg, DeprecationWarning)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Radial distribution')"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(OHrdf.bins, OHrdf.rdf)\n",
        "plt.xlabel(\"Radius (angstrom)\")\n",
        "plt.ylabel(\"Radial distribution\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "YeT-d_udMAE0",
        "outputId": "5bc61ef7-1629-45f8-d663-fac14d42ebef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:417: DeprecationWarning: The `bins` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.bins` instead\n",
            "  warnings.warn(wmsg, DeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:425: DeprecationWarning: The `rdf` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.rdf` instead\n",
            "  warnings.warn(wmsg, DeprecationWarning)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Radial distribution')"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(HHrdf.bins, HHrdf.rdf)\n",
        "plt.xlabel(\"Radius (angstrom)\")\n",
        "plt.ylabel(\"Radial distribution\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "my-short-nequip-tutorial.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "c9be9acec9edbd902b751bf46a8fbd7b71bbc5f0438c72d3ebaee4bffeb5e5e4"
    },
    "kernelspec": {
      "display_name": "Python 3.7.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b20111fd465408498d5e18cede15336": {
          "model_module": "nglview-js-widgets",
          "model_module_version": "3.0.1",
          "model_name": "NGLModel",
          "state": {
            "_camera_orientation": [
              18.783115818947113,
              0,
              0,
              0,
              0,
              18.783115818947113,
              0,
              0,
              0,
              0,
              18.783115818947113,
              0,
              -4.905500038526952,
              -4.932000007480383,
              -4.892499856650829,
              1
            ],
            "_camera_str": "orthographic",
            "_dom_classes": [],
            "_gui_theme": null,
            "_ibtn_fullscreen": "IPY_MODEL_85df14fcd2fe45fdbf076f0566563fc6",
            "_igui": null,
            "_iplayer": "IPY_MODEL_773d52906773488f98bc32dff9484206",
            "_model_module": "nglview-js-widgets",
            "_model_module_version": "3.0.1",
            "_model_name": "NGLModel",
            "_ngl_color_dict": {},
            "_ngl_coordinate_resource": {},
            "_ngl_full_stage_parameters": {
              "ambientColor": 14540253,
              "ambientIntensity": 0.2,
              "backgroundColor": "white",
              "cameraEyeSep": 0.3,
              "cameraFov": 40,
              "cameraType": "orthographic",
              "clipDist": 0,
              "clipFar": 100,
              "clipNear": 0,
              "fogFar": 100,
              "fogNear": 50,
              "hoverTimeout": 0,
              "impostor": true,
              "lightColor": 14540253,
              "lightIntensity": 1,
              "mousePreset": "default",
              "panSpeed": 1,
              "quality": "medium",
              "rotateSpeed": 2,
              "sampleLevel": 0,
              "tooltip": true,
              "workerDefault": true,
              "zoomSpeed": 1.2
            },
            "_ngl_msg_archive": [
              {
                "args": [
                  {
                    "binary": false,
                    "data": "CRYST1    9.850    9.850    9.850  90.00  90.00  90.00 P 1\nMODEL     1\nATOM      1    H MOL     1       3.486   9.794   8.779  1.00  0.00           H  \nATOM      2    H MOL     1       4.653   9.231   7.816  1.00  0.00           H  \nATOM      3    H MOL     1       0.530   7.838   6.931  1.00  0.00           H  \nATOM      4    H MOL     1       9.006   7.162   7.052  1.00  0.00           H  \nATOM      5    H MOL     1       6.698   9.334   4.942  1.00  0.00           H  \nATOM      6    H MOL     1       8.146   9.809   5.464  1.00  0.00           H  \nATOM      7    H MOL     1       8.061   7.485   4.378  1.00  0.00           H  \nATOM      8    H MOL     1       9.116   6.499   4.666  1.00  0.00           H  \nATOM      9    H MOL     1       7.608   9.019   2.377  1.00  0.00           H  \nATOM     10    H MOL     1       9.056   9.582   2.794  1.00  0.00           H  \nATOM     11    H MOL     1       4.753   2.220   6.588  1.00  0.00           H  \nATOM     12    H MOL     1       3.694   1.303   7.070  1.00  0.00           H  \nATOM     13    H MOL     1       9.179   4.796   6.374  1.00  0.00           H  \nATOM     14    H MOL     1       8.798   3.567   7.287  1.00  0.00           H  \nATOM     15    H MOL     1       3.314   3.206   1.347  1.00  0.00           H  \nATOM     16    H MOL     1       1.740   3.087   1.077  1.00  0.00           H  \nATOM     17    H MOL     1       4.202   6.712   0.273  1.00  0.00           H  \nATOM     18    H MOL     1       5.114   6.186   9.097  1.00  0.00           H  \nATOM     19    H MOL     1       3.317   5.125   3.038  1.00  0.00           H  \nATOM     20    H MOL     1       2.163   5.589   3.867  1.00  0.00           H  \nATOM     21    H MOL     1       2.878   9.118   0.998  1.00  0.00           H  \nATOM     22    H MOL     1       3.093   0.921   0.934  1.00  0.00           H  \nATOM     23    H MOL     1       1.935   4.678   7.846  1.00  0.00           H  \nATOM     24    H MOL     1       2.767   3.621   6.917  1.00  0.00           H  \nATOM     25    H MOL     1       6.962   3.171   6.132  1.00  0.00           H  \nATOM     26    H MOL     1       6.827   1.604   5.938  1.00  0.00           H  \nATOM     27    H MOL     1       2.474   7.068   2.138  1.00  0.00           H  \nATOM     28    H MOL     1       1.851   7.032   0.689  1.00  0.00           H  \nATOM     29    H MOL     1       3.976   6.291   4.714  1.00  0.00           H  \nATOM     30    H MOL     1       5.090   6.585   5.947  1.00  0.00           H  \nATOM     31    H MOL     1       0.656   9.545   0.281  1.00  0.00           H  \nATOM     32    H MOL     1       0.078   8.755   8.760  1.00  0.00           H  \nATOM     33    H MOL     1       0.304   1.958   3.840  1.00  0.00           H  \nATOM     34    H MOL     1       8.784   2.697   3.526  1.00  0.00           H  \nATOM     35    H MOL     1       0.782   2.630   7.958  1.00  0.00           H  \nATOM     36    H MOL     1       9.770   1.394   8.432  1.00  0.00           H  \nATOM     37    H MOL     1       6.838   8.812   8.805  1.00  0.00           H  \nATOM     38    H MOL     1       6.771   9.006   7.203  1.00  0.00           H  \nATOM     39    H MOL     1       0.449   4.286   4.380  1.00  0.00           H  \nATOM     40    H MOL     1       1.223   4.811   5.650  1.00  0.00           H  \nATOM     41    H MOL     1       0.659   6.331   8.558  1.00  0.00           H  \nATOM     42    H MOL     1       0.840   5.175   9.596  1.00  0.00           H  \nATOM     43    H MOL     1       6.632   2.439   4.015  1.00  0.00           H  \nATOM     44    H MOL     1       7.261   1.271   3.042  1.00  0.00           H  \nATOM     45    H MOL     1       5.287   9.378   0.832  1.00  0.00           H  \nATOM     46    H MOL     1       5.847   7.923   0.758  1.00  0.00           H  \nATOM     47    H MOL     1       2.572   9.117   6.960  1.00  0.00           H  \nATOM     48    H MOL     1       2.668   7.845   6.117  1.00  0.00           H  \nATOM     49    H MOL     1       6.828   5.545   6.904  1.00  0.00           H  \nATOM     50    H MOL     1       6.262   6.849   7.335  1.00  0.00           H  \nATOM     51    H MOL     1       0.095   1.065   6.164  1.00  0.00           H  \nATOM     52    H MOL     1       0.829   9.783   5.438  1.00  0.00           H  \nATOM     53    H MOL     1       4.665   8.194   4.338  1.00  0.00           H  \nATOM     54    H MOL     1       5.234   8.825   2.953  1.00  0.00           H  \nATOM     55    H MOL     1       1.065   1.147   1.757  1.00  0.00           H  \nATOM     56    H MOL     1       1.926   0.655   2.906  1.00  0.00           H  \nATOM     57    H MOL     1       9.321   3.504   0.509  1.00  0.00           H  \nATOM     58    H MOL     1       9.795   3.728   1.977  1.00  0.00           H  \nATOM     59    H MOL     1       4.586   3.097   2.807  1.00  0.00           H  \nATOM     60    H MOL     1       5.405   4.080   1.907  1.00  0.00           H  \nATOM     61    H MOL     1       3.703   1.527   4.656  1.00  0.00           H  \nATOM     62    H MOL     1       4.243   0.501   3.660  1.00  0.00           H  \nATOM     63    H MOL     1       7.271   2.621   1.283  1.00  0.00           H  \nATOM     64    H MOL     1       8.005   1.565   0.365  1.00  0.00           H  \nATOM     65    O MOL     1       3.810   9.787   7.859  1.00  0.00           O  \nATOM     66    O MOL     1       9.694   7.537   7.591  1.00  0.00           O  \nATOM     67    O MOL     1       7.192   9.766   5.709  1.00  0.00           O  \nATOM     68    O MOL     1       8.242   6.762   5.015  1.00  0.00           O  \nATOM     69    O MOL     1       8.131   9.422   3.098  1.00  0.00           O  \nATOM     70    O MOL     1       3.767   2.095   6.472  1.00  0.00           O  \nATOM     71    O MOL     1       8.445   4.361   6.857  1.00  0.00           O  \nATOM     72    O MOL     1       2.632   2.661   0.908  1.00  0.00           O  \nATOM     73    O MOL     1       5.104   6.413   0.189  1.00  0.00           O  \nATOM     74    O MOL     1       2.893   5.947   3.296  1.00  0.00           O  \nATOM     75    O MOL     1       3.410   0.055   0.609  1.00  0.00           O  \nATOM     76    O MOL     1       1.878   4.016   7.124  1.00  0.00           O  \nATOM     77    O MOL     1       6.356   2.497   5.799  1.00  0.00           O  \nATOM     78    O MOL     1       2.483   7.475   1.262  1.00  0.00           O  \nATOM     79    O MOL     1       4.275   6.898   5.472  1.00  0.00           O  \nATOM     80    O MOL     1       0.057   9.543   9.385  1.00  0.00           O  \nATOM     81    O MOL     1       9.785   2.869   3.606  1.00  0.00           O  \nATOM     82    O MOL     1       9.736   2.229   7.927  1.00  0.00           O  \nATOM     83    O MOL     1       6.330   8.652   7.972  1.00  0.00           O  \nATOM     84    O MOL     1       0.808   5.089   4.786  1.00  0.00           O  \nATOM     85    O MOL     1       1.310   5.833   9.056  1.00  0.00           O  \nATOM     86    O MOL     1       7.018   2.242   3.108  1.00  0.00           O  \nATOM     87    O MOL     1       6.109   8.853   1.025  1.00  0.00           O  \nATOM     88    O MOL     1       2.035   8.538   6.351  1.00  0.00           O  \nATOM     89    O MOL     1       5.999   5.944   7.121  1.00  0.00           O  \nATOM     90    O MOL     1       0.016   0.468   5.417  1.00  0.00           O  \nATOM     91    O MOL     1       5.212   8.893   3.939  1.00  0.00           O  \nATOM     92    O MOL     1       1.085   0.402   2.393  1.00  0.00           O  \nATOM     93    O MOL     1       0.123   4.013   1.051  1.00  0.00           O  \nATOM     94    O MOL     1       4.577   3.970   2.343  1.00  0.00           O  \nATOM     95    O MOL     1       3.567   1.234   3.713  1.00  0.00           O  \nATOM     96    O MOL     1       7.650   2.505   0.392  1.00  0.00           O  \nENDMDL\n",
                    "type": "blob"
                  }
                ],
                "kwargs": {
                  "defaultRepresentation": false,
                  "ext": "pdb",
                  "name": "nglview.adaptor.ASETrajectory"
                },
                "methodName": "loadFile",
                "reconstruc_color_scheme": false,
                "target": "Stage",
                "type": "call_method"
              },
              {
                "args": [
                  "500px",
                  "500px"
                ],
                "kwargs": {},
                "methodName": "setSize",
                "reconstruc_color_scheme": false,
                "target": "Widget",
                "type": "call_method"
              },
              {
                "args": [
                  "unitcell"
                ],
                "component_index": 0,
                "kwargs": {
                  "sele": "all"
                },
                "methodName": "addRepresentation",
                "reconstruc_color_scheme": false,
                "target": "compList",
                "type": "call_method"
              },
              {
                "args": [
                  "spacefill"
                ],
                "component_index": 0,
                "kwargs": {
                  "sele": "all"
                },
                "methodName": "addRepresentation",
                "reconstruc_color_scheme": false,
                "target": "compList",
                "type": "call_method"
              },
              {
                "args": [],
                "kwargs": {
                  "cameraType": "orthographic"
                },
                "methodName": "setParameters",
                "reconstruc_color_scheme": false,
                "target": "Stage",
                "type": "call_method"
              },
              {
                "args": [
                  {
                    "clipDist": 0
                  }
                ],
                "kwargs": {},
                "methodName": "setParameters",
                "reconstruc_color_scheme": false,
                "target": "Widget",
                "type": "call_method"
              },
              {
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "colorScale": "rainbow",
                  "colorScheme": "element",
                  "radiusScale": 0.5,
                  "radiusType": "covalent"
                },
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "target": "Widget",
                "type": "call_method"
              },
              {
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "colorScale": "rainbow",
                  "colorScheme": "element",
                  "radiusScale": 0.5,
                  "radiusType": "covalent"
                },
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "target": "Widget",
                "type": "call_method"
              },
              {
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "colorScale": "rainbow",
                  "colorScheme": "element",
                  "radiusScale": 0.49999999999999994,
                  "radiusType": "covalent"
                },
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "target": "Widget",
                "type": "call_method"
              },
              {
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "colorScale": "rainbow",
                  "colorScheme": "element",
                  "radiusScale": 0.49999999999999994,
                  "radiusType": "covalent"
                },
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "target": "Widget",
                "type": "call_method"
              },
              {
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "colorScale": "rainbow",
                  "colorScheme": "element",
                  "radiusScale": 0.49999999999999994,
                  "radiusType": "covalent"
                },
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "target": "Widget",
                "type": "call_method"
              },
              {
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "colorScale": "rainbow",
                  "colorScheme": "element",
                  "radiusScale": 0.57,
                  "radiusType": "covalent"
                },
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "target": "Widget",
                "type": "call_method"
              },
              {
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "colorScale": "rainbow",
                  "colorScheme": "element",
                  "radiusScale": 0.57,
                  "radiusType": "covalent"
                },
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "target": "Widget",
                "type": "call_method"
              },
              {
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "colorScale": "rainbow",
                  "colorScheme": "element",
                  "radiusScale": 0.57,
                  "radiusType": "covalent"
                },
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "target": "Widget",
                "type": "call_method"
              },
              {
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "colorScale": "rainbow",
                  "colorScheme": "element",
                  "radiusScale": 1.5,
                  "radiusType": "covalent"
                },
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "target": "Widget",
                "type": "call_method"
              },
              {
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "colorScale": "rainbow",
                  "colorScheme": "element",
                  "radiusScale": 1.5,
                  "radiusType": "covalent"
                },
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "target": "Widget",
                "type": "call_method"
              }
            ],
            "_ngl_original_stage_parameters": {
              "ambientColor": 14540253,
              "ambientIntensity": 0.2,
              "backgroundColor": "white",
              "cameraEyeSep": 0.3,
              "cameraFov": 40,
              "cameraType": "perspective",
              "clipDist": 10,
              "clipFar": 100,
              "clipNear": 0,
              "fogFar": 100,
              "fogNear": 50,
              "hoverTimeout": 0,
              "impostor": true,
              "lightColor": 14540253,
              "lightIntensity": 1,
              "mousePreset": "default",
              "panSpeed": 1,
              "quality": "medium",
              "rotateSpeed": 2,
              "sampleLevel": 0,
              "tooltip": true,
              "workerDefault": true,
              "zoomSpeed": 1.2
            },
            "_ngl_repr_dict": {
              "0": {
                "0": {
                  "params": {
                    "clipCenter": {
                      "x": 0,
                      "y": 0,
                      "z": 0
                    },
                    "clipNear": 0,
                    "clipRadius": 0,
                    "colorMode": "hcl",
                    "colorReverse": false,
                    "colorScale": "",
                    "colorScheme": "element",
                    "colorValue": "orange",
                    "defaultAssembly": "",
                    "depthWrite": true,
                    "diffuse": 16777215,
                    "diffuseInterior": false,
                    "disableImpostor": false,
                    "disablePicking": false,
                    "flatShaded": false,
                    "interiorColor": 2236962,
                    "interiorDarkening": 0,
                    "lazy": false,
                    "matrix": {
                      "elements": [
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1
                      ]
                    },
                    "metalness": 0,
                    "opacity": 1,
                    "quality": "medium",
                    "radialSegments": 10,
                    "radiusData": {},
                    "radiusScale": 1,
                    "radiusSize": 0.04924999783811635,
                    "radiusType": "vdw",
                    "roughness": 0.4,
                    "sele": "all",
                    "side": "double",
                    "sphereDetail": 1,
                    "useInteriorColor": true,
                    "visible": true,
                    "wireframe": false
                  },
                  "type": "unitcell"
                },
                "1": {
                  "params": {
                    "clipCenter": {
                      "x": 0,
                      "y": 0,
                      "z": 0
                    },
                    "clipNear": 0,
                    "clipRadius": 0,
                    "colorMode": "hcl",
                    "colorReverse": false,
                    "colorScale": "",
                    "colorScheme": "element",
                    "colorValue": "orange",
                    "defaultAssembly": "",
                    "depthWrite": true,
                    "diffuse": 16777215,
                    "diffuseInterior": false,
                    "disableImpostor": false,
                    "disablePicking": false,
                    "flatShaded": false,
                    "interiorColor": 2236962,
                    "interiorDarkening": 0,
                    "lazy": false,
                    "matrix": {
                      "elements": [
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1
                      ]
                    },
                    "metalness": 0,
                    "opacity": 1,
                    "quality": "medium",
                    "radialSegments": 10,
                    "radiusData": {},
                    "radiusScale": 1,
                    "radiusSize": 0.04924999783811635,
                    "radiusType": "vdw",
                    "roughness": 0.4,
                    "sele": "all",
                    "side": "double",
                    "sphereDetail": 1,
                    "useInteriorColor": true,
                    "visible": true,
                    "wireframe": false
                  },
                  "type": "unitcell"
                },
                "2": {
                  "params": {
                    "assembly": "default",
                    "clipCenter": {
                      "x": 0,
                      "y": 0,
                      "z": 0
                    },
                    "clipNear": 0,
                    "clipRadius": 0,
                    "colorMode": "hcl",
                    "colorReverse": false,
                    "colorScale": "rainbow",
                    "colorScheme": "element",
                    "colorValue": 9474192,
                    "defaultAssembly": "",
                    "depthWrite": true,
                    "diffuse": 16777215,
                    "diffuseInterior": false,
                    "disableImpostor": false,
                    "disablePicking": false,
                    "flatShaded": false,
                    "interiorColor": 2236962,
                    "interiorDarkening": 0,
                    "lazy": false,
                    "matrix": {
                      "elements": [
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1
                      ]
                    },
                    "metalness": 0,
                    "opacity": 1,
                    "quality": "medium",
                    "radiusData": {},
                    "radiusScale": 1.5,
                    "radiusSize": 1,
                    "radiusType": "covalent",
                    "roughness": 0.4,
                    "sele": "all",
                    "side": "double",
                    "sphereDetail": 1,
                    "useInteriorColor": true,
                    "visible": true,
                    "wireframe": false
                  },
                  "type": "spacefill"
                },
                "3": {
                  "params": {
                    "assembly": "default",
                    "clipCenter": {
                      "x": 0,
                      "y": 0,
                      "z": 0
                    },
                    "clipNear": 0,
                    "clipRadius": 0,
                    "colorMode": "hcl",
                    "colorReverse": false,
                    "colorScale": "rainbow",
                    "colorScheme": "element",
                    "colorValue": 9474192,
                    "defaultAssembly": "",
                    "depthWrite": true,
                    "diffuse": 16777215,
                    "diffuseInterior": false,
                    "disableImpostor": false,
                    "disablePicking": false,
                    "flatShaded": false,
                    "interiorColor": 2236962,
                    "interiorDarkening": 0,
                    "lazy": false,
                    "matrix": {
                      "elements": [
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1
                      ]
                    },
                    "metalness": 0,
                    "opacity": 1,
                    "quality": "medium",
                    "radiusData": {},
                    "radiusScale": 1.5,
                    "radiusSize": 1,
                    "radiusType": "covalent",
                    "roughness": 0.4,
                    "sele": "all",
                    "side": "double",
                    "sphereDetail": 1,
                    "useInteriorColor": true,
                    "visible": true,
                    "wireframe": false
                  },
                  "type": "spacefill"
                }
              },
              "1": {}
            },
            "_ngl_serialize": false,
            "_ngl_version": "2.0.0-dev.36",
            "_ngl_view_id": [
              "61DAFDCF-5BE5-4A7A-908E-FC57CF4A64E3"
            ],
            "_player_dict": {},
            "_scene_position": {},
            "_scene_rotation": {},
            "_synced_model_ids": [],
            "_synced_repr_model_ids": [],
            "_view_count": null,
            "_view_height": "",
            "_view_module": "nglview-js-widgets",
            "_view_module_version": "3.0.1",
            "_view_name": "NGLView",
            "_view_width": "",
            "background": "white",
            "frame": 100,
            "gui_style": null,
            "layout": "IPY_MODEL_8e38d82c9a364c5f9af95d37d46469ff",
            "max_frame": 100,
            "n_components": 2,
            "picked": {}
          }
        },
        "0c7f34bbc26e454ab7c0a4e6b00aedc6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ff7f5767d3d4cd981e0f3d0fd1a176c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "123c8f3742b94d2f88f5e9de681159aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "218966dd7fa6471fbeea95790d5a1d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9101a0a83624dd5ad3f01efd501b676",
              "IPY_MODEL_5593bd983e06477ebe6f61d308d0de00",
              "IPY_MODEL_dcd4125d82544a94a7e18e126febbfea",
              "IPY_MODEL_aad778fab43b45f4a1b2b0acc869b718"
            ],
            "layout": "IPY_MODEL_0ff7f5767d3d4cd981e0f3d0fd1a176c"
          }
        },
        "29bccb94ca334c09a588ca903054f167": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c7c333da3f347cca7446932479e9a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "SliderStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "38bd9de63edf488bb62178a622b59e7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "IntSliderModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_b9fdf07bb0e14da3bfc8c0ec0ec257e3",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 1,
            "style": "IPY_MODEL_8538ff68402b4a70a5d4269193a03080",
            "value": 100
          }
        },
        "4d2ea2c966e24183b0879e416592a7c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "34px"
          }
        },
        "5593bd983e06477ebe6f61d308d0de00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DropdownModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              " ",
              "picking",
              "random",
              "uniform",
              "atomindex",
              "residueindex",
              "chainindex",
              "modelindex",
              "sstruc",
              "element",
              "resname",
              "bfactor",
              "hydrophobicity",
              "value",
              "volume",
              "occupancy"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Color scheme",
            "description_tooltip": null,
            "disabled": false,
            "index": 9,
            "layout": "IPY_MODEL_781a4063245448bb9408db98b2adaaee",
            "style": "IPY_MODEL_d699f01053394ddd919c442a5205e9ec"
          }
        },
        "587e46469c1b40b7a7fd0dddab12fd40": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "670cdc83f2f54137a2fff88e1be17fcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PlayModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PlayModel",
            "_playing": false,
            "_repeat": false,
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PlayView",
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "interval": 100,
            "layout": "IPY_MODEL_e5b9207e29bd4528b192e362bb6a5189",
            "max": 100,
            "min": 0,
            "show_repeat": true,
            "step": 1,
            "style": "IPY_MODEL_b98edcf4c077434b8802166872507b5d",
            "value": 100
          }
        },
        "6cd6a128f3eb48d98f2ad3239224fc07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "773d52906773488f98bc32dff9484206": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_670cdc83f2f54137a2fff88e1be17fcf",
              "IPY_MODEL_38bd9de63edf488bb62178a622b59e7f"
            ],
            "layout": "IPY_MODEL_9471a2080aad4e08b846958314bdde39"
          }
        },
        "781a4063245448bb9408db98b2adaaee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8538ff68402b4a70a5d4269193a03080": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "SliderStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "85df14fcd2fe45fdbf076f0566563fc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "",
            "disabled": false,
            "icon": "compress",
            "layout": "IPY_MODEL_4d2ea2c966e24183b0879e416592a7c0",
            "style": "IPY_MODEL_123c8f3742b94d2f88f5e9de681159aa",
            "tooltip": ""
          }
        },
        "8c489d789b554721b386c847f58fb18c": {
          "model_module": "nglview-js-widgets",
          "model_module_version": "3.0.1",
          "model_name": "ColormakerRegistryModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "nglview-js-widgets",
            "_model_module_version": "3.0.1",
            "_model_name": "ColormakerRegistryModel",
            "_msg_ar": [],
            "_msg_q": [],
            "_ready": false,
            "_view_count": null,
            "_view_module": "nglview-js-widgets",
            "_view_module_version": "3.0.1",
            "_view_name": "ColormakerRegistryView",
            "layout": "IPY_MODEL_ab02488078ab415aa15ba5f20843a9e6"
          }
        },
        "8e38d82c9a364c5f9af95d37d46469ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9471a2080aad4e08b846958314bdde39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c74398308864fd595e5d1ef3c305bef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aad778fab43b45f4a1b2b0acc869b718": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "IntSliderModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_29bccb94ca334c09a588ca903054f167",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 1,
            "style": "IPY_MODEL_ec349da3a00a4505b896b6b7a2d6e380",
            "value": 100
          }
        },
        "b98edcf4c077434b8802166872507b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9fdf07bb0e14da3bfc8c0ec0ec257e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbe09d800dff4befad106bb6f379c3ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b20111fd465408498d5e18cede15336",
              "IPY_MODEL_218966dd7fa6471fbeea95790d5a1d20"
            ],
            "layout": "IPY_MODEL_587e46469c1b40b7a7fd0dddab12fd40"
          }
        },
        "d699f01053394ddd919c442a5205e9ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcd4125d82544a94a7e18e126febbfea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatSliderModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FloatSliderView",
            "continuous_update": true,
            "description": "Ball size",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_6cd6a128f3eb48d98f2ad3239224fc07",
            "max": 1.5,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": ".2f",
            "step": 0.01,
            "style": "IPY_MODEL_2c7c333da3f347cca7446932479e9a76",
            "value": 1.5
          }
        },
        "e5b9207e29bd4528b192e362bb6a5189": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9101a0a83624dd5ad3f01efd501b676": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DropdownModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "All",
              "O",
              "H"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Show",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_0c7f34bbc26e454ab7c0a4e6b00aedc6",
            "style": "IPY_MODEL_9c74398308864fd595e5d1ef3c305bef"
          }
        },
        "ec349da3a00a4505b896b6b7a2d6e380": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "SliderStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}