{"cells":[{"cell_type":"markdown","metadata":{"id":"CFpAi8g9XmUU"},"source":["# Molecular Dynamics with NequIP \n","\n","### NequIP Authors: Simon Batzner, Albert Musaelian, Lixin Sun, Anders Johansson, Boris Kozinsky"]},{"cell_type":"markdown","metadata":{"id":"6qtq3KJqLq9F"},"source":["<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/nequip3.png?raw=true\" width=\"60%\">"]},{"cell_type":"markdown","metadata":{"id":"0m1W9W9yvuKA"},"source":["### Tutorial for MD for bulk water modified by Gabriele Tocci"]},{"cell_type":"markdown","metadata":{"id":"gMg3YvVF2aBQ"},"source":["### The water trajectory is obtained with the SCAN functional at 300 K and is the last 5 ps (10k frames) of a trajectory used in the paper by Herrero et al.: [Connection between water's dynamical and structural properties: Insights from ab initio simulations ](https://www.pnas.org/doi/10.1073/pnas.2121641119)"]},{"cell_type":"markdown","metadata":{"id":"2shNisM863Wy"},"source":["### Open in colab and change the runtime to use the GPU\n","<a href=\"https://colab.research.google.com/github/gabriele16/nequip/blob/main/colab/nequip-water-tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K7bW4JWmmuyD","outputId":"f4ddf12b-a679-4952-9673-214a6f9d2bc4","executionInfo":{"status":"ok","timestamp":1668710998308,"user_tz":-60,"elapsed":96214,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.10\n","  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[K     |██████████████████████████████▎ | 834.1 MB 57.2 MB/s eta 0:00:01tcmalloc: large alloc 1147494400 bytes == 0x66266000 @  0x7f079f859615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n","\u001b[K     |████████████████████████████████| 881.9 MB 18 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10) (4.1.1)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.10.0 which is incompatible.\n","torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.10.0 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.10.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.10.0\n"]}],"source":["!pip install torch==1.10"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ZOLIFOJZaeZ5","executionInfo":{"status":"ok","timestamp":1668710998564,"user_tz":-60,"elapsed":269,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[],"source":["import warnings\n","import os\n","\n","data_dir = \"/content/nequip/datasets\""]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uZpOvFtImsy2","outputId":"999a517a-3eac-4179-9941-8e75a9fb28c6","executionInfo":{"status":"ok","timestamp":1668710999884,"user_tz":-60,"elapsed":1323,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1.10.0+cu102\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}],"source":["import torch\n","\n","print(torch.__version__)\n","torch.cuda.is_available()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t7Uh7nyHnR-w","outputId":"088151a2-0bb1-4c57-e988-af65e55912a1","executionInfo":{"status":"ok","timestamp":1668711000104,"user_tz":-60,"elapsed":223,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2021 NVIDIA Corporation\n","Built on Sun_Feb_14_21:12:58_PST_2021\n","Cuda compilation tools, release 11.2, V11.2.152\n","Build cuda_11.2.r11.2/compiler.29618528_0\n"]}],"source":["!nvcc --version"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uIMyrDOEm2IB","outputId":"10e2eaf6-832c-4319-dbf3-5a932a7e640c","executionInfo":{"status":"ok","timestamp":1668711015034,"user_tz":-60,"elapsed":14935,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove 'lammps': No such file or directory\n","Cloning into 'lammps'...\n","remote: Enumerating objects: 11732, done.\u001b[K\n","remote: Counting objects: 100% (11732/11732), done.\u001b[K\n","remote: Compressing objects: 100% (8609/8609), done.\u001b[K\n","remote: Total 11732 (delta 3932), reused 6311 (delta 2924), pack-reused 0\n","Receiving objects: 100% (11732/11732), 110.00 MiB | 19.84 MiB/s, done.\n","Resolving deltas: 100% (3932/3932), done.\n","Note: checking out '7586adbb6a61254125992709ef2fda9134cfca6c'.\n","\n","You are in 'detached HEAD' state. You can look around, make experimental\n","changes and commit them, and you can discard any commits you make in this\n","state without impacting any branches by performing another checkout.\n","\n","If you want to create a new branch to retain commits you create, you may\n","do so (now or later) by using -b with the checkout command again. Example:\n","\n","  git checkout -b <new-branch-name>\n","\n","Checking out files: 100% (11058/11058), done.\n","Cloning into 'pair_nequip'...\n","remote: Enumerating objects: 420, done.\u001b[K\n","remote: Counting objects: 100% (118/118), done.\u001b[K\n","remote: Compressing objects: 100% (44/44), done.\u001b[K\n","remote: Total 420 (delta 80), reused 87 (delta 66), pack-reused 302\u001b[K\n","Receiving objects: 100% (420/420), 427.40 KiB | 8.22 MiB/s, done.\n","Resolving deltas: 100% (209/209), done.\n","Copying files...\n","Updating CMakeLists.txt...\n","Done!\n"]}],"source":["!rm -r lammps\n","!git clone -b \"stable_29Sep2021_update2\" --depth 1 \"https://github.com/lammps/lammps.git\"\n","!git clone https://github.com/mir-group/pair_nequip\n","!cd pair_nequip && ./patch_lammps.sh /content/lammps/\n","!cd .."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"100Be8B6m5am","executionInfo":{"status":"ok","timestamp":1668711015396,"user_tz":-60,"elapsed":378,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[],"source":["!cp /content/pair_nequip/*.cpp /content/lammps/src/\n","!cp /content/pair_nequip/*.h /content/lammps/src/\n","! sed -i 's/CMAKE_CXX_STANDARD 11/CMAKE_CXX_STANDARD 14/g'  /content/lammps/cmake/CMakeLists.txt"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KlgRSmyom9VQ","outputId":"e3601ae6-e50c-489c-bf79-068f8606278b","executionInfo":{"status":"ok","timestamp":1668711019063,"user_tz":-60,"elapsed":3669,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mkl in /usr/local/lib/python3.7/dist-packages (2019.0)\n","Collecting mkl-include\n","  Downloading mkl_include-2022.2.1-py2.py3-none-manylinux1_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 4.7 MB/s \n","\u001b[?25hRequirement already satisfied: intel-openmp in /usr/local/lib/python3.7/dist-packages (from mkl) (2022.2.1)\n","Installing collected packages: mkl-include\n","Successfully installed mkl-include-2022.2.1\n"]}],"source":["!pip install mkl mkl-include"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t8vY6rwbnEgK","outputId":"0f6b08c9-f95b-42c5-c103-b69d381e12c5","executionInfo":{"status":"ok","timestamp":1668711202828,"user_tz":-60,"elapsed":183769,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["-- The CXX compiler identification is GNU 7.5.0\n","-- Detecting CXX compiler ABI info\n","-- Detecting CXX compiler ABI info - done\n","-- Check for working CXX compiler: /usr/bin/c++ - skipped\n","-- Detecting CXX compile features\n","-- Detecting CXX compile features - done\n","-- Found Git: /usr/bin/git (found version \"2.17.1\") \n","-- Appending /usr/local/cuda/lib64/stubs to CMAKE_LIBRARY_PATH: /usr/local/cuda/lib64/stubs\n","-- Running check for auto-generated files from make-based build system\n","-- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so (found version \"3.1\") \n","-- Found MPI: TRUE (found version \"3.1\")  \n","-- Looking for C++ include omp.h\n","-- Looking for C++ include omp.h - found\n","-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n","-- Found OpenMP: TRUE (found version \"4.5\")  \n","-- Found JPEG: /usr/lib/x86_64-linux-gnu/libjpeg.so (found version \"80\") \n","-- Found PNG: /usr/lib/x86_64-linux-gnu/libpng.so (found version \"1.6.34\") \n","-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n","-- Found GZIP: /bin/gzip  \n","-- Found FFMPEG: /usr/bin/ffmpeg  \n","-- Looking for C++ include cmath\n","-- Looking for C++ include cmath - found\n","-- Generating style headers...\n","-- Generating package headers...\n","-- Generating lmpinstalledpkgs.h...\n","-- Could NOT find ClangFormat (missing: ClangFormat_EXECUTABLE) (Required is at least version \"8.0\")\n","-- The following tools and libraries have been found and configured:\n"," * Git\n"," * MPI\n"," * OpenMP\n"," * JPEG\n"," * PNG\n"," * ZLIB\n","\n","-- <<< Build configuration >>>\n","   Operating System: Linux Ubuntu 18.04\n","   Build type:       RelWithDebInfo\n","   Install path:     /root/.local\n","   Generator:        Unix Makefiles using /usr/bin/make\n","-- Enabled packages: <None>\n","-- <<< Compilers and Flags: >>>\n","-- C++ Compiler:     /usr/bin/c++\n","      Type:          GNU\n","      Version:       7.5.0\n","      C++ Flags:     -O2 -g -DNDEBUG\n","      Defines:       LAMMPS_SMALLBIG;LAMMPS_MEMALIGN=64;LAMMPS_OMP_COMPAT=3;LAMMPS_JPEG;LAMMPS_PNG;LAMMPS_GZIP;LAMMPS_FFMPEG\n","-- <<< Linker flags: >>>\n","-- Executable name:  lmp\n","-- Static library flags:    \n","-- <<< MPI flags >>>\n","-- MPI_defines:      MPICH_SKIP_MPICXX;OMPI_SKIP_MPICXX;_MPICC_H\n","-- MPI includes:     /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include;/usr/lib/x86_64-linux-gnu/openmpi/include\n","-- MPI libraries:    /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so;\n","-- Looking for C++ include pthread.h\n","-- Looking for C++ include pthread.h - found\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n","-- Looking for pthread_create in pthreads\n","-- Looking for pthread_create in pthreads - not found\n","-- Looking for pthread_create in pthread\n","-- Looking for pthread_create in pthread - found\n","-- Found Threads: TRUE  \n","-- Found CUDA: /usr/local/cuda (found version \"11.2\") \n","-- Caffe2: CUDA detected: 11.2\n","-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\n","-- Caffe2: CUDA toolkit directory: /usr/local/cuda\n","-- Caffe2: Header version is: 11.2\n","-- Found CUDNN: /usr/lib/x86_64-linux-gnu/libcudnn.so  \n","-- Found cuDNN: v8.1.1  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)\n","-- /usr/local/cuda/lib64/libnvrtc.so shorthash is 369df368\n","-- Autodetected CUDA architecture(s):  7.5\n","-- Added CUDA NVCC flags for: -gencode;arch=compute_75,code=sm_75\n","\u001b[33mCMake Warning at /usr/local/lib/python3.7/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\n","  static library kineto_LIBRARY-NOTFOUND not found.\n","Call Stack (most recent call first):\n","  /usr/local/lib/python3.7/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:127 (append_torchlib_if_found)\n","  CMakeLists.txt:922 (find_package)\n","\n","\u001b[0m\n","-- Found Torch: /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so  \n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/lammps/build\n","[  0%] \u001b[34m\u001b[1mGenerating includes/lammps/atom.h\u001b[0m\n","[  0%] \u001b[34m\u001b[1mGenerating includes/lammps/angle.h\u001b[0m\n","[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/variable.h\u001b[0m\n","-- Git Directory: /content/lammps/.git\n","[  1%] Built target atom.h\n","[  1%] Built target angle.h\n","[  1%] Built target variable.h\n","[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/citeme.h\u001b[0m\n","[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/comm.h\u001b[0m\n","[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/bond.h\u001b[0m\n","[  1%] Built target comm.h\n","[  1%] Built target citeme.h\n","[  1%] Built target bond.h\n","[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/dihedral.h\u001b[0m\n","[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/compute.h\u001b[0m\n","[  2%] Built target dihedral.h\n","[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/domain.h\u001b[0m\n","[  2%] Built target compute.h\n","[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/error.h\u001b[0m\n","[  2%] Built target domain.h\n","[  2%] Built target error.h\n","[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/fix.h\u001b[0m\n","[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/force.h\u001b[0m\n","[  2%] Built target fix.h\n","[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/group.h\u001b[0m\n","[  3%] Built target force.h\n","[  3%] Built target group.h\n","[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/improper.h\u001b[0m\n","[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/input.h\u001b[0m\n","[  3%] Built target improper.h\n","[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/info.h\u001b[0m\n","[  3%] Built target input.h\n","[  3%] Built target info.h\n","[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/kspace.h\u001b[0m\n","[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lattice.h\u001b[0m\n","[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lammps.h\u001b[0m\n","[  4%] Built target kspace.h\n","[  4%] Built target lammps.h\n","[  4%] Built target lattice.h\n","[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/library.h\u001b[0m\n","[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/lmppython.h\u001b[0m\n","[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/lmptype.h\u001b[0m\n","[  5%] Built target lmptype.h\n","[  5%] Built target lmppython.h\n","[  5%] Built target library.h\n","[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/memory.h\u001b[0m\n","[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/modify.h\u001b[0m\n","[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/neighbor.h\u001b[0m\n","[  6%] Built target neighbor.h\n","[  6%] Built target modify.h\n","[  6%] Built target memory.h\n","[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/output.h\u001b[0m\n","[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/pair.h\u001b[0m\n","[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/neigh_list.h\u001b[0m\n","[  7%] Built target output.h\n","[  7%] Built target pair.h\n","[  7%] Built target neigh_list.h\n","[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/pointers.h\u001b[0m\n","[  7%] Built target pointers.h\n","[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/timer.h\u001b[0m\n","[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/region.h\u001b[0m\n","[  8%] Built target timer.h\n","-- Generating lmpgitversion.h...\n","[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/universe.h\u001b[0m\n","[  8%] Built target region.h\n","[  8%] Built target gitversion\n","[  8%] Built target universe.h\n","[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/update.h\u001b[0m\n","[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/utils.h\u001b[0m\n","[  8%] Built target update.h\n","[  8%] Built target utils.h\n","[  8%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_deprecated.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_zero.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_hybrid.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/arg_info.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_map.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec.cpp.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_atomic.cpp.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_body.cpp.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_charge.cpp.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_ellipsoid.cpp.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_hybrid.cpp.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_line.cpp.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_sphere.cpp.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_tri.cpp.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/balance.cpp.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/body.cpp.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond.cpp.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_deprecated.cpp.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_hybrid.cpp.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_zero.cpp.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/change_box.cpp.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/citeme.cpp.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_brick.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_tiled.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_aggregate_atom.cpp.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle.cpp.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle_local.cpp.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angmom_chunk.cpp.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond_local.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centro_atom.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centroid_stress_atom.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_atom.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_spread_atom.cpp.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cluster_atom.cpp.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cna_atom.cpp.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com.cpp.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com_chunk.cpp.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_coord_atom.cpp.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_deprecated.cpp.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral.cpp.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral_local.cpp.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole.cpp.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole_chunk.cpp.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_displace_atom.cpp.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere_atom.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_fragment_atom.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_global_atom.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_group_group.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration.cpp.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration_chunk.cpp.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_heat_flux.cpp.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper.cpp.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper_local.cpp.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_inertia_chunk.cpp.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke.cpp.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke_atom.cpp.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd.cpp.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd_chunk.cpp.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_omega_chunk.cpp.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_orientorder_atom.cpp.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair.cpp.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair_local.cpp.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe.cpp.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe_atom.cpp.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pressure.cpp.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_atom.cpp.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_chunk.cpp.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_local.cpp.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_rdf.cpp.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce.cpp.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_chunk.cpp.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_region.cpp.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_slice.cpp.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_stress_atom.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_chunk.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_com.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_deform.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_partial.cpp.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_profile.cpp.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_ramp.cpp.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_region.cpp.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_sphere.cpp.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_torque_chunk.cpp.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vacf.cpp.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vcm_chunk.cpp.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_atoms.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_bonds.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_box.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_atoms.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_bonds.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/deprecated.cpp.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral.cpp.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_deprecated.cpp.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_hybrid.cpp.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_zero.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/displace_atoms.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/domain.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_atom.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_cfg.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_custom.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_deprecated.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_image.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_local.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_movie.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_xyz.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/error.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/finish.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_adapt.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_addforce.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_atom.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_chunk.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_correlate.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo_weight.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_time.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_aveforce.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_balance.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_box_relax.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deform.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deposit.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deprecated.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dt_reset.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dummy.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_efield.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_enforce2d.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_evaporate.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_external.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_gravity.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_group.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_halt.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_heat.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_indent.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_langevin.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_lineforce.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_minimize.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_momentum.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_move.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_neigh_history.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh_sphere.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph_sphere.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt_sphere.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_limit.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_noforce.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_sphere.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sllod.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sphere.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_planeforce.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_press_berendsen.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_print.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_property_atom.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_read_restart.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_recenter.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_respa.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_restrain.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_setforce.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_chunk.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_self.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_force.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_state.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_berendsen.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_rescale.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_thermal_conductivity.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_vector.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_viscous.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_harmonic.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj1043.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj126.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj93.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_morse.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_reflect.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_region.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_format.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_os.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/force.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/gridcomm.cpp.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/group.cpp.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/hashlittle.cpp.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/image.cpp.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_group.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_neigh.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_store.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_time.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_var.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_deprecated.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_hybrid.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_zero.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/info.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/input.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/integrate.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/irregular.cpp.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace.cpp.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace_deprecated.cpp.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lammps.cpp.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lattice.cpp.o\u001b[0m\n","[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/library.cpp.o\u001b[0m\n","[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lmppython.cpp.o\u001b[0m\n","[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_eigen.cpp.o\u001b[0m\n","[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_extra.cpp.o\u001b[0m\n","[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_special.cpp.o\u001b[0m\n","[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/memory.cpp.o\u001b[0m\n","[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min.cpp.o\u001b[0m\n","[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_cg.cpp.o\u001b[0m\n","[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_fire.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_fire_old.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_hftn.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_linesearch.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_quickmin.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_sd.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/minimize.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/modify.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/molecule.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_page.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_pool_chunk.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_multi.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_standard.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_list.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_request.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neighbor.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_copy.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin_atomonly.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_bin_ghost.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_multi.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_multi_old.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_nsq.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_full_nsq_ghost.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_atomonly_newton.cpp.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newtoff.cpp.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newtoff_ghost.cpp.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newton.cpp.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_bin_newton_tri.cpp.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newtoff.cpp.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newton.cpp.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_newton_tri.cpp.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newtoff.cpp.o\u001b[0m\n","[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newton.cpp.o\u001b[0m\n","[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_multi_old_newton_tri.cpp.o\u001b[0m\n","[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newtoff.cpp.o\u001b[0m\n","[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newtoff_ghost.cpp.o\u001b[0m\n","[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_nsq_newton.cpp.o\u001b[0m\n","[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newtoff.cpp.o\u001b[0m\n","[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newton.cpp.o\u001b[0m\n","[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_bin_newton_tri.cpp.o\u001b[0m\n","[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_nsq_newtoff.cpp.o\u001b[0m\n","[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_respa_nsq_newton.cpp.o\u001b[0m\n","[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newtoff.cpp.o\u001b[0m\n","[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newton.cpp.o\u001b[0m\n","[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_bin_newton_tri.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newtoff.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newton.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_newton_tri.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newtoff.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newton.cpp.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_multi_old_newton_tri.cpp.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_nsq_newtoff.cpp.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_half_size_nsq_newton.cpp.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_halffull_newtoff.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_halffull_newton.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_respa.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size.cpp.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on.cpp.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on_oneside.cpp.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil.cpp.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_bin_2d.cpp.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_bin_3d.cpp.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_ghost_bin_2d.cpp.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_ghost_bin_3d.cpp.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_2d.cpp.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_3d.cpp.o\u001b[0m\n","[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_old_2d.cpp.o\u001b[0m\n","[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_full_multi_old_3d.cpp.o\u001b[0m\n","[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_2d.cpp.o\u001b[0m\n","[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_2d_tri.cpp.o\u001b[0m\n","[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_3d.cpp.o\u001b[0m\n","[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_bin_3d_tri.cpp.o\u001b[0m\n","[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_2d.cpp.o\u001b[0m\n","[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_2d_tri.cpp.o\u001b[0m\n","[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_3d.cpp.o\u001b[0m\n","[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_3d_tri.cpp.o\u001b[0m\n","[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_2d.cpp.o\u001b[0m\n","[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_2d_tri.cpp.o\u001b[0m\n","[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_3d.cpp.o\u001b[0m\n","[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_half_multi_old_3d_tri.cpp.o\u001b[0m\n","[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo.cpp.o\u001b[0m\n","[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_all.cpp.o\u001b[0m\n","[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_partial.cpp.o\u001b[0m\n","[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_template.cpp.o\u001b[0m\n","[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_all.cpp.o\u001b[0m\n","[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_partial.cpp.o\u001b[0m\n","[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_template.cpp.o\u001b[0m\n","[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_all.cpp.o\u001b[0m\n","[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_partial.cpp.o\u001b[0m\n","[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_template.cpp.o\u001b[0m\n","[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_all.cpp.o\u001b[0m\n","[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_partial.cpp.o\u001b[0m\n","[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_template.cpp.o\u001b[0m\n","[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/output.cpp.o\u001b[0m\n","[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair.cpp.o\u001b[0m\n","[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_born.cpp.o\u001b[0m\n","[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck.cpp.o\u001b[0m\n","[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck_coul_cut.cpp.o\u001b[0m\n","[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_cut.cpp.o\u001b[0m\n","[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_debye.cpp.o\u001b[0m\n","[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_dsf.cpp.o\u001b[0m\n","[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_wolf.cpp.o\u001b[0m\n","[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_deprecated.cpp.o\u001b[0m\n","[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid.cpp.o\u001b[0m\n","[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_overlay.cpp.o\u001b[0m\n","[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_scaled.cpp.o\u001b[0m\n","[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut.cpp.o\u001b[0m\n","[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut_coul_cut.cpp.o\u001b[0m\n","[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_expand.cpp.o\u001b[0m\n","[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_morse.cpp.o\u001b[0m\n","[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_nequip.cpp.o\u001b[0m\n","[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_soft.cpp.o\u001b[0m\n","[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_table.cpp.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_yukawa.cpp.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zbl.cpp.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zero.cpp.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/potential_file_reader.cpp.o\u001b[0m\n","[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/procmap.cpp.o\u001b[0m\n","[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_mars.cpp.o\u001b[0m\n","[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_park.cpp.o\u001b[0m\n","[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rcb.cpp.o\u001b[0m\n","[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_data.cpp.o\u001b[0m\n","[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_dump.cpp.o\u001b[0m\n","[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_restart.cpp.o\u001b[0m\n","[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader.cpp.o\u001b[0m\n","[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_native.cpp.o\u001b[0m\n","[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_xyz.cpp.o\u001b[0m\n","[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region.cpp.o\u001b[0m\n","[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_block.cpp.o\u001b[0m\n","[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cone.cpp.o\u001b[0m\n","[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cylinder.cpp.o\u001b[0m\n","[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_deprecated.cpp.o\u001b[0m\n","[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_intersect.cpp.o\u001b[0m\n","[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_plane.cpp.o\u001b[0m\n","[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_prism.cpp.o\u001b[0m\n","[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_sphere.cpp.o\u001b[0m\n","[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_union.cpp.o\u001b[0m\n","[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/replicate.cpp.o\u001b[0m\n","[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rerun.cpp.o\u001b[0m\n","[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atom_ids.cpp.o\u001b[0m\n","[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_mol_ids.cpp.o\u001b[0m\n","[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/respa.cpp.o\u001b[0m\n","[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/run.cpp.o\u001b[0m\n","[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/set.cpp.o\u001b[0m\n","[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/special.cpp.o\u001b[0m\n","[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/table_file_reader.cpp.o\u001b[0m\n","[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tabular_function.cpp.o\u001b[0m\n","[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/text_file_reader.cpp.o\u001b[0m\n","[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/thermo.cpp.o\u001b[0m\n","[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/timer.cpp.o\u001b[0m\n","[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tokenizer.cpp.o\u001b[0m\n","[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/universe.cpp.o\u001b[0m\n","[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/update.cpp.o\u001b[0m\n","[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/utils.cpp.o\u001b[0m\n","[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/variable.cpp.o\u001b[0m\n","\u001b[01m\u001b[K/content/lammps/src/variable.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint LAMMPS_NS::Variable::next(int, char**)\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/lammps/src/variable.cpp:714:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n","         \u001b[01;35m\u001b[Kfread(buf,1,64,fp)\u001b[m\u001b[K;\n","         \u001b[01;35m\u001b[K~~~~~^~~~~~~~~~~~~\u001b[m\u001b[K\n","[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/velocity.cpp.o\u001b[0m\n","[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/verlet.cpp.o\u001b[0m\n","[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_coeff.cpp.o\u001b[0m\n","[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_data.cpp.o\u001b[0m\n","[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_dump.cpp.o\u001b[0m\n","[100%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_restart.cpp.o\u001b[0m\n","[100%] \u001b[32m\u001b[1mLinking CXX static library liblammps.a\u001b[0m\n","[100%] Built target lammps\n","[100%] \u001b[32mBuilding CXX object CMakeFiles/lmp.dir/content/lammps/src/main.cpp.o\u001b[0m\n","[100%] \u001b[32m\u001b[1mLinking CXX executable lmp\u001b[0m\n","[100%] Built target lmp\n"]}],"source":["!cd lammps && mkdir -p build && cd build && cmake ../cmake -DCMAKE_PREFIX_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` && make -j4"]},{"cell_type":"markdown","metadata":{"id":"aG8wqxg43tiZ"},"source":["### Clone nequip repository with the AIMD data file"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hoYyUGf70c4H","outputId":"f36bfdb5-6d10-40c2-96e7-86593523e683","executionInfo":{"status":"ok","timestamp":1668711204986,"user_tz":-60,"elapsed":2189,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'nequip'...\n","remote: Enumerating objects: 191, done.\u001b[K\n","remote: Counting objects: 100% (191/191), done.\u001b[K\n","remote: Compressing objects: 100% (185/185), done.\u001b[K\n","remote: Total 191 (delta 7), reused 60 (delta 0), pack-reused 0\u001b[K\n","Receiving objects: 100% (191/191), 39.50 MiB | 30.37 MiB/s, done.\n","Resolving deltas: 100% (7/7), done.\n","Already up to date.\n"]}],"source":["! git clone --depth 1 \"https://github.com/gabriele16/nequip.git\"\n","! cd nequip && git pull"]},{"cell_type":"markdown","metadata":{"id":"FrjXfT6L3S51"},"source":["### Extract configurations and forces of AIMD trajectory"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PHCh2aC7WfKj","outputId":"05b49374-800c-455a-a3e7-48a2e9e9798f","executionInfo":{"status":"ok","timestamp":1668711206240,"user_tz":-60,"elapsed":1257,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["._AIMD_data\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n","AIMD_data/\n","AIMD_data/._WATER-frc-10k-1.xyz\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n","AIMD_data/WATER-frc-10k-1.xyz\n","AIMD_data/._celldata.dat\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n","AIMD_data/celldata.dat\n","AIMD_data/._WATER-pos-10k-1.xyz\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.google.drivefs.item-id#S'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n","AIMD_data/WATER-pos-10k-1.xyz\n","celldata.dat  WATER-frc-10k-1.xyz  WATER-pos-10k-1.xyz\n"]}],"source":["! tar -xzvf  /content/nequip/datasets/AIMD_data.tar.gz -C /content/nequip/datasets/\n","! ls /content/nequip/datasets/AIMD_data"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J51CA0Bod1Jv","outputId":"341b867a-af12-47ec-f73f-c56e27bc4886","executionInfo":{"status":"ok","timestamp":1668711230283,"user_tz":-60,"elapsed":24046,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wandb\n","  Downloading wandb-0.13.5-py2.py3-none-any.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 4.7 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.11.0-py2.py3-none-any.whl (168 kB)\n","\u001b[K     |████████████████████████████████| 168 kB 66.1 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.19.6)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Collecting setproctitle\n","  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 53.2 MB/s \n","\u001b[?25hCollecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n","\u001b[K     |████████████████████████████████| 166 kB 60.7 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n","\u001b[K     |████████████████████████████████| 166 kB 74.6 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n","\u001b[K     |████████████████████████████████| 162 kB 63.6 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n","\u001b[K     |████████████████████████████████| 162 kB 71.0 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n","\u001b[K     |████████████████████████████████| 158 kB 61.0 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 65.3 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 57.9 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 57.8 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 60.4 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 51.6 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 61.3 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 45.0 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n","\u001b[K     |████████████████████████████████| 156 kB 45.1 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=d930e8cb71c1130531e5e7ab3c92602b592d1d5a4d0d3943c1c34dfaad7e8e51\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built pathtools\n","Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n","Successfully installed GitPython-3.1.29 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 wandb-0.13.5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing ./nequip\n","\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (1.21.6)\n","Collecting ase\n","  Downloading ase-3.22.1-py3-none-any.whl (2.2 MB)\n","\u001b[K     |████████████████████████████████| 2.2 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (4.64.1)\n","Requirement already satisfied: torch!=1.9.0,<=1.12,>=1.8 in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (1.10.0)\n","Collecting e3nn<0.6.0,>=0.3.5\n","  Downloading e3nn-0.5.0-py3-none-any.whl (117 kB)\n","\u001b[K     |████████████████████████████████| 117 kB 50.4 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (6.0)\n","Collecting torch-runstats>=0.2.0\n","  Downloading torch_runstats-0.2.0-py3-none-any.whl (8.1 kB)\n","Collecting torch-ema>=0.3.0\n","  Downloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\n","Collecting scikit_learn<=1.0.1\n","  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n","\u001b[K     |████████████████████████████████| 23.2 MB 223 kB/s \n","\u001b[?25hRequirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from nequip==0.5.5) (4.1.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.7.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.7.3)\n","Collecting opt-einsum-fx>=0.1.4\n","  Downloading opt_einsum_fx-0.1.4-py3-none-any.whl (13 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (21.3)\n","Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (3.3.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn<=1.0.1->nequip==0.5.5) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn<=1.0.1->nequip==0.5.5) (3.1.0)\n","Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from ase->nequip==0.5.5) (3.2.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase->nequip==0.5.5) (3.0.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1.0->ase->nequip==0.5.5) (1.15.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.2.1)\n","Building wheels for collected packages: nequip\n","  Building wheel for nequip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nequip: filename=nequip-0.5.5-py3-none-any.whl size=138682 sha256=005784a579573f1a1cab811ba4cf0a037cbc2452aae4c1803255fff065435af3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-rdwb7kra/wheels/a8/8f/18/b30c4402c2d6ab52853310650b85822afe26aaae5f6d00356b\n","Successfully built nequip\n","Installing collected packages: opt-einsum-fx, torch-runstats, torch-ema, scikit-learn, e3nn, ase, nequip\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.0.2\n","    Uninstalling scikit-learn-1.0.2:\n","      Successfully uninstalled scikit-learn-1.0.2\n","Successfully installed ase-3.22.1 e3nn-0.5.0 nequip-0.5.5 opt-einsum-fx-0.1.4 scikit-learn-1.0.1 torch-ema-0.3 torch-runstats-0.2.0\n"]},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fbc89452b10>"]},"metadata":{},"execution_count":11}],"source":["import numpy as np\n","import pandas as pd\n","\n","# install wandb\n","!pip install wandb\n","# install nequip\n","!pip install nequip/\n","# fix colab imports\n","import site\n","\n","site.main()\n","# set to allow anonymous WandB\n","import os\n","\n","os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n","import numpy as np\n","from ase.io import read, write\n","\n","np.random.seed(0)\n","torch.manual_seed(0)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0,"referenced_widgets":["baaa03010f81450c97509f0242e8dcf0"]},"id":"ZixXbCiPcMGg","outputId":"d10bd7a9-88f2-4ed0-ed64-96d39058b892","executionInfo":{"status":"ok","timestamp":1668711249168,"user_tz":-60,"elapsed":18899,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting nglview\n","  Downloading nglview-3.0.3.tar.gz (5.7 MB)\n","\u001b[K     |████████████████████████████████| 5.7 MB 4.8 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ipywidgets>=7 in /usr/local/lib/python3.7/dist-packages (from nglview) (7.7.1)\n","Requirement already satisfied: jupyterlab-widgets in /usr/local/lib/python3.7/dist-packages (from nglview) (3.0.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nglview) (1.21.6)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (3.6.1)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (5.3.4)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (7.9.0)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (0.2.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7->nglview) (5.1.1)\n","Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7->nglview) (6.0.4)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7->nglview) (6.1.12)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (2.6.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (4.4.2)\n","Collecting jedi>=0.10\n","  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 40.3 MB/s \n","\u001b[?25hRequirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (4.8.0)\n","Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (2.0.10)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (0.7.5)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (0.2.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7->nglview) (57.4.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets>=7->nglview) (0.8.3)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7->nglview) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7->nglview) (0.2.5)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (5.7.16)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.15.0)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (23.2.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (1.8.0)\n","Requirement already satisfied: nbconvert<6.0 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (5.6.1)\n","Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (4.11.2)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.13.3)\n","Requirement already satisfied: jinja2<=3.0.0 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (2.11.3)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (5.7.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<=3.0.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7->nglview) (2.8.2)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.8.4)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.4)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (5.0.1)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.6.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.7.1)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (1.5.0)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (2.16.2)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (4.3.3)\n","Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (4.13.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (3.10.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.19.2)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (22.1.0)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (5.10.0)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.7.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7->nglview) (0.5.1)\n","Building wheels for collected packages: nglview\n","  Building wheel for nglview (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nglview: filename=nglview-3.0.3-py3-none-any.whl size=8057550 sha256=4749818d32868b568825b992eddbb4f62150c90fc2a6db736b1d22515ddeeeaf\n","  Stored in directory: /root/.cache/pip/wheels/01/0c/49/c6f79d8edba8fe89752bf20de2d99040bfa57db0548975c5d5\n","Successfully built nglview\n","Installing collected packages: jedi, nglview\n","Successfully installed jedi-0.18.1 nglview-3.0.3\n","Enabling notebook extension nglview-js-widgets/extension...\n","Paths used for configuration of notebook: \n","    \t/usr/etc/jupyter/nbconfig/notebook.json\n","Paths used for configuration of notebook: \n","    \t\n","      - Validating: \u001b[32mOK\u001b[0m\n","Paths used for configuration of notebook: \n","    \t/usr/etc/jupyter/nbconfig/notebook.json\n"]},{"output_type":"display_data","data":{"text/plain":[],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baaa03010f81450c97509f0242e8dcf0"}},"metadata":{}}],"source":["!pip3 install nglview\n","!jupyter-nbextension enable nglview --py --sys-prefix\n","import nglview as nv"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"jDJ9Re0arOb0","executionInfo":{"status":"ok","timestamp":1668711249169,"user_tz":-60,"elapsed":7,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[],"source":["def MD_reader_xyz(f, data_dir, no_skip=0):\n","    filename = os.path.join(data_dir, f)\n","    fo = open(filename, \"r\")\n","    natoms_str = fo.read().rsplit(\" i = \")[0]\n","    natoms = int(natoms_str.split(\"\\n\")[0])\n","    fo.close()\n","    fo = open(filename, \"r\")\n","    samples = fo.read().split(natoms_str)[1:]\n","    steps = []\n","    xyz = []\n","    temperatures = []\n","    energies = []\n","    for sample in samples[::no_skip]:\n","        entries = sample.split(\"\\n\")[:-1]\n","        energies.append(float(entries[0].split(\"=\")[-1]))\n","        temp = np.array([list(map(float, lv.split()[1:])) for lv in entries[1:]])\n","        xyz.append(temp[:, :])\n","    return natoms_str, np.array(xyz), np.array(energies)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"DziQT_zjMKS4","executionInfo":{"status":"ok","timestamp":1668711249169,"user_tz":-60,"elapsed":6,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[],"source":["from ase.build import sort\n","\n","\n","def MD_writer_xyz(\n","    positions, forces, cell_vec_abc, energies, data_dir, f, conv_frc=1.0, conv_ener=1.0\n","):\n","\n","    filename = os.path.join(data_dir, f)\n","    fo = open(filename, \"w\")\n","\n","    for it, frame in enumerate(positions):\n","        natoms = len(frame)\n","        fo.write(\"{:5d}\\n\".format(natoms))\n","        fo.write(\n","            'Lattice=\"{:.5f} 0.0 0.0 0.0 {:.5f} 0.0 0.0 0.0 {:.5f}\" \\\n","    Properties=\"species:S:1:pos:R:3:forces:R:3\" \\\n","    energy={:.10f} pbc=\"T T T\"\\n'.format(\n","                cell_vec_abc[0],\n","                cell_vec_abc[1],\n","                cell_vec_abc[2],\n","                energies[it] * conv_ener,\n","            )\n","        )\n","        if it % 1000 == 0.0:\n","            print(it)\n","\n","        sorted_frame = sort(frame)\n","        sorted_forces = sort(forces[it])\n","\n","        fo.write(\n","            \"\".join(\n","                \"{:8s} {:.8f} {:16.8f} {:16.8f}\\\n","     {:16.8f} {:16.8f} {:16.8f}\\n\".format(\n","                    sorted_frame[iat].symbol,\n","                    sorted_frame[iat].position[0],\n","                    sorted_frame[iat].position[1],\n","                    sorted_frame[iat].position[2],\n","                    sorted_forces[iat].position[0] * conv_frc,\n","                    sorted_forces[iat].position[1] * conv_frc,\n","                    sorted_forces[iat].position[2] * conv_frc,\n","                )\n","                for iat in range(len(frame))\n","            )\n","        )"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z2U5i3IZqLBI","outputId":"b1661718-320b-4aaf-b518-9786d05e2283","executionInfo":{"status":"ok","timestamp":1668711249526,"user_tz":-60,"elapsed":10,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([9.85, 9.85, 9.85])"]},"metadata":{},"execution_count":15}],"source":["def read_cell(f, data_dir):\n","    filename = os.path.join(data_dir, f)\n","    fo = open(filename, \"r\")\n","    cell_list_abc = fo.read().split(\"\\n\")[:-1]\n","    cell_vec_abc = np.array(\n","        [list(map(float, lv.split())) for lv in cell_list_abc]\n","    ).squeeze()\n","    return cell_vec_abc\n","\n","\n","cell_vec_abc = read_cell(\"celldata.dat\", data_dir + \"/AIMD_data\")\n","cell_vec_abc"]},{"cell_type":"markdown","metadata":{"id":"wUcVzREa7XNR"},"source":["### Read positions, energies, forces from of a 32 water molecules box in .xyz format generated with CP2K using the SCAN functional\n","### The Energy is in Hartree while the forces are in eV/angstrom, therefore we convert also the energy in eV to make it consistent with LAMMPS \"metal\" units."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"X9fcRMYnu5-V","executionInfo":{"status":"ok","timestamp":1668711260774,"user_tz":-60,"elapsed":11251,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[],"source":["wat_traj = read(data_dir + \"/AIMD_data/WATER-pos-10k-1.xyz\", index=\":\")\n","wat_frc = read(data_dir + \"/AIMD_data/WATER-frc-10k-1.xyz\", index=\":\")"]},{"cell_type":"markdown","metadata":{"id":"tFK_vOgL7ul8"},"source":["### The reader below is required to get the energies."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"jT7B4ryYu82t","executionInfo":{"status":"ok","timestamp":1668711263419,"user_tz":-60,"elapsed":2660,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[],"source":["natoms, positions, energies = MD_reader_xyz(\n","    \"WATER-pos-10k-1.xyz\", data_dir + \"/AIMD_data/\", no_skip=1\n",")"]},{"cell_type":"markdown","metadata":{"id":"BivsnMaz8NoG"},"source":["### The writer below is useful to convert 2 separate ase Atom objects of the positions and forces, and np.arrays of energies and cell, to an .extxyz file that can be read by nequip."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xgmmJAU5l5F2","outputId":"d974224d-eba4-4738-a9ba-8e958a8ccd60","executionInfo":{"status":"ok","timestamp":1668711293608,"user_tz":-60,"elapsed":30198,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n"]}],"source":["MD_writer_xyz(\n","    wat_traj,\n","    wat_frc,\n","    cell_vec_abc,\n","    energies,\n","    data_dir + \"/AIMD_data/\",\n","    \"wat_pos_frc-10k.extxyz\",\n","    conv_frc=1.0,\n","    conv_ener=27.211399,\n",")"]},{"cell_type":"markdown","metadata":{"id":"ZPqnt-SAXyvL"},"source":["### Turn on GPU\n","\n","Make sure Runtime --> Change runtime type is set to GPU"]},{"cell_type":"markdown","metadata":{"id":"-HDmxkn3z8_m"},"source":["## 3 Steps: \n","* Train: using a data set, train the neural network 🧠 \n","* Deploy: convert the Python-based model into a stand-alone potential file for fast execution ⚡\n","* Run: run Molecular Dynamics, Monte Carlo, Structural Minimization, ...  with it in LAMMPS 🏃"]},{"cell_type":"markdown","metadata":{"id":"6OD71eeDz7dA"},"source":["<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/all.png?raw=true\" width=\"60%\">"]},{"cell_type":"markdown","metadata":{"id":"62aEgq6QYFIn"},"source":["### Train a model"]},{"cell_type":"markdown","metadata":{"id":"ELdBzH_8z4_2"},"source":["<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/train.png?raw=true\" width=\"60%\">"]},{"cell_type":"markdown","metadata":{"id":"1KuOIippfVfd"},"source":["Here, we will train a NequIP potential on the following system\n","\n","* Water\n","* sampled at T=300K from AIMD\n","* Using 1000 training configurations\n","* The units of the reference data are in eV and A."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"mgoydrJW5lg0","outputId":"e6e70f43-2c10-4789-a70c-8d1531db8c34","executionInfo":{"status":"ok","timestamp":1668711293609,"user_tz":-60,"elapsed":24,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'0.5.5'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}],"source":["import nequip\n","\n","nequip.__version__"]},{"cell_type":"markdown","metadata":{"id":"ff4YA7aK1o9d"},"source":["### Below is the configuration file used by nequip. We provide the dataset in an .extxyz file."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fFCszShRk2RP","outputId":"c28bc18b-b8cb-43c0-e574-5f0e6aa681ae","executionInfo":{"status":"ok","timestamp":1668711293612,"user_tz":-60,"elapsed":11,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'root': 'results/water', 'run_name': 'example-run-water', 'seed': 123, 'dataset_seed': 456, 'append': True, 'default_dtype': 'float32', 'allow_tf32': False, 'r_max': 4.0, 'num_layers': 4, 'l_max': 1, 'parity': True, 'num_features': 32, 'nonlinearity_type': 'gate', 'resnet': False, 'nonlinearity_scalars': {'e': 'silu', 'o': 'tanh'}, 'nonlinearity_gates': {'e': 'silu', 'o': 'tanh'}, 'num_basis': 8, 'BesselBasis_trainable': True, 'PolynomialCutoff_p': 6, 'invariant_layers': 2, 'invariant_neurons': 64, 'avg_num_neighbors': 'auto', 'use_sc': True, 'include_keys': ['user_label'], 'key_mapping': {'user_label': 'label0'}, 'chemical_symbols': ['H', 'O'], 'dataset': 'ase', 'dataset_file_name': './nequip/datasets/AIMD_data/wat_pos_frc-10k.extxyz', 'ase_args': {'format': 'extxyz'}, 'verbose': 'info', 'log_batch_freq': 1, 'log_epoch_freq': 1, 'save_checkpoint_freq': -1, 'save_ema_checkpoint_freq': -1, 'n_train': 100, 'n_val': 50, 'learning_rate': 0.005, 'batch_size': 5, 'validation_batch_size': 10, 'max_epochs': 100000, 'train_val_split': 'random', 'shuffle': True, 'metrics_key': 'validation_loss', 'use_ema': True, 'ema_decay': 0.99, 'ema_use_num_updates': True, 'report_init_validation': True, 'early_stopping_patiences': {'validation_loss': 50}, 'early_stopping_delta': {'validation_loss': 0.005}, 'early_stopping_cumulative_delta': False, 'early_stopping_lower_bounds': {'LR': 1e-05}, 'early_stopping_upper_bounds': {'cumulative_wall': 1e+100}, 'loss_coeffs': {'forces': 1, 'total_energy': [1, 'PerAtomMSELoss']}, 'metrics_components': [['forces', 'mae'], ['forces', 'rmse'], ['forces', 'mae', {'PerSpecies': True, 'report_per_component': False}], ['forces', 'rmse', {'PerSpecies': True, 'report_per_component': False}], ['total_energy', 'mae'], ['total_energy', 'mae', {'PerAtom': True}]], 'optimizer_name': 'Adam', 'optimizer_amsgrad': False, 'optimizer_betas': (0.9, 0.999), 'optimizer_eps': 1e-08, 'optimizer_weight_decay': 0, 'max_gradient_norm': None, 'lr_scheduler_name': 'ReduceLROnPlateau', 'lr_scheduler_patience': 100, 'lr_scheduler_factor': 0.5, 'per_species_rescale_scales_trainable': False, 'per_species_rescale_shifts_trainable': False, 'per_species_rescale_shifts': 'dataset_per_atom_total_energy_mean', 'per_species_rescale_scales': 'dataset_forces_rms', 'global_rescale_shift': None, 'global_rescale_scale': 'dataset_forces_rms', 'global_rescale_shift_trainable': False, 'global_rescale_scale_trainable': False}"]},"metadata":{},"execution_count":20}],"source":["from nequip.utils import Config\n","\n","config = Config.from_file(\"/content/nequip/configs/water-example.yaml\")\n","config"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ukSnt_QD5avu","outputId":"a1c133a7-85ec-4e1b-b102-c09bc059c7f7","executionInfo":{"status":"ok","timestamp":1668712046780,"user_tz":-60,"elapsed":753175,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Torch device: cuda\n","Processing dataset...\n","Loaded data: Batch(atomic_numbers=[960000, 1], batch=[960000], cell=[10000, 3, 3], edge_cell_shift=[25556838, 3], edge_index=[2, 25556838], forces=[960000, 3], pbc=[10000, 3], pos=[960000, 3], ptr=[10001], total_energy=[10000, 1])\n","Cached processed data to disk\n","Done!\n","Successfully loaded the data set of type ASEDataset(10000)...\n","Replace string dataset_forces_rms to 0.7736424803733826\n","Replace string dataset_per_atom_total_energy_mean to -156.0919189453125\n","Atomic outputs are scaled by: [H, O: 0.773642], shifted by [H, O: -156.091919].\n","Replace string dataset_forces_rms to 0.7736424803733826\n","Initially outputs are globally scaled by: 0.7736424803733826, total_energy are globally shifted by None.\n","Successfully built the network...\n","Number of weights: 154200\n","! Starting training ...\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      0     1          1.1         1.09       0.0142        0.606        0.807        0.515        0.787        0.651        0.688            1        0.846         8.82       0.0919\n","      0     2         1.02         1.01       0.0135        0.577        0.776        0.491        0.749         0.62        0.662        0.965        0.813         8.61       0.0896\n","      0     3        0.992        0.979       0.0132        0.574        0.765        0.494        0.734        0.614        0.659        0.943        0.801         8.52       0.0888\n","      0     4        0.973         0.96       0.0134        0.564        0.758         0.48        0.733        0.606        0.653        0.933        0.793         8.58       0.0894\n","      0     5        0.969        0.956       0.0131        0.571        0.756        0.491        0.732        0.612         0.65        0.934        0.792          8.5       0.0885\n","\n","\n","  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Initial Validation          0    4.827    0.005        0.998       0.0135         1.01        0.578        0.773        0.494        0.747        0.621        0.663        0.956        0.809         8.61       0.0896\n","Wall time: 4.82777500799989\n","! Best model        0    1.011\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      1     1        0.886        0.873       0.0135         0.55        0.723        0.481        0.687        0.584        0.633        0.875        0.754          8.6       0.0896\n","      1     2         1.02         1.01       0.0064        0.576        0.778        0.498        0.733        0.616        0.664        0.968        0.816         5.93       0.0618\n","      1     3         1.05         1.05      0.00212        0.586        0.792        0.499        0.759        0.629        0.675        0.986         0.83         3.37       0.0351\n","      1     4         1.01         1.01     0.000403        0.573        0.777        0.492        0.735        0.613        0.672        0.954        0.813          1.4       0.0146\n","      1     5        0.962        0.962        5e-05        0.576        0.759        0.499         0.73        0.615        0.655        0.933        0.794        0.481      0.00501\n","      1     6        0.956        0.956      2.6e-05        0.573        0.756        0.491        0.737        0.614        0.647        0.938        0.792         0.33      0.00344\n","      1     7        0.904        0.904     6.71e-05        0.553        0.736        0.472        0.715        0.594        0.632        0.908         0.77        0.529      0.00552\n","      1     8        0.891        0.891     3.62e-05         0.53         0.73        0.457        0.677        0.567        0.626        0.903        0.764        0.413       0.0043\n","      1     9        0.826        0.826     0.000137        0.533        0.703        0.457        0.685        0.571        0.597        0.878        0.738        0.817      0.00851\n","      1    10        0.893        0.893     0.000659        0.558        0.731        0.474        0.726          0.6        0.627        0.903        0.765         1.89       0.0197\n","      1    11        0.769        0.767      0.00131        0.505        0.678        0.435        0.646        0.541        0.581        0.839         0.71         2.69        0.028\n","      1    12        0.654        0.651        0.003        0.474        0.624        0.403        0.615        0.509        0.526        0.785        0.655         4.05       0.0422\n","      1    13        0.596        0.591      0.00545        0.458        0.595        0.394        0.586         0.49         0.51        0.735        0.623         5.47        0.057\n","      1    14         0.59        0.581      0.00867        0.445         0.59        0.381        0.574        0.477          0.5        0.737        0.619          6.9       0.0719\n","      1    15        0.528        0.516       0.0123         0.41        0.556        0.343        0.544        0.444        0.463        0.705        0.584         8.23       0.0858\n","      1    16         0.46        0.445       0.0148        0.394        0.516        0.331         0.52        0.425        0.434         0.65        0.542         9.04       0.0941\n","      1    17        0.346        0.333       0.0131        0.345        0.446        0.303        0.428        0.365        0.388        0.545        0.466         8.49       0.0884\n","      1    18        0.335        0.325      0.00993         0.34        0.441        0.302        0.415        0.359        0.386        0.535         0.46         7.38       0.0769\n","      1    19        0.295        0.293      0.00247         0.33        0.419        0.287        0.416        0.351         0.36        0.516        0.438         3.67       0.0382\n","      1    20        0.273        0.272     0.000177        0.313        0.404        0.279        0.381         0.33        0.355        0.487        0.421        0.965       0.0101\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      1     1        0.298        0.298     0.000639        0.326        0.422        0.281        0.415        0.348        0.357        0.529        0.443         1.84       0.0191\n","      1     2        0.273        0.272     0.000526        0.312        0.404        0.272        0.391        0.332        0.348        0.497        0.422         1.65       0.0172\n","      1     3        0.253        0.253     0.000496        0.304        0.389        0.272        0.368         0.32        0.343        0.467        0.405         1.62       0.0169\n","      1     4        0.264        0.263     0.000611        0.311        0.397        0.274        0.385        0.329        0.344        0.487        0.415         1.79       0.0187\n","      1     5        0.265        0.264     0.000581        0.308        0.398        0.266        0.392        0.329        0.338        0.496        0.417         1.77       0.0184\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               1    9.580    0.005        0.707      0.00473        0.712        0.481        0.651        0.414        0.616        0.515        0.558        0.805        0.681         4.03        0.042\n","! Validation          1    9.580    0.005         0.27     0.000571        0.271        0.312        0.402        0.273         0.39        0.332        0.346        0.495        0.421         1.73       0.0181\n","Wall time: 9.580611645999852\n","! Best model        1    0.271\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      2     1         0.27        0.266      0.00439        0.312        0.399        0.277        0.382         0.33         0.35        0.482        0.416         4.87       0.0508\n","      2     2        0.263        0.254      0.00901        0.299         0.39        0.269        0.358        0.313        0.351        0.457        0.404         7.02       0.0731\n","      2     3        0.212          0.2       0.0126        0.265        0.346         0.23        0.335        0.283        0.297        0.426        0.362         8.31       0.0866\n","      2     4        0.207        0.199       0.0088        0.266        0.345        0.233        0.333        0.283        0.298        0.424        0.361         6.94       0.0723\n","      2     5        0.204        0.202      0.00223        0.268        0.347        0.232        0.338        0.285        0.299        0.428        0.364         3.49       0.0364\n","      2     6        0.161        0.161     0.000149        0.239         0.31         0.21        0.299        0.254        0.269        0.379        0.324        0.696      0.00725\n","      2     7        0.167        0.166     0.000341        0.246        0.315        0.213        0.312        0.263        0.272        0.389         0.33         1.21       0.0126\n","      2     8        0.152        0.152     0.000398         0.23        0.301        0.197        0.295        0.246        0.253         0.38        0.316         1.25        0.013\n","      2     9        0.174        0.173     0.000924        0.246        0.322        0.211        0.317        0.264        0.272        0.404        0.338         2.24       0.0233\n","      2    10        0.154        0.154     0.000707        0.236        0.303        0.204          0.3        0.252         0.26        0.375        0.317         1.93       0.0201\n","      2    11        0.146        0.145     0.000939        0.231        0.295        0.204        0.284        0.244        0.256         0.36        0.308         2.23       0.0232\n","      2    12        0.163        0.162     0.000592        0.246        0.312        0.215        0.309        0.262        0.271         0.38        0.325         1.78       0.0186\n","      2    13        0.142        0.141     0.000747        0.226         0.29        0.195        0.289        0.242        0.245        0.364        0.305         1.99       0.0207\n","      2    14         0.13        0.128      0.00154        0.214        0.277        0.184        0.274        0.229        0.233        0.349        0.291         2.88         0.03\n","      2    15        0.138        0.136      0.00201         0.22        0.286        0.194        0.273        0.233        0.247        0.351        0.299          3.3       0.0344\n","      2    16        0.147        0.146      0.00121        0.224        0.295        0.189        0.292        0.241        0.242         0.38        0.311         2.54       0.0264\n","      2    17        0.137        0.136     0.000425        0.222        0.286        0.194        0.279        0.236        0.244        0.355        0.299         1.48       0.0155\n","      2    18        0.137        0.137     3.24e-05         0.22        0.286         0.19        0.278        0.234        0.244        0.356          0.3        0.353      0.00367\n","      2    19        0.122        0.122     4.68e-05        0.208         0.27        0.182        0.262        0.222        0.233        0.332        0.282        0.402      0.00419\n","      2    20        0.127        0.126     0.000242        0.211        0.275        0.179        0.274        0.227        0.231        0.347        0.289         1.14       0.0119\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      2     1        0.128        0.128     4.02e-05        0.211        0.277        0.181        0.272        0.226        0.229        0.353        0.291        0.433      0.00451\n","      2     2        0.132        0.132      3.8e-05        0.215        0.281        0.186        0.275         0.23        0.239         0.35        0.294         0.42      0.00437\n","      2     3        0.114        0.114     2.33e-05        0.203        0.261        0.178        0.253        0.216        0.226        0.319        0.272         0.29      0.00302\n","      2     4         0.12         0.12     4.85e-05        0.205        0.268        0.179        0.257        0.218        0.229        0.333        0.281        0.392      0.00408\n","      2     5        0.128        0.127     3.44e-05        0.211        0.276        0.179        0.274        0.227        0.229        0.353        0.291        0.366      0.00381\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               2   13.489    0.005        0.165      0.00237        0.168        0.241        0.314         0.21        0.304        0.257        0.271        0.388        0.329          2.8       0.0292\n","! Validation          2   13.489    0.005        0.124     3.69e-05        0.124        0.209        0.273        0.181        0.266        0.223         0.23        0.342        0.286         0.38      0.00396\n","Wall time: 13.489929331999974\n","! Best model        2    0.124\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      3     1        0.123        0.122      0.00115        0.205        0.271        0.169        0.278        0.223        0.221        0.349        0.285         2.47       0.0258\n","      3     2        0.126        0.124      0.00129        0.209        0.273        0.181        0.265        0.223        0.231        0.341        0.286         2.63       0.0274\n","      3     3        0.128        0.127     0.000888        0.212        0.275        0.186        0.265        0.225        0.239        0.336        0.288         2.16       0.0225\n","      3     4        0.104        0.104      0.00017        0.192        0.249        0.165        0.245        0.205        0.212        0.311        0.261        0.896      0.00933\n","      3     5        0.122        0.122     0.000148        0.204         0.27        0.176        0.261        0.218        0.233        0.332        0.283        0.863      0.00898\n","      3     6       0.0961       0.0959     0.000121        0.184         0.24        0.157        0.238        0.198        0.199        0.305        0.252        0.734      0.00764\n","      3     7        0.104        0.103      0.00029        0.193        0.249        0.166        0.246        0.206        0.211        0.311        0.261         1.15        0.012\n","      3     8        0.114        0.114     0.000186        0.199        0.261        0.171        0.256        0.214        0.221        0.325        0.273        0.973       0.0101\n","      3     9        0.109        0.109     4.67e-05        0.196        0.255        0.169        0.252         0.21         0.22        0.314        0.267        0.334      0.00348\n","      3    10       0.0941       0.0939     0.000195        0.184        0.237        0.162        0.228        0.195        0.205         0.29        0.248         1.01       0.0105\n","      3    11        0.106        0.106     0.000142        0.189        0.252        0.164        0.239        0.201        0.213        0.314        0.264        0.786      0.00818\n","      3    12       0.0905       0.0905     1.81e-05        0.181        0.233        0.157        0.228        0.193        0.199        0.289        0.244        0.252      0.00262\n","      3    13        0.102        0.102     4.52e-05        0.193        0.247        0.169         0.24        0.204        0.214        0.302        0.258        0.485      0.00505\n","      3    14       0.0898       0.0896     0.000191         0.18        0.232         0.16        0.221        0.191        0.205        0.277        0.241        0.887      0.00924\n","      3    15       0.0931        0.093     3.22e-05        0.185        0.236        0.159        0.236        0.197        0.201        0.294        0.247        0.389      0.00405\n","      3    16       0.0853       0.0851     0.000143        0.174        0.226        0.146         0.23        0.188        0.188        0.287        0.237        0.841      0.00876\n","      3    17       0.0794       0.0791     0.000293        0.169        0.218        0.149         0.21        0.179        0.194        0.259        0.226         1.21       0.0126\n","      3    18       0.0847       0.0844     0.000328        0.174        0.225        0.149        0.226        0.187        0.191         0.28        0.236         1.25       0.0131\n","      3    19        0.098        0.098     3.49e-05        0.187        0.242        0.161        0.238          0.2        0.206        0.302        0.254         0.38      0.00396\n","      3    20       0.0859       0.0858     8.83e-05        0.175        0.227         0.15        0.226        0.188        0.193        0.282        0.238        0.653       0.0068\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      3     1       0.0955       0.0955     2.49e-05        0.182        0.239        0.155        0.234        0.195        0.199        0.304        0.251        0.297      0.00309\n","      3     2       0.0993       0.0992     3.77e-05        0.186        0.244        0.161        0.238        0.199         0.21          0.3        0.255        0.396      0.00412\n","      3     3       0.0819       0.0819     2.86e-05        0.171        0.221        0.151        0.212        0.182        0.194        0.268        0.231        0.308       0.0032\n","      3     4       0.0904       0.0904     1.67e-05        0.177        0.233        0.154        0.224        0.189        0.199        0.289        0.244        0.283      0.00295\n","      3     5       0.0939       0.0939     1.97e-05        0.181        0.237        0.154        0.235        0.195        0.198        0.301        0.249        0.247      0.00258\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               3   17.396    0.005        0.101      0.00029        0.102        0.189        0.246        0.163        0.241        0.202         0.21        0.306        0.258         1.02       0.0106\n","! Validation          3   17.396    0.005       0.0922     2.55e-05       0.0922         0.18        0.235        0.155        0.228        0.192          0.2        0.292        0.246        0.306      0.00319\n","Wall time: 17.396972486999857\n","! Best model        3    0.092\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      4     1       0.0771        0.077     4.66e-05        0.165        0.215        0.142        0.211        0.177        0.183        0.268        0.225        0.425      0.00443\n","      4     2       0.0834       0.0834     5.56e-05        0.175        0.223         0.15        0.227        0.188        0.187        0.282        0.235        0.491      0.00512\n","      4     3       0.0764       0.0763     8.12e-05        0.169        0.214        0.146        0.215        0.181        0.183        0.265        0.224        0.609      0.00634\n","      4     4       0.0777       0.0775      0.00013        0.166        0.215        0.148        0.202        0.175        0.195        0.252        0.223        0.795      0.00828\n","      4     5       0.0776       0.0775     9.66e-05        0.166        0.215        0.144        0.208        0.176        0.184        0.267        0.226         0.69      0.00719\n","      4     6       0.0768       0.0762     0.000654        0.163        0.214        0.142        0.205        0.174        0.183        0.264        0.223         1.89       0.0196\n","      4     7       0.0787       0.0782     0.000551        0.166        0.216        0.144        0.211        0.177        0.185        0.268        0.226         1.72       0.0179\n","      4     8       0.0756       0.0756     3.22e-05        0.165        0.213        0.143        0.209        0.176        0.183        0.262        0.223        0.361      0.00376\n","      4     9        0.074       0.0738     0.000186        0.161         0.21        0.138        0.207        0.172        0.177        0.265        0.221        0.937      0.00976\n","      4    10       0.0778       0.0773     0.000587        0.166        0.215        0.142        0.215        0.178        0.185        0.265        0.225         1.77       0.0184\n","      4    11       0.0726       0.0725     7.93e-05        0.159        0.208        0.135        0.207        0.171        0.175        0.263        0.219        0.619      0.00645\n","      4    12       0.0609       0.0609     6.43e-05        0.148        0.191        0.128        0.189        0.158        0.164        0.235          0.2        0.546      0.00569\n","      4    13       0.0593       0.0592     6.21e-05        0.146        0.188        0.127        0.185        0.156        0.162        0.231        0.197         0.56      0.00584\n","      4    14       0.0751       0.0751     9.72e-06        0.163        0.212        0.141        0.207        0.174        0.182        0.262        0.222        0.196      0.00204\n","      4    15       0.0636       0.0634     0.000251        0.152        0.195        0.131        0.195        0.163        0.168         0.24        0.204         1.16       0.0121\n","      4    16        0.078       0.0776     0.000349        0.163        0.216        0.142        0.206        0.174        0.185        0.267        0.226         1.32       0.0138\n","      4    17        0.053        0.053     5.15e-05        0.137        0.178         0.12         0.17        0.145        0.158        0.213        0.185        0.433      0.00451\n","      4    18        0.065       0.0648     0.000272        0.151        0.197         0.13        0.195        0.162        0.168        0.244        0.206         1.17       0.0122\n","      4    19       0.0543       0.0542     0.000137         0.14         0.18        0.124        0.172        0.148        0.158        0.218        0.188        0.864        0.009\n","      4    20       0.0703       0.0702     9.19e-05         0.16        0.205         0.14        0.201         0.17         0.18        0.248        0.214        0.628      0.00655\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      4     1       0.0703       0.0703     1.68e-05        0.156        0.205        0.135          0.2        0.167        0.174        0.257        0.215        0.258      0.00268\n","      4     2       0.0727       0.0727     2.65e-05        0.159        0.209         0.14        0.199        0.169        0.184        0.251        0.217        0.314      0.00327\n","      4     3       0.0591       0.0591     1.29e-05        0.146        0.188         0.13        0.178        0.154        0.166        0.225        0.196        0.227      0.00237\n","      4     4       0.0644       0.0644     1.17e-05        0.151        0.196        0.131        0.191        0.161        0.168        0.243        0.206        0.205      0.00214\n","      4     5       0.0637       0.0637     1.16e-05        0.149        0.195        0.127        0.192         0.16        0.167        0.243        0.205        0.193      0.00201\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               4   21.323    0.005       0.0712     0.000189       0.0714        0.159        0.206        0.138        0.202         0.17        0.178        0.254        0.216        0.859      0.00895\n","! Validation          4   21.323    0.005        0.066     1.59e-05        0.066        0.152        0.199        0.132        0.192        0.162        0.172        0.244        0.208        0.239      0.00249\n","Wall time: 21.32396722999988\n","! Best model        4    0.066\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      5     1        0.066       0.0657     0.000242        0.151        0.198        0.133        0.186        0.159        0.174         0.24        0.207          1.1       0.0114\n","      5     2        0.068        0.068     5.87e-05        0.155        0.202        0.136        0.193        0.164        0.175        0.246        0.211         0.52      0.00542\n","      5     3       0.0584       0.0581     0.000315        0.142        0.186        0.122         0.18        0.151        0.161         0.23        0.195         1.24       0.0129\n","      5     4        0.055       0.0545     0.000554        0.139        0.181         0.12        0.177        0.148        0.154        0.224        0.189         1.73        0.018\n","      5     5       0.0494       0.0493      0.00014        0.133        0.172        0.117        0.166        0.141        0.149        0.209        0.179        0.837      0.00871\n","      5     6       0.0516       0.0515     8.56e-05        0.136        0.176        0.118        0.173        0.146        0.153        0.214        0.183        0.619      0.00645\n","      5     7       0.0525       0.0522     0.000236        0.138        0.177        0.119        0.178        0.148        0.149        0.222        0.186         1.11       0.0116\n","      5     8       0.0561       0.0557     0.000401        0.142        0.183        0.125        0.175         0.15        0.161         0.22         0.19         1.47       0.0154\n","      5     9       0.0497       0.0496     1.22e-05        0.133        0.172        0.116        0.166        0.141        0.152        0.208         0.18        0.225      0.00235\n","      5    10       0.0543       0.0541      0.00018         0.14         0.18        0.124        0.171        0.148        0.159        0.216        0.187        0.972       0.0101\n","      5    11       0.0636       0.0634     0.000142         0.15        0.195         0.13        0.189        0.159        0.169        0.239        0.204        0.873       0.0091\n","      5    12       0.0526       0.0526     8.18e-06        0.138        0.177        0.122        0.171        0.146        0.157        0.213        0.185         0.16      0.00166\n","      5    13       0.0531       0.0529     0.000262        0.137        0.178        0.122        0.167        0.145        0.158        0.212        0.185         1.18       0.0122\n","      5    14       0.0441       0.0438     0.000332        0.127        0.162        0.112        0.158        0.135        0.142        0.196        0.169         1.35       0.0141\n","      5    15       0.0456       0.0454     0.000246        0.127        0.165        0.111         0.16        0.136        0.142        0.203        0.172         1.16       0.0121\n","      5    16       0.0502       0.0502     7.74e-06        0.135        0.173        0.123        0.158        0.141         0.16        0.198        0.179        0.159      0.00166\n","      5    17       0.0464       0.0457     0.000677        0.126        0.165        0.108        0.162        0.135        0.137        0.211        0.174         1.92         0.02\n","      5    18       0.0435       0.0423      0.00118        0.122        0.159        0.108        0.149        0.129        0.142        0.188        0.165         2.55       0.0265\n","      5    19       0.0496       0.0493     0.000295        0.134        0.172        0.117        0.169        0.143        0.151        0.208        0.179         1.26       0.0131\n","      5    20       0.0523       0.0523     1.45e-05        0.135        0.177        0.121        0.163        0.142        0.157        0.211        0.184        0.277      0.00288\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      5     1       0.0568       0.0568     2.65e-05        0.141        0.184        0.123        0.177         0.15        0.159        0.227        0.193        0.315      0.00328\n","      5     2        0.058       0.0579     3.86e-05        0.143        0.186        0.127        0.175        0.151        0.167         0.22        0.193        0.408      0.00425\n","      5     3       0.0471       0.0471     5.02e-05         0.13        0.168        0.117        0.156        0.136        0.151        0.198        0.174        0.469      0.00488\n","      5     4        0.051        0.051     3.31e-05        0.135        0.175        0.117         0.17        0.144        0.151        0.214        0.183        0.369      0.00385\n","      5     5       0.0494       0.0494     4.74e-05        0.132        0.172        0.114        0.167        0.141        0.149         0.21         0.18        0.459      0.00479\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               5   25.240    0.005       0.0528     0.000269       0.0531        0.137        0.178         0.12        0.171        0.145        0.155        0.216        0.186         1.04       0.0108\n","! Validation          5   25.240    0.005       0.0524     3.92e-05       0.0525        0.136        0.177        0.119        0.169        0.144        0.155        0.214        0.185        0.404      0.00421\n","Wall time: 25.240811112999836\n","! Best model        5    0.052\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      6     1       0.0443       0.0442     0.000107        0.125        0.163        0.108         0.16        0.134         0.14        0.201         0.17        0.749       0.0078\n","      6     2       0.0422        0.042     0.000277        0.124        0.158        0.109        0.152        0.131        0.138        0.193        0.165         1.24       0.0129\n","      6     3       0.0457       0.0455     0.000197        0.128        0.165        0.115        0.155        0.135        0.149        0.192        0.171         1.02       0.0107\n","      6     4       0.0522       0.0522     8.31e-06        0.136        0.177        0.121        0.164        0.143        0.157        0.211        0.184         0.21      0.00219\n","      6     5       0.0449       0.0448     7.62e-05        0.127        0.164        0.113        0.156        0.135        0.144        0.197        0.171        0.577      0.00601\n","      6     6       0.0492       0.0492     9.74e-06         0.13        0.172        0.115         0.16        0.137        0.153        0.203        0.178        0.212      0.00221\n","      6     7       0.0431       0.0431     1.13e-05        0.126        0.161        0.113        0.151        0.132        0.143        0.191        0.167        0.225      0.00234\n","      6     8       0.0443        0.044     0.000307        0.125        0.162        0.108        0.157        0.133        0.139          0.2         0.17         1.27       0.0133\n","      6     9       0.0378       0.0377      6.5e-05        0.118         0.15        0.103        0.148        0.126        0.132        0.181        0.157        0.596      0.00621\n","      6    10       0.0475       0.0474     1.05e-05        0.129        0.169        0.112        0.162        0.137        0.147        0.205        0.176        0.184      0.00191\n","      6    11       0.0531        0.053     5.87e-05        0.136        0.178        0.119        0.169        0.144        0.157        0.215        0.186        0.452      0.00471\n","      6    12       0.0462       0.0458     0.000342        0.129        0.166        0.115        0.157        0.136        0.146        0.199        0.173         1.36       0.0141\n","      6    13       0.0472       0.0468     0.000329         0.13        0.167        0.115        0.158        0.137        0.149          0.2        0.174         1.29       0.0134\n","      6    14        0.045        0.045     2.38e-05        0.128        0.164        0.114        0.156        0.135        0.147        0.193         0.17        0.322      0.00336\n","      6    15       0.0397       0.0397     9.63e-06        0.118        0.154        0.104        0.146        0.125        0.133        0.189        0.161        0.192        0.002\n","      6    16       0.0443       0.0442     7.95e-05        0.126        0.163        0.111        0.156        0.134        0.142        0.197         0.17        0.651      0.00678\n","      6    17       0.0428       0.0425     0.000283        0.122         0.16        0.112        0.144        0.128        0.147        0.182        0.164         1.24       0.0129\n","      6    18       0.0357       0.0357     1.29e-05        0.115        0.146        0.101        0.141        0.121        0.129        0.175        0.152         0.23       0.0024\n","      6    19       0.0489       0.0487     0.000193        0.131        0.171        0.116        0.161        0.139        0.151        0.205        0.178         1.01       0.0106\n","      6    20       0.0429       0.0429     1.12e-05        0.125         0.16        0.111        0.153        0.132        0.143         0.19        0.167        0.215      0.00224\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      6     1       0.0488       0.0488     8.03e-06        0.131        0.171        0.115        0.163        0.139        0.149        0.208        0.179        0.175      0.00182\n","      6     2        0.049        0.049      1.6e-05        0.132        0.171        0.118         0.16        0.139        0.154        0.202        0.178        0.246      0.00256\n","      6     3       0.0404       0.0404     1.72e-05        0.121        0.156        0.109        0.144        0.127         0.14        0.182        0.161        0.265      0.00276\n","      6     4       0.0433       0.0433     1.15e-05        0.125        0.161        0.108        0.157        0.133         0.14        0.197        0.168        0.189      0.00197\n","      6     5        0.042        0.042     1.92e-05        0.121        0.158        0.106        0.152        0.129        0.139        0.191        0.165        0.269       0.0028\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               6   29.154    0.005       0.0447     0.000121       0.0449        0.126        0.164        0.112        0.155        0.134        0.145        0.196         0.17        0.662       0.0069\n","! Validation          6   29.154    0.005       0.0447     1.44e-05       0.0447        0.126        0.164        0.111        0.155        0.133        0.144        0.196         0.17        0.229      0.00238\n","Wall time: 29.155051469\n","! Best model        6    0.045\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      7     1       0.0443       0.0442     0.000101        0.125        0.163        0.112        0.152        0.132        0.146        0.192        0.169        0.731      0.00762\n","      7     2       0.0428       0.0428     5.51e-05        0.123         0.16        0.107        0.156        0.131        0.139        0.196        0.167        0.527      0.00549\n","      7     3       0.0455       0.0453     0.000155        0.127        0.165         0.11        0.161        0.135        0.141        0.204        0.172        0.922       0.0096\n","      7     4       0.0437       0.0437     5.97e-05        0.125        0.162        0.115        0.146         0.13        0.147        0.187        0.167        0.522      0.00544\n","      7     5       0.0417       0.0414     0.000279         0.12        0.157        0.108        0.145        0.127        0.143        0.183        0.163         1.19       0.0124\n","      7     6       0.0383       0.0377     0.000555        0.118         0.15        0.105        0.142        0.124        0.136        0.175        0.156         1.75       0.0182\n","      7     7       0.0407       0.0406     2.55e-05        0.121        0.156        0.109        0.145        0.127        0.138        0.186        0.162         0.34      0.00354\n","      7     8       0.0393       0.0393     5.55e-06        0.118        0.153        0.105        0.143        0.124        0.137        0.182         0.16        0.132      0.00138\n","      7     9       0.0413       0.0413     1.73e-05         0.12        0.157        0.109        0.143        0.126        0.139        0.188        0.164        0.264      0.00275\n","      7    10       0.0417       0.0415     0.000165        0.122        0.158        0.106        0.154         0.13        0.135        0.195        0.165        0.926      0.00965\n","      7    11       0.0487       0.0484     0.000286        0.131         0.17         0.12        0.153        0.137        0.159        0.191        0.175         1.25        0.013\n","      7    12       0.0407       0.0407     2.26e-05         0.12        0.156        0.107        0.147        0.127         0.14        0.185        0.162        0.316      0.00329\n","      7    13       0.0411       0.0408     0.000251         0.12        0.156        0.109        0.143        0.126        0.142        0.182        0.162         1.17       0.0122\n","      7    14       0.0446       0.0446     6.89e-06        0.127        0.163        0.114        0.154        0.134        0.146        0.193         0.17        0.159      0.00165\n","      7    15       0.0349       0.0349     5.35e-05        0.112        0.144       0.0978         0.14        0.119        0.125        0.177        0.151        0.528       0.0055\n","      7    16       0.0436       0.0436     7.98e-06        0.124        0.162        0.115        0.143        0.129        0.151         0.18        0.166        0.167      0.00174\n","      7    17       0.0332       0.0331     4.75e-05        0.109        0.141       0.0959        0.135        0.115        0.122        0.172        0.147        0.471      0.00491\n","      7    18       0.0402       0.0402     6.03e-05        0.121        0.155         0.11        0.142        0.126         0.14        0.181        0.161        0.564      0.00587\n","      7    19       0.0379       0.0378     0.000113        0.116         0.15        0.101        0.145        0.123        0.131        0.183        0.157        0.776      0.00809\n","      7    20       0.0397       0.0395     0.000182        0.118        0.154        0.104        0.146        0.125        0.134        0.187         0.16         0.97       0.0101\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      7     1       0.0437       0.0437     5.73e-06        0.124        0.162        0.109        0.155        0.132        0.141        0.197        0.169        0.154      0.00161\n","      7     2        0.043        0.043     1.14e-05        0.124         0.16         0.11        0.151        0.131        0.143         0.19        0.167        0.199      0.00207\n","      7     3       0.0362       0.0362     1.29e-05        0.114        0.147        0.103        0.138         0.12        0.132        0.174        0.153        0.232      0.00242\n","      7     4       0.0385       0.0385      8.2e-06        0.117        0.152        0.102        0.148        0.125        0.132        0.185        0.159        0.156      0.00162\n","      7     5       0.0376       0.0376     1.47e-05        0.115         0.15        0.101        0.143        0.122        0.132        0.181        0.156        0.234      0.00244\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               7   33.086    0.005       0.0411     0.000122       0.0412        0.121        0.157        0.108        0.147        0.127         0.14        0.186        0.163        0.683      0.00712\n","! Validation          7   33.086    0.005       0.0398     1.06e-05       0.0398        0.119        0.154        0.105        0.147        0.126        0.136        0.185        0.161        0.195      0.00203\n","Wall time: 33.087012899\n","! Best model        7    0.040\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      8     1       0.0302       0.0302     4.91e-07        0.104        0.134       0.0922        0.127         0.11        0.119        0.162         0.14       0.0447     0.000466\n","      8     2       0.0361       0.0361     4.15e-05        0.112        0.147        0.101        0.134        0.117        0.133        0.172        0.152        0.462      0.00481\n","      8     3       0.0392       0.0391     5.96e-05        0.119        0.153        0.108        0.141        0.125        0.141        0.175        0.158        0.502      0.00523\n","      8     4       0.0396       0.0395     5.87e-05         0.12        0.154        0.106        0.147        0.127        0.137        0.183         0.16        0.548       0.0057\n","      8     5       0.0413       0.0413     5.74e-05         0.12        0.157        0.105         0.15        0.128        0.137        0.191        0.164        0.498      0.00519\n","      8     6       0.0398       0.0397     4.95e-05        0.117        0.154        0.105        0.142        0.123        0.136        0.185        0.161        0.459      0.00479\n","      8     7       0.0334       0.0333     0.000187         0.11        0.141        0.095        0.139        0.117        0.122        0.174        0.148            1       0.0105\n","      8     8        0.039       0.0389     5.89e-05        0.116        0.153        0.103        0.142        0.122        0.134        0.184        0.159        0.525      0.00547\n","      8     9       0.0291        0.029     7.11e-05        0.103        0.132       0.0926        0.125        0.109        0.117        0.158        0.137        0.623      0.00649\n","      8    10       0.0377       0.0376     8.19e-06        0.116         0.15        0.104        0.141        0.123        0.134        0.178        0.156        0.122      0.00127\n","      8    11       0.0377       0.0375     0.000189        0.116         0.15        0.103        0.144        0.123        0.131        0.181        0.156        0.999       0.0104\n","      8    12       0.0327       0.0324      0.00035        0.108        0.139       0.0938        0.135        0.114         0.12        0.171        0.146         1.38       0.0144\n","      8    13       0.0296       0.0295     0.000109        0.104        0.133       0.0933        0.124        0.109        0.122        0.153        0.137        0.755      0.00786\n","      8    14       0.0333       0.0333     1.16e-05        0.109        0.141       0.0955        0.135        0.115        0.124         0.17        0.147         0.23       0.0024\n","      8    15       0.0361        0.036     0.000134        0.112        0.147       0.0994        0.136        0.118        0.132        0.173        0.152         0.85      0.00886\n","      8    16       0.0339       0.0336      0.00029         0.11        0.142       0.0984        0.134        0.116        0.128        0.166        0.147         1.26       0.0131\n","      8    17       0.0305       0.0305     4.11e-05        0.106        0.135        0.092        0.133        0.113        0.117        0.166        0.141        0.412       0.0043\n","      8    18       0.0375       0.0374     0.000108        0.115         0.15       0.0992        0.146        0.122        0.129        0.184        0.156        0.766      0.00798\n","      8    19       0.0355       0.0354     9.54e-05        0.112        0.146       0.0989        0.139        0.119        0.129        0.175        0.152        0.707      0.00736\n","      8    20       0.0295       0.0295      4.8e-06        0.104        0.133       0.0906        0.131        0.111        0.115        0.162        0.139         0.13      0.00135\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      8     1       0.0394       0.0394     4.64e-06        0.118        0.154        0.103        0.147        0.125        0.134        0.186         0.16        0.135      0.00141\n","      8     2       0.0382       0.0382     8.06e-06        0.117        0.151        0.104        0.144        0.124        0.135         0.18        0.157        0.162      0.00169\n","      8     3       0.0328       0.0327     9.33e-06        0.109         0.14       0.0975        0.132        0.115        0.125        0.166        0.146        0.201       0.0021\n","      8     4       0.0347       0.0347     5.59e-06        0.111        0.144       0.0969         0.14        0.118        0.126        0.175         0.15        0.116      0.00121\n","      8     5        0.034        0.034     1.02e-05         0.11        0.143       0.0963        0.136        0.116        0.126        0.171        0.149        0.187      0.00194\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               8   37.017    0.005        0.035     9.63e-05       0.0351        0.112        0.145       0.0988        0.137        0.118        0.128        0.173        0.151        0.614      0.00639\n","! Validation          8   37.017    0.005       0.0358     7.56e-06       0.0358        0.113        0.146       0.0997         0.14         0.12        0.129        0.176        0.153         0.16      0.00167\n","Wall time: 37.01820312099994\n","! Best model        8    0.036\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      9     1       0.0371        0.037     0.000153        0.113        0.149          0.1        0.139         0.12        0.131        0.179        0.155        0.886      0.00922\n","      9     2       0.0337       0.0332     0.000567        0.109        0.141       0.0963        0.134        0.115        0.123        0.171        0.147         1.76       0.0183\n","      9     3        0.031       0.0309     0.000145        0.106        0.136        0.095        0.129        0.112        0.121        0.162        0.141        0.866      0.00903\n","      9     4        0.034       0.0338     0.000211        0.109        0.142          0.1        0.127        0.114        0.132        0.161        0.146         1.07       0.0112\n","      9     5       0.0257       0.0253     0.000391       0.0966        0.123       0.0859        0.118        0.102        0.111        0.145        0.128         1.46       0.0152\n","      9     6       0.0362       0.0361     0.000175        0.114        0.147        0.103        0.138         0.12        0.132        0.173        0.152        0.956      0.00996\n","      9     7       0.0319       0.0318     9.91e-05        0.107        0.138       0.0944        0.132        0.113        0.122        0.166        0.144        0.732      0.00763\n","      9     8       0.0346       0.0346     4.58e-06        0.112        0.144       0.0976        0.141        0.119        0.126        0.175         0.15        0.132      0.00138\n","      9     9       0.0312        0.031     0.000217        0.104        0.136       0.0928        0.128         0.11        0.121        0.162        0.142         1.08       0.0112\n","      9    10       0.0322       0.0322     1.42e-05        0.108        0.139       0.0937        0.136        0.115        0.119        0.171        0.145        0.241      0.00251\n","      9    11        0.033        0.033     1.38e-05        0.109        0.141        0.097        0.133        0.115        0.123         0.17        0.147        0.253      0.00263\n","      9    12       0.0354       0.0353     4.77e-05        0.113        0.145          0.1        0.138        0.119        0.129        0.174        0.151        0.503      0.00524\n","      9    13       0.0302       0.0299     0.000234        0.103        0.134       0.0908        0.126        0.109        0.117        0.163         0.14         1.12       0.0116\n","      9    14       0.0348       0.0348     2.58e-05        0.113        0.144        0.104        0.131        0.118        0.132        0.166        0.149        0.351      0.00365\n","      9    15       0.0305       0.0305     6.25e-06        0.104        0.135       0.0932        0.127         0.11        0.119        0.162        0.141        0.145      0.00151\n","      9    16       0.0369       0.0369     4.72e-05        0.114        0.149       0.0991        0.143        0.121        0.129        0.181        0.155        0.502      0.00523\n","      9    17       0.0306       0.0302     0.000393        0.105        0.134       0.0943        0.125         0.11        0.121        0.157        0.139         1.47       0.0153\n","      9    18       0.0338       0.0336     0.000177        0.109        0.142       0.0964        0.135        0.116        0.125        0.171        0.148        0.979       0.0102\n","      9    19       0.0325       0.0325      9.9e-06        0.106        0.139       0.0961        0.126        0.111        0.128         0.16        0.144        0.181      0.00189\n","      9    20       0.0244       0.0244     5.28e-05       0.0944        0.121       0.0814        0.121        0.101        0.103         0.15        0.127        0.531      0.00553\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","      9     1       0.0357       0.0357     5.26e-06        0.112        0.146       0.0984         0.14        0.119        0.128        0.178        0.153        0.139      0.00145\n","      9     2       0.0344       0.0344     6.35e-06        0.111        0.143       0.0985        0.137        0.118        0.127        0.171        0.149        0.136      0.00142\n","      9     3       0.0296       0.0296     7.22e-06        0.104        0.133       0.0925        0.126        0.109        0.118        0.159        0.139        0.171      0.00178\n","      9     4       0.0313       0.0313      4.6e-06        0.105        0.137       0.0922        0.132        0.112         0.12        0.166        0.143        0.115      0.00119\n","      9     5       0.0308       0.0308     6.52e-06        0.105        0.136       0.0916        0.131        0.111         0.12        0.164        0.142        0.147      0.00154\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train               9   40.944    0.005       0.0323     0.000149       0.0325        0.107        0.139       0.0956        0.131        0.113        0.123        0.166        0.145        0.761      0.00793\n","! Validation          9   40.944    0.005       0.0324     5.99e-06       0.0324        0.107        0.139       0.0946        0.133        0.114        0.123        0.168        0.145        0.142      0.00148\n","Wall time: 40.94467349599995\n","! Best model        9    0.032\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     10     1       0.0302       0.0302     1.31e-05        0.104        0.134       0.0914        0.128         0.11        0.121        0.158        0.139        0.239      0.00249\n","     10     2       0.0306       0.0305     2.54e-05        0.105        0.135       0.0935        0.128        0.111        0.121         0.16         0.14        0.333      0.00347\n","     10     3       0.0313       0.0313     1.33e-05        0.104        0.137       0.0909        0.129         0.11         0.12        0.166        0.143        0.216      0.00225\n","     10     4       0.0314       0.0314     9.08e-06        0.105        0.137       0.0924        0.131        0.112        0.121        0.165        0.143        0.192        0.002\n","     10     5        0.029       0.0289     3.62e-05        0.104        0.132       0.0911        0.129         0.11        0.116        0.158        0.137        0.412      0.00429\n","     10     6       0.0286       0.0285     9.95e-05        0.101        0.131       0.0918        0.119        0.105        0.119        0.152        0.135        0.737      0.00767\n","     10     7       0.0252       0.0252     4.43e-06       0.0951        0.123       0.0853        0.115          0.1        0.109        0.146        0.128        0.142      0.00148\n","     10     8       0.0291       0.0291     2.75e-05        0.101        0.132       0.0849        0.133        0.109        0.108         0.17        0.139        0.352      0.00366\n","     10     9       0.0265       0.0264     5.68e-05       0.0961        0.126       0.0839         0.12        0.102        0.109        0.154        0.131        0.516      0.00538\n","     10    10         0.03       0.0299     7.55e-05        0.102        0.134       0.0901        0.127        0.108        0.118         0.16        0.139        0.626      0.00652\n","     10    11       0.0288       0.0288     1.04e-05       0.0998        0.131       0.0868        0.126        0.106        0.114         0.16        0.137        0.229      0.00238\n","     10    12       0.0254       0.0253     2.65e-05       0.0951        0.123        0.083        0.119        0.101        0.106        0.151        0.129        0.346       0.0036\n","     10    13       0.0268       0.0268     2.61e-05       0.0985        0.127       0.0874        0.121        0.104        0.112        0.152        0.132        0.368      0.00383\n","     10    14        0.029        0.029     7.45e-06        0.102        0.132       0.0883        0.129        0.109        0.113        0.162        0.138        0.158      0.00165\n","     10    15       0.0252       0.0252     1.87e-05       0.0963        0.123       0.0824        0.124        0.103        0.104        0.153        0.129        0.298       0.0031\n","     10    16       0.0324       0.0324     2.87e-05        0.107        0.139       0.0925        0.135        0.114        0.121        0.169        0.145         0.38      0.00396\n","     10    17        0.024        0.024     1.86e-05       0.0931         0.12       0.0817        0.116       0.0989        0.103        0.148        0.125         0.27      0.00282\n","     10    18       0.0243       0.0241     0.000286       0.0951         0.12       0.0873        0.111       0.0991        0.111        0.136        0.123         1.25       0.0131\n","     10    19       0.0259       0.0257     0.000132       0.0952        0.124       0.0858        0.114          0.1         0.11        0.149        0.129        0.851      0.00886\n","     10    20       0.0258       0.0258     1.11e-05       0.0951        0.124        0.084        0.117        0.101        0.109        0.151         0.13        0.204      0.00213\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     10     1       0.0324       0.0324     4.92e-06        0.107        0.139       0.0934        0.133        0.113        0.121         0.17        0.145        0.132      0.00138\n","     10     2       0.0309       0.0309     5.35e-06        0.105        0.136       0.0928        0.131        0.112         0.12        0.164        0.142        0.135      0.00141\n","     10     3       0.0268       0.0268     6.81e-06       0.0985        0.127       0.0873        0.121        0.104        0.112        0.153        0.132        0.164      0.00171\n","     10     4       0.0283       0.0283     3.85e-06          0.1         0.13       0.0876        0.126        0.107        0.114        0.158        0.136        0.105       0.0011\n","     10     5       0.0279       0.0279     5.58e-06       0.0996        0.129       0.0869        0.125        0.106        0.113        0.156        0.135        0.138      0.00144\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              10   44.939    0.005       0.0279     4.63e-05        0.028       0.0997        0.129       0.0877        0.124        0.106        0.113        0.156        0.135        0.406      0.00423\n","! Validation         10   44.939    0.005       0.0293      5.3e-06       0.0293        0.102        0.132       0.0896        0.127        0.108        0.116         0.16        0.138        0.135      0.00141\n","Wall time: 44.93977463599981\n","! Best model       10    0.029\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     11     1       0.0268       0.0267     0.000115       0.0954        0.126       0.0817        0.123        0.102        0.107        0.159        0.133        0.777       0.0081\n","     11     2       0.0264       0.0263     0.000102       0.0978        0.126       0.0852        0.123        0.104        0.108        0.155        0.131         0.72       0.0075\n","     11     3       0.0248       0.0248     1.54e-05       0.0943        0.122       0.0825        0.118          0.1        0.106        0.148        0.127        0.238      0.00248\n","     11     4       0.0266       0.0266      1.3e-05       0.0978        0.126        0.088        0.117        0.103        0.113        0.149        0.131        0.257      0.00268\n","     11     5       0.0234       0.0233      5.2e-06       0.0917        0.118       0.0811        0.113        0.097        0.105        0.142        0.123        0.138      0.00144\n","     11     6       0.0257       0.0256     0.000103       0.0959        0.124       0.0832        0.122        0.102        0.107        0.152         0.13        0.747      0.00778\n","     11     7       0.0256       0.0255     0.000119       0.0962        0.124        0.085        0.119        0.102         0.11        0.148        0.129        0.799      0.00833\n","     11     8        0.022        0.022     3.09e-05        0.087        0.115       0.0766        0.108       0.0922          0.1         0.14         0.12          0.4      0.00417\n","     11     9       0.0242       0.0238     0.000418       0.0919        0.119       0.0799        0.116       0.0978        0.103        0.147        0.125         1.51       0.0158\n","     11    10       0.0251       0.0246     0.000468       0.0928        0.121       0.0809        0.117       0.0988        0.106        0.147        0.127          1.6       0.0167\n","     11    11        0.023       0.0229     0.000151       0.0916        0.117       0.0815        0.112       0.0967        0.104        0.139        0.122        0.906      0.00944\n","     11    12       0.0225       0.0223     0.000166       0.0897        0.116       0.0778        0.113       0.0956       0.0994        0.142        0.121        0.939      0.00979\n","     11    13       0.0255       0.0249     0.000655       0.0953        0.122       0.0833        0.119        0.101        0.107        0.147        0.127          1.9       0.0198\n","     11    14       0.0276       0.0274     0.000139       0.0978        0.128       0.0846        0.124        0.104        0.111        0.156        0.134        0.859      0.00895\n","     11    15       0.0231       0.0231     2.26e-05       0.0898        0.118       0.0807        0.108       0.0943        0.105        0.139        0.122        0.348      0.00363\n","     11    16       0.0278       0.0277     6.49e-05       0.0972        0.129       0.0856         0.12        0.103        0.113        0.155        0.134        0.586       0.0061\n","     11    17       0.0278       0.0277     8.94e-05       0.0969        0.129       0.0849        0.121        0.103        0.113        0.156        0.134        0.692      0.00721\n","     11    18       0.0268       0.0266     0.000122       0.0966        0.126       0.0831        0.124        0.103        0.109        0.156        0.132        0.813      0.00847\n","     11    19       0.0237       0.0235     0.000167       0.0916        0.119       0.0794        0.116       0.0977        0.101        0.147        0.124        0.952      0.00991\n","     11    20       0.0211       0.0211     1.01e-05       0.0868        0.112       0.0747        0.111       0.0928       0.0966        0.139        0.118        0.159      0.00166\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     11     1       0.0292       0.0292     6.01e-06        0.101        0.132       0.0883        0.127        0.108        0.115        0.162        0.138        0.139      0.00145\n","     11     2       0.0278       0.0278     5.09e-06       0.0997        0.129       0.0872        0.125        0.106        0.113        0.156        0.135        0.146      0.00152\n","     11     3       0.0242       0.0242     6.18e-06       0.0935         0.12       0.0822        0.116       0.0991        0.105        0.146        0.126        0.157      0.00164\n","     11     4       0.0255       0.0255     4.23e-06        0.095        0.124       0.0829        0.119        0.101        0.108         0.15        0.129        0.125       0.0013\n","     11     5       0.0253       0.0253      4.1e-06       0.0949        0.123       0.0824         0.12        0.101        0.107         0.15        0.129        0.122      0.00127\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              11   49.017    0.005       0.0248     0.000149        0.025       0.0937        0.122        0.082        0.117       0.0996        0.106        0.148        0.127        0.767      0.00799\n","! Validation         11   49.017    0.005       0.0264     5.12e-06       0.0264       0.0968        0.126       0.0846        0.121        0.103         0.11        0.153        0.131        0.138      0.00144\n","Wall time: 49.01773114999992\n","! Best model       11    0.026\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     12     1       0.0248       0.0245     0.000267       0.0951        0.121       0.0815        0.122        0.102        0.103        0.151        0.127         1.21       0.0126\n","     12     2       0.0345       0.0339     0.000634        0.112        0.142        0.102        0.131        0.116         0.13        0.164        0.147         1.86       0.0194\n","     12     3       0.0285       0.0285     2.93e-05        0.101        0.131       0.0907        0.123        0.107        0.117        0.154        0.136        0.389      0.00405\n","     12     4         0.02       0.0199     0.000129        0.085        0.109       0.0748        0.105       0.0901       0.0949        0.133        0.114        0.815      0.00849\n","     12     5       0.0297       0.0293      0.00041        0.102        0.132       0.0882        0.129        0.108        0.113        0.164        0.139          1.5       0.0156\n","     12     6       0.0348       0.0344     0.000388        0.112        0.143        0.105        0.127        0.116        0.132        0.163        0.148         1.46       0.0152\n","     12     7       0.0294       0.0294     4.55e-05        0.104        0.133       0.0935        0.124        0.109        0.119        0.157        0.138        0.464      0.00484\n","     12     8       0.0224       0.0221     0.000211       0.0895        0.115       0.0782        0.112       0.0952       0.0995        0.141         0.12         1.08       0.0112\n","     12     9       0.0334       0.0334     9.19e-06        0.111        0.141        0.105        0.123        0.114        0.135        0.154        0.144        0.181      0.00188\n","     12    10       0.0279       0.0278     3.69e-05          0.1        0.129       0.0855        0.129        0.107        0.109        0.162        0.135        0.439      0.00457\n","     12    11       0.0208       0.0207     0.000121       0.0856        0.111       0.0749        0.107        0.091       0.0957        0.137        0.116        0.809      0.00843\n","     12    12        0.028        0.028     3.35e-05        0.102        0.129       0.0881        0.129        0.108        0.112        0.158        0.135        0.352      0.00366\n","     12    13       0.0273       0.0272     0.000118       0.0967        0.128       0.0826        0.125        0.104        0.108         0.16        0.134        0.772      0.00804\n","     12    14       0.0215       0.0213     0.000152       0.0857        0.113       0.0755        0.106       0.0908       0.0991        0.136        0.118        0.905      0.00943\n","     12    15       0.0217       0.0217     7.97e-05       0.0884        0.114       0.0764        0.112       0.0944       0.0988        0.139        0.119        0.656      0.00684\n","     12    16       0.0207       0.0207     8.27e-06       0.0866        0.111       0.0786        0.103       0.0906        0.101        0.129        0.115        0.209      0.00218\n","     12    17       0.0226       0.0226     7.77e-05       0.0893        0.116       0.0774        0.113       0.0952          0.1        0.143        0.122        0.646      0.00673\n","     12    18       0.0249       0.0247     0.000177       0.0925        0.122       0.0811        0.115       0.0983        0.109        0.144        0.126        0.988       0.0103\n","     12    19       0.0213       0.0213     1.08e-05       0.0873        0.113       0.0753        0.111       0.0933       0.0976        0.139        0.118        0.207      0.00215\n","     12    20       0.0239       0.0239     4.01e-05       0.0911        0.119       0.0802        0.113       0.0965        0.105        0.144        0.125        0.459      0.00478\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     12     1       0.0266       0.0266     7.64e-06       0.0964        0.126       0.0837        0.122        0.103        0.109        0.155        0.132        0.163       0.0017\n","     12     2       0.0254       0.0254     5.41e-06       0.0947        0.123       0.0822         0.12        0.101        0.107        0.151        0.129        0.157      0.00164\n","     12     3       0.0222       0.0221     6.36e-06       0.0891        0.115       0.0777        0.112       0.0948       0.0997        0.141         0.12        0.154      0.00161\n","     12     4       0.0234       0.0234     5.76e-06       0.0908        0.118       0.0789        0.115       0.0967        0.103        0.144        0.123        0.146      0.00152\n","     12     5       0.0231       0.0231     3.56e-06       0.0908        0.118       0.0784        0.116        0.097        0.102        0.145        0.123        0.117      0.00121\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              12   52.978    0.005       0.0258     0.000149       0.0259       0.0958        0.124       0.0847        0.118        0.101         0.11        0.149        0.129        0.769      0.00802\n","! Validation         12   52.978    0.005       0.0241     5.75e-06       0.0241       0.0923         0.12       0.0802        0.117       0.0984        0.104        0.147        0.126        0.147      0.00154\n","Wall time: 52.97917400399979\n","! Best model       12    0.024\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     13     1        0.024        0.024     1.04e-06        0.092         0.12       0.0791        0.118       0.0985        0.104        0.147        0.125       0.0662      0.00069\n","     13     2       0.0206       0.0205     6.28e-05       0.0865        0.111       0.0742        0.111       0.0926       0.0958        0.136        0.116        0.583      0.00608\n","     13     3       0.0239       0.0239     4.24e-05       0.0912         0.12       0.0767         0.12       0.0984        0.101         0.15        0.125        0.456      0.00475\n","     13     4       0.0197       0.0197     8.34e-06       0.0849        0.109       0.0745        0.106       0.0901       0.0943        0.133        0.113        0.199      0.00208\n","     13     5         0.02         0.02     4.94e-05       0.0848        0.109       0.0731        0.108       0.0907        0.093        0.136        0.115        0.513      0.00534\n","     13     6       0.0272        0.027     0.000157       0.0999        0.127        0.092        0.116        0.104        0.117        0.146        0.131        0.922       0.0096\n","     13     7       0.0232       0.0231     5.45e-05       0.0926        0.118       0.0795        0.119       0.0991          0.1        0.146        0.123        0.542      0.00565\n","     13     8       0.0202       0.0202     1.03e-05       0.0837         0.11       0.0731        0.105        0.089       0.0962        0.133        0.115         0.22      0.00229\n","     13     9       0.0249       0.0248     4.61e-05       0.0938        0.122       0.0804         0.12          0.1        0.105        0.151        0.128        0.484      0.00504\n","     13    10       0.0209       0.0207     0.000104       0.0849        0.111       0.0764        0.102       0.0891       0.0999        0.132        0.116        0.746      0.00777\n","     13    11       0.0234       0.0234     1.81e-05       0.0916        0.118       0.0793        0.116       0.0978        0.103        0.145        0.124        0.251      0.00262\n","     13    12       0.0252       0.0251     7.74e-05       0.0957        0.123       0.0856        0.116        0.101         0.11        0.145        0.127        0.651      0.00679\n","     13    13       0.0207       0.0206     5.46e-05       0.0855        0.111       0.0741        0.108       0.0913       0.0951        0.138        0.116        0.541      0.00564\n","     13    14       0.0247       0.0247     2.35e-06        0.092        0.122       0.0785        0.119       0.0988        0.106        0.149        0.127        0.111      0.00115\n","     13    15       0.0221       0.0221     6.25e-06       0.0876        0.115       0.0764         0.11       0.0932       0.0988        0.142         0.12         0.18      0.00187\n","     13    16       0.0191        0.019      2.3e-05       0.0827        0.107       0.0718        0.105       0.0882       0.0921        0.131        0.112        0.291      0.00303\n","     13    17       0.0219       0.0219     1.22e-05        0.087        0.114       0.0743        0.113       0.0934        0.098        0.142         0.12        0.229      0.00238\n","     13    18       0.0218       0.0217     5.08e-05       0.0864        0.114       0.0767        0.106       0.0913       0.0992        0.139        0.119        0.503      0.00523\n","     13    19       0.0203       0.0203     1.21e-05       0.0854         0.11       0.0743        0.108       0.0909       0.0951        0.135        0.115        0.228      0.00237\n","     13    20        0.023        0.023     9.25e-05       0.0904        0.117       0.0792        0.113       0.0961        0.103        0.141        0.122        0.709      0.00739\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     13     1       0.0245       0.0244     9.96e-06       0.0923        0.121       0.0798        0.117       0.0986        0.104         0.15        0.127         0.19      0.00198\n","     13     2       0.0236       0.0236     6.29e-06       0.0908        0.119       0.0781        0.116       0.0971        0.102        0.146        0.124        0.166      0.00173\n","     13     3       0.0205       0.0205     7.23e-06       0.0855        0.111       0.0739        0.109       0.0913       0.0952        0.137        0.116        0.161      0.00167\n","     13     4       0.0216       0.0216     8.04e-06       0.0872        0.114       0.0756         0.11        0.093       0.0987        0.139        0.119        0.173      0.00181\n","     13     5       0.0214       0.0214      4.2e-06       0.0872        0.113       0.0748        0.112       0.0934       0.0967         0.14        0.118        0.123      0.00128\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              13   56.969    0.005       0.0223     4.43e-05       0.0223       0.0889        0.116       0.0775        0.112       0.0947          0.1        0.141        0.121        0.421      0.00439\n","! Validation         13   56.969    0.005       0.0223     7.14e-06       0.0223       0.0886        0.115       0.0764        0.113       0.0947       0.0993        0.142        0.121        0.163      0.00169\n","Wall time: 56.96953313699987\n","! Best model       13    0.022\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     14     1       0.0214       0.0213     8.35e-05       0.0866        0.113        0.075         0.11       0.0924       0.0962        0.141        0.118        0.675      0.00703\n","     14     2       0.0196       0.0195     7.49e-05        0.084        0.108       0.0742        0.103       0.0889       0.0945        0.131        0.113        0.614       0.0064\n","     14     3       0.0184       0.0184     2.45e-06       0.0816        0.105       0.0706        0.104       0.0871       0.0894        0.131         0.11         0.11      0.00114\n","     14     4       0.0242        0.024     0.000163       0.0931         0.12       0.0812        0.117        0.099        0.105        0.146        0.125        0.929      0.00968\n","     14     5       0.0218       0.0216     0.000201       0.0887        0.114       0.0749        0.116       0.0955       0.0963        0.143        0.119         1.03       0.0107\n","     14     6       0.0209       0.0209     6.54e-05       0.0876        0.112       0.0794        0.104       0.0917        0.101        0.131        0.116        0.586       0.0061\n","     14     7       0.0187       0.0187     5.14e-06       0.0814        0.106       0.0689        0.106       0.0876       0.0886        0.134        0.111         0.14      0.00145\n","     14     8       0.0218       0.0217     5.54e-05       0.0878        0.114       0.0782        0.107       0.0925        0.101        0.136        0.119        0.536      0.00558\n","     14     9       0.0253       0.0253     8.42e-05       0.0938        0.123       0.0795        0.122        0.101        0.101        0.158        0.129        0.662      0.00689\n","     14    10       0.0209       0.0208     5.41e-05       0.0839        0.112       0.0725        0.107       0.0896       0.0975        0.135        0.116        0.541      0.00564\n","     14    11       0.0189       0.0189     3.24e-05       0.0817        0.106       0.0696        0.106       0.0878       0.0893        0.134        0.112        0.404      0.00421\n","     14    12       0.0175       0.0175     2.51e-06       0.0795        0.102       0.0692          0.1       0.0847       0.0894        0.124        0.107        0.116      0.00121\n","     14    13       0.0165       0.0164     5.68e-05       0.0758       0.0991        0.065       0.0976       0.0813       0.0852        0.122        0.104        0.554      0.00577\n","     14    14       0.0222       0.0222     3.68e-05       0.0861        0.115       0.0727        0.113       0.0928       0.0981        0.144        0.121        0.445      0.00463\n","     14    15       0.0201       0.0201     1.85e-06       0.0834         0.11       0.0718        0.107       0.0892        0.094        0.136        0.115       0.0939     0.000979\n","     14    16       0.0177       0.0177     1.33e-05       0.0791        0.103       0.0692       0.0988        0.084         0.09        0.125        0.107        0.242      0.00252\n","     14    17       0.0212       0.0212     3.15e-06       0.0867        0.113       0.0738        0.112       0.0931       0.0956        0.141        0.118        0.112      0.00117\n","     14    18       0.0197       0.0196     6.55e-06       0.0837        0.108       0.0729        0.105       0.0891       0.0937        0.133        0.113        0.172      0.00179\n","     14    19       0.0196       0.0195     3.89e-05       0.0821        0.108       0.0703        0.106        0.088       0.0919        0.135        0.113        0.456      0.00475\n","     14    20       0.0169       0.0169     1.92e-05       0.0785        0.101       0.0675          0.1        0.084        0.085        0.126        0.105        0.309      0.00322\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     14     1       0.0228       0.0227     8.55e-06       0.0889        0.117       0.0764        0.114       0.0952       0.0992        0.145        0.122        0.177      0.00184\n","     14     2        0.022        0.022     5.16e-06       0.0875        0.115       0.0747        0.113       0.0939       0.0981        0.142         0.12        0.157      0.00164\n","     14     3       0.0191       0.0191     6.02e-06       0.0825        0.107       0.0708        0.106       0.0884       0.0911        0.133        0.112        0.148      0.00154\n","     14     4       0.0201       0.0201     6.62e-06        0.084         0.11       0.0725        0.107       0.0897       0.0948        0.135        0.115         0.15      0.00157\n","     14     5         0.02         0.02     3.43e-06       0.0842        0.109       0.0717        0.109       0.0904       0.0925        0.137        0.115         0.11      0.00115\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              14   60.928    0.005       0.0201        5e-05       0.0202       0.0842         0.11       0.0728        0.107         0.09       0.0942        0.135        0.115        0.436      0.00455\n","! Validation         14   60.928    0.005       0.0208     5.95e-06       0.0208       0.0854        0.112       0.0732         0.11       0.0915       0.0952        0.139        0.117        0.148      0.00155\n","Wall time: 60.92864881399987\n","! Best model       14    0.021\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     15     1       0.0193       0.0193     1.53e-05        0.082        0.108       0.0702        0.106       0.0879       0.0922        0.133        0.113        0.273      0.00285\n","     15     2       0.0182       0.0181     0.000114       0.0801        0.104       0.0689        0.103       0.0857        0.089        0.129        0.109        0.777      0.00809\n","     15     3       0.0174       0.0173     2.26e-05       0.0786        0.102       0.0669        0.102       0.0844       0.0854        0.129        0.107        0.332      0.00346\n","     15     4       0.0201       0.0201     9.22e-06       0.0846         0.11       0.0739        0.106       0.0899       0.0949        0.135        0.115        0.191      0.00199\n","     15     5       0.0197       0.0197      2.2e-05       0.0834        0.109       0.0713        0.108       0.0895       0.0921        0.136        0.114        0.323      0.00337\n","     15     6       0.0182       0.0182     1.62e-06       0.0817        0.104       0.0722        0.101       0.0865       0.0913        0.126        0.109       0.0836     0.000871\n","     15     7       0.0202       0.0202     3.25e-06       0.0837         0.11       0.0721        0.107       0.0895       0.0938        0.136        0.115        0.114      0.00119\n","     15     8       0.0194       0.0194     2.64e-06       0.0829        0.108       0.0713        0.106       0.0888       0.0914        0.134        0.113        0.117      0.00122\n","     15     9       0.0215       0.0215      1.4e-05       0.0879        0.113       0.0776        0.109        0.093        0.101        0.135        0.118        0.241      0.00251\n","     15    10       0.0215       0.0214     0.000101       0.0867        0.113       0.0774        0.105       0.0913          0.1        0.136        0.118        0.739      0.00769\n","     15    11       0.0197       0.0197     2.48e-05       0.0848        0.109       0.0731        0.108       0.0906       0.0923        0.135        0.114        0.329      0.00343\n","     15    12       0.0221        0.022     6.32e-05        0.088        0.115       0.0771         0.11       0.0934          0.1         0.14         0.12        0.583      0.00608\n","     15    13       0.0268       0.0267     0.000138        0.098        0.126        0.088        0.118        0.103        0.112         0.15        0.131        0.866      0.00902\n","     15    14       0.0189       0.0188     0.000149        0.081        0.106       0.0705        0.102       0.0862       0.0905        0.132        0.111        0.904      0.00941\n","     15    15        0.021        0.021     1.27e-06       0.0854        0.112       0.0726        0.111       0.0918       0.0966        0.138        0.117       0.0709     0.000739\n","     15    16       0.0218       0.0217     0.000127       0.0904        0.114       0.0812        0.109        0.095        0.101        0.135        0.118         0.82      0.00854\n","     15    17       0.0168       0.0168     8.28e-05       0.0774          0.1       0.0659          0.1       0.0832       0.0854        0.125        0.105        0.668      0.00696\n","     15    18       0.0222       0.0222     8.79e-07       0.0867        0.115       0.0737        0.113       0.0932       0.0981        0.143        0.121         0.05     0.000521\n","     15    19        0.019        0.019     1.31e-05       0.0833        0.107       0.0748          0.1       0.0875       0.0951        0.126        0.111        0.247      0.00257\n","     15    20       0.0198       0.0198     2.18e-05       0.0816        0.109       0.0675         0.11       0.0887       0.0884        0.141        0.115        0.315      0.00328\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     15     1       0.0213       0.0213     8.35e-06        0.086        0.113       0.0735        0.111       0.0922       0.0954        0.142        0.119        0.176      0.00184\n","     15     2       0.0207       0.0207     4.99e-06       0.0846        0.111       0.0717         0.11        0.091       0.0945        0.139        0.117        0.155      0.00162\n","     15     3       0.0179       0.0179     5.71e-06       0.0799        0.104        0.068        0.104       0.0858       0.0876         0.13        0.109        0.146      0.00152\n","     15     4        0.019        0.019     6.66e-06       0.0814        0.107         0.07        0.104       0.0871       0.0916        0.132        0.112        0.151      0.00157\n","     15     5       0.0188       0.0188     3.42e-06       0.0815        0.106        0.069        0.107       0.0878       0.0889        0.134        0.111         0.11      0.00114\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              15   64.913    0.005       0.0201     4.64e-05       0.0202       0.0844         0.11       0.0733        0.107         0.09       0.0948        0.135        0.115        0.402      0.00419\n","! Validation         15   64.913    0.005       0.0195     5.83e-06       0.0195       0.0827        0.108       0.0704        0.107       0.0888       0.0916        0.135        0.113        0.148      0.00154\n","Wall time: 64.91344824999987\n","! Best model       15    0.020\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     16     1       0.0185       0.0185     4.49e-06       0.0829        0.105        0.074        0.101       0.0874       0.0939        0.125        0.109        0.153       0.0016\n","     16     2       0.0212       0.0212     5.65e-05       0.0863        0.113       0.0728        0.113       0.0931        0.094        0.143        0.118        0.545      0.00567\n","     16     3       0.0212       0.0211     3.28e-06       0.0849        0.113       0.0702        0.114       0.0923       0.0934        0.143        0.118        0.107      0.00112\n","     16     4       0.0186       0.0186     3.16e-05       0.0812        0.105       0.0684        0.107       0.0877       0.0872        0.135        0.111        0.411      0.00428\n","     16     5        0.018        0.018     3.55e-05       0.0796        0.104       0.0694          0.1       0.0847        0.089        0.128        0.109        0.425      0.00443\n","     16     6       0.0177       0.0177     2.34e-05       0.0789        0.103       0.0677        0.101       0.0845       0.0872        0.129        0.108        0.325      0.00339\n","     16     7       0.0183       0.0183     2.58e-05        0.081        0.105       0.0718       0.0994       0.0856        0.093        0.124        0.109         0.37      0.00385\n","     16     8       0.0207       0.0206     0.000102       0.0858        0.111        0.074        0.109       0.0917       0.0942        0.138        0.116        0.745      0.00776\n","     16     9       0.0208       0.0208     1.93e-05       0.0837        0.112        0.071        0.109       0.0901       0.0956        0.138        0.117        0.272      0.00283\n","     16    10       0.0175       0.0174     9.29e-06       0.0774        0.102        0.065        0.102       0.0836       0.0853         0.13        0.107         0.21      0.00218\n","     16    11        0.024       0.0239     9.01e-05       0.0916         0.12       0.0778        0.119       0.0985       0.0999        0.152        0.126        0.665      0.00692\n","     16    12       0.0174       0.0174     5.44e-05       0.0782        0.102       0.0691       0.0964       0.0827       0.0896        0.123        0.106        0.531      0.00554\n","     16    13        0.017        0.017     1.35e-06       0.0767        0.101       0.0649          0.1       0.0827        0.086        0.126        0.106       0.0703     0.000732\n","     16    14       0.0182       0.0182     4.35e-06        0.079        0.104       0.0675        0.102       0.0848       0.0882        0.131        0.109        0.131      0.00137\n","     16    15       0.0154       0.0154        2e-06       0.0738       0.0959       0.0622       0.0971       0.0796       0.0812         0.12        0.101       0.0971      0.00101\n","     16    16       0.0166       0.0166      1.7e-06       0.0768       0.0995        0.066       0.0983       0.0822       0.0856        0.123        0.104        0.073     0.000761\n","     16    17       0.0176       0.0176     4.02e-06       0.0787        0.103       0.0653        0.105       0.0853       0.0851        0.131        0.108        0.135      0.00141\n","     16    18       0.0157       0.0157     4.75e-06       0.0741       0.0969       0.0622       0.0978         0.08        0.081        0.123        0.102        0.125       0.0013\n","     16    19       0.0139       0.0138     3.54e-05       0.0707        0.091       0.0611       0.0899       0.0755       0.0765        0.115       0.0956        0.434      0.00452\n","     16    20       0.0165       0.0165     7.15e-06       0.0759       0.0994       0.0648       0.0981       0.0815       0.0838        0.125        0.104        0.179      0.00186\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     16     1       0.0202       0.0202     7.01e-06       0.0836         0.11        0.071        0.109       0.0898       0.0923        0.138        0.115         0.16      0.00167\n","     16     2       0.0196       0.0196      4.1e-06       0.0821        0.108       0.0691        0.108       0.0886       0.0913        0.136        0.114        0.142      0.00148\n","     16     3        0.017        0.017     4.81e-06       0.0776        0.101       0.0657        0.101       0.0835       0.0848        0.127        0.106        0.135      0.00141\n","     16     4        0.018        0.018     5.26e-06       0.0793        0.104        0.068        0.102       0.0849       0.0888        0.129        0.109        0.137      0.00143\n","     16     5       0.0178       0.0178     2.85e-06        0.079        0.103       0.0665        0.104       0.0853       0.0858        0.131        0.108        0.105       0.0011\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              16   68.888    0.005       0.0182     2.58e-05       0.0182       0.0799        0.104       0.0683        0.103       0.0857       0.0887         0.13        0.109          0.3      0.00313\n","! Validation         16   68.888    0.005       0.0185      4.8e-06       0.0185       0.0803        0.105       0.0681        0.105       0.0864       0.0887        0.132         0.11        0.136      0.00142\n","Wall time: 68.88857397499987\n","! Best model       16    0.019\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     17     1       0.0154       0.0154     4.32e-06       0.0743       0.0961       0.0623       0.0984       0.0804       0.0796        0.123        0.101        0.121      0.00127\n","     17     2       0.0151        0.015     2.17e-05       0.0734       0.0949       0.0626       0.0949       0.0787       0.0796         0.12       0.0997         0.33      0.00344\n","     17     3       0.0167       0.0167     6.02e-05       0.0775       0.0999       0.0662          0.1       0.0831       0.0843        0.125        0.105        0.564      0.00588\n","     17     4       0.0188       0.0187     3.32e-05       0.0801        0.106       0.0685        0.103        0.086       0.0905        0.131        0.111        0.384        0.004\n","     17     5       0.0159       0.0159     9.77e-06       0.0743       0.0975       0.0642       0.0943       0.0793       0.0843         0.12        0.102         0.22      0.00229\n","     17     6       0.0159       0.0158     5.54e-05        0.074       0.0972       0.0618       0.0984       0.0801       0.0795        0.125        0.102        0.522      0.00544\n","     17     7       0.0164       0.0163     6.19e-05       0.0761       0.0988       0.0635        0.101       0.0824       0.0818        0.126        0.104        0.575      0.00599\n","     17     8       0.0192       0.0192     5.67e-06       0.0808        0.107       0.0676        0.107       0.0874       0.0895        0.136        0.113        0.144       0.0015\n","     17     9       0.0151        0.015     0.000129       0.0719       0.0947       0.0601       0.0957       0.0779       0.0797        0.119       0.0994         0.84      0.00875\n","     17    10       0.0157       0.0155     0.000232       0.0748       0.0962       0.0639       0.0967       0.0803       0.0815         0.12        0.101         1.13       0.0118\n","     17    11       0.0183       0.0182     1.24e-05        0.078        0.104       0.0643        0.105       0.0848       0.0852        0.135         0.11        0.205      0.00214\n","     17    12       0.0164       0.0161      0.00022       0.0763       0.0983       0.0645       0.0999       0.0822       0.0821        0.125        0.103         1.09       0.0113\n","     17    13       0.0164       0.0161     0.000381        0.074        0.098       0.0636       0.0949       0.0793       0.0838        0.122        0.103         1.45       0.0151\n","     17    14       0.0153       0.0153     1.47e-06       0.0745       0.0958       0.0636       0.0961       0.0799       0.0798        0.122        0.101       0.0793     0.000826\n","     17    15       0.0178       0.0173     0.000498       0.0774        0.102       0.0671       0.0981       0.0826       0.0878        0.125        0.106         1.65       0.0172\n","     17    16       0.0154       0.0151     0.000378       0.0733        0.095       0.0599          0.1       0.0801       0.0755        0.125          0.1         1.44        0.015\n","     17    17       0.0163       0.0163     6.38e-06       0.0756       0.0988       0.0646       0.0974        0.081       0.0833        0.124        0.104         0.16      0.00166\n","     17    18       0.0174        0.017     0.000416       0.0766        0.101       0.0651       0.0996       0.0824       0.0856        0.126        0.106         1.51       0.0158\n","     17    19       0.0187       0.0184     0.000328       0.0799        0.105       0.0667        0.106       0.0865       0.0887        0.132         0.11         1.34        0.014\n","     17    20       0.0154       0.0154     7.62e-06       0.0738       0.0961       0.0627        0.096       0.0793       0.0808        0.121        0.101        0.181      0.00188\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     17     1       0.0191       0.0191     7.17e-06       0.0813        0.107       0.0687        0.106       0.0875       0.0895        0.135        0.112        0.165      0.00172\n","     17     2       0.0185       0.0185     3.96e-06       0.0797        0.105       0.0667        0.106       0.0862       0.0883        0.133        0.111        0.138      0.00143\n","     17     3       0.0161       0.0161     4.87e-06       0.0754       0.0982       0.0635       0.0992       0.0814        0.082        0.124        0.103        0.137      0.00143\n","     17     4       0.0171       0.0171     5.56e-06       0.0772        0.101        0.066       0.0996       0.0828       0.0861        0.126        0.106        0.141      0.00147\n","     17     5       0.0169       0.0169     2.89e-06       0.0769          0.1       0.0643        0.102       0.0831       0.0831        0.128        0.106       0.0992      0.00103\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              17   72.866    0.005       0.0164     0.000143       0.0166       0.0758       0.0992       0.0641       0.0992       0.0817       0.0832        0.125        0.104        0.697      0.00726\n","! Validation         17   72.866    0.005       0.0176     4.89e-06       0.0176       0.0781        0.102       0.0658        0.103       0.0842       0.0858         0.13        0.108        0.136      0.00142\n","Wall time: 72.86706521299993\n","! Best model       17    0.018\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     18     1       0.0173        0.017     0.000271       0.0756        0.101       0.0661       0.0946       0.0803       0.0879        0.123        0.105         1.22       0.0127\n","     18     2       0.0149       0.0148     0.000147       0.0734        0.094       0.0622       0.0958        0.079       0.0782         0.12       0.0989        0.898      0.00935\n","     18     3       0.0187       0.0187     3.44e-06       0.0803        0.106       0.0656         0.11       0.0876       0.0865        0.136        0.111       0.0992      0.00103\n","     18     4       0.0169       0.0168      0.00015       0.0765          0.1       0.0633        0.103       0.0831       0.0816         0.13        0.106        0.886      0.00923\n","     18     5       0.0156       0.0155     0.000134       0.0723       0.0963       0.0595       0.0979       0.0787       0.0795        0.123        0.101        0.853      0.00889\n","     18     6       0.0149       0.0149     7.07e-06       0.0714       0.0945       0.0602       0.0938        0.077       0.0792        0.119       0.0993        0.149      0.00155\n","     18     7        0.016       0.0159     0.000112       0.0743       0.0976       0.0615          0.1       0.0808       0.0804        0.125        0.103        0.771      0.00804\n","     18     8       0.0166       0.0165     2.71e-05       0.0772       0.0995       0.0666       0.0984       0.0825       0.0846        0.124        0.104         0.38      0.00396\n","     18     9       0.0158       0.0158     2.12e-06       0.0755       0.0972       0.0654       0.0958       0.0806        0.083        0.121        0.102       0.0893      0.00093\n","     18    10       0.0169       0.0169     2.66e-05       0.0772          0.1       0.0662       0.0993       0.0828       0.0864        0.124        0.105        0.365      0.00381\n","     18    11        0.015        0.015     1.12e-06       0.0738       0.0947       0.0648       0.0917       0.0782       0.0822        0.116       0.0989       0.0676     0.000704\n","     18    12       0.0147       0.0147     2.74e-06       0.0726       0.0938       0.0612       0.0953       0.0783        0.077        0.121       0.0988        0.109      0.00114\n","     18    13        0.016        0.016     9.79e-06       0.0762       0.0977        0.066       0.0965       0.0813       0.0855        0.119        0.102         0.22      0.00229\n","     18    14       0.0169       0.0169     5.33e-06       0.0784        0.101        0.068       0.0992       0.0836       0.0856        0.126        0.106        0.139      0.00145\n","     18    15       0.0178       0.0178     6.46e-06       0.0794        0.103       0.0664        0.105       0.0858       0.0845        0.133        0.109        0.179      0.00187\n","     18    16       0.0154       0.0153     3.96e-05       0.0747       0.0958       0.0655       0.0931       0.0793       0.0829        0.117          0.1        0.459      0.00478\n","     18    17       0.0144       0.0143     6.11e-05       0.0703       0.0925        0.059        0.093        0.076       0.0766        0.118       0.0974        0.565      0.00589\n","     18    18       0.0152       0.0152     6.29e-06       0.0725       0.0954       0.0635       0.0905        0.077       0.0828        0.117       0.0997        0.154       0.0016\n","     18    19        0.019        0.019     8.43e-06       0.0824        0.107        0.069        0.109        0.089        0.088        0.136        0.112        0.198      0.00206\n","     18    20       0.0177       0.0176     3.13e-05       0.0791        0.103        0.069       0.0991       0.0841       0.0899        0.125        0.107        0.413       0.0043\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     18     1       0.0181       0.0181     6.98e-06       0.0791        0.104       0.0665        0.104       0.0853       0.0867        0.132         0.11        0.163       0.0017\n","     18     2       0.0176       0.0176     3.62e-06       0.0776        0.103       0.0647        0.103        0.084       0.0855         0.13        0.108         0.13      0.00135\n","     18     3       0.0153       0.0153     4.73e-06       0.0734       0.0958       0.0614       0.0973       0.0794       0.0794        0.122        0.101        0.136      0.00142\n","     18     4       0.0164       0.0164     5.41e-06       0.0754       0.0989       0.0642       0.0977       0.0809       0.0837        0.124        0.104         0.14      0.00146\n","     18     5        0.016        0.016      2.9e-06       0.0748        0.098       0.0622       0.0999       0.0811       0.0806        0.126        0.103       0.0979      0.00102\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              18   76.845    0.005       0.0162     5.26e-05       0.0163       0.0757       0.0986       0.0645        0.098       0.0813       0.0832        0.124        0.103        0.411      0.00428\n","! Validation         18   76.845    0.005       0.0167     4.73e-06       0.0167        0.076          0.1       0.0638        0.101       0.0822       0.0832        0.127        0.105        0.133      0.00139\n","Wall time: 76.84593903699988\n","! Best model       18    0.017\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     19     1       0.0156       0.0156     3.93e-05       0.0733       0.0965       0.0646       0.0906       0.0776       0.0857        0.115          0.1        0.458      0.00477\n","     19     2       0.0133       0.0133     4.31e-05       0.0674       0.0892       0.0555       0.0912       0.0733       0.0726        0.115        0.094        0.479      0.00499\n","     19     3       0.0135       0.0135     9.34e-07       0.0695         0.09       0.0612       0.0861       0.0736       0.0784         0.11        0.094       0.0518     0.000539\n","     19     4       0.0143       0.0142     8.91e-05       0.0709       0.0923       0.0591       0.0946       0.0769       0.0755        0.119       0.0972        0.699      0.00728\n","     19     5       0.0159       0.0158     7.67e-05       0.0742       0.0972        0.063       0.0966       0.0798       0.0829        0.121        0.102        0.645      0.00672\n","     19     6       0.0162       0.0162     2.38e-05       0.0758       0.0985       0.0642        0.099       0.0816       0.0825        0.124        0.103        0.322      0.00335\n","     19     7       0.0148       0.0148     1.58e-05       0.0718        0.094       0.0582        0.099       0.0786       0.0756        0.123       0.0992        0.283      0.00295\n","     19     8       0.0132        0.013     0.000197       0.0675       0.0882       0.0579       0.0867       0.0723       0.0747         0.11       0.0925         1.04       0.0108\n","     19     9       0.0142       0.0141     0.000141       0.0703       0.0918       0.0584        0.094       0.0762       0.0755        0.118       0.0966        0.879      0.00916\n","     19    10       0.0188       0.0188     5.75e-06       0.0804        0.106       0.0668        0.107       0.0871       0.0873        0.136        0.112        0.155      0.00161\n","     19    11       0.0241       0.0241     3.25e-05       0.0931         0.12       0.0815        0.116       0.0989        0.105        0.146        0.125        0.403       0.0042\n","     19    12       0.0244       0.0241     0.000252       0.0959         0.12       0.0882        0.111       0.0997         0.11        0.139        0.124         1.17       0.0122\n","     19    13       0.0173       0.0173      2.3e-06       0.0783        0.102       0.0694       0.0961       0.0828       0.0903        0.122        0.106       0.0914     0.000952\n","     19    14       0.0172       0.0172     1.88e-05       0.0782        0.101       0.0674          0.1       0.0837       0.0875        0.125        0.106        0.316      0.00329\n","     19    15       0.0242       0.0241     6.29e-05       0.0957         0.12       0.0912        0.105        0.098        0.114        0.133        0.123        0.588      0.00612\n","     19    16       0.0214       0.0214     3.11e-06        0.089        0.113         0.08        0.107       0.0935       0.0997        0.136        0.118        0.111      0.00116\n","     19    17       0.0158       0.0158     1.52e-06       0.0742       0.0971        0.062       0.0986       0.0803       0.0816        0.122        0.102       0.0771     0.000804\n","     19    18       0.0158       0.0157     3.93e-05       0.0745        0.097       0.0634       0.0966         0.08       0.0825        0.121        0.102        0.439      0.00457\n","     19    19       0.0164       0.0162     0.000106       0.0748       0.0986       0.0645       0.0956         0.08       0.0842        0.122        0.103        0.764      0.00795\n","     19    20       0.0175       0.0175     5.59e-06       0.0768        0.102       0.0646        0.101        0.083       0.0857        0.129        0.107        0.166      0.00173\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     19     1       0.0173       0.0173     5.44e-06       0.0772        0.102       0.0646        0.102       0.0834       0.0844         0.13        0.107         0.14      0.00146\n","     19     2       0.0168       0.0168     2.91e-06       0.0757          0.1       0.0629        0.101       0.0821       0.0832        0.128        0.105        0.118      0.00123\n","     19     3       0.0146       0.0146      3.7e-06       0.0716       0.0935       0.0597       0.0953       0.0775       0.0773        0.119       0.0984        0.123      0.00128\n","     19     4       0.0157       0.0157     3.74e-06       0.0737       0.0969       0.0626        0.096       0.0793       0.0816        0.122        0.102        0.119      0.00124\n","     19     5       0.0154       0.0154     2.25e-06        0.073       0.0959       0.0606       0.0979       0.0792       0.0785        0.123        0.101       0.0947     0.000987\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              19   80.822    0.005       0.0171     5.78e-05       0.0172       0.0776        0.101       0.0671       0.0986       0.0828       0.0873        0.125        0.106        0.457      0.00476\n","! Validation         19   80.822    0.005        0.016     3.61e-06        0.016       0.0742       0.0977       0.0621       0.0985       0.0803        0.081        0.125        0.103        0.119      0.00124\n","Wall time: 80.82326317599995\n","! Best model       19    0.016\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     20     1       0.0157       0.0157     6.61e-06       0.0746       0.0969       0.0635       0.0968       0.0802       0.0824        0.121        0.102        0.138      0.00144\n","     20     2       0.0134       0.0134     6.62e-06       0.0687       0.0895       0.0578       0.0904       0.0741        0.074        0.114       0.0941        0.177      0.00184\n","     20     3       0.0169       0.0169     2.41e-05       0.0761          0.1       0.0653       0.0977       0.0815       0.0858        0.125        0.105        0.337      0.00351\n","     20     4       0.0176       0.0176     4.57e-06       0.0778        0.103       0.0652        0.103       0.0841       0.0857         0.13        0.108        0.139      0.00144\n","     20     5       0.0144       0.0144     1.81e-05       0.0706       0.0928       0.0586       0.0944       0.0765       0.0779        0.117       0.0975        0.302      0.00315\n","     20     6       0.0139       0.0139     2.31e-05       0.0705       0.0911       0.0595       0.0927       0.0761       0.0762        0.115       0.0958        0.349      0.00363\n","     20     7       0.0141       0.0141     9.37e-06       0.0696       0.0918       0.0578       0.0933       0.0755       0.0751        0.118       0.0967        0.207      0.00215\n","     20     8       0.0125       0.0125     4.17e-06       0.0665       0.0866       0.0544       0.0908       0.0726       0.0695        0.113       0.0915        0.119      0.00124\n","     20     9       0.0131       0.0131     5.61e-06       0.0679       0.0886        0.058       0.0877       0.0729       0.0752        0.111       0.0929        0.161      0.00168\n","     20    10       0.0178       0.0177     9.28e-05       0.0802        0.103       0.0674        0.106       0.0866       0.0848        0.132        0.108        0.698      0.00728\n","     20    11       0.0169       0.0169     1.35e-05       0.0768        0.101       0.0662        0.098       0.0821        0.087        0.123        0.105        0.247      0.00258\n","     20    12       0.0137       0.0137     4.03e-06       0.0694       0.0904       0.0582       0.0916       0.0749       0.0753        0.115        0.095        0.124      0.00129\n","     20    13       0.0131       0.0131     2.75e-05        0.067       0.0884       0.0549       0.0912        0.073       0.0703        0.117       0.0934        0.386      0.00402\n","     20    14       0.0133       0.0132     3.17e-05       0.0686        0.089        0.058         0.09        0.074       0.0735        0.114       0.0936        0.393       0.0041\n","     20    15       0.0137       0.0137     1.12e-05       0.0695       0.0904       0.0578       0.0928       0.0753        0.075        0.115       0.0951        0.233      0.00242\n","     20    16       0.0136       0.0136     7.56e-06       0.0678       0.0903       0.0556       0.0924        0.074       0.0731        0.117       0.0952        0.197      0.00205\n","     20    17       0.0162       0.0162     1.03e-05       0.0751       0.0984       0.0638       0.0977       0.0808       0.0822        0.125        0.103        0.237      0.00246\n","     20    18       0.0176       0.0176     6.64e-06       0.0806        0.103        0.072       0.0979       0.0849       0.0916        0.122        0.107        0.128      0.00133\n","     20    19         0.02       0.0199     1.11e-05       0.0828        0.109       0.0703        0.108       0.0891       0.0937        0.135        0.114        0.225      0.00234\n","     20    20       0.0133       0.0132      3.4e-05       0.0683        0.089       0.0578       0.0893       0.0736        0.075        0.112       0.0934        0.422      0.00439\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     20     1       0.0166       0.0166     5.03e-06       0.0755       0.0997       0.0631          0.1       0.0817       0.0824        0.127        0.105        0.133      0.00139\n","     20     2       0.0161       0.0161     2.67e-06        0.074       0.0981       0.0613       0.0993       0.0803        0.081        0.125        0.103        0.111      0.00116\n","     20     3        0.014        0.014     3.48e-06         0.07       0.0916       0.0582       0.0937       0.0759       0.0754        0.118       0.0965         0.12      0.00125\n","     20     4       0.0151       0.0151     3.43e-06       0.0722        0.095       0.0612       0.0944       0.0778       0.0797         0.12       0.0998        0.115      0.00119\n","     20     5       0.0147       0.0147     2.12e-06       0.0713       0.0938        0.059        0.096       0.0775       0.0766        0.121       0.0989       0.0929     0.000967\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              20   84.801    0.005        0.015     1.76e-05        0.015       0.0724       0.0948       0.0611       0.0951       0.0781       0.0794         0.12       0.0996        0.261      0.00272\n","! Validation         20   84.801    0.005       0.0153     3.35e-06       0.0153       0.0726       0.0957       0.0606       0.0967       0.0786       0.0791        0.122        0.101        0.114      0.00119\n","Wall time: 84.80194028999995\n","! Best model       20    0.015\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     21     1       0.0124       0.0123     7.51e-05       0.0661       0.0857       0.0559       0.0867       0.0713       0.0706         0.11       0.0902        0.638      0.00664\n","     21     2       0.0171       0.0171     1.69e-06       0.0779        0.101       0.0655        0.103       0.0841        0.086        0.126        0.106        0.076     0.000791\n","     21     3        0.017       0.0169     4.82e-05       0.0767        0.101       0.0655       0.0993       0.0824       0.0848        0.127        0.106        0.494      0.00514\n","     21     4       0.0176       0.0175     0.000153       0.0797        0.102         0.07       0.0992       0.0846        0.089        0.124        0.107        0.915      0.00953\n","     21     5       0.0151       0.0151     5.63e-06       0.0738       0.0951       0.0619       0.0975       0.0797       0.0791        0.121          0.1         0.16      0.00166\n","     21     6       0.0147       0.0147        5e-06       0.0717       0.0936       0.0607       0.0938       0.0772       0.0787        0.118       0.0983        0.153       0.0016\n","     21     7        0.016       0.0159     7.51e-05       0.0729       0.0975       0.0609       0.0969       0.0789       0.0788        0.127        0.103        0.631      0.00657\n","     21     8        0.019       0.0189     0.000145       0.0835        0.106       0.0744        0.102       0.0881       0.0934        0.128        0.111        0.891      0.00928\n","     21     9       0.0122       0.0122      1.9e-06       0.0664       0.0854       0.0569       0.0856       0.0712       0.0713        0.108       0.0897       0.0799     0.000832\n","     21    10       0.0124       0.0123     5.63e-05       0.0669       0.0858       0.0568        0.087       0.0719       0.0716        0.109       0.0902        0.547       0.0057\n","     21    11       0.0156       0.0155     8.44e-05       0.0752       0.0964       0.0675       0.0906       0.0791       0.0849        0.116          0.1        0.671      0.00699\n","     21    12       0.0152       0.0151     7.28e-05       0.0723       0.0951       0.0598       0.0972       0.0785        0.079        0.121          0.1         0.62      0.00646\n","     21    13       0.0138       0.0138     2.96e-06       0.0685        0.091       0.0575       0.0903       0.0739       0.0755        0.116       0.0957        0.124       0.0013\n","     21    14       0.0161       0.0161     4.42e-05       0.0751       0.0981       0.0608        0.104       0.0823       0.0795        0.127        0.103         0.49       0.0051\n","     21    15       0.0148       0.0147     2.58e-05       0.0718       0.0939       0.0626       0.0902       0.0764       0.0801        0.117       0.0984        0.364      0.00379\n","     21    16        0.015        0.015     8.52e-06       0.0728       0.0947       0.0607        0.097       0.0789       0.0787        0.121       0.0997        0.191      0.00199\n","     21    17       0.0129       0.0129     3.35e-06       0.0666       0.0877       0.0538       0.0923       0.0731       0.0693        0.116       0.0927        0.119      0.00124\n","     21    18       0.0148       0.0148     1.63e-06       0.0726       0.0941       0.0629       0.0922       0.0775       0.0798        0.118       0.0987       0.0918     0.000956\n","     21    19       0.0163       0.0163     2.75e-05       0.0753       0.0987       0.0652       0.0956       0.0804       0.0837        0.123        0.104        0.381      0.00397\n","     21    20        0.015        0.015     2.46e-05       0.0727       0.0947       0.0632       0.0916       0.0774       0.0821        0.116        0.099        0.362      0.00377\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     21     1        0.016        0.016     5.25e-06        0.074       0.0977       0.0616       0.0986       0.0801       0.0806        0.125        0.103        0.139      0.00145\n","     21     2       0.0154       0.0154     2.51e-06       0.0724       0.0961       0.0598       0.0976       0.0787       0.0791        0.123        0.101        0.107      0.00111\n","     21     3       0.0135       0.0135      3.6e-06       0.0686       0.0898       0.0569       0.0921       0.0745       0.0736        0.116       0.0946        0.121      0.00126\n","     21     4       0.0146       0.0146      3.7e-06       0.0708       0.0933       0.0598       0.0929       0.0764       0.0779        0.118       0.0981        0.118      0.00123\n","     21     5       0.0142       0.0142     2.25e-06         0.07       0.0921       0.0578       0.0943       0.0761        0.075        0.119       0.0971       0.0915     0.000953\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              21   88.789    0.005       0.0151     4.31e-05       0.0151       0.0729       0.0951       0.0621       0.0946       0.0783         0.08         0.12       0.0998          0.4      0.00417\n","! Validation         21   88.789    0.005       0.0147     3.46e-06       0.0147       0.0712       0.0939       0.0592       0.0951       0.0771       0.0773         0.12       0.0988        0.115       0.0012\n","Wall time: 88.78932270999985\n","! Best model       21    0.015\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     22     1       0.0139       0.0138     8.54e-05       0.0687       0.0909       0.0574       0.0911       0.0743       0.0751        0.116       0.0957        0.678      0.00706\n","     22     2       0.0168       0.0168     6.21e-05       0.0764          0.1       0.0655       0.0981       0.0818        0.085        0.125        0.105        0.573      0.00597\n","     22     3       0.0183       0.0183     4.89e-06       0.0795        0.105       0.0702       0.0982       0.0842       0.0937        0.123        0.109        0.112      0.00116\n","     22     4       0.0134       0.0134     8.75e-05       0.0695       0.0894       0.0597        0.089       0.0744       0.0751        0.113       0.0939        0.692      0.00721\n","     22     5       0.0129       0.0128     9.43e-05       0.0679       0.0875       0.0581       0.0875       0.0728       0.0739         0.11       0.0919        0.712      0.00741\n","     22     6       0.0148       0.0148      4.2e-06       0.0718       0.0942       0.0596       0.0964        0.078       0.0759        0.123       0.0994        0.122      0.00127\n","     22     7       0.0157       0.0157     8.16e-07       0.0737        0.097       0.0605          0.1       0.0803        0.078        0.127        0.102       0.0604     0.000629\n","     22     8       0.0154       0.0153     5.72e-05       0.0736       0.0957        0.062       0.0967       0.0793       0.0786        0.123        0.101         0.56      0.00583\n","     22     9        0.013        0.013     3.24e-05       0.0682       0.0881       0.0564       0.0917        0.074       0.0706        0.115        0.093        0.414      0.00431\n","     22    10       0.0153       0.0153      5.7e-06        0.073       0.0958       0.0613       0.0964       0.0789       0.0802        0.121        0.101        0.137      0.00143\n","     22    11       0.0127       0.0127     2.44e-06       0.0657       0.0871       0.0548       0.0876       0.0712        0.072        0.111       0.0916       0.0959     0.000999\n","     22    12       0.0137       0.0137     5.33e-06        0.069       0.0906       0.0575        0.092       0.0747        0.075        0.116       0.0953        0.166      0.00173\n","     22    13       0.0121       0.0121     3.05e-05       0.0638        0.085       0.0524       0.0866       0.0695       0.0695         0.11       0.0896        0.404      0.00421\n","     22    14        0.012        0.012     2.79e-06        0.066       0.0849       0.0552       0.0877       0.0714       0.0703        0.108       0.0893       0.0977      0.00102\n","     22    15       0.0135       0.0135     1.26e-05       0.0693       0.0899       0.0565       0.0949       0.0757       0.0735        0.116       0.0947        0.255      0.00266\n","     22    16       0.0117       0.0116     5.66e-06       0.0639       0.0835       0.0538       0.0842        0.069       0.0692        0.107       0.0878        0.166      0.00173\n","     22    17       0.0126       0.0126     3.26e-05       0.0657       0.0869       0.0543       0.0883       0.0713       0.0718        0.111       0.0914        0.409      0.00426\n","     22    18       0.0118       0.0118     1.72e-05       0.0634       0.0841       0.0526       0.0849       0.0688       0.0688        0.108       0.0887        0.294      0.00306\n","     22    19        0.012        0.012     9.92e-06       0.0651       0.0847       0.0561       0.0832       0.0697       0.0711        0.107        0.089        0.221      0.00231\n","     22    20        0.013        0.013     4.46e-05       0.0669       0.0881       0.0537       0.0933       0.0735       0.0685        0.118       0.0932        0.491      0.00511\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     22     1       0.0153       0.0153     4.74e-06       0.0725       0.0958       0.0602        0.097       0.0786       0.0788        0.123        0.101         0.13      0.00135\n","     22     2       0.0148       0.0148     2.19e-06        0.071       0.0942       0.0584        0.096       0.0772       0.0772        0.121       0.0992       0.0994      0.00104\n","     22     3        0.013        0.013     3.21e-06       0.0673       0.0882       0.0557       0.0907       0.0732        0.072        0.114        0.093        0.116       0.0012\n","     22     4       0.0141       0.0141      3.2e-06       0.0695       0.0917       0.0586       0.0915        0.075       0.0762        0.117       0.0965        0.111      0.00116\n","     22     5       0.0137       0.0137     2.14e-06       0.0686       0.0904       0.0566       0.0927       0.0746       0.0735        0.117       0.0953       0.0908     0.000946\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              22   92.759    0.005       0.0137     2.99e-05       0.0137       0.0691       0.0906       0.0579       0.0914       0.0746        0.075        0.116       0.0953        0.333      0.00347\n","! Validation         22   92.759    0.005       0.0142     3.09e-06       0.0142       0.0698       0.0921       0.0579       0.0936       0.0757       0.0756        0.118        0.097        0.109      0.00114\n","Wall time: 92.75934245099984\n","! Best model       22    0.014\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     23     1       0.0115       0.0115     3.77e-07       0.0641       0.0831       0.0534       0.0855       0.0695       0.0693        0.105       0.0873       0.0443     0.000462\n","     23     2       0.0127       0.0126     2.57e-05       0.0683        0.087       0.0583       0.0883       0.0733       0.0736        0.109       0.0913        0.369      0.00385\n","     23     3       0.0134       0.0134     5.17e-05       0.0682       0.0894       0.0562       0.0922       0.0742       0.0735        0.115       0.0942        0.531      0.00553\n","     23     4       0.0141       0.0141     1.56e-05       0.0693       0.0918       0.0566       0.0946       0.0756       0.0747        0.119       0.0968        0.281      0.00293\n","     23     5       0.0128       0.0127     9.26e-05       0.0658       0.0872       0.0547       0.0881       0.0714       0.0712        0.113       0.0919        0.705      0.00735\n","     23     6       0.0142       0.0141     1.84e-05       0.0702        0.092       0.0583        0.094       0.0761       0.0758        0.118       0.0969        0.314      0.00327\n","     23     7       0.0127       0.0127     6.26e-05       0.0664        0.087       0.0554       0.0885        0.072       0.0726         0.11       0.0915        0.571      0.00595\n","     23     8       0.0141       0.0141     7.64e-05       0.0697       0.0918       0.0571       0.0951       0.0761       0.0747        0.119       0.0967        0.629      0.00655\n","     23     9       0.0132       0.0132     6.06e-06       0.0669        0.089       0.0561       0.0887       0.0724       0.0728        0.115       0.0938        0.157      0.00164\n","     23    10       0.0125       0.0124     8.18e-05       0.0657       0.0862        0.055       0.0869        0.071        0.072        0.109       0.0906        0.654      0.00682\n","     23    11       0.0136       0.0136     1.17e-05       0.0692       0.0901       0.0588       0.0899       0.0743       0.0748        0.115       0.0947        0.236      0.00246\n","     23    12       0.0132       0.0132     5.64e-06       0.0687       0.0888       0.0584       0.0893       0.0738       0.0741        0.112       0.0933        0.152      0.00158\n","     23    13       0.0116       0.0115     0.000105       0.0635        0.083       0.0528       0.0849       0.0688       0.0689        0.106       0.0874        0.757      0.00789\n","     23    14       0.0127       0.0127     3.69e-06       0.0672       0.0872       0.0569       0.0876       0.0723       0.0729         0.11       0.0916        0.126      0.00131\n","     23    15       0.0166       0.0165      7.6e-05       0.0778       0.0995       0.0718         0.09       0.0809       0.0899        0.116        0.103        0.638      0.00665\n","     23    16        0.016        0.016     5.18e-06       0.0773       0.0979       0.0702       0.0916       0.0809       0.0884        0.115        0.101        0.144       0.0015\n","     23    17       0.0144       0.0143     4.33e-06       0.0714       0.0927       0.0595       0.0952       0.0774       0.0748        0.121       0.0978        0.153      0.00159\n","     23    18       0.0146       0.0146     4.32e-05       0.0701       0.0934       0.0582       0.0939        0.076       0.0777        0.119       0.0982        0.479      0.00499\n","     23    19       0.0142       0.0142     1.25e-06       0.0709       0.0921       0.0573       0.0981       0.0777       0.0724        0.122       0.0974       0.0629     0.000655\n","     23    20       0.0187       0.0186     9.46e-05       0.0828        0.106       0.0734        0.102       0.0875       0.0915        0.129         0.11        0.715      0.00745\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     23     1       0.0148       0.0148     4.52e-06       0.0712       0.0941        0.059       0.0955       0.0772       0.0772        0.121       0.0991        0.129      0.00135\n","     23     2       0.0143       0.0143     1.94e-06       0.0695       0.0924       0.0571       0.0945       0.0758       0.0755        0.119       0.0974       0.0925     0.000963\n","     23     3       0.0126       0.0126     3.06e-06       0.0661       0.0867       0.0545       0.0892       0.0719       0.0705        0.112       0.0914        0.113      0.00117\n","     23     4       0.0136       0.0136     2.97e-06       0.0683       0.0902       0.0574       0.0901       0.0738       0.0747        0.115       0.0949        0.108      0.00112\n","     23     5       0.0132       0.0132     2.03e-06       0.0673       0.0888       0.0554       0.0911       0.0733       0.0721        0.115       0.0937       0.0877     0.000913\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              23   96.721    0.005       0.0138     3.91e-05       0.0138       0.0697       0.0909       0.0589       0.0912       0.0751       0.0761        0.115       0.0955        0.386      0.00402\n","! Validation         23   96.721    0.005       0.0137      2.9e-06       0.0137       0.0685       0.0905       0.0567       0.0921       0.0744        0.074        0.117       0.0953        0.106       0.0011\n","Wall time: 96.72210447799989\n","! Best model       23    0.014\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     24     1       0.0197       0.0197     1.16e-06       0.0857        0.109       0.0781        0.101       0.0895       0.0978        0.127        0.113       0.0654     0.000682\n","     24     2       0.0143       0.0143     1.56e-05       0.0709       0.0924       0.0611       0.0905       0.0758       0.0779        0.116        0.097        0.286      0.00298\n","     24     3       0.0117       0.0117     4.35e-05       0.0632       0.0837        0.052       0.0855       0.0688       0.0676        0.109       0.0882        0.474      0.00494\n","     24     4       0.0149       0.0149     5.15e-06       0.0739       0.0946       0.0635       0.0946        0.079       0.0797        0.119       0.0993        0.164      0.00171\n","     24     5       0.0136       0.0135     9.23e-05       0.0687         0.09       0.0583       0.0896        0.074       0.0749        0.115       0.0947        0.712      0.00742\n","     24     6       0.0123       0.0122     2.76e-05       0.0642       0.0855       0.0526       0.0874         0.07       0.0701         0.11       0.0901        0.375      0.00391\n","     24     7       0.0127       0.0127     2.18e-05       0.0674       0.0871       0.0572       0.0878       0.0725       0.0732         0.11       0.0915        0.336       0.0035\n","     24     8       0.0138       0.0138     2.47e-05         0.07       0.0909       0.0594       0.0911       0.0753       0.0762        0.115       0.0955        0.354      0.00369\n","     24     9       0.0137       0.0136     1.94e-05        0.068       0.0903       0.0555        0.093       0.0743       0.0743        0.116       0.0951        0.312      0.00325\n","     24    10       0.0133       0.0133     7.25e-06       0.0691       0.0891       0.0581       0.0909       0.0745       0.0746        0.113       0.0936        0.191      0.00199\n","     24    11       0.0146       0.0145     7.82e-05       0.0718       0.0932       0.0583       0.0989       0.0786       0.0734        0.124       0.0985        0.651      0.00678\n","     24    12       0.0127       0.0127     4.35e-06       0.0656        0.087       0.0561       0.0847       0.0704       0.0734        0.109       0.0913        0.151      0.00157\n","     24    13       0.0126       0.0126     1.61e-05       0.0678       0.0867       0.0589       0.0856       0.0723       0.0746        0.107       0.0908        0.278       0.0029\n","     24    14       0.0128       0.0128     1.42e-05       0.0672       0.0875       0.0557       0.0901       0.0729       0.0713        0.113       0.0922        0.259       0.0027\n","     24    15       0.0164       0.0164     6.61e-06       0.0759       0.0991        0.065       0.0979       0.0814       0.0837        0.124        0.104        0.187      0.00194\n","     24    16       0.0134       0.0133     7.09e-05       0.0687       0.0893       0.0558       0.0947       0.0752       0.0702        0.119       0.0944        0.617      0.00642\n","     24    17        0.014        0.014      4.5e-06       0.0706       0.0915       0.0575        0.097       0.0772       0.0732         0.12       0.0966        0.122      0.00128\n","     24    18       0.0124       0.0124     3.95e-05        0.067       0.0862       0.0571       0.0867       0.0719       0.0732        0.108       0.0903        0.465      0.00485\n","     24    19       0.0122       0.0121     2.85e-05       0.0642       0.0853       0.0533       0.0861       0.0697       0.0696         0.11       0.0898        0.391      0.00407\n","     24    20       0.0112       0.0112     6.81e-07       0.0627       0.0817       0.0532       0.0816       0.0674       0.0685        0.103       0.0859       0.0494     0.000515\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     24     1       0.0143       0.0143     3.89e-06         0.07       0.0925       0.0579       0.0941        0.076       0.0758        0.119       0.0974         0.12      0.00125\n","     24     2       0.0138       0.0138     1.82e-06       0.0684        0.091        0.056       0.0932       0.0746       0.0741        0.118       0.0959        0.089     0.000927\n","     24     3       0.0122       0.0122     2.69e-06        0.065       0.0853       0.0535       0.0879       0.0707       0.0692        0.111         0.09        0.107      0.00112\n","     24     4       0.0132       0.0132     2.43e-06       0.0672       0.0889       0.0564       0.0889       0.0726       0.0733        0.114       0.0936          0.1      0.00104\n","     24     5       0.0128       0.0128     1.86e-06       0.0662       0.0875       0.0545       0.0897       0.0721       0.0709        0.114       0.0922       0.0853     0.000888\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              24  100.693    0.005       0.0136     2.61e-05       0.0136       0.0691       0.0902       0.0583       0.0907       0.0745       0.0751        0.114       0.0948        0.322      0.00336\n","! Validation         24  100.693    0.005       0.0133     2.54e-06       0.0133       0.0674       0.0891       0.0557       0.0907       0.0732       0.0727        0.115       0.0939          0.1      0.00105\n","Wall time: 100.6934081129998\n","! Best model       24    0.013\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     25     1       0.0149       0.0149     4.65e-05       0.0715       0.0943       0.0597       0.0951       0.0774       0.0789        0.119       0.0991        0.498      0.00519\n","     25     2       0.0138       0.0138     7.03e-06       0.0695        0.091       0.0571       0.0941       0.0756       0.0722         0.12       0.0961        0.139      0.00145\n","     25     3       0.0132       0.0132     2.47e-06       0.0688       0.0888       0.0596       0.0871       0.0733       0.0746        0.112       0.0933       0.0814     0.000848\n","     25     4       0.0119       0.0119     5.14e-05       0.0658       0.0843       0.0561       0.0851       0.0706       0.0712        0.106       0.0884        0.503      0.00524\n","     25     5       0.0109       0.0109     1.27e-06       0.0606       0.0807       0.0503       0.0813       0.0658       0.0659        0.104        0.085       0.0715     0.000745\n","     25     6        0.012       0.0118     0.000191        0.065       0.0841       0.0558       0.0835       0.0696       0.0713        0.105       0.0881         1.02       0.0107\n","     25     7       0.0135       0.0134     6.69e-05       0.0656       0.0897       0.0523       0.0921       0.0722       0.0722        0.117       0.0947        0.607      0.00632\n","     25     8       0.0117       0.0116     3.87e-05        0.064       0.0835       0.0538       0.0843        0.069       0.0685        0.107       0.0879        0.454      0.00473\n","     25     9       0.0111        0.011     9.98e-05       0.0627       0.0813        0.052       0.0839        0.068       0.0661        0.105       0.0857        0.741      0.00772\n","     25    10       0.0121        0.012     7.65e-05       0.0653       0.0849       0.0544       0.0871       0.0708       0.0691         0.11       0.0895        0.644      0.00671\n","     25    11       0.0127       0.0127     1.97e-05       0.0676       0.0872       0.0587       0.0854        0.072       0.0742        0.109       0.0914        0.327      0.00341\n","     25    12       0.0121        0.012     6.87e-05        0.065       0.0848       0.0537       0.0876       0.0707       0.0684         0.11       0.0895        0.612      0.00637\n","     25    13       0.0134       0.0134     6.18e-07       0.0678       0.0896       0.0568       0.0899       0.0733       0.0749        0.113       0.0942       0.0506     0.000527\n","     25    14       0.0114       0.0114     2.29e-05       0.0632       0.0824       0.0536       0.0823        0.068       0.0683        0.105       0.0867         0.34      0.00354\n","     25    15       0.0119       0.0119     1.41e-05       0.0644       0.0845       0.0538       0.0856       0.0697       0.0701        0.108       0.0889        0.255      0.00266\n","     25    16       0.0116       0.0116     2.08e-05       0.0619       0.0834       0.0491       0.0876       0.0683       0.0652        0.111       0.0882        0.322      0.00336\n","     25    17       0.0124       0.0124      4.8e-05       0.0653       0.0862       0.0528       0.0903       0.0716       0.0674        0.115       0.0911        0.504      0.00524\n","     25    18        0.012        0.012     2.19e-06       0.0643       0.0846       0.0525       0.0879       0.0702       0.0687         0.11       0.0892       0.0914     0.000952\n","     25    19       0.0125       0.0125     2.89e-05       0.0661       0.0865        0.054       0.0905       0.0722       0.0711        0.111       0.0911         0.39      0.00406\n","     25    20       0.0106       0.0106     9.61e-06       0.0616       0.0795       0.0502       0.0842       0.0672       0.0628        0.105        0.084        0.229      0.00238\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     25     1       0.0138       0.0138     3.64e-06       0.0689        0.091       0.0569       0.0928       0.0749       0.0745        0.117       0.0959        0.116      0.00121\n","     25     2       0.0134       0.0134     1.66e-06       0.0672       0.0895       0.0549       0.0919       0.0734       0.0726        0.116       0.0944       0.0849     0.000884\n","     25     3       0.0118       0.0118     2.48e-06        0.064       0.0841       0.0526       0.0867       0.0697        0.068        0.109       0.0887        0.104      0.00108\n","     25     4       0.0128       0.0128     1.99e-06       0.0662       0.0877       0.0553       0.0878       0.0716       0.0719        0.113       0.0923        0.092     0.000958\n","     25     5       0.0124       0.0124     1.86e-06       0.0652       0.0861       0.0536       0.0883       0.0709       0.0698        0.112       0.0908       0.0847     0.000882\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              25  104.729    0.005       0.0123     4.08e-05       0.0123       0.0653       0.0856       0.0543       0.0873       0.0708       0.0702         0.11       0.0902        0.394      0.00411\n","! Validation         25  104.729    0.005       0.0129     2.33e-06       0.0129       0.0663       0.0877       0.0547       0.0895       0.0721       0.0714        0.114       0.0925       0.0962        0.001\n","Wall time: 104.73008567299985\n","! Best model       25    0.013\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     26     1       0.0135       0.0135      1.2e-05        0.068       0.0899        0.056       0.0918       0.0739       0.0738        0.115       0.0946        0.186      0.00194\n","     26     2       0.0127       0.0127      5.4e-06       0.0674       0.0871       0.0594       0.0835       0.0715       0.0757        0.106        0.091        0.154       0.0016\n","     26     3       0.0121       0.0121     7.85e-06       0.0658        0.085       0.0557       0.0861       0.0709       0.0702        0.109       0.0895        0.202       0.0021\n","     26     4       0.0124       0.0124     1.57e-05       0.0654       0.0862       0.0536        0.089       0.0713       0.0702        0.112       0.0909        0.284      0.00296\n","     26     5         0.01      0.00999     2.19e-05       0.0587       0.0773       0.0469       0.0824       0.0646       0.0606        0.103       0.0818        0.343      0.00358\n","     26     6       0.0103       0.0103      1.9e-06         0.06       0.0786       0.0482       0.0836       0.0659        0.062        0.104       0.0831       0.0844     0.000879\n","     26     7       0.0119       0.0119     1.72e-05        0.063       0.0843       0.0516       0.0859       0.0687       0.0682         0.11       0.0889        0.305      0.00318\n","     26     8       0.0126       0.0126     4.08e-05       0.0665       0.0867       0.0566       0.0864       0.0715       0.0716        0.111       0.0913        0.469      0.00489\n","     26     9       0.0137       0.0137      2.3e-06       0.0695       0.0904       0.0597       0.0891       0.0744       0.0762        0.114       0.0949       0.0947     0.000987\n","     26    10        0.014       0.0139     1.79e-05        0.069       0.0914        0.057       0.0931       0.0751        0.075        0.117       0.0962        0.301      0.00314\n","     26    11       0.0109       0.0109     1.99e-05       0.0612       0.0807       0.0501       0.0835       0.0668       0.0642        0.106       0.0852        0.329      0.00343\n","     26    12       0.0105       0.0105     1.02e-05       0.0617       0.0794       0.0518       0.0816       0.0667       0.0654        0.102       0.0836        0.205      0.00214\n","     26    13       0.0135       0.0135     5.16e-06       0.0679       0.0898       0.0564        0.091       0.0737       0.0721        0.118       0.0948        0.157      0.00163\n","     26    14       0.0169       0.0168     1.92e-05       0.0787          0.1        0.071        0.094       0.0825       0.0901        0.118        0.104        0.315      0.00328\n","     26    15        0.016        0.016     2.25e-06       0.0771       0.0979       0.0714       0.0886         0.08       0.0896        0.113        0.101       0.0871     0.000907\n","     26    16       0.0116       0.0116     1.25e-05       0.0641       0.0832       0.0539       0.0845       0.0692       0.0686        0.107       0.0876        0.258      0.00268\n","     26    17       0.0117       0.0117      4.4e-05       0.0642       0.0837       0.0545       0.0836       0.0691       0.0706        0.105       0.0879        0.482      0.00502\n","     26    18       0.0167       0.0166     1.32e-05       0.0772       0.0998       0.0675       0.0966       0.0821        0.088         0.12        0.104        0.263      0.00274\n","     26    19        0.019       0.0188     0.000183       0.0841        0.106       0.0767       0.0989       0.0878       0.0952        0.125         0.11            1       0.0104\n","     26    20       0.0188       0.0188     7.82e-05       0.0834        0.106       0.0716        0.107       0.0893       0.0891        0.133        0.111        0.652      0.00679\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     26     1       0.0134       0.0134     3.45e-06       0.0678       0.0896        0.056       0.0915       0.0737       0.0732        0.116       0.0944        0.111      0.00115\n","     26     2        0.013        0.013     1.61e-06       0.0662       0.0881       0.0539       0.0907       0.0723       0.0713        0.115        0.093       0.0838     0.000873\n","     26     3       0.0115       0.0115     2.22e-06        0.063       0.0829       0.0517       0.0855       0.0686       0.0668        0.108       0.0875       0.0973      0.00101\n","     26     4       0.0125       0.0125     1.65e-06       0.0652       0.0864       0.0544       0.0868       0.0706       0.0707        0.111       0.0911        0.084     0.000875\n","     26     5        0.012        0.012     1.86e-06       0.0641       0.0849       0.0527       0.0869       0.0698       0.0688         0.11       0.0895       0.0841     0.000876\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              26  108.781    0.005       0.0134     2.65e-05       0.0134       0.0687       0.0896       0.0585        0.089       0.0737       0.0755        0.113       0.0941        0.309      0.00322\n","! Validation         26  108.781    0.005       0.0125     2.16e-06       0.0125       0.0653       0.0864       0.0537       0.0883        0.071       0.0702        0.112       0.0911        0.092     0.000958\n","Wall time: 108.78158092599983\n","! Best model       26    0.012\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     27     1       0.0142       0.0142     7.22e-06        0.071       0.0921       0.0604       0.0922       0.0763       0.0759        0.118       0.0969        0.195      0.00203\n","     27     2       0.0112        0.011     0.000204       0.0606        0.081       0.0507       0.0802       0.0655       0.0653        0.106       0.0855         1.06       0.0111\n","     27     3        0.014       0.0138     0.000217       0.0701       0.0909       0.0583       0.0937        0.076       0.0743        0.117       0.0957         1.09       0.0113\n","     27     4       0.0225       0.0225     5.84e-05       0.0927        0.116       0.0865        0.105       0.0957        0.109         0.13        0.119        0.563      0.00587\n","     27     5       0.0166       0.0166      1.8e-05       0.0775       0.0997       0.0696       0.0933       0.0815       0.0881         0.12        0.104        0.303      0.00316\n","     27     6       0.0142       0.0142     3.24e-05        0.068       0.0921       0.0542       0.0957       0.0749       0.0746         0.12       0.0971        0.421      0.00439\n","     27     7        0.014        0.014     2.22e-06       0.0716       0.0917       0.0636       0.0877       0.0756       0.0807         0.11       0.0955       0.0977      0.00102\n","     27     8       0.0193       0.0192     5.28e-05        0.085        0.107       0.0755        0.104       0.0898       0.0929        0.132        0.112        0.529      0.00551\n","     27     9       0.0171       0.0171     1.45e-05        0.078        0.101       0.0674       0.0993       0.0833       0.0855        0.126        0.106        0.238      0.00248\n","     27    10        0.011        0.011     4.07e-06       0.0615        0.081       0.0509       0.0829       0.0669       0.0659        0.105       0.0854        0.114      0.00119\n","     27    11       0.0174       0.0174     4.06e-05       0.0805        0.102       0.0714       0.0987        0.085       0.0897        0.123        0.106        0.461       0.0048\n","     27    12       0.0151       0.0151     1.78e-06       0.0737       0.0951       0.0662       0.0887       0.0775        0.085        0.113       0.0988       0.0896     0.000934\n","     27    13       0.0116       0.0115     2.39e-05       0.0642       0.0831       0.0519       0.0889       0.0704       0.0658         0.11       0.0878        0.346      0.00361\n","     27    14       0.0176       0.0175     4.05e-05       0.0806        0.102       0.0748       0.0922       0.0835       0.0941        0.117        0.106        0.465      0.00485\n","     27    15       0.0177       0.0176     6.27e-05       0.0808        0.103       0.0732       0.0959       0.0845       0.0909        0.123        0.107        0.584      0.00609\n","     27    16       0.0112       0.0111     4.76e-05       0.0618       0.0817       0.0517        0.082       0.0669        0.067        0.105        0.086        0.507      0.00528\n","     27    17       0.0117       0.0117     8.84e-06       0.0656       0.0838       0.0567       0.0834       0.0701       0.0721        0.103       0.0877        0.214      0.00223\n","     27    18       0.0155       0.0155     7.39e-06       0.0766       0.0963        0.065       0.0997       0.0824       0.0803        0.122        0.101        0.194      0.00202\n","     27    19       0.0143       0.0143     2.24e-05       0.0704       0.0924         0.06       0.0911       0.0756       0.0792        0.114       0.0968        0.351      0.00366\n","     27    20       0.0138       0.0138     4.44e-05        0.069       0.0908       0.0565       0.0939       0.0752       0.0739        0.117       0.0957        0.479      0.00499\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     27     1       0.0131       0.0131      3.4e-06        0.067       0.0885       0.0552       0.0906       0.0729       0.0721        0.115       0.0933        0.113      0.00117\n","     27     2       0.0127       0.0127     1.41e-06       0.0653       0.0871       0.0531       0.0896       0.0714       0.0703        0.113       0.0919       0.0792     0.000825\n","     27     3       0.0112       0.0112     2.18e-06       0.0622       0.0819       0.0511       0.0845       0.0678       0.0659        0.107       0.0864       0.0979      0.00102\n","     27     4       0.0122       0.0122     1.76e-06       0.0645       0.0855       0.0536       0.0862       0.0699       0.0697        0.111       0.0902       0.0867     0.000903\n","     27     5       0.0118       0.0118     1.82e-06       0.0633       0.0839        0.052       0.0857       0.0689        0.068        0.109       0.0884       0.0809     0.000842\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              27  112.761    0.005        0.015     4.56e-05        0.015        0.073       0.0946       0.0632       0.0924       0.0778       0.0813        0.117        0.099        0.415      0.00432\n","! Validation         27  112.761    0.005       0.0122     2.12e-06       0.0122       0.0644       0.0854        0.053       0.0873       0.0702       0.0692        0.111       0.0901       0.0914     0.000953\n","Wall time: 112.76203040799987\n","! Best model       27    0.012\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     28     1       0.0164       0.0164     1.45e-06       0.0751        0.099       0.0632       0.0988        0.081       0.0837        0.124        0.104       0.0814     0.000848\n","     28     2       0.0136       0.0135     4.06e-05       0.0688       0.0899        0.058       0.0902       0.0741       0.0758        0.113       0.0944        0.452       0.0047\n","     28     3       0.0126       0.0126      1.2e-06       0.0655       0.0868       0.0555       0.0855       0.0705       0.0726         0.11       0.0912       0.0668     0.000696\n","     28     4        0.015       0.0149     4.64e-05       0.0738       0.0945       0.0637       0.0941       0.0789       0.0809        0.117        0.099        0.491      0.00512\n","     28     5       0.0158       0.0158     1.83e-05       0.0758       0.0973       0.0652        0.097       0.0811       0.0824        0.122        0.102        0.309      0.00322\n","     28     6        0.013        0.013     7.82e-06        0.067       0.0882       0.0559       0.0891       0.0725       0.0723        0.113       0.0929        0.188      0.00195\n","     28     7       0.0117       0.0116     3.44e-05        0.065       0.0835       0.0539       0.0874       0.0706       0.0684        0.108        0.088        0.426      0.00444\n","     28     8       0.0134       0.0133     7.98e-05       0.0687       0.0893       0.0575       0.0911       0.0743        0.073        0.115       0.0941        0.655      0.00683\n","     28     9       0.0123       0.0123     1.01e-06       0.0657       0.0858       0.0525        0.092       0.0723       0.0664        0.115       0.0907       0.0639     0.000665\n","     28    10       0.0103       0.0103     2.51e-05        0.059       0.0785       0.0492       0.0787       0.0639       0.0649          0.1       0.0827         0.36      0.00375\n","     28    11       0.0108       0.0108     1.16e-05       0.0611       0.0805       0.0519       0.0795       0.0657       0.0676        0.101       0.0845        0.233      0.00243\n","     28    12        0.012        0.012     2.03e-06       0.0654       0.0848       0.0544       0.0874       0.0709       0.0682        0.111       0.0895       0.0879     0.000916\n","     28    13       0.0101       0.0101     2.43e-05       0.0589       0.0777       0.0492       0.0784       0.0638        0.064       0.0997       0.0818        0.357      0.00372\n","     28    14      0.00913      0.00912     6.93e-06       0.0564       0.0739       0.0461       0.0769       0.0615       0.0585       0.0976       0.0781        0.186      0.00194\n","     28    15       0.0114       0.0113     7.58e-05       0.0634       0.0823        0.052       0.0861       0.0691       0.0671        0.106       0.0868        0.641      0.00668\n","     28    16       0.0113       0.0113     5.93e-06       0.0628       0.0824       0.0529       0.0827       0.0678       0.0681        0.105       0.0867        0.163       0.0017\n","     28    17      0.00974      0.00968     5.89e-05       0.0586       0.0761       0.0489       0.0781       0.0635       0.0627       0.0976       0.0801        0.564      0.00588\n","     28    18       0.0128       0.0127     7.74e-05       0.0669       0.0872       0.0559       0.0889       0.0724       0.0719        0.112       0.0918        0.653       0.0068\n","     28    19       0.0137       0.0137     1.81e-06       0.0683       0.0905       0.0571       0.0906       0.0738       0.0751        0.115       0.0952        0.082     0.000854\n","     28    20       0.0111       0.0109     0.000152       0.0619       0.0808       0.0513       0.0831       0.0672        0.066        0.104       0.0852        0.913      0.00951\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     28     1       0.0128       0.0128     3.04e-06       0.0662       0.0875       0.0545       0.0897       0.0721       0.0712        0.113       0.0922        0.102      0.00106\n","     28     2       0.0124       0.0124     1.67e-06       0.0646       0.0861       0.0525       0.0888       0.0707       0.0693        0.113       0.0909       0.0832     0.000867\n","     28     3        0.011        0.011     2.09e-06       0.0615       0.0811       0.0505       0.0837       0.0671       0.0651        0.106       0.0856       0.0918     0.000956\n","     28     4        0.012        0.012     1.35e-06       0.0638       0.0847       0.0529       0.0854       0.0692       0.0688         0.11       0.0893       0.0774     0.000807\n","     28     5       0.0115       0.0115     1.98e-06       0.0625       0.0829       0.0514       0.0847       0.0681       0.0672        0.108       0.0874       0.0854      0.00089\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              28  116.743    0.005       0.0123     3.36e-05       0.0123       0.0654       0.0857       0.0547       0.0868       0.0707       0.0708         0.11       0.0902        0.349      0.00363\n","! Validation         28  116.743    0.005       0.0119     2.03e-06       0.0119       0.0637       0.0845       0.0524       0.0865       0.0694       0.0684         0.11       0.0891        0.088     0.000917\n","Wall time: 116.74379447199999\n","! Best model       28    0.012\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     29     1       0.0108       0.0108     6.83e-05       0.0611       0.0802       0.0506        0.082       0.0663       0.0655        0.104       0.0845        0.612      0.00637\n","     29     2       0.0139       0.0139     3.48e-06       0.0709       0.0913       0.0607       0.0912       0.0759       0.0774        0.114       0.0958        0.118      0.00122\n","     29     3       0.0117       0.0117     3.33e-05       0.0633       0.0835       0.0529       0.0839       0.0684       0.0686        0.107        0.088        0.413      0.00431\n","     29     4       0.0098      0.00969     0.000109       0.0582       0.0762       0.0474       0.0798       0.0636       0.0611       0.0996       0.0804         0.77      0.00802\n","     29     5       0.0124       0.0123     2.86e-05       0.0662       0.0859       0.0552       0.0881       0.0717       0.0687        0.113       0.0907        0.371      0.00387\n","     29     6       0.0118       0.0118        6e-05       0.0636        0.084        0.053       0.0846       0.0688       0.0685        0.109       0.0885        0.558      0.00582\n","     29     7       0.0094      0.00933     7.19e-05        0.057       0.0747       0.0463       0.0783       0.0623        0.059        0.099        0.079        0.627      0.00653\n","     29     8       0.0103       0.0103     1.04e-06       0.0602       0.0786       0.0491       0.0823       0.0657       0.0625        0.104        0.083       0.0535     0.000557\n","     29     9       0.0146       0.0146     1.77e-05       0.0717       0.0934       0.0614       0.0923       0.0768       0.0784        0.118       0.0981        0.289      0.00301\n","     29    10       0.0131       0.0131     1.65e-05       0.0665       0.0886       0.0554       0.0886        0.072       0.0728        0.114       0.0933        0.298       0.0031\n","     29    11       0.0128       0.0128     5.34e-06       0.0658       0.0876       0.0542       0.0889       0.0716       0.0735        0.111        0.092         0.16      0.00166\n","     29    12        0.015        0.015     4.74e-05       0.0732       0.0947        0.067       0.0857       0.0763       0.0867        0.109       0.0978         0.51      0.00531\n","     29    13       0.0118       0.0118     2.62e-06       0.0663       0.0839       0.0583       0.0821       0.0702       0.0722        0.103       0.0878       0.0832     0.000867\n","     29    14      0.00959      0.00958     9.51e-06       0.0575       0.0757       0.0477       0.0772       0.0624       0.0606       0.0992       0.0799        0.207      0.00215\n","     29    15       0.0167       0.0167     3.61e-05        0.078          0.1       0.0673       0.0993       0.0833       0.0854        0.124        0.105         0.44      0.00459\n","     29    16       0.0182       0.0182     7.25e-05       0.0812        0.104       0.0718       0.0999       0.0859       0.0911        0.127        0.109        0.629      0.00656\n","     29    17       0.0123       0.0122     4.94e-05       0.0651       0.0855       0.0547       0.0861       0.0704       0.0707        0.109       0.0899        0.516      0.00538\n","     29    18       0.0109       0.0108     0.000113       0.0609       0.0804       0.0493        0.084       0.0667       0.0637        0.106        0.085        0.781      0.00813\n","     29    19        0.013        0.013     8.73e-07       0.0699       0.0883        0.064       0.0819       0.0729       0.0795        0.104       0.0916       0.0658     0.000686\n","     29    20       0.0137       0.0137     2.07e-05       0.0694       0.0905       0.0587       0.0906       0.0747       0.0744        0.116       0.0953        0.335      0.00349\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     29     1       0.0125       0.0125     3.05e-06       0.0655       0.0865       0.0538       0.0888       0.0713       0.0704        0.112       0.0912        0.105      0.00109\n","     29     2       0.0121       0.0121     1.33e-06       0.0639       0.0852       0.0518        0.088       0.0699       0.0683        0.111       0.0899       0.0749      0.00078\n","     29     3       0.0108       0.0108     2.08e-06       0.0609       0.0803       0.0499       0.0829       0.0664       0.0643        0.105       0.0847       0.0934     0.000972\n","     29     4       0.0117       0.0117     1.47e-06       0.0631       0.0838       0.0523       0.0847       0.0685       0.0679        0.109       0.0884        0.079     0.000823\n","     29     5       0.0112       0.0112     1.85e-06       0.0618        0.082       0.0508       0.0838       0.0673       0.0664        0.107       0.0865       0.0811     0.000844\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              29  120.731    0.005       0.0126     3.84e-05       0.0126       0.0663       0.0867       0.0563       0.0863       0.0713       0.0725         0.11       0.0911        0.392      0.00408\n","! Validation         29  120.731    0.005       0.0117     1.96e-06       0.0117        0.063       0.0836       0.0517       0.0856       0.0687       0.0675        0.109       0.0882       0.0867     0.000903\n","Wall time: 120.73183489499979\n","! Best model       29    0.012\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     30     1       0.0129       0.0128     6.72e-05       0.0671       0.0876        0.055       0.0913       0.0732       0.0709        0.114       0.0924        0.605       0.0063\n","     30     2       0.0112       0.0112     7.55e-06        0.063       0.0817       0.0551       0.0786       0.0669       0.0698        0.101       0.0856        0.189      0.00197\n","     30     3       0.0136       0.0136     4.14e-05       0.0697       0.0902       0.0592       0.0907       0.0749       0.0738        0.116       0.0951        0.473      0.00493\n","     30     4       0.0128       0.0128     1.33e-05       0.0653       0.0876       0.0523       0.0914       0.0719       0.0697        0.115       0.0926        0.255      0.00266\n","     30     5      0.00994      0.00994     2.02e-07       0.0585       0.0771       0.0475       0.0803       0.0639       0.0612        0.102       0.0815       0.0301     0.000313\n","     30     6       0.0108       0.0108     1.17e-05       0.0618       0.0804       0.0518       0.0819       0.0668       0.0668        0.102       0.0845        0.243      0.00253\n","     30     7       0.0094      0.00939     2.81e-06       0.0579        0.075       0.0487       0.0762       0.0625       0.0612       0.0969        0.079       0.0918     0.000956\n","     30     8        0.011        0.011     2.24e-06       0.0623       0.0812       0.0535       0.0798       0.0667       0.0687        0.102       0.0852       0.0938     0.000977\n","     30     9       0.0116       0.0116      1.3e-06       0.0636       0.0834       0.0521       0.0865       0.0693       0.0673        0.109        0.088       0.0682      0.00071\n","     30    10       0.0118       0.0118     5.41e-05       0.0624        0.084       0.0508       0.0857       0.0683        0.067         0.11       0.0887        0.545      0.00568\n","     30    11       0.0123       0.0122     4.02e-05       0.0653       0.0855       0.0538       0.0883       0.0711       0.0697        0.111       0.0901        0.468      0.00488\n","     30    12       0.0113       0.0112     3.84e-06       0.0619        0.082       0.0515       0.0827       0.0671       0.0678        0.105       0.0863        0.127      0.00133\n","     30    13       0.0106       0.0106     1.85e-05       0.0607       0.0795        0.049       0.0841       0.0666        0.063        0.105        0.084        0.287      0.00298\n","     30    14       0.0102       0.0101     6.97e-05       0.0598       0.0778       0.0512       0.0769       0.0641       0.0661       0.0971       0.0816        0.619      0.00645\n","     30    15       0.0111       0.0111     5.75e-06       0.0623       0.0815       0.0519       0.0831       0.0675        0.067        0.105       0.0858        0.153       0.0016\n","     30    16       0.0121       0.0121     7.73e-07       0.0649       0.0853       0.0562       0.0823       0.0693       0.0722        0.107       0.0895       0.0584     0.000608\n","     30    17       0.0162       0.0162      1.3e-06       0.0783       0.0985       0.0688       0.0973       0.0831       0.0848        0.121        0.103       0.0707     0.000736\n","     30    18       0.0146       0.0145     5.69e-05       0.0716       0.0932       0.0596       0.0956       0.0776       0.0749        0.122       0.0983        0.558      0.00582\n","     30    19       0.0105       0.0105     1.08e-05       0.0604       0.0792       0.0493       0.0826       0.0659       0.0637        0.104       0.0836        0.229      0.00239\n","     30    20       0.0138       0.0138     7.05e-06       0.0706       0.0908       0.0592       0.0934       0.0763       0.0745        0.117       0.0956        0.154       0.0016\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     30     1       0.0122       0.0122        3e-06       0.0648       0.0856       0.0531        0.088       0.0706       0.0695        0.111       0.0902        0.105      0.00109\n","     30     2       0.0119       0.0119     1.27e-06       0.0633       0.0843       0.0513       0.0872       0.0693       0.0675        0.111        0.089        0.072      0.00075\n","     30     3       0.0106       0.0106     2.01e-06       0.0603       0.0796       0.0493       0.0822       0.0658       0.0636        0.104        0.084       0.0917     0.000955\n","     30     4       0.0115       0.0115     1.38e-06       0.0624       0.0831       0.0516       0.0841       0.0679       0.0671        0.108       0.0876       0.0772     0.000805\n","     30     5        0.011        0.011     1.86e-06       0.0611       0.0812       0.0503       0.0828       0.0665       0.0658        0.105       0.0856       0.0815     0.000849\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              30  124.734    0.005       0.0119     2.08e-05       0.0119       0.0644       0.0843       0.0538       0.0854       0.0696       0.0692        0.108       0.0887        0.266      0.00277\n","! Validation         30  124.734    0.005       0.0114     1.91e-06       0.0114       0.0624       0.0828       0.0511       0.0849        0.068       0.0667        0.108       0.0873       0.0855      0.00089\n","Wall time: 124.73489451499995\n","! Best model       30    0.011\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     31     1       0.0142       0.0142     4.35e-06       0.0713       0.0922       0.0627       0.0885       0.0756       0.0796        0.113       0.0964         0.15      0.00156\n","     31     2       0.0122       0.0122     2.31e-05       0.0662       0.0854       0.0571       0.0843       0.0707       0.0736        0.105       0.0893        0.348      0.00362\n","     31     3       0.0102       0.0101     3.52e-05       0.0588       0.0778       0.0485       0.0793       0.0639       0.0623        0.102       0.0821        0.438      0.00456\n","     31     4       0.0112       0.0112     1.14e-05        0.063       0.0818       0.0549       0.0793       0.0671       0.0702        0.101       0.0857        0.245      0.00256\n","     31     5       0.0106       0.0106     8.68e-06       0.0612       0.0798       0.0499       0.0839       0.0669       0.0639        0.104       0.0842        0.182       0.0019\n","     31     6      0.00973      0.00973     3.55e-06       0.0594       0.0763       0.0505       0.0773       0.0639       0.0634       0.0971       0.0803        0.111      0.00115\n","     31     7       0.0121       0.0121      5.6e-05       0.0647        0.085       0.0539       0.0863       0.0701       0.0701        0.109       0.0895        0.554      0.00577\n","     31     8       0.0106       0.0105     4.12e-05       0.0607       0.0794       0.0485        0.085       0.0667       0.0622        0.106        0.084        0.462      0.00481\n","     31     9       0.0107       0.0106     3.45e-06       0.0607       0.0798       0.0497       0.0825       0.0661        0.064        0.105       0.0843        0.132      0.00138\n","     31    10       0.0121       0.0121     2.37e-06       0.0656       0.0852       0.0559        0.085       0.0705       0.0703        0.109       0.0897       0.0988      0.00103\n","     31    11         0.01      0.00998     3.92e-05        0.059       0.0773        0.049       0.0789        0.064       0.0625        0.101       0.0815        0.458      0.00477\n","     31    12       0.0119       0.0119      3.6e-05       0.0638       0.0844       0.0527       0.0859       0.0693       0.0695        0.108       0.0889        0.442      0.00461\n","     31    13       0.0108       0.0108     9.76e-07        0.061       0.0803       0.0509       0.0813       0.0661       0.0658        0.103       0.0846        0.067     0.000698\n","     31    14       0.0101       0.0101      9.4e-06       0.0575       0.0778       0.0448       0.0829       0.0638       0.0601        0.105       0.0823        0.203      0.00212\n","     31    15       0.0099      0.00988     1.78e-05       0.0581       0.0769       0.0466       0.0813       0.0639       0.0606        0.102       0.0813        0.307      0.00319\n","     31    16      0.00976      0.00976     2.83e-06       0.0575       0.0764       0.0476       0.0774       0.0625       0.0615       0.0998       0.0807        0.102      0.00106\n","     31    17       0.0107       0.0107     2.16e-05       0.0603       0.0801        0.049        0.083        0.066       0.0644        0.105       0.0845        0.337      0.00351\n","     31    18      0.00931      0.00931     4.61e-07       0.0572       0.0746       0.0475       0.0767       0.0621       0.0605       0.0969       0.0787       0.0357     0.000372\n","     31    19        0.011       0.0109     2.36e-05       0.0613        0.081       0.0499        0.084       0.0669       0.0646        0.106       0.0855        0.344      0.00358\n","     31    20         0.01         0.01     1.65e-06        0.059       0.0775        0.048       0.0811       0.0645       0.0615        0.102       0.0819       0.0701      0.00073\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     31     1        0.012        0.012     2.86e-06       0.0641       0.0847       0.0525       0.0873       0.0699       0.0686         0.11       0.0893          0.1      0.00104\n","     31     2       0.0116       0.0116     1.29e-06       0.0626       0.0835       0.0506       0.0865       0.0686       0.0666         0.11       0.0881        0.072      0.00075\n","     31     3       0.0104       0.0104     1.89e-06       0.0597       0.0789       0.0488       0.0816       0.0652       0.0629        0.104       0.0833       0.0878     0.000915\n","     31     4       0.0113       0.0113     1.23e-06       0.0618       0.0823        0.051       0.0835       0.0672       0.0662        0.107       0.0868       0.0744     0.000775\n","     31     5       0.0108       0.0108     1.92e-06       0.0605       0.0803       0.0497        0.082       0.0659       0.0651        0.104       0.0847       0.0799     0.000832\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              31  128.741    0.005       0.0108     1.71e-05       0.0109       0.0613       0.0806       0.0509       0.0822       0.0665       0.0657        0.104       0.0849        0.254      0.00265\n","! Validation         31  128.741    0.005       0.0112     1.84e-06       0.0112       0.0617       0.0819       0.0505       0.0842       0.0674       0.0659        0.107       0.0865       0.0829     0.000863\n","Wall time: 128.74157543499996\n","! Best model       31    0.011\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     32     1       0.0108       0.0108     1.22e-05       0.0609       0.0803       0.0504       0.0818       0.0661        0.067        0.102       0.0844        0.248      0.00259\n","     32     2      0.00862       0.0086      1.9e-05       0.0556       0.0718       0.0443       0.0783       0.0613       0.0558        0.096       0.0759        0.318      0.00331\n","     32     3        0.013        0.013     1.43e-06       0.0678       0.0882       0.0582        0.087       0.0726       0.0746        0.111       0.0926       0.0742     0.000773\n","     32     4       0.0112       0.0111     5.67e-05       0.0642       0.0816       0.0568        0.079       0.0679       0.0711       0.0994       0.0852        0.547       0.0057\n","     32     5      0.00981      0.00978     2.26e-05       0.0581       0.0765       0.0464       0.0814       0.0639       0.0592        0.103        0.081        0.352      0.00366\n","     32     6       0.0105       0.0105     5.35e-06        0.061       0.0793       0.0513       0.0805       0.0659       0.0651        0.102       0.0835        0.148      0.00154\n","     32     7       0.0128       0.0127      8.2e-05       0.0654       0.0873       0.0534       0.0893       0.0714       0.0708        0.113        0.092        0.669      0.00697\n","     32     8        0.011       0.0109     0.000125        0.062       0.0807       0.0513       0.0832       0.0673       0.0648        0.106       0.0852        0.829      0.00863\n","     32     9       0.0112       0.0111     2.62e-05       0.0625       0.0816       0.0526       0.0824       0.0675       0.0676        0.104       0.0859        0.379      0.00395\n","     32    10       0.0119       0.0117     0.000173        0.064       0.0836       0.0528       0.0864       0.0696       0.0674        0.109       0.0882        0.974       0.0101\n","     32    11       0.0122       0.0122     4.59e-05       0.0656       0.0854        0.056       0.0847       0.0704       0.0735        0.105       0.0894        0.493      0.00514\n","     32    12       0.0098       0.0098        3e-06       0.0586       0.0766       0.0475       0.0809       0.0642       0.0608        0.101       0.0809        0.112      0.00117\n","     32    13      0.00984      0.00976     7.63e-05       0.0583       0.0764       0.0506       0.0737       0.0622       0.0656       0.0945         0.08        0.643       0.0067\n","     32    14       0.0103       0.0103      3.9e-05       0.0605       0.0783       0.0509       0.0796       0.0652        0.064        0.101       0.0826        0.451       0.0047\n","     32    15       0.0107       0.0107      1.5e-05         0.06         0.08       0.0493       0.0814       0.0653       0.0648        0.104       0.0844        0.285      0.00297\n","     32    16      0.00999      0.00994     5.11e-05       0.0586       0.0771       0.0488       0.0781       0.0635       0.0617        0.101       0.0814         0.52      0.00541\n","     32    17       0.0114       0.0114     8.27e-06       0.0619       0.0825       0.0498       0.0859       0.0679       0.0641        0.111       0.0873        0.208      0.00216\n","     32    18        0.013        0.013      2.7e-05       0.0681       0.0881         0.06       0.0842       0.0721       0.0765        0.108       0.0921        0.382      0.00398\n","     32    19       0.0106       0.0105     3.87e-05       0.0614       0.0794       0.0527       0.0788       0.0658        0.068       0.0983       0.0831        0.448      0.00467\n","     32    20       0.0118       0.0118     9.74e-06       0.0627       0.0839       0.0508       0.0865       0.0686       0.0673         0.11       0.0886         0.22      0.00229\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     32     1       0.0117       0.0117      2.9e-06       0.0634       0.0838       0.0518       0.0865       0.0692       0.0678        0.109       0.0883        0.103      0.00107\n","     32     2       0.0114       0.0114     1.19e-06        0.062       0.0826       0.0501       0.0858       0.0679       0.0658        0.109       0.0873       0.0694     0.000723\n","     32     3       0.0102       0.0102     1.92e-06       0.0592       0.0782       0.0483        0.081       0.0646       0.0622        0.103       0.0826       0.0883      0.00092\n","     32     4       0.0111       0.0111     1.31e-06       0.0612       0.0815       0.0504       0.0829       0.0666       0.0655        0.107       0.0861       0.0765     0.000797\n","     32     5       0.0106       0.0106     1.84e-06       0.0599       0.0796       0.0492       0.0812       0.0652       0.0645        0.103       0.0839       0.0771     0.000803\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              32  132.740    0.005        0.011     4.19e-05        0.011       0.0619        0.081       0.0517       0.0822       0.0669       0.0667        0.104       0.0853        0.415      0.00432\n","! Validation         32  132.740    0.005        0.011     1.83e-06        0.011       0.0611       0.0812         0.05       0.0835       0.0667       0.0652        0.106       0.0857       0.0828     0.000863\n","Wall time: 132.74043337399985\n","! Best model       32    0.011\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     33     1       0.0105       0.0103     0.000174       0.0614       0.0785       0.0532       0.0778       0.0655       0.0671       0.0974       0.0823        0.979       0.0102\n","     33     2       0.0132       0.0132     2.64e-06       0.0668        0.089       0.0519       0.0964       0.0742       0.0661        0.123       0.0944         0.11      0.00115\n","     33     3       0.0109       0.0109     2.81e-07       0.0624       0.0807       0.0518       0.0835       0.0677       0.0641        0.106       0.0852        0.035     0.000364\n","     33     4      0.00956       0.0095     6.49e-05       0.0577       0.0754       0.0477       0.0776       0.0626       0.0604       0.0988       0.0796        0.594      0.00619\n","     33     5       0.0134       0.0134     1.82e-05       0.0672       0.0896       0.0533        0.095       0.0742       0.0694         0.12       0.0948        0.311      0.00324\n","     33     6        0.017       0.0169     5.62e-05       0.0795        0.101       0.0726       0.0933       0.0829       0.0907        0.118        0.104        0.555      0.00578\n","     33     7       0.0148       0.0148     8.63e-06        0.074        0.094       0.0676       0.0868       0.0772       0.0845         0.11       0.0975         0.21      0.00219\n","     33     8         0.01         0.01     4.87e-06       0.0591       0.0774       0.0485       0.0804       0.0644       0.0617        0.102       0.0818         0.15      0.00157\n","     33     9       0.0127       0.0127     3.49e-06       0.0676       0.0873       0.0596       0.0835       0.0715       0.0755        0.107       0.0913        0.137      0.00142\n","     33    10       0.0144       0.0144     1.06e-05       0.0709       0.0928       0.0592       0.0944       0.0768       0.0761        0.119       0.0977        0.232      0.00242\n","     33    11       0.0113       0.0112     5.86e-05       0.0613       0.0819       0.0498       0.0842        0.067       0.0642        0.109       0.0866        0.567      0.00591\n","     33    12        0.011        0.011     2.38e-05       0.0617       0.0811       0.0486       0.0878       0.0682       0.0628        0.109       0.0858        0.356      0.00371\n","     33    13       0.0132       0.0131     5.72e-05       0.0682       0.0886       0.0574       0.0899       0.0737       0.0739        0.112       0.0931         0.56      0.00583\n","     33    14       0.0123       0.0123     1.24e-05       0.0667       0.0857        0.059       0.0821       0.0706       0.0742        0.105       0.0896        0.237      0.00246\n","     33    15      0.00947      0.00947     2.86e-06       0.0575       0.0753       0.0478       0.0767       0.0623       0.0614       0.0973       0.0793        0.109      0.00114\n","     33    16      0.00965      0.00964     9.47e-06       0.0588        0.076       0.0498       0.0767       0.0633       0.0638       0.0958       0.0798        0.221      0.00231\n","     33    17       0.0111        0.011     2.37e-05       0.0621       0.0813       0.0502       0.0858        0.068       0.0662        0.105       0.0857        0.336       0.0035\n","     33    18       0.0113       0.0113     4.38e-06        0.062       0.0821       0.0507       0.0848       0.0677       0.0668        0.106       0.0866        0.133      0.00139\n","     33    19       0.0113       0.0113        6e-06       0.0631       0.0823       0.0525       0.0843       0.0684        0.068        0.105       0.0866        0.157      0.00164\n","     33    20      0.00992      0.00992     5.57e-07       0.0575        0.077       0.0477       0.0771       0.0624       0.0633        0.099       0.0811       0.0402     0.000419\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     33     1       0.0115       0.0115     2.82e-06       0.0628        0.083       0.0512       0.0859       0.0685        0.067        0.108       0.0875          0.1      0.00105\n","     33     2       0.0112       0.0112      1.2e-06       0.0614       0.0819       0.0496       0.0851       0.0673       0.0652        0.108       0.0865       0.0679     0.000707\n","     33     3         0.01         0.01     1.84e-06       0.0587       0.0775       0.0478       0.0804       0.0641       0.0616        0.102       0.0819       0.0861     0.000897\n","     33     4       0.0109       0.0109     1.23e-06       0.0607       0.0809       0.0498       0.0824       0.0661       0.0647        0.106       0.0854       0.0748     0.000779\n","     33     5       0.0104       0.0104     1.76e-06       0.0593       0.0789       0.0488       0.0805       0.0646       0.0639        0.103       0.0832       0.0757     0.000788\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              33  136.750    0.005       0.0118     2.72e-05       0.0118       0.0643       0.0841       0.0539       0.0849       0.0694       0.0695        0.108       0.0885        0.302      0.00314\n","! Validation         33  136.750    0.005       0.0108     1.77e-06       0.0108       0.0606       0.0805       0.0494       0.0828       0.0661       0.0645        0.105       0.0849        0.081     0.000844\n","Wall time: 136.7506914869998\n","! Best model       33    0.011\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     34     1       0.0102       0.0102     1.62e-06       0.0596       0.0782       0.0497       0.0794       0.0646        0.063        0.102       0.0825       0.0689     0.000718\n","     34     2       0.0124       0.0124     4.24e-06       0.0659        0.086       0.0546       0.0883       0.0715       0.0696        0.112       0.0907        0.144       0.0015\n","     34     3      0.00927      0.00927     1.19e-06       0.0583       0.0745       0.0498       0.0752       0.0625       0.0622       0.0944       0.0783       0.0551     0.000574\n","     34     4       0.0113       0.0113     6.63e-06       0.0618       0.0823       0.0502        0.085       0.0676       0.0669        0.107       0.0868        0.171      0.00179\n","     34     5      0.00994      0.00994     2.13e-06       0.0578       0.0771       0.0484       0.0767       0.0626       0.0644       0.0977       0.0811       0.0916     0.000954\n","     34     6       0.0102       0.0102     3.36e-05       0.0601       0.0781       0.0511       0.0781       0.0646       0.0654       0.0989       0.0821        0.429      0.00447\n","     34     7      0.00904      0.00904     4.73e-06       0.0555       0.0735       0.0443       0.0778       0.0611       0.0575       0.0981       0.0778        0.147      0.00153\n","     34     8      0.00976      0.00975     6.85e-06       0.0583       0.0764       0.0478       0.0793       0.0636       0.0619       0.0993       0.0806        0.182      0.00189\n","     34     9      0.00992      0.00991     1.35e-06       0.0587        0.077       0.0475       0.0811       0.0643       0.0607        0.102       0.0814       0.0826     0.000861\n","     34    10      0.00897      0.00896     7.99e-06        0.056       0.0732       0.0463       0.0755       0.0609       0.0593       0.0951       0.0772        0.189      0.00197\n","     34    11       0.0105       0.0105     6.15e-06       0.0606       0.0792       0.0511       0.0795       0.0653       0.0644        0.103       0.0835        0.165      0.00172\n","     34    12       0.0102       0.0102     3.06e-06       0.0592        0.078       0.0471       0.0833       0.0652       0.0615        0.103       0.0824       0.0969      0.00101\n","     34    13       0.0106       0.0106     4.57e-07       0.0598       0.0795       0.0484       0.0825       0.0654       0.0635        0.104        0.084       0.0441      0.00046\n","     34    14       0.0106       0.0106     1.04e-06        0.059       0.0798       0.0469       0.0832        0.065       0.0616        0.107       0.0844       0.0672       0.0007\n","     34    15      0.00892      0.00892     1.29e-06       0.0566       0.0731       0.0466       0.0765       0.0615       0.0593       0.0948        0.077        0.065     0.000677\n","     34    16      0.00966      0.00966     4.02e-06       0.0571        0.076       0.0451       0.0812       0.0631        0.058        0.103       0.0805        0.137      0.00142\n","     34    17       0.0104       0.0104     2.37e-05       0.0591        0.079       0.0486         0.08       0.0643       0.0618        0.105       0.0835        0.359      0.00374\n","     34    18       0.0121       0.0121     1.13e-06       0.0653        0.085       0.0561       0.0836       0.0699       0.0715        0.107       0.0893         0.05     0.000521\n","     34    19        0.012        0.012     3.35e-06       0.0646       0.0847       0.0546       0.0845       0.0696       0.0702        0.108       0.0891        0.115       0.0012\n","     34    20      0.00991      0.00986     4.62e-05       0.0587       0.0768       0.0479       0.0802       0.0641       0.0612        0.101       0.0811        0.504      0.00525\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     34     1       0.0113       0.0113     2.73e-06       0.0622       0.0822       0.0507       0.0852       0.0679       0.0662        0.107       0.0867       0.0987      0.00103\n","     34     2        0.011        0.011     1.19e-06       0.0608       0.0812       0.0491       0.0844       0.0667       0.0644        0.107       0.0857       0.0663     0.000691\n","     34     3      0.00989      0.00989     1.79e-06       0.0582       0.0769       0.0473         0.08       0.0636        0.061        0.102       0.0813       0.0833     0.000868\n","     34     4       0.0107       0.0107     1.14e-06       0.0601       0.0802       0.0493       0.0818       0.0656        0.064        0.105       0.0847       0.0719     0.000749\n","     34     5       0.0102       0.0102     1.84e-06       0.0588       0.0782       0.0483       0.0797        0.064       0.0633        0.102       0.0824       0.0758     0.000789\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              34  140.745    0.005       0.0103     8.04e-06       0.0103       0.0596       0.0785       0.0491       0.0805       0.0648       0.0633        0.102       0.0828        0.158      0.00165\n","! Validation         34  140.745    0.005       0.0106     1.74e-06       0.0106         0.06       0.0798       0.0489       0.0822       0.0656       0.0638        0.105       0.0842       0.0792     0.000825\n","Wall time: 140.74610045999998\n","! Best model       34    0.011\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     35     1      0.00981      0.00974     6.57e-05       0.0575       0.0764       0.0462       0.0803       0.0632       0.0604        0.101       0.0807        0.597      0.00622\n","     35     2      0.00996      0.00996     1.99e-06       0.0591       0.0772       0.0509       0.0756       0.0632       0.0649       0.0972       0.0811       0.0828     0.000863\n","     35     3       0.0103       0.0103     2.51e-05        0.059       0.0786       0.0477       0.0816       0.0646       0.0632        0.103       0.0829        0.369      0.00384\n","     35     4      0.00981      0.00975     5.25e-05       0.0578       0.0764       0.0476        0.078       0.0628       0.0619       0.0992       0.0806        0.532      0.00554\n","     35     5       0.0115       0.0115     8.56e-06       0.0628       0.0829       0.0512        0.086       0.0686       0.0664        0.109       0.0875        0.193      0.00201\n","     35     6        0.011       0.0109     7.96e-05       0.0613       0.0807       0.0504        0.083       0.0667       0.0647        0.106       0.0852         0.66      0.00688\n","     35     7       0.0109       0.0109     1.35e-06       0.0609       0.0809       0.0502       0.0823       0.0663       0.0651        0.106       0.0854       0.0756     0.000787\n","     35     8       0.0101       0.0101     1.53e-05       0.0583       0.0776       0.0471       0.0807       0.0639       0.0617        0.102        0.082        0.282      0.00294\n","     35     9       0.0103       0.0103     1.33e-06       0.0609       0.0786       0.0512       0.0801       0.0657       0.0653          0.1       0.0826       0.0834     0.000869\n","     35    10       0.0132       0.0132     2.17e-06       0.0683       0.0889       0.0577       0.0894       0.0736       0.0744        0.112       0.0934       0.0998      0.00104\n","     35    11       0.0132       0.0132      2.6e-05       0.0684       0.0888       0.0552       0.0949        0.075       0.0696        0.118       0.0939        0.363      0.00378\n","     35    12       0.0129       0.0129     1.39e-05       0.0674        0.088       0.0563       0.0895       0.0729       0.0716        0.114       0.0927        0.256      0.00266\n","     35    13       0.0108       0.0108      1.1e-05       0.0607       0.0803       0.0497       0.0828       0.0662       0.0645        0.105       0.0847         0.24       0.0025\n","     35    14       0.0082       0.0082     6.29e-06       0.0543         0.07       0.0444       0.0741       0.0593       0.0564       0.0914       0.0739        0.181      0.00189\n","     35    15      0.00997      0.00997     2.01e-07        0.058       0.0773       0.0476       0.0787       0.0632       0.0613        0.102       0.0816       0.0293     0.000305\n","     35    16       0.0107       0.0106     6.92e-05        0.062       0.0797       0.0523       0.0813       0.0668       0.0659        0.102       0.0839        0.611      0.00636\n","     35    17       0.0125       0.0125     1.86e-06       0.0665       0.0863       0.0586       0.0822       0.0704       0.0748        0.106       0.0902       0.0848     0.000883\n","     35    18      0.00934      0.00931     2.16e-05       0.0577       0.0747       0.0485       0.0761       0.0623       0.0615       0.0957       0.0786        0.342      0.00356\n","     35    19      0.00814      0.00808     5.72e-05       0.0535       0.0695       0.0449       0.0707       0.0578       0.0569       0.0897       0.0733        0.553      0.00576\n","     35    20       0.0108       0.0108     1.07e-05       0.0615       0.0802       0.0521       0.0802       0.0661       0.0666        0.102       0.0844         0.22      0.00229\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     35     1       0.0111       0.0111     2.78e-06       0.0616       0.0814       0.0502       0.0846       0.0674       0.0655        0.106       0.0859        0.101      0.00105\n","     35     2       0.0108       0.0108     1.12e-06       0.0603       0.0805       0.0486       0.0837       0.0662       0.0637        0.106        0.085       0.0653     0.000681\n","     35     3      0.00975      0.00975     1.71e-06       0.0577       0.0764       0.0469       0.0795       0.0632       0.0604        0.101       0.0807       0.0821     0.000856\n","     35     4       0.0106       0.0106      1.2e-06       0.0596       0.0796       0.0487       0.0813        0.065       0.0633        0.105        0.084       0.0736     0.000767\n","     35     5         0.01         0.01     1.77e-06       0.0583       0.0775       0.0479       0.0791       0.0635       0.0628        0.101       0.0818       0.0734     0.000765\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              35  144.748    0.005       0.0106     2.36e-05       0.0107       0.0608       0.0798       0.0505       0.0814       0.0659        0.065        0.103       0.0841        0.293      0.00305\n","! Validation         35  144.748    0.005       0.0105     1.72e-06       0.0105       0.0595       0.0791       0.0485       0.0816       0.0651       0.0632        0.104       0.0835       0.0792     0.000825\n","Wall time: 144.7490244789999\n","! Best model       35    0.010\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     36     1       0.0112       0.0111     0.000107       0.0634       0.0815       0.0531       0.0841       0.0686       0.0689        0.102       0.0856        0.766      0.00798\n","     36     2      0.00975      0.00969     6.11e-05       0.0589       0.0762       0.0502       0.0763       0.0633       0.0641       0.0958         0.08        0.578      0.00602\n","     36     3      0.00913      0.00913     2.34e-06       0.0564       0.0739        0.047       0.0752       0.0611       0.0599        0.096       0.0779        0.106       0.0011\n","     36     4       0.0125       0.0124     2.09e-05       0.0662       0.0863       0.0585       0.0817       0.0701       0.0756        0.104         0.09        0.329      0.00343\n","     36     5       0.0112       0.0112      2.1e-05       0.0628       0.0819       0.0544       0.0795       0.0669         0.07        0.102       0.0859         0.33      0.00344\n","     36     6      0.00946      0.00946     3.09e-06       0.0571       0.0752       0.0457       0.0799       0.0628       0.0591          0.1       0.0796       0.0877     0.000913\n","     36     7       0.0109       0.0109     3.67e-06       0.0621       0.0806       0.0524       0.0815        0.067       0.0681        0.101       0.0846        0.115       0.0012\n","     36     8       0.0106       0.0106     3.17e-05       0.0612       0.0797       0.0501       0.0835       0.0668       0.0641        0.104       0.0841         0.41      0.00427\n","     36     9      0.00897      0.00897     4.09e-06       0.0554       0.0733       0.0426       0.0808       0.0617       0.0537        0.102       0.0777        0.138      0.00143\n","     36    10      0.00885      0.00885     9.27e-07       0.0557       0.0728       0.0455       0.0761       0.0608       0.0572       0.0967        0.077        0.065     0.000677\n","     36    11      0.00848      0.00848     7.86e-07        0.054       0.0712       0.0439       0.0742       0.0591       0.0556       0.0951       0.0753       0.0494     0.000515\n","     36    12      0.00966      0.00965     7.37e-06       0.0575        0.076       0.0476       0.0773       0.0625       0.0623       0.0979       0.0801        0.195      0.00203\n","     36    13      0.00954      0.00954     1.14e-06       0.0576       0.0756       0.0468       0.0792        0.063       0.0588        0.101       0.0799       0.0592     0.000616\n","     36    14      0.00942      0.00942     3.25e-06       0.0562       0.0751       0.0448       0.0791       0.0619       0.0575        0.102       0.0795        0.105       0.0011\n","     36    15      0.00891      0.00889     1.27e-05       0.0558        0.073        0.046       0.0755       0.0607       0.0573        0.097       0.0771        0.251      0.00261\n","     36    16       0.0117       0.0117     2.61e-06       0.0627       0.0837        0.051       0.0859       0.0685       0.0677        0.109       0.0882        0.105       0.0011\n","     36    17       0.0118       0.0118      3.3e-05       0.0638        0.084       0.0533       0.0847        0.069       0.0692        0.108       0.0884        0.424      0.00441\n","     36    18       0.0103       0.0103     2.07e-05       0.0596       0.0785       0.0502       0.0784       0.0643       0.0643        0.101       0.0827         0.32      0.00333\n","     36    19      0.00891      0.00889     1.41e-05       0.0556        0.073       0.0459       0.0749       0.0604       0.0586       0.0953        0.077        0.272      0.00283\n","     36    20       0.0105       0.0105     2.74e-06       0.0607       0.0792       0.0522       0.0778        0.065       0.0671       0.0992       0.0831        0.102      0.00106\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     36     1       0.0109       0.0109     2.76e-06       0.0611       0.0807       0.0497        0.084       0.0668       0.0649        0.106       0.0852        0.102      0.00107\n","     36     2       0.0106       0.0106     1.01e-06       0.0598       0.0798       0.0481       0.0832       0.0656       0.0631        0.106       0.0843       0.0613     0.000639\n","     36     3      0.00961      0.00961     1.72e-06       0.0573       0.0758       0.0464        0.079       0.0627       0.0598          0.1       0.0801       0.0824     0.000859\n","     36     4       0.0104       0.0104     1.21e-06       0.0591       0.0789       0.0483       0.0808       0.0645       0.0627        0.104       0.0834       0.0722     0.000752\n","     36     5      0.00988      0.00988     1.72e-06       0.0579       0.0769       0.0475       0.0786        0.063       0.0622       0.0999       0.0811         0.07     0.000729\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              36  148.750    0.005       0.0101     1.77e-05       0.0101       0.0591       0.0777       0.0491       0.0793       0.0642       0.0632          0.1       0.0818         0.24       0.0025\n","! Validation         36  148.750    0.005       0.0103     1.68e-06       0.0103        0.059       0.0785        0.048       0.0811       0.0645       0.0626        0.103       0.0829       0.0777     0.000809\n","Wall time: 148.75110567899992\n","! Best model       36    0.010\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     37     1       0.0112       0.0112     3.76e-06       0.0632        0.082       0.0525       0.0847       0.0686       0.0665        0.106       0.0864        0.138      0.00144\n","     37     2       0.0107       0.0107     2.08e-05       0.0609       0.0801       0.0479       0.0871       0.0675       0.0618        0.108       0.0847        0.317       0.0033\n","     37     3       0.0102       0.0102     9.05e-06       0.0583       0.0783       0.0467       0.0816       0.0641       0.0612        0.104       0.0828        0.218      0.00227\n","     37     4       0.0105       0.0105     5.33e-06       0.0589       0.0794       0.0453        0.086       0.0656       0.0604        0.108       0.0841        0.166      0.00173\n","     37     5       0.0097      0.00965     4.82e-05       0.0571        0.076       0.0464       0.0785       0.0625       0.0606       0.0999       0.0802        0.511      0.00532\n","     37     6       0.0097      0.00968     2.29e-05       0.0574       0.0761       0.0471       0.0781       0.0626       0.0594        0.102       0.0805        0.352      0.00367\n","     37     7       0.0101       0.0101     4.77e-05       0.0585       0.0776       0.0474       0.0806        0.064       0.0604        0.104       0.0821        0.507      0.00529\n","     37     8      0.00961       0.0095     0.000103       0.0567       0.0754       0.0464       0.0774       0.0619       0.0611        0.098       0.0795        0.746      0.00778\n","     37     9       0.0101         0.01     2.89e-05       0.0591       0.0775       0.0501        0.077       0.0636       0.0636       0.0997       0.0816        0.398      0.00415\n","     37    10         0.01         0.01      2.4e-05        0.059       0.0774       0.0482       0.0807       0.0645        0.062        0.101       0.0817        0.359      0.00374\n","     37    11       0.0091      0.00895     0.000151       0.0561       0.0732       0.0457       0.0768       0.0613       0.0586       0.0958       0.0772        0.911      0.00948\n","     37    12       0.0101       0.0101     8.58e-06       0.0597       0.0776       0.0483       0.0825       0.0654       0.0626        0.101       0.0819        0.209      0.00218\n","     37    13      0.00877      0.00876     7.71e-06       0.0564       0.0724       0.0477       0.0738       0.0608       0.0592       0.0934       0.0763        0.172      0.00179\n","     37    14      0.00871      0.00869     1.86e-05        0.055       0.0721       0.0438       0.0773       0.0605       0.0558       0.0969       0.0763        0.307       0.0032\n","     37    15       0.0086      0.00858     1.04e-05       0.0548       0.0717       0.0447        0.075       0.0599       0.0575       0.0939       0.0757        0.225      0.00234\n","     37    16       0.0107       0.0107     9.15e-06       0.0605         0.08       0.0483        0.085       0.0666        0.063        0.106       0.0846        0.204      0.00213\n","     37    17      0.00855      0.00855        6e-06       0.0542       0.0715       0.0455       0.0718       0.0586       0.0591       0.0915       0.0753        0.157      0.00164\n","     37    18      0.00855      0.00854     1.36e-05       0.0546       0.0715       0.0442       0.0753       0.0598       0.0563       0.0949       0.0756         0.26      0.00271\n","     37    19      0.00932      0.00932     2.96e-06       0.0569       0.0747       0.0463       0.0781       0.0622       0.0594       0.0983       0.0789        0.107      0.00111\n","     37    20      0.00936      0.00933     3.67e-05       0.0567       0.0747       0.0458       0.0787       0.0622       0.0587       0.0992        0.079        0.447      0.00466\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     37     1       0.0107       0.0107     2.87e-06       0.0606       0.0801       0.0493       0.0834       0.0663       0.0642        0.105       0.0845        0.106       0.0011\n","     37     2       0.0105       0.0105     9.31e-07       0.0592       0.0792       0.0476       0.0825       0.0651       0.0624        0.105       0.0837       0.0593     0.000617\n","     37     3      0.00948      0.00948     1.73e-06       0.0569       0.0753        0.046       0.0785       0.0623       0.0593       0.0999       0.0796        0.082     0.000854\n","     37     4       0.0103       0.0103      1.3e-06       0.0587       0.0784       0.0478       0.0804       0.0641        0.062        0.104       0.0828       0.0754     0.000785\n","     37     5      0.00972      0.00972     1.69e-06       0.0574       0.0763       0.0471        0.078       0.0626       0.0617       0.0992       0.0804       0.0695     0.000724\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              37  152.761    0.005      0.00966     2.89e-05      0.00969       0.0577        0.076       0.0469       0.0793       0.0631       0.0604          0.1       0.0803        0.336       0.0035\n","! Validation         37  152.761    0.005       0.0101      1.7e-06       0.0101       0.0586       0.0779       0.0476       0.0806       0.0641        0.062        0.103       0.0822       0.0784     0.000816\n","Wall time: 152.76214335299983\n","! Best model       37    0.010\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     38     1       0.0086      0.00856     3.55e-05       0.0548       0.0716       0.0444       0.0756         0.06       0.0565       0.0948       0.0756        0.438      0.00457\n","     38     2       0.0102       0.0102     5.49e-06       0.0586       0.0782       0.0459       0.0842        0.065       0.0597        0.106       0.0828        0.151      0.00157\n","     38     3      0.00959      0.00955     3.48e-05       0.0579       0.0756       0.0475       0.0786       0.0631       0.0602       0.0995       0.0799        0.434      0.00452\n","     38     4       0.0101         0.01     2.93e-05       0.0586       0.0775       0.0487       0.0784       0.0636       0.0637       0.0994       0.0816        0.395      0.00411\n","     38     5      0.00873      0.00873     1.41e-06       0.0556       0.0723        0.046       0.0748       0.0604       0.0589       0.0934       0.0762        0.084     0.000875\n","     38     6       0.0103       0.0103     1.11e-06       0.0605       0.0786       0.0519       0.0776       0.0648       0.0664       0.0985       0.0825       0.0703     0.000732\n","     38     7      0.00859      0.00859     3.29e-06       0.0541       0.0717       0.0447       0.0728       0.0588       0.0572       0.0942       0.0757        0.132      0.00138\n","     38     8      0.00908      0.00907     1.45e-05       0.0562       0.0737       0.0457       0.0771       0.0614       0.0587       0.0969       0.0778        0.258      0.00269\n","     38     9       0.0103       0.0103     4.16e-05       0.0596       0.0785       0.0493         0.08       0.0647       0.0634        0.102       0.0829        0.463      0.00482\n","     38    10       0.0108       0.0108     1.52e-05       0.0602       0.0803       0.0483       0.0841       0.0662       0.0633        0.106       0.0849        0.281      0.00293\n","     38    11       0.0105       0.0104     5.56e-05       0.0614        0.079       0.0516       0.0812       0.0664       0.0663       0.0996        0.083         0.55      0.00573\n","     38    12      0.00996      0.00993     2.16e-05       0.0597       0.0771       0.0515       0.0763       0.0639       0.0645       0.0976        0.081        0.339      0.00354\n","     38    13       0.0105       0.0105     7.28e-07       0.0607       0.0793       0.0492       0.0838       0.0665       0.0638        0.104       0.0837       0.0486     0.000507\n","     38    14      0.00941      0.00938     3.66e-05       0.0572       0.0749       0.0465       0.0787       0.0626       0.0594       0.0989       0.0792        0.446      0.00464\n","     38    15       0.0111        0.011     7.68e-06       0.0614       0.0813        0.049       0.0861       0.0676       0.0635        0.109        0.086        0.191      0.00199\n","     38    16      0.00887      0.00876     0.000102       0.0549       0.0724       0.0438       0.0769       0.0604       0.0552       0.0982       0.0767         0.75      0.00781\n","     38    17      0.00866      0.00863      3.1e-05       0.0543       0.0719       0.0441       0.0748       0.0595       0.0563       0.0957        0.076        0.409      0.00426\n","     38    18      0.00954      0.00952     1.97e-05       0.0555       0.0755       0.0453        0.076       0.0606       0.0597       0.0998       0.0798        0.324      0.00338\n","     38    19      0.00819      0.00808     0.000119       0.0528       0.0695       0.0435       0.0713       0.0574       0.0552       0.0917       0.0734        0.808      0.00842\n","     38    20      0.00886      0.00883        3e-05       0.0558       0.0727       0.0461       0.0752       0.0606       0.0592        0.094       0.0766         0.39      0.00407\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     38     1       0.0105       0.0105     2.76e-06       0.0602       0.0794       0.0488       0.0829       0.0658       0.0636        0.104       0.0839        0.103      0.00108\n","     38     2       0.0103       0.0103     9.36e-07       0.0587       0.0785       0.0471       0.0819       0.0645       0.0618        0.104        0.083       0.0586      0.00061\n","     38     3      0.00935      0.00935     1.65e-06       0.0565       0.0748       0.0456       0.0781       0.0619       0.0588       0.0994       0.0791       0.0796     0.000829\n","     38     4       0.0101       0.0101     1.23e-06       0.0582       0.0778       0.0474         0.08       0.0637       0.0614        0.103       0.0822        0.072      0.00075\n","     38     5      0.00956      0.00956      1.7e-06        0.057       0.0756       0.0467       0.0774       0.0621       0.0612       0.0984       0.0798       0.0691      0.00072\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              38  156.776    0.005      0.00956     3.04e-05      0.00959       0.0575       0.0757       0.0471       0.0782       0.0627       0.0607       0.0991       0.0799        0.348      0.00363\n","! Validation         38  156.776    0.005      0.00997     1.65e-06      0.00997       0.0581       0.0773       0.0471       0.0801       0.0636       0.0614        0.102       0.0816       0.0765     0.000797\n","Wall time: 156.77648531399996\n","! Best model       38    0.010\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     39     1       0.0106       0.0106      8.2e-05       0.0607       0.0795       0.0502       0.0817        0.066        0.065        0.103       0.0838        0.667      0.00695\n","     39     2       0.0109       0.0109     5.86e-05       0.0612       0.0807       0.0501       0.0834       0.0668       0.0644        0.106       0.0852        0.565      0.00589\n","     39     3       0.0108       0.0108     2.11e-06       0.0608       0.0805       0.0493       0.0838       0.0665       0.0639        0.106        0.085       0.0857     0.000893\n","     39     4       0.0106       0.0104     0.000106       0.0606       0.0791       0.0492       0.0835       0.0663       0.0619        0.105       0.0836        0.763      0.00795\n","     39     5        0.009        0.009     3.56e-06       0.0556       0.0734       0.0445        0.078       0.0612       0.0574       0.0978       0.0776        0.114      0.00119\n","     39     6       0.0102       0.0102     3.53e-05       0.0592        0.078       0.0502       0.0773       0.0637       0.0657       0.0982       0.0819        0.435      0.00453\n","     39     7      0.00876      0.00875     3.22e-06       0.0554       0.0724       0.0479       0.0704       0.0592       0.0608       0.0913        0.076        0.122      0.00128\n","     39     8       0.0104       0.0104     1.39e-05       0.0602       0.0788       0.0495       0.0816       0.0655       0.0624        0.104       0.0833        0.268      0.00279\n","     39     9       0.0114       0.0113     9.08e-05       0.0625       0.0822       0.0527       0.0821       0.0674       0.0678        0.105       0.0865        0.706      0.00735\n","     39    10       0.0103       0.0103     5.33e-05       0.0614       0.0785       0.0537       0.0768       0.0653       0.0678       0.0964       0.0821        0.541      0.00564\n","     39    11       0.0104       0.0104     2.79e-05       0.0606       0.0787       0.0514        0.079       0.0652       0.0665       0.0988       0.0826        0.383      0.00399\n","     39    12      0.00735      0.00735     2.09e-06       0.0501       0.0663        0.041       0.0683       0.0546       0.0522       0.0879       0.0701       0.0965      0.00101\n","     39    13       0.0101       0.0101     3.78e-05       0.0588       0.0778       0.0495       0.0774       0.0634       0.0639          0.1       0.0819        0.445      0.00464\n","     39    14         0.01         0.01     2.13e-05       0.0594       0.0775       0.0489       0.0805       0.0647       0.0626        0.101       0.0817        0.315      0.00328\n","     39    15       0.0097      0.00961     9.43e-05       0.0572       0.0758       0.0453       0.0809       0.0631       0.0588        0.102       0.0802        0.721      0.00751\n","     39    16      0.00783      0.00776     7.16e-05       0.0518       0.0681       0.0424       0.0707       0.0565       0.0544       0.0895       0.0719         0.62      0.00646\n","     39    17       0.0106       0.0106     4.49e-05       0.0611       0.0796       0.0516       0.0802       0.0659        0.066        0.101       0.0837        0.487      0.00507\n","     39    18         0.01       0.0099     0.000123       0.0577        0.077       0.0465       0.0799       0.0632       0.0602        0.103       0.0814        0.824      0.00859\n","     39    19      0.00945      0.00943     2.48e-05        0.057       0.0751       0.0458       0.0794       0.0626       0.0596       0.0991       0.0794        0.351      0.00365\n","     39    20      0.00987       0.0098      6.5e-05        0.058       0.0766       0.0472       0.0795       0.0633       0.0609        0.101       0.0809        0.594      0.00618\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     39     1       0.0104       0.0104     2.75e-06       0.0597       0.0788       0.0483       0.0823       0.0653       0.0629        0.103       0.0832        0.103      0.00107\n","     39     2       0.0102       0.0102      9.7e-07       0.0583        0.078       0.0468       0.0814       0.0641       0.0613        0.104       0.0825       0.0593     0.000617\n","     39     3      0.00924      0.00924     1.65e-06       0.0561       0.0744       0.0452       0.0778       0.0615       0.0583       0.0989       0.0786       0.0803     0.000836\n","     39     4      0.00997      0.00997     1.18e-06       0.0578       0.0772       0.0469       0.0796       0.0632       0.0608        0.102       0.0816       0.0724     0.000754\n","     39     5      0.00941      0.00941     1.71e-06       0.0565       0.0751       0.0463        0.077       0.0616       0.0607       0.0977       0.0792       0.0695     0.000724\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              39  160.785    0.005      0.00988     4.81e-05      0.00992       0.0585       0.0769       0.0483       0.0787       0.0635       0.0622       0.0999       0.0811        0.455      0.00474\n","! Validation         39  160.785    0.005      0.00983     1.65e-06      0.00983       0.0577       0.0767       0.0467       0.0796       0.0632       0.0608        0.101        0.081       0.0769     0.000801\n","Wall time: 160.78577292599994\n","! Best model       39    0.010\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     40     1      0.00887      0.00881     5.91e-05       0.0558       0.0726       0.0456       0.0761       0.0608       0.0577       0.0957       0.0767        0.565      0.00588\n","     40     2      0.00922      0.00921     1.01e-05       0.0558       0.0743       0.0454       0.0765       0.0609       0.0583       0.0987       0.0785        0.221       0.0023\n","     40     3       0.0102       0.0101     2.59e-05       0.0582       0.0779       0.0464       0.0816        0.064       0.0605        0.104       0.0824        0.375      0.00391\n","     40     4         0.01         0.01     9.28e-07       0.0607       0.0775       0.0528       0.0765       0.0646       0.0669       0.0954       0.0811       0.0713     0.000743\n","     40     5       0.0148       0.0148     1.82e-06       0.0753       0.0941        0.069       0.0878       0.0784       0.0858        0.109       0.0973       0.0877     0.000913\n","     40     6       0.0148       0.0148     1.61e-05       0.0726       0.0941       0.0594       0.0988       0.0791       0.0768        0.121       0.0992        0.291      0.00303\n","     40     7       0.0117       0.0116     9.41e-05       0.0636       0.0834       0.0522       0.0863       0.0692       0.0662         0.11       0.0881        0.714      0.00744\n","     40     8      0.00988      0.00987     4.27e-06       0.0584       0.0769       0.0488       0.0777       0.0632       0.0628       0.0992        0.081        0.126      0.00131\n","     40     9       0.0104       0.0103      7.5e-05       0.0588       0.0786       0.0477       0.0809       0.0643       0.0623        0.104        0.083        0.642      0.00668\n","     40    10      0.00978      0.00969     9.74e-05       0.0569       0.0761       0.0449       0.0809       0.0629       0.0591        0.102       0.0806        0.731      0.00761\n","     40    11      0.00984      0.00984     5.98e-06       0.0574       0.0767       0.0456       0.0811       0.0633       0.0595        0.103       0.0812        0.175      0.00183\n","     40    12      0.00998      0.00989     8.83e-05       0.0582       0.0769       0.0467       0.0811       0.0639       0.0605        0.102       0.0813        0.697      0.00726\n","     40    13       0.0115       0.0115     4.33e-05       0.0654       0.0829       0.0607        0.075       0.0678       0.0761        0.095       0.0856        0.483      0.00503\n","     40    14       0.0111       0.0111     1.21e-05       0.0625       0.0815       0.0543       0.0789       0.0666       0.0696        0.101       0.0854        0.243      0.00253\n","     40    15       0.0104       0.0104     3.83e-06       0.0593       0.0791       0.0467       0.0846       0.0656       0.0592        0.108       0.0838         0.14      0.00146\n","     40    16      0.00893       0.0089     2.52e-05       0.0556        0.073       0.0438       0.0793       0.0615       0.0551       0.0995       0.0773        0.342      0.00356\n","     40    17      0.00915      0.00914     3.31e-06       0.0563        0.074        0.046        0.077       0.0615       0.0592        0.097       0.0781        0.092     0.000958\n","     40    18      0.00933      0.00932     5.27e-06       0.0573       0.0747       0.0463       0.0793       0.0628       0.0594       0.0984       0.0789        0.149      0.00155\n","     40    19       0.0095      0.00949     5.76e-06        0.058       0.0754         0.05        0.074        0.062       0.0627       0.0958       0.0793        0.173       0.0018\n","     40    20      0.00943      0.00943     2.46e-06       0.0581       0.0751       0.0492       0.0759       0.0625       0.0627       0.0953        0.079        0.114      0.00118\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     40     1       0.0102       0.0102     2.68e-06       0.0593       0.0782       0.0479        0.082       0.0649       0.0623        0.103       0.0826        0.102      0.00106\n","     40     2         0.01         0.01     9.33e-07       0.0579       0.0774       0.0463       0.0809       0.0636       0.0607        0.103       0.0819        0.057     0.000594\n","     40     3      0.00913      0.00913     1.63e-06       0.0557       0.0739       0.0449       0.0774       0.0611       0.0579       0.0985       0.0782       0.0786     0.000819\n","     40     4      0.00984      0.00984      1.2e-06       0.0574       0.0767       0.0465       0.0792       0.0628       0.0602        0.102       0.0811       0.0729      0.00076\n","     40     5      0.00928      0.00928     1.69e-06       0.0562       0.0745        0.046       0.0765       0.0612       0.0602       0.0971       0.0786        0.068     0.000708\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              40  164.949    0.005       0.0104      2.9e-05       0.0104       0.0602        0.079       0.0501       0.0805       0.0653       0.0645        0.102       0.0832        0.322      0.00335\n","! Validation         40  164.949    0.005       0.0097     1.62e-06       0.0097       0.0573       0.0762       0.0463       0.0792       0.0628       0.0603        0.101       0.0805       0.0757     0.000789\n","Wall time: 164.9497575339999\n","! Best model       40    0.010\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     41     1      0.00965      0.00962     3.08e-05       0.0577       0.0759       0.0462       0.0806       0.0634       0.0592        0.101       0.0802        0.409      0.00426\n","     41     2      0.00916      0.00914     1.75e-05       0.0561        0.074       0.0455       0.0772       0.0614       0.0586       0.0977       0.0781        0.293      0.00305\n","     41     3      0.00833      0.00829      3.8e-05       0.0536       0.0704       0.0437       0.0735       0.0586       0.0552       0.0937       0.0745        0.455      0.00474\n","     41     4      0.00946      0.00946     4.89e-07       0.0589       0.0753       0.0511       0.0745       0.0628        0.064       0.0937       0.0789       0.0469     0.000488\n","     41     5       0.0106       0.0106     1.88e-06       0.0619       0.0797       0.0532       0.0793       0.0663       0.0666        0.101       0.0838         0.09     0.000938\n","     41     6      0.00887      0.00885     1.93e-05       0.0546       0.0728       0.0435       0.0768       0.0602       0.0556       0.0986       0.0771        0.324      0.00338\n","     41     7      0.00806      0.00806     2.45e-06       0.0532       0.0695       0.0429       0.0738       0.0584        0.055       0.0918       0.0734        0.106       0.0011\n","     41     8      0.00844      0.00844     4.44e-06       0.0539       0.0711       0.0453       0.0712       0.0583       0.0584       0.0913       0.0748         0.14      0.00146\n","     41     9      0.00853      0.00853     1.19e-06        0.054       0.0715       0.0446       0.0728       0.0587       0.0567       0.0942       0.0755       0.0715     0.000745\n","     41    10      0.00922      0.00922     6.03e-06       0.0569       0.0743       0.0481       0.0747       0.0614       0.0616       0.0946       0.0781        0.161      0.00167\n","     41    11      0.00946      0.00944     1.09e-05       0.0552       0.0752       0.0424       0.0809       0.0617       0.0557        0.104       0.0797        0.235      0.00245\n","     41    12         0.01         0.01     3.67e-05       0.0586       0.0774        0.049       0.0779       0.0634       0.0633       0.0998       0.0815        0.448      0.00466\n","     41    13      0.00906      0.00905     9.87e-07       0.0557       0.0736       0.0457       0.0757       0.0607       0.0601        0.095       0.0776       0.0525     0.000547\n","     41    14       0.0087      0.00868     1.43e-05       0.0541       0.0721       0.0419       0.0784       0.0601       0.0531       0.0998       0.0764        0.265      0.00276\n","     41    15      0.00865      0.00865     7.13e-07        0.055        0.072       0.0446       0.0756       0.0601       0.0566       0.0956       0.0761       0.0572     0.000596\n","     41    16       0.0139       0.0139     1.75e-06       0.0692       0.0913       0.0581       0.0914       0.0747       0.0756        0.116        0.096       0.0883      0.00092\n","     41    17       0.0145       0.0144     0.000122       0.0734       0.0929        0.065       0.0903       0.0776       0.0818        0.112       0.0968        0.813      0.00847\n","     41    18       0.0127       0.0127     2.22e-06       0.0694       0.0873       0.0617       0.0847       0.0732       0.0767        0.105        0.091       0.0867     0.000903\n","     41    19         0.01      0.00998     5.06e-05       0.0593       0.0773       0.0479       0.0822        0.065       0.0603        0.103       0.0818        0.525      0.00547\n","     41    20      0.00953      0.00941     0.000118       0.0562       0.0751       0.0461       0.0763       0.0612       0.0597       0.0988       0.0793        0.804      0.00837\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     41     1       0.0101       0.0101     2.76e-06       0.0589       0.0777       0.0475       0.0816       0.0646       0.0618        0.102       0.0821        0.104      0.00108\n","     41     2       0.0099       0.0099     9.25e-07       0.0575        0.077        0.046       0.0805       0.0632       0.0602        0.103       0.0814       0.0559     0.000582\n","     41     3      0.00904      0.00904     1.59e-06       0.0554       0.0736       0.0446       0.0771       0.0608       0.0574       0.0981       0.0778       0.0788     0.000821\n","     41     4      0.00972      0.00972     1.23e-06        0.057       0.0763       0.0461       0.0789       0.0625       0.0597        0.102       0.0806       0.0732     0.000763\n","     41     5      0.00916      0.00916     1.72e-06       0.0558        0.074       0.0456       0.0761       0.0609       0.0597       0.0965       0.0781       0.0694     0.000723\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              41  169.000    0.005      0.00983      2.4e-05      0.00985       0.0583       0.0767       0.0483       0.0784       0.0634       0.0622       0.0996       0.0809        0.274      0.00285\n","! Validation         41  169.000    0.005      0.00958     1.65e-06      0.00958       0.0569       0.0757        0.046       0.0788       0.0624       0.0598          0.1         0.08       0.0762     0.000793\n","Wall time: 169.001145705\n","! Best model       41    0.010\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     42     1       0.0105       0.0105     2.58e-05       0.0591       0.0792       0.0486       0.0803       0.0644       0.0641        0.103       0.0835        0.366      0.00381\n","     42     2       0.0101      0.00981     0.000283       0.0576       0.0766       0.0463       0.0801       0.0632       0.0599        0.102        0.081         1.25        0.013\n","     42     3        0.012        0.012     1.32e-05       0.0665       0.0847       0.0594       0.0806         0.07       0.0751        0.101       0.0881        0.259      0.00269\n","     42     4       0.0123       0.0123     7.69e-05       0.0676       0.0857       0.0616       0.0796       0.0706       0.0763        0.102       0.0891        0.649      0.00676\n","     42     5       0.0112        0.011     0.000189       0.0619       0.0812       0.0504       0.0849       0.0677       0.0646        0.107       0.0858         1.02       0.0106\n","     42     6      0.00962       0.0096     1.81e-05       0.0573       0.0758       0.0463       0.0791       0.0627         0.06          0.1       0.0801        0.313      0.00326\n","     42     7      0.00826      0.00815     0.000108       0.0532       0.0699       0.0428       0.0738       0.0583       0.0542       0.0936       0.0739        0.769      0.00801\n","     42     8      0.00892      0.00892     7.06e-07       0.0545       0.0731       0.0429       0.0777       0.0603       0.0549       0.0999       0.0774       0.0543     0.000566\n","     42     9      0.00886      0.00879      6.8e-05       0.0542       0.0725       0.0443        0.074       0.0591       0.0585       0.0946       0.0765        0.609      0.00635\n","     42    10      0.00802      0.00799     3.01e-05       0.0525       0.0692       0.0419       0.0736       0.0578       0.0536       0.0928       0.0732        0.403      0.00419\n","     42    11      0.00828      0.00825     2.99e-05       0.0537       0.0703       0.0444       0.0721       0.0583       0.0563       0.0921       0.0742        0.403       0.0042\n","     42    12      0.00902      0.00893     8.79e-05       0.0554       0.0731       0.0444       0.0776        0.061       0.0569       0.0978       0.0774         0.68      0.00708\n","     42    13      0.00855      0.00855     2.07e-06       0.0542       0.0715       0.0437       0.0753       0.0595       0.0562       0.0951       0.0756          0.1      0.00105\n","     42    14      0.00853       0.0085     2.99e-05       0.0539       0.0713       0.0439       0.0739       0.0589       0.0562       0.0945       0.0754        0.398      0.00414\n","     42    15      0.00886      0.00884     1.97e-05       0.0545       0.0727       0.0435       0.0764         0.06        0.057       0.0967       0.0769        0.323      0.00336\n","     42    16      0.00856      0.00856     9.11e-07       0.0543       0.0716       0.0417       0.0794       0.0606       0.0527        0.099       0.0759       0.0639     0.000665\n","     42    17       0.0102       0.0102     4.06e-05       0.0602        0.078       0.0505       0.0794        0.065       0.0641          0.1       0.0822         0.47      0.00489\n","     42    18       0.0133       0.0132      1.9e-05       0.0696        0.089        0.061       0.0867       0.0738       0.0768        0.109       0.0931        0.312      0.00326\n","     42    19        0.013        0.013     1.75e-06       0.0698       0.0884       0.0617       0.0862       0.0739       0.0758        0.109       0.0925       0.0934     0.000972\n","     42    20       0.0097       0.0097     5.49e-07       0.0581       0.0762       0.0478       0.0787       0.0632       0.0605          0.1       0.0805        0.043     0.000448\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     42     1      0.00996      0.00996     2.82e-06       0.0585       0.0772       0.0472       0.0812       0.0642       0.0613        0.102       0.0816        0.104      0.00109\n","     42     2      0.00978      0.00978     9.04e-07       0.0571       0.0765       0.0457         0.08       0.0628       0.0598        0.102       0.0809       0.0559     0.000582\n","     42     3      0.00895      0.00895     1.62e-06       0.0551       0.0732       0.0443       0.0768       0.0605       0.0571       0.0977       0.0774         0.08     0.000833\n","     42     4       0.0096       0.0096     1.29e-06       0.0567       0.0758       0.0457       0.0786       0.0621       0.0592        0.101       0.0802        0.074     0.000771\n","     42     5      0.00904      0.00904     1.67e-06       0.0555       0.0736       0.0453       0.0758       0.0605       0.0593       0.0959       0.0776       0.0692     0.000721\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              42  172.991    0.005      0.00984     5.23e-05      0.00989       0.0584       0.0767       0.0484       0.0785       0.0634       0.0622       0.0997       0.0809        0.429      0.00447\n","! Validation         42  172.991    0.005      0.00947     1.66e-06      0.00947       0.0566       0.0753       0.0456       0.0785        0.062       0.0593       0.0998       0.0796       0.0767     0.000799\n","Wall time: 172.99229504999994\n","! Best model       42    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     43     1      0.00831      0.00831     2.09e-06       0.0524       0.0705       0.0418       0.0738       0.0578       0.0549       0.0944       0.0746        0.102      0.00107\n","     43     2       0.0123       0.0123     1.34e-05       0.0651       0.0858       0.0566       0.0821       0.0694       0.0739        0.106       0.0898        0.259       0.0027\n","     43     3        0.015       0.0149     5.63e-05       0.0748       0.0945       0.0662       0.0921       0.0791       0.0819        0.116       0.0988        0.548      0.00571\n","     43     4       0.0141       0.0141     3.61e-06       0.0722       0.0919       0.0628       0.0911       0.0769       0.0786        0.114       0.0963        0.136      0.00141\n","     43     5      0.00987      0.00984     2.99e-05       0.0587       0.0767       0.0493       0.0777       0.0635       0.0629       0.0988       0.0808        0.402      0.00418\n","     43     6      0.00849      0.00849     5.88e-06       0.0532       0.0713       0.0417       0.0762       0.0589       0.0537       0.0973       0.0755        0.158      0.00164\n","     43     7      0.00711      0.00706     4.86e-05       0.0491        0.065       0.0393       0.0688        0.054       0.0494       0.0883       0.0689        0.515      0.00536\n","     43     8      0.00859      0.00859     9.57e-06       0.0542       0.0717       0.0442        0.074       0.0591       0.0574       0.0939       0.0757        0.207      0.00216\n","     43     9      0.00877      0.00877     2.84e-06       0.0548       0.0724       0.0447       0.0752       0.0599       0.0579        0.095       0.0765       0.0887     0.000924\n","     43    10      0.00804      0.00803     1.13e-05       0.0524       0.0693       0.0422        0.073       0.0576       0.0541       0.0925       0.0733        0.244      0.00254\n","     43    11      0.00868      0.00868      1.9e-06        0.055       0.0721       0.0449        0.075         0.06       0.0573        0.095       0.0761       0.0832     0.000867\n","     43    12      0.00831      0.00829     2.16e-05       0.0528       0.0704       0.0429       0.0726       0.0577       0.0551       0.0939       0.0745        0.343      0.00357\n","     43    13      0.00949      0.00949     4.84e-06       0.0567       0.0753       0.0463       0.0776        0.062       0.0601       0.0991       0.0796        0.153      0.00159\n","     43    14      0.00824      0.00816     8.38e-05       0.0532       0.0699       0.0438       0.0721       0.0579       0.0554       0.0923       0.0738        0.672        0.007\n","     43    15      0.00932      0.00926     6.21e-05       0.0562       0.0744       0.0456       0.0773       0.0614       0.0593        0.098       0.0786        0.581      0.00605\n","     43    16      0.00807      0.00801     5.91e-05       0.0519       0.0692       0.0425       0.0708       0.0566       0.0545       0.0919       0.0732        0.569      0.00593\n","     43    17      0.00878      0.00861      0.00017       0.0544       0.0718       0.0446       0.0741       0.0594       0.0568        0.095       0.0759        0.966       0.0101\n","     43    18      0.00821      0.00821     4.83e-06       0.0529       0.0701       0.0419       0.0748       0.0584       0.0535        0.095       0.0742        0.154      0.00161\n","     43    19      0.00892      0.00886     5.51e-05       0.0553       0.0728       0.0435       0.0789       0.0612        0.055       0.0993       0.0772        0.549      0.00572\n","     43    20      0.00872      0.00867     5.45e-05       0.0548        0.072       0.0441       0.0763       0.0602       0.0563       0.0961       0.0762        0.546      0.00569\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     43     1      0.00984      0.00984     2.75e-06       0.0581       0.0767       0.0468       0.0808       0.0638       0.0608        0.101       0.0811        0.104      0.00108\n","     43     2      0.00967      0.00967     9.21e-07       0.0568       0.0761       0.0453       0.0796       0.0625       0.0593        0.102       0.0805       0.0556     0.000579\n","     43     3      0.00886      0.00886      1.6e-06       0.0548       0.0728        0.044       0.0765       0.0602       0.0567       0.0973        0.077       0.0784     0.000817\n","     43     4      0.00949      0.00949     1.22e-06       0.0563       0.0754       0.0453       0.0782       0.0618       0.0587        0.101       0.0797       0.0729     0.000759\n","     43     5      0.00893      0.00893     1.64e-06       0.0552       0.0731        0.045       0.0754       0.0602       0.0589       0.0954       0.0771       0.0674     0.000702\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              43  177.002    0.005      0.00933     3.51e-05      0.00937       0.0565       0.0747       0.0464       0.0767       0.0616         0.06       0.0978       0.0789        0.364      0.00379\n","! Validation         43  177.002    0.005      0.00936     1.63e-06      0.00936       0.0562       0.0748       0.0453       0.0781       0.0617       0.0589       0.0993       0.0791       0.0756     0.000788\n","Wall time: 177.00281986999994\n","! Best model       43    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     44     1      0.00792      0.00791     4.98e-06       0.0521       0.0688        0.042       0.0723       0.0572        0.054       0.0915       0.0728         0.16      0.00167\n","     44     2       0.0093      0.00928     2.26e-05       0.0558       0.0745       0.0457        0.076       0.0608         0.06       0.0972       0.0786        0.345      0.00359\n","     44     3      0.00923      0.00923     1.13e-07       0.0571       0.0743       0.0472        0.077       0.0621         0.06       0.0968       0.0784       0.0207     0.000216\n","     44     4      0.00878      0.00873     5.53e-05       0.0547       0.0723       0.0435        0.077       0.0603       0.0567       0.0961       0.0764        0.542      0.00565\n","     44     5      0.00822      0.00822     2.98e-06       0.0531       0.0701       0.0425       0.0744       0.0585       0.0545       0.0939       0.0742        0.113      0.00118\n","     44     6       0.0103       0.0103     8.03e-05       0.0598       0.0784       0.0501       0.0792       0.0646       0.0649          0.1       0.0825         0.66      0.00688\n","     44     7      0.00921      0.00916     4.37e-05       0.0568       0.0741       0.0471       0.0762       0.0617         0.06       0.0961       0.0781        0.487      0.00507\n","     44     8      0.00946      0.00946     2.56e-06       0.0583       0.0752       0.0501       0.0747       0.0624       0.0628       0.0954       0.0791       0.0945     0.000985\n","     44     9      0.00876      0.00865     0.000106       0.0541        0.072       0.0428       0.0765       0.0597       0.0546       0.0979       0.0762        0.758      0.00789\n","     44    10      0.00892      0.00888     4.06e-05       0.0551       0.0729       0.0436        0.078       0.0608       0.0556       0.0987       0.0772        0.459      0.00478\n","     44    11      0.00934      0.00934     1.86e-06       0.0556       0.0748        0.044       0.0788       0.0614       0.0573        0.101       0.0792       0.0951     0.000991\n","     44    12       0.0114       0.0114     9.11e-06       0.0635       0.0827        0.051       0.0884       0.0697       0.0646         0.11       0.0875        0.217      0.00226\n","     44    13       0.0117       0.0116     0.000117       0.0648       0.0834       0.0568       0.0806       0.0687       0.0713        0.103       0.0873        0.802      0.00836\n","     44    14       0.0108       0.0108     1.22e-05       0.0622       0.0803       0.0544        0.078       0.0662         0.07       0.0979       0.0839        0.238      0.00248\n","     44    15      0.00867      0.00863     4.28e-05       0.0543       0.0719       0.0436       0.0758       0.0597       0.0555       0.0966       0.0761        0.478      0.00498\n","     44    16      0.00952      0.00952     5.23e-07       0.0575       0.0755       0.0488       0.0749       0.0618       0.0626       0.0962       0.0794       0.0475     0.000494\n","     44    17       0.0103       0.0103     6.86e-07       0.0602       0.0786        0.052       0.0766       0.0643       0.0659       0.0992       0.0825       0.0537     0.000559\n","     44    18       0.0089      0.00889     4.61e-06       0.0553        0.073       0.0463       0.0734       0.0598       0.0583       0.0957        0.077        0.153       0.0016\n","     44    19      0.00855      0.00855     2.45e-06       0.0536       0.0715       0.0426       0.0756       0.0591       0.0543       0.0972       0.0758       0.0852     0.000887\n","     44    20      0.00997      0.00996     6.29e-06       0.0578       0.0772       0.0468       0.0798       0.0633       0.0614        0.102       0.0816        0.161      0.00168\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     44     1      0.00972      0.00972      2.7e-06       0.0578       0.0763       0.0465       0.0804       0.0634       0.0604        0.101       0.0806        0.102      0.00107\n","     44     2      0.00957      0.00957     8.97e-07       0.0564       0.0757        0.045       0.0793       0.0621       0.0589        0.101         0.08        0.054     0.000563\n","     44     3      0.00877      0.00877     1.56e-06       0.0545       0.0725       0.0437       0.0762       0.0599       0.0563        0.097       0.0767       0.0785     0.000818\n","     44     4      0.00939      0.00939     1.22e-06        0.056        0.075        0.045       0.0779       0.0615       0.0582          0.1       0.0793        0.073     0.000761\n","     44     5      0.00883      0.00883     1.61e-06       0.0549       0.0727       0.0448       0.0751       0.0599       0.0585        0.095       0.0767       0.0673     0.000701\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              44  181.018    0.005      0.00944     2.78e-05      0.00947       0.0571       0.0752        0.047       0.0772       0.0621       0.0604       0.0982       0.0793        0.299      0.00311\n","! Validation         44  181.018    0.005      0.00926      1.6e-06      0.00926       0.0559       0.0744        0.045       0.0778       0.0614       0.0585       0.0989       0.0787       0.0751     0.000782\n","Wall time: 181.0189583729998\n","! Best model       44    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     45     1      0.00941       0.0094     1.15e-05       0.0575        0.075       0.0478       0.0769       0.0624       0.0603        0.098       0.0791        0.234      0.00244\n","     45     2       0.0103       0.0103     2.26e-06       0.0606       0.0787       0.0516       0.0786       0.0651       0.0659       0.0995       0.0827       0.0879     0.000916\n","     45     3      0.00888      0.00888     1.29e-06       0.0554       0.0729       0.0462       0.0737       0.0599       0.0585       0.0954       0.0769        0.068     0.000708\n","     45     4       0.0104       0.0104     4.84e-06         0.06        0.079       0.0507       0.0785       0.0646       0.0654        0.101       0.0831        0.111      0.00116\n","     45     5       0.0146       0.0146     1.99e-05       0.0741       0.0934       0.0669       0.0883       0.0776       0.0828        0.112       0.0972        0.321      0.00334\n","     45     6        0.013       0.0129     1.27e-05       0.0679        0.088       0.0574       0.0888       0.0731       0.0722        0.113       0.0927        0.255      0.00266\n","     45     7      0.00903      0.00903     6.03e-07       0.0564       0.0735        0.045       0.0791        0.062       0.0571       0.0985       0.0778       0.0547      0.00057\n","     45     8      0.00731      0.00731     2.86e-06       0.0497       0.0661       0.0393       0.0704       0.0548       0.0499       0.0903       0.0701        0.115       0.0012\n","     45     9      0.00948      0.00947     6.53e-06        0.057       0.0753       0.0463       0.0782       0.0623       0.0591          0.1       0.0796        0.185      0.00192\n","     45    10       0.0112       0.0111      1.2e-06       0.0633       0.0817       0.0534        0.083       0.0682       0.0674        0.105        0.086        0.067     0.000698\n","     45    11       0.0118       0.0118     2.73e-05       0.0657       0.0841       0.0583       0.0804       0.0694       0.0743        0.101       0.0876        0.385      0.00401\n","     45    12      0.00903      0.00902        4e-06       0.0558       0.0735       0.0436       0.0803       0.0619       0.0556          0.1       0.0779        0.119      0.00124\n","     45    13      0.00875      0.00871     3.11e-05       0.0544       0.0722       0.0436       0.0761       0.0598       0.0575       0.0951       0.0763        0.401      0.00417\n","     45    14      0.00959      0.00957     2.13e-05        0.057       0.0757       0.0464       0.0782       0.0623       0.0594        0.101         0.08        0.336       0.0035\n","     45    15      0.00932      0.00931     5.55e-06       0.0564       0.0746       0.0454       0.0784       0.0619       0.0588       0.0991       0.0789        0.166      0.00173\n","     45    16       0.0105       0.0104     8.42e-05       0.0595        0.079       0.0485       0.0816        0.065       0.0635        0.103       0.0834        0.678      0.00706\n","     45    17      0.00821      0.00815     6.35e-05       0.0529       0.0698       0.0423        0.074       0.0581       0.0531       0.0949        0.074        0.591      0.00616\n","     45    18       0.0103       0.0103     2.69e-05       0.0602       0.0784       0.0537       0.0732       0.0635       0.0687       0.0948       0.0817        0.383      0.00399\n","     45    19      0.00921      0.00913     8.47e-05       0.0564       0.0739       0.0466        0.076       0.0613       0.0597       0.0963        0.078        0.674      0.00702\n","     45    20      0.00996      0.00996     6.45e-06        0.059       0.0772       0.0482       0.0806       0.0644        0.062        0.101       0.0815        0.162      0.00169\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     45     1      0.00961      0.00961     2.73e-06       0.0574       0.0758       0.0461       0.0801       0.0631       0.0599          0.1       0.0801        0.103      0.00107\n","     45     2      0.00947      0.00947     8.98e-07       0.0561       0.0753       0.0447       0.0789       0.0618       0.0585        0.101       0.0797       0.0536     0.000558\n","     45     3       0.0087       0.0087     1.55e-06       0.0543       0.0721       0.0434       0.0759       0.0597        0.056       0.0967       0.0763       0.0775     0.000808\n","     45     4      0.00929      0.00929     1.25e-06       0.0557       0.0746       0.0447       0.0777       0.0612       0.0578          0.1       0.0789       0.0739      0.00077\n","     45     5      0.00873      0.00872     1.58e-06       0.0546       0.0723       0.0444       0.0748       0.0596        0.058       0.0945       0.0763       0.0666     0.000694\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              45  185.038    0.005      0.00999     2.09e-05         0.01        0.059       0.0773       0.0491       0.0787       0.0639        0.063          0.1       0.0815         0.27      0.00281\n","! Validation         45  185.038    0.005      0.00916      1.6e-06      0.00916       0.0556        0.074       0.0447       0.0775       0.0611       0.0581       0.0985       0.0783       0.0749     0.000781\n","Wall time: 185.03918552999994\n","! Best model       45    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     46     1      0.00805      0.00794     0.000107       0.0519       0.0689       0.0413       0.0731       0.0572       0.0534       0.0925       0.0729        0.766      0.00798\n","     46     2       0.0114       0.0113     8.85e-05       0.0623       0.0824       0.0508       0.0854       0.0681        0.065        0.109       0.0871        0.694      0.00722\n","     46     3       0.0109       0.0108     9.63e-05       0.0614       0.0803       0.0517       0.0807       0.0662       0.0668        0.102       0.0844        0.722      0.00752\n","     46     4      0.00976      0.00968     8.33e-05       0.0596       0.0761       0.0533       0.0723       0.0628       0.0665       0.0923       0.0794        0.674      0.00702\n","     46     5       0.0093       0.0093     4.28e-07        0.057       0.0746       0.0456         0.08       0.0628        0.058       0.0998       0.0789       0.0402     0.000419\n","     46     6      0.00816      0.00804     0.000115       0.0537       0.0694       0.0431       0.0749        0.059       0.0543       0.0925       0.0734        0.795      0.00828\n","     46     7      0.00886      0.00885     9.06e-06       0.0552       0.0728        0.045       0.0755       0.0603       0.0581       0.0957       0.0769        0.217      0.00226\n","     46     8      0.00994      0.00984     0.000106       0.0579       0.0767       0.0478       0.0783        0.063       0.0618          0.1        0.081        0.764      0.00795\n","     46     9         0.01         0.01     7.37e-06       0.0607       0.0774       0.0523       0.0774       0.0649       0.0659       0.0964       0.0812        0.179      0.00187\n","     46    10       0.0106       0.0106     2.51e-05       0.0607       0.0795       0.0512       0.0797       0.0654       0.0656        0.102       0.0837        0.369      0.00384\n","     46    11      0.00784      0.00775     9.08e-05       0.0516       0.0681       0.0392       0.0765       0.0578       0.0499       0.0945       0.0722        0.701       0.0073\n","     46    12      0.00723      0.00723     1.13e-06       0.0488       0.0658        0.039       0.0683       0.0537       0.0498       0.0896       0.0697       0.0707     0.000736\n","     46    13         0.01      0.00993     7.08e-05       0.0577       0.0771       0.0458       0.0815       0.0636       0.0593        0.104       0.0816         0.62      0.00645\n","     46    14      0.00728      0.00728      3.3e-06       0.0507        0.066       0.0411       0.0698       0.0555       0.0522       0.0873       0.0697         0.11      0.00115\n","     46    15      0.00924      0.00923     6.48e-06       0.0566       0.0743       0.0473        0.075       0.0612       0.0617       0.0947       0.0782         0.16      0.00166\n","     46    16      0.00874      0.00874     1.01e-06       0.0546       0.0723       0.0445       0.0747       0.0596       0.0563       0.0967       0.0765       0.0617     0.000643\n","     46    17      0.00974      0.00973     5.02e-06       0.0584       0.0763       0.0486        0.078       0.0633        0.062        0.099       0.0805        0.163       0.0017\n","     46    18       0.0101       0.0101      7.6e-06       0.0594       0.0776       0.0485        0.081       0.0648       0.0625        0.101       0.0819        0.172      0.00179\n","     46    19      0.00961      0.00956     4.23e-05       0.0571       0.0757       0.0455       0.0802       0.0629       0.0585        0.102       0.0801        0.465      0.00484\n","     46    20       0.0084      0.00839        1e-05       0.0536       0.0709        0.043       0.0749       0.0589       0.0557       0.0942       0.0749        0.221      0.00231\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     46     1      0.00951       0.0095     2.68e-06       0.0571       0.0754       0.0458       0.0797       0.0627       0.0595       0.0999       0.0797        0.102      0.00106\n","     46     2      0.00939      0.00939     9.78e-07       0.0559        0.075       0.0445       0.0787       0.0616       0.0582          0.1       0.0793       0.0558     0.000581\n","     46     3      0.00863      0.00863     1.52e-06        0.054       0.0719       0.0431       0.0757       0.0594       0.0556       0.0964        0.076       0.0761     0.000792\n","     46     4       0.0092       0.0092     1.22e-06       0.0554       0.0742       0.0443       0.0774       0.0609       0.0574       0.0997       0.0785       0.0727     0.000757\n","     46     5      0.00863      0.00863     1.59e-06       0.0542       0.0719       0.0441       0.0745       0.0593       0.0576       0.0941       0.0758       0.0668     0.000696\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              46  189.048    0.005      0.00921     4.38e-05      0.00926       0.0564       0.0743       0.0462       0.0769       0.0615       0.0594       0.0974       0.0784        0.398      0.00415\n","! Validation         46  189.048    0.005      0.00907      1.6e-06      0.00907       0.0553       0.0737       0.0444       0.0772       0.0608       0.0577       0.0981       0.0779       0.0746     0.000777\n","Wall time: 189.04890502199987\n","! Best model       46    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     47     1       0.0102       0.0102     2.92e-06       0.0578       0.0781       0.0465       0.0804       0.0634       0.0611        0.104       0.0826        0.112      0.00117\n","     47     2       0.0108       0.0108     1.07e-05       0.0621       0.0804       0.0534       0.0795       0.0665       0.0673        0.102       0.0845        0.211       0.0022\n","     47     3      0.00864      0.00863     1.66e-05        0.055       0.0719        0.045        0.075         0.06       0.0576       0.0941       0.0759        0.301      0.00314\n","     47     4       0.0087       0.0087     5.26e-06       0.0545       0.0722       0.0443        0.075       0.0596       0.0566       0.0959       0.0763        0.158      0.00164\n","     47     5      0.00976      0.00976     1.78e-06       0.0582       0.0764       0.0455       0.0835       0.0645        0.057        0.105        0.081       0.0705     0.000734\n","     47     6       0.0108       0.0108     1.26e-05       0.0607       0.0804       0.0488       0.0846       0.0667       0.0633        0.107        0.085        0.261      0.00271\n","     47     7      0.00969      0.00965     3.64e-05       0.0595        0.076       0.0526       0.0734        0.063        0.066       0.0928       0.0794        0.441       0.0046\n","     47     8      0.00748      0.00746     2.17e-05       0.0509       0.0668       0.0417       0.0693       0.0555       0.0532       0.0879       0.0706         0.34      0.00354\n","     47     9      0.00913      0.00912     8.93e-06       0.0559       0.0739       0.0467       0.0742       0.0605       0.0588       0.0972        0.078        0.209      0.00218\n","     47    10       0.0121       0.0121     1.63e-05       0.0659       0.0852       0.0563       0.0852       0.0707       0.0708        0.108       0.0896        0.295      0.00307\n","     47    11       0.0107       0.0106     3.17e-05       0.0618       0.0797       0.0523        0.081       0.0666       0.0663        0.101       0.0838          0.4      0.00417\n","     47    12      0.00879      0.00876     3.02e-05       0.0549       0.0724        0.043       0.0785       0.0608       0.0565       0.0966       0.0766        0.393      0.00409\n","     47    13      0.00963      0.00956     7.02e-05        0.059       0.0757       0.0507       0.0754       0.0631       0.0647       0.0938       0.0793        0.617      0.00643\n","     47    14       0.0118       0.0118     1.11e-05       0.0649       0.0839       0.0557       0.0832       0.0695       0.0706        0.106       0.0881        0.244      0.00255\n","     47    15       0.0113       0.0113     3.38e-05        0.064       0.0821       0.0544       0.0833       0.0688       0.0681        0.105       0.0864        0.426      0.00443\n","     47    16      0.00918      0.00917     5.17e-06       0.0559       0.0741       0.0437       0.0804       0.0621       0.0558        0.101       0.0785        0.167      0.00174\n","     47    17      0.00849      0.00847     2.28e-05        0.054       0.0712       0.0426       0.0769       0.0597       0.0555       0.0951       0.0753        0.342      0.00356\n","     47    18       0.0124       0.0124     2.63e-05       0.0666       0.0862        0.058       0.0838       0.0709       0.0727        0.108       0.0904         0.38      0.00396\n","     47    19       0.0138       0.0137     3.59e-05       0.0721       0.0907       0.0655       0.0853       0.0754       0.0802        0.109       0.0945        0.438      0.00457\n","     47    20      0.00845      0.00844     1.71e-05       0.0553       0.0711       0.0473       0.0712       0.0592       0.0597       0.0896       0.0746        0.304      0.00317\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     47     1       0.0094       0.0094     2.64e-06       0.0568        0.075       0.0455       0.0794       0.0624        0.059       0.0995       0.0793        0.101      0.00105\n","     47     2      0.00929      0.00929     9.68e-07       0.0556       0.0746       0.0442       0.0783       0.0612       0.0578          0.1       0.0789       0.0565     0.000589\n","     47     3      0.00855      0.00854     1.49e-06       0.0537       0.0715       0.0429       0.0754       0.0592       0.0553       0.0961       0.0757       0.0743     0.000774\n","     47     4      0.00911       0.0091     1.18e-06       0.0551       0.0738        0.044       0.0771       0.0606       0.0569       0.0993       0.0781       0.0718     0.000748\n","     47     5      0.00855      0.00855     1.55e-06        0.054       0.0715       0.0439       0.0742       0.0591       0.0573       0.0937       0.0755       0.0645     0.000671\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              47  193.078    0.005       0.0101     2.09e-05       0.0101       0.0595       0.0776       0.0497       0.0789       0.0643       0.0635          0.1       0.0818        0.305      0.00318\n","! Validation         47  193.078    0.005      0.00898     1.57e-06      0.00898        0.055       0.0733       0.0441       0.0769       0.0605       0.0573       0.0978       0.0775       0.0736     0.000767\n","Wall time: 193.07858714099984\n","! Best model       47    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     48     1      0.00929      0.00929     2.35e-06       0.0571       0.0746       0.0474       0.0765       0.0619         0.06       0.0974       0.0787        0.108      0.00113\n","     48     2       0.0151       0.0151     2.68e-06       0.0761       0.0951       0.0693       0.0898       0.0796       0.0856        0.112       0.0987       0.0963        0.001\n","     48     3       0.0134       0.0134     8.74e-06       0.0703       0.0896       0.0645       0.0818       0.0732       0.0805        0.106        0.093        0.199      0.00207\n","     48     4      0.00944      0.00943     1.84e-06       0.0551       0.0751       0.0434       0.0785       0.0609       0.0582        0.101       0.0795       0.0867     0.000903\n","     48     5      0.00934      0.00932     2.23e-05       0.0568       0.0747        0.047       0.0763       0.0617       0.0597       0.0981       0.0789        0.331      0.00345\n","     48     6       0.0116       0.0116     9.05e-05       0.0642       0.0831       0.0544       0.0837       0.0691       0.0689        0.106       0.0875        0.701       0.0073\n","     48     7      0.00898      0.00896     2.21e-05       0.0553       0.0732        0.046        0.074         0.06        0.059       0.0955       0.0772        0.344      0.00358\n","     48     8        0.008      0.00787     0.000127       0.0519       0.0686       0.0411       0.0737       0.0574       0.0526       0.0927       0.0727        0.833      0.00868\n","     48     9      0.00943       0.0094     2.31e-05       0.0577        0.075       0.0485       0.0759       0.0622       0.0617       0.0963        0.079        0.352      0.00367\n","     48    10      0.00992      0.00992     1.21e-06       0.0598       0.0771       0.0515       0.0763       0.0639       0.0653       0.0964       0.0808       0.0529     0.000551\n","     48    11      0.00819      0.00812     7.92e-05       0.0525       0.0697       0.0424       0.0727       0.0575       0.0549       0.0924       0.0737        0.658      0.00685\n","     48    12       0.0088      0.00877      2.5e-05       0.0558       0.0725       0.0457        0.076       0.0609       0.0573       0.0959       0.0766        0.369      0.00385\n","     48    13      0.00858      0.00858     1.32e-06       0.0548       0.0717       0.0437        0.077       0.0603       0.0558       0.0958       0.0758       0.0717     0.000747\n","     48    14      0.00958      0.00958     3.32e-06       0.0567       0.0757       0.0442       0.0817       0.0629        0.059        0.101       0.0801        0.128      0.00133\n","     48    15      0.00892      0.00892     9.52e-07       0.0554       0.0731       0.0452       0.0756       0.0604        0.058       0.0963       0.0772       0.0662      0.00069\n","     48    16      0.00874      0.00874     1.42e-06       0.0552       0.0723       0.0438        0.078       0.0609        0.056        0.097       0.0765       0.0707     0.000736\n","     48    17      0.00804      0.00803     5.47e-06       0.0529       0.0693        0.043       0.0728       0.0579        0.055       0.0915       0.0732        0.151      0.00157\n","     48    18      0.00815      0.00812     2.85e-05       0.0533       0.0697       0.0425        0.075       0.0587       0.0542       0.0933       0.0738        0.376      0.00391\n","     48    19      0.00787      0.00786     4.96e-06       0.0514       0.0686       0.0412       0.0719       0.0565       0.0528       0.0924       0.0726        0.147      0.00153\n","     48    20      0.00806      0.00806     2.05e-06       0.0515       0.0695       0.0409       0.0727       0.0568       0.0534       0.0936       0.0735       0.0928     0.000966\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     48     1      0.00932      0.00932     2.72e-06       0.0565       0.0747       0.0452       0.0791       0.0621       0.0587       0.0992       0.0789        0.104      0.00108\n","     48     2      0.00922      0.00922        9e-07       0.0553       0.0743       0.0439        0.078        0.061       0.0575       0.0997       0.0786       0.0534     0.000556\n","     48     3      0.00848      0.00848     1.45e-06       0.0535       0.0712       0.0426       0.0752       0.0589        0.055       0.0958       0.0754       0.0743     0.000774\n","     48     4      0.00902      0.00902     1.26e-06       0.0548       0.0735       0.0438       0.0769       0.0603       0.0566        0.099       0.0778       0.0737     0.000768\n","     48     5      0.00845      0.00845     1.56e-06       0.0537       0.0711       0.0436       0.0739       0.0588       0.0569       0.0933       0.0751       0.0646     0.000673\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              48  197.098    0.005      0.00945     2.27e-05      0.00948       0.0572       0.0752       0.0473        0.077       0.0621        0.061       0.0976       0.0793        0.262      0.00273\n","! Validation         48  197.098    0.005       0.0089     1.58e-06       0.0089       0.0548        0.073       0.0438       0.0766       0.0602       0.0569       0.0974       0.0772        0.074      0.00077\n","Wall time: 197.09906785999988\n","! Best model       48    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     49     1      0.00795      0.00791     4.34e-05       0.0516       0.0688       0.0425         0.07       0.0562       0.0542       0.0913       0.0727        0.477      0.00497\n","     49     2       0.0082      0.00818     1.84e-05       0.0525         0.07       0.0424       0.0727       0.0576        0.055       0.0929        0.074        0.316       0.0033\n","     49     3      0.00815      0.00815     2.49e-06       0.0527       0.0698       0.0426       0.0728       0.0577       0.0549       0.0927       0.0738        0.107      0.00111\n","     49     4      0.00861       0.0086     4.83e-06        0.054       0.0717       0.0436       0.0749       0.0593       0.0569       0.0947       0.0758        0.144       0.0015\n","     49     5       0.0101      0.00999     5.97e-05       0.0597       0.0773       0.0503       0.0783       0.0643       0.0636       0.0992       0.0814        0.569      0.00592\n","     49     6       0.0109       0.0109     9.82e-06       0.0622       0.0806       0.0531       0.0805       0.0668       0.0681        0.101       0.0846        0.221      0.00231\n","     49     7      0.00808      0.00808     9.71e-07       0.0533       0.0695       0.0431       0.0736       0.0584       0.0548       0.0922       0.0735       0.0512     0.000533\n","     49     8      0.00886      0.00886     3.37e-06       0.0555       0.0728       0.0453       0.0759       0.0606       0.0571       0.0969        0.077        0.126      0.00131\n","     49     9      0.00927      0.00921      5.5e-05       0.0591       0.0743       0.0531       0.0709        0.062       0.0652       0.0897       0.0774        0.546      0.00569\n","     49    10      0.00792      0.00791      6.8e-06       0.0525       0.0688       0.0425       0.0724       0.0574       0.0542       0.0913       0.0727        0.175      0.00183\n","     49    11        0.009      0.00899     2.32e-06        0.055       0.0734       0.0458       0.0735       0.0596       0.0593       0.0955       0.0774       0.0916     0.000954\n","     49    12       0.0111       0.0111     1.25e-05       0.0636       0.0816       0.0546       0.0815        0.068       0.0682        0.103       0.0857        0.257      0.00267\n","     49    13       0.0138       0.0137        6e-05       0.0699       0.0906       0.0572       0.0953       0.0763        0.072        0.119       0.0957        0.572      0.00596\n","     49    14       0.0108       0.0108     9.88e-06       0.0616       0.0804       0.0501       0.0847       0.0674       0.0632        0.107        0.085        0.229      0.00238\n","     49    15       0.0082       0.0082     5.82e-06       0.0543         0.07        0.045       0.0728       0.0589       0.0567        0.091       0.0739         0.16      0.00167\n","     49    16      0.00982       0.0098     1.49e-05       0.0573       0.0766        0.048        0.076        0.062       0.0631       0.0981       0.0806        0.286      0.00298\n","     49    17       0.0129       0.0129     1.39e-05       0.0695        0.088       0.0636       0.0813       0.0725       0.0797        0.103       0.0911        0.261      0.00272\n","     49    18      0.00934      0.00932     1.63e-05       0.0574       0.0747       0.0476       0.0771       0.0623       0.0601       0.0975       0.0788        0.281      0.00293\n","     49    19      0.00752      0.00749     2.28e-05       0.0504        0.067       0.0399       0.0713       0.0556       0.0497       0.0923        0.071        0.348      0.00362\n","     49    20      0.00861       0.0086     9.15e-06       0.0544       0.0718       0.0434       0.0764       0.0599       0.0558        0.096       0.0759        0.221       0.0023\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     49     1      0.00923      0.00922     2.68e-06       0.0562       0.0743       0.0449       0.0788       0.0618       0.0583       0.0988       0.0785        0.103      0.00107\n","     49     2      0.00913      0.00913     9.81e-07        0.055       0.0739       0.0437       0.0778       0.0607       0.0571       0.0994       0.0782       0.0561     0.000584\n","     49     3      0.00842      0.00842     1.48e-06       0.0533        0.071       0.0424       0.0751       0.0587       0.0547       0.0956       0.0751       0.0738     0.000769\n","     49     4      0.00894      0.00894      1.2e-06       0.0545       0.0732       0.0435       0.0766         0.06       0.0562       0.0987       0.0774       0.0727     0.000757\n","     49     5      0.00837      0.00836     1.56e-06       0.0534       0.0708       0.0433       0.0736       0.0585       0.0565       0.0929       0.0747       0.0655     0.000683\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              49  201.119    0.005      0.00944     1.86e-05      0.00945       0.0573       0.0751       0.0477       0.0766       0.0621        0.061       0.0975       0.0792        0.272      0.00283\n","! Validation         49  201.119    0.005      0.00882     1.58e-06      0.00882       0.0545       0.0726       0.0436       0.0764         0.06       0.0566       0.0971       0.0768       0.0742     0.000773\n","Wall time: 201.1199231459998\n","! Best model       49    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     50     1      0.00875      0.00869     5.99e-05       0.0541       0.0721       0.0433       0.0758       0.0595       0.0548        0.098       0.0764        0.572      0.00596\n","     50     2      0.00814      0.00814        4e-06       0.0528       0.0698        0.043       0.0722       0.0576       0.0544       0.0932       0.0738        0.145      0.00151\n","     50     3      0.00835      0.00834     1.12e-05        0.053       0.0706       0.0429       0.0732        0.058       0.0565       0.0927       0.0746        0.235      0.00245\n","     50     4      0.00785      0.00783     1.47e-05       0.0521       0.0685       0.0412       0.0739       0.0576       0.0517       0.0934       0.0725        0.279       0.0029\n","     50     5      0.00758      0.00758     1.14e-06       0.0513       0.0674       0.0419       0.0702        0.056       0.0529       0.0895       0.0712       0.0781     0.000814\n","     50     6      0.00725      0.00724     2.04e-06       0.0503       0.0658       0.0401       0.0706       0.0554       0.0502       0.0893       0.0697       0.0939     0.000979\n","     50     7      0.00876      0.00875     1.15e-05       0.0557       0.0723       0.0483       0.0705       0.0594       0.0612       0.0906       0.0759        0.239      0.00249\n","     50     8      0.00945      0.00943     1.51e-05       0.0571       0.0751       0.0484       0.0746       0.0615       0.0629       0.0951        0.079        0.264      0.00275\n","     50     9      0.00779      0.00779     2.03e-06       0.0522       0.0683       0.0421       0.0724       0.0572       0.0536       0.0908       0.0722       0.0969      0.00101\n","     50    10         0.01      0.00999     2.08e-05       0.0594       0.0773       0.0506        0.077       0.0638        0.065       0.0973       0.0812        0.336       0.0035\n","     50    11       0.0106       0.0106     7.42e-07       0.0612       0.0796       0.0507       0.0822       0.0664       0.0638        0.104        0.084       0.0535     0.000557\n","     50    12      0.00799      0.00799     7.79e-06       0.0517       0.0691       0.0408       0.0736       0.0572       0.0532       0.0931       0.0732        0.194      0.00202\n","     50    13      0.00878      0.00877     1.19e-05       0.0547       0.0724       0.0432       0.0777       0.0604       0.0555       0.0979       0.0767        0.224      0.00233\n","     50    14      0.00744      0.00742     2.31e-05       0.0511       0.0666       0.0414       0.0704       0.0559       0.0526       0.0882       0.0704        0.352      0.00367\n","     50    15      0.00942      0.00942     4.47e-07       0.0559       0.0751       0.0434       0.0808       0.0621       0.0577        0.101       0.0795       0.0344     0.000358\n","     50    16      0.00838      0.00838     6.61e-06       0.0532       0.0708       0.0417        0.076       0.0589       0.0546       0.0953       0.0749         0.15      0.00157\n","     50    17      0.00986      0.00986     7.45e-07       0.0582       0.0768       0.0465       0.0815        0.064       0.0585        0.104       0.0814       0.0535     0.000557\n","     50    18      0.00934      0.00934     1.11e-06       0.0571       0.0748       0.0447       0.0818       0.0632       0.0568        0.102       0.0792       0.0643     0.000669\n","     50    19      0.00745      0.00745     1.75e-06       0.0502       0.0668       0.0404       0.0699       0.0551        0.052       0.0893       0.0706       0.0953     0.000993\n","     50    20      0.00969      0.00969     1.54e-06       0.0568       0.0762       0.0447       0.0811       0.0629       0.0584        0.103       0.0806       0.0621     0.000647\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     50     1      0.00913      0.00913     2.71e-06       0.0559       0.0739       0.0446       0.0784       0.0615       0.0579       0.0984       0.0782        0.103      0.00107\n","     50     2      0.00906      0.00905     1.01e-06       0.0548       0.0736       0.0435       0.0775       0.0605       0.0568       0.0991       0.0779       0.0559     0.000582\n","     50     3      0.00836      0.00836     1.49e-06       0.0531       0.0707       0.0422       0.0748       0.0585       0.0544       0.0953       0.0749       0.0728     0.000758\n","     50     4      0.00886      0.00886     1.19e-06       0.0542       0.0728       0.0432       0.0763       0.0598       0.0558       0.0983       0.0771       0.0717     0.000747\n","     50     5      0.00829      0.00829     1.54e-06       0.0532       0.0704       0.0431       0.0734       0.0582       0.0562       0.0925       0.0744       0.0646     0.000673\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              50  205.157    0.005      0.00863     9.91e-06      0.00864       0.0544       0.0719        0.044       0.0753       0.0596       0.0565       0.0955        0.076        0.181      0.00189\n","! Validation         50  205.157    0.005      0.00874     1.59e-06      0.00874       0.0542       0.0723       0.0433       0.0761       0.0597       0.0562       0.0968       0.0765       0.0736     0.000766\n","Wall time: 205.15798153699984\n","! Best model       50    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     51     1       0.0113       0.0113     4.34e-05       0.0644       0.0822       0.0556       0.0821       0.0689       0.0714        0.101       0.0859        0.484      0.00504\n","     51     2      0.00958      0.00958     7.81e-07       0.0586       0.0757       0.0493       0.0773       0.0633       0.0613       0.0984       0.0798       0.0631     0.000657\n","     51     3      0.00813      0.00813     9.66e-07       0.0519       0.0698       0.0403       0.0751       0.0577       0.0512       0.0967        0.074       0.0643     0.000669\n","     51     4      0.00982       0.0098     1.41e-05        0.058       0.0766       0.0464       0.0812       0.0638       0.0595        0.103        0.081        0.267      0.00278\n","     51     5       0.0137       0.0136     0.000103       0.0703       0.0902       0.0618       0.0872       0.0745        0.077        0.112       0.0945        0.746      0.00777\n","     51     6        0.012       0.0119     4.09e-05       0.0662       0.0845       0.0595       0.0797       0.0696       0.0749        0.101       0.0879        0.475      0.00494\n","     51     7      0.00841      0.00839      2.3e-05       0.0537       0.0709       0.0438       0.0735       0.0586       0.0561       0.0936       0.0749        0.354      0.00368\n","     51     8      0.00874      0.00873     5.71e-06       0.0553       0.0723       0.0452       0.0755       0.0603       0.0579       0.0948       0.0763        0.126      0.00131\n","     51     9      0.00943      0.00942     4.96e-06       0.0587       0.0751       0.0503       0.0754       0.0629       0.0636        0.094       0.0788         0.16      0.00167\n","     51    10      0.00982      0.00981     7.55e-06       0.0588       0.0766       0.0481       0.0802       0.0641       0.0612        0.101       0.0809        0.192        0.002\n","     51    11      0.00951      0.00947     4.35e-05       0.0577       0.0753        0.046        0.081       0.0635       0.0584        0.101       0.0796        0.489      0.00509\n","     51    12      0.00842      0.00841     8.66e-06       0.0537        0.071       0.0432       0.0746       0.0589       0.0548       0.0954       0.0751         0.21      0.00219\n","     51    13      0.00824      0.00815     8.88e-05       0.0526       0.0699       0.0423       0.0733       0.0578       0.0545       0.0933       0.0739        0.699      0.00728\n","     51    14       0.0094      0.00938     1.79e-05        0.057       0.0749       0.0477       0.0758       0.0617       0.0615       0.0964       0.0789        0.299      0.00312\n","     51    15       0.0089      0.00885     5.79e-05        0.055       0.0728       0.0441       0.0766       0.0604       0.0562       0.0978        0.077        0.562      0.00586\n","     51    16      0.00851      0.00842     8.14e-05       0.0536        0.071       0.0432       0.0745       0.0588       0.0545       0.0959       0.0752        0.666      0.00693\n","     51    17       0.0073       0.0073     2.09e-06        0.052       0.0661       0.0453       0.0654       0.0554       0.0567       0.0816       0.0692       0.0814     0.000848\n","     51    18      0.00832      0.00824     8.24e-05        0.053       0.0702       0.0422       0.0748       0.0585       0.0538       0.0949       0.0743        0.674      0.00702\n","     51    19       0.0116       0.0116     6.48e-06       0.0623       0.0832       0.0492       0.0885       0.0688       0.0641        0.112       0.0881        0.185      0.00193\n","     51    20      0.00921      0.00921     1.86e-06       0.0576       0.0742       0.0495       0.0738       0.0616       0.0633       0.0923       0.0778       0.0814     0.000848\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     51     1      0.00905      0.00905     2.67e-06       0.0556       0.0736       0.0443       0.0782       0.0612       0.0576       0.0981       0.0778        0.103      0.00107\n","     51     2      0.00898      0.00898     9.87e-07       0.0546       0.0733       0.0432       0.0773       0.0603       0.0564       0.0987       0.0776       0.0544     0.000567\n","     51     3       0.0083       0.0083     1.43e-06       0.0529       0.0705        0.042       0.0746       0.0583       0.0542       0.0951       0.0746       0.0733     0.000764\n","     51     4      0.00878      0.00878     1.23e-06        0.054       0.0725       0.0429       0.0761       0.0595       0.0555        0.098       0.0767       0.0731     0.000762\n","     51     5      0.00821      0.00821     1.53e-06       0.0529       0.0701       0.0429       0.0731        0.058       0.0559       0.0922        0.074       0.0641     0.000667\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              51  209.200    0.005      0.00948     3.18e-05      0.00951       0.0575       0.0753       0.0476       0.0773       0.0625        0.061       0.0979       0.0795        0.344      0.00358\n","! Validation         51  209.200    0.005      0.00866     1.57e-06      0.00866        0.054        0.072       0.0431       0.0759       0.0595       0.0559       0.0965       0.0762       0.0735     0.000766\n","Wall time: 209.20101238599977\n","! Best model       51    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     52     1      0.00831      0.00831     7.46e-06       0.0528       0.0705       0.0414       0.0756       0.0585       0.0527       0.0968       0.0747         0.18      0.00187\n","     52     2       0.0104       0.0104     9.89e-06       0.0621       0.0789       0.0559       0.0747       0.0653       0.0693       0.0951       0.0822        0.219      0.00228\n","     52     3      0.00891       0.0089     1.38e-06       0.0558        0.073       0.0477       0.0721       0.0599       0.0614        0.092       0.0767       0.0789     0.000822\n","     52     4      0.00804      0.00804      9.6e-07       0.0526       0.0694       0.0411       0.0755       0.0583       0.0532       0.0937       0.0734        0.059     0.000614\n","     52     5      0.00778      0.00776     2.13e-05        0.052       0.0682       0.0418       0.0724       0.0571       0.0536       0.0905       0.0721        0.323      0.00337\n","     52     6      0.00916      0.00915     2.25e-06       0.0564        0.074       0.0453       0.0785       0.0619       0.0584       0.0981       0.0782        0.103      0.00108\n","     52     7       0.0085      0.00848      1.7e-05       0.0546       0.0712       0.0432       0.0773       0.0602       0.0543       0.0966       0.0754        0.303      0.00315\n","     52     8      0.00953      0.00953     3.67e-06       0.0567       0.0755       0.0459       0.0782        0.062       0.0598       0.0998       0.0798       0.0934     0.000972\n","     52     9      0.00787      0.00783     3.48e-05       0.0516       0.0685        0.042       0.0708       0.0564       0.0535       0.0913       0.0724        0.437      0.00455\n","     52    10      0.00908      0.00908     1.12e-06       0.0567       0.0737       0.0479       0.0743       0.0611       0.0606       0.0946       0.0776       0.0734     0.000765\n","     52    11      0.00902      0.00896     6.35e-05       0.0565       0.0732       0.0487        0.072       0.0603       0.0625       0.0909       0.0767        0.587      0.00612\n","     52    12       0.0076      0.00757     3.09e-05       0.0514       0.0673       0.0426       0.0689       0.0558       0.0536       0.0885       0.0711        0.408      0.00425\n","     52    13      0.00824      0.00822     2.08e-05       0.0525       0.0701       0.0427       0.0721       0.0574       0.0554       0.0928       0.0741        0.337      0.00351\n","     52    14      0.00937      0.00931     6.67e-05       0.0576       0.0746       0.0475       0.0778       0.0626         0.06       0.0976       0.0788        0.604       0.0063\n","     52    15       0.0101       0.0101     4.73e-06       0.0584       0.0779       0.0454       0.0843       0.0649       0.0582        0.107       0.0825        0.135       0.0014\n","     52    16      0.00734      0.00733     8.09e-06        0.051       0.0663       0.0421       0.0688       0.0554       0.0525       0.0875         0.07        0.196      0.00205\n","     52    17      0.00879      0.00879     1.23e-06       0.0535       0.0725       0.0415       0.0774       0.0595       0.0542       0.0995       0.0769       0.0709     0.000739\n","     52    18      0.00874      0.00874     3.94e-06        0.056       0.0723       0.0457       0.0768       0.0612       0.0577        0.095       0.0764        0.144       0.0015\n","     52    19      0.00815      0.00815     6.66e-07       0.0521       0.0698        0.041       0.0743       0.0576       0.0532       0.0948        0.074       0.0594     0.000618\n","     52    20       0.0094      0.00938     1.58e-05       0.0578       0.0749        0.051       0.0714       0.0612       0.0658       0.0904       0.0781        0.275      0.00286\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     52     1      0.00896      0.00896      2.7e-06       0.0553       0.0732       0.0441       0.0778       0.0609       0.0572       0.0977       0.0775        0.103      0.00107\n","     52     2       0.0089       0.0089     1.07e-06       0.0543        0.073        0.043        0.077         0.06       0.0561       0.0984       0.0772        0.057     0.000594\n","     52     3      0.00824      0.00824     1.44e-06       0.0527       0.0702       0.0418       0.0744       0.0581       0.0539       0.0949       0.0744       0.0716     0.000746\n","     52     4       0.0087       0.0087     1.23e-06       0.0537       0.0721       0.0427       0.0758       0.0593       0.0551       0.0977       0.0764       0.0736     0.000767\n","     52     5      0.00813      0.00813     1.49e-06       0.0527       0.0697       0.0426       0.0728       0.0577       0.0555       0.0918       0.0737       0.0634      0.00066\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              52  213.233    0.005       0.0087     1.58e-05      0.00872       0.0549       0.0722        0.045       0.0747       0.0598       0.0577       0.0947       0.0762        0.234      0.00244\n","! Validation         52  213.233    0.005      0.00858     1.59e-06      0.00859       0.0538       0.0717       0.0428       0.0756       0.0592       0.0556       0.0961       0.0758       0.0737     0.000768\n","Wall time: 213.23346349199983\n","! Best model       52    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     53     1      0.00923       0.0092     2.58e-05       0.0575       0.0742       0.0474       0.0777       0.0625       0.0604       0.0961       0.0782         0.37      0.00385\n","     53     2       0.0102       0.0102     1.19e-05       0.0604       0.0782       0.0476       0.0859       0.0668       0.0599        0.106       0.0827        0.245      0.00255\n","     53     3      0.00793      0.00791     1.85e-05       0.0517       0.0688        0.041       0.0732       0.0571       0.0518        0.094       0.0729         0.31      0.00323\n","     53     4      0.00806      0.00804     1.85e-05       0.0535       0.0694       0.0442       0.0723       0.0582       0.0557       0.0907       0.0732          0.3      0.00313\n","     53     5      0.00844      0.00844      8.8e-07       0.0545       0.0711       0.0454       0.0726        0.059       0.0567       0.0934       0.0751        0.065     0.000677\n","     53     6      0.00977      0.00973     4.79e-05       0.0565       0.0763       0.0447       0.0801       0.0624       0.0584        0.103       0.0808        0.511      0.00532\n","     53     7      0.00742       0.0074     2.17e-05       0.0512       0.0666       0.0414       0.0709       0.0561       0.0519       0.0889       0.0704        0.342      0.00356\n","     53     8      0.00977      0.00973     4.61e-05       0.0588       0.0763       0.0499       0.0765       0.0632       0.0638       0.0966       0.0802        0.496      0.00517\n","     53     9      0.00707      0.00703      3.6e-05       0.0491       0.0649       0.0382       0.0711       0.0546       0.0484       0.0891       0.0687        0.441       0.0046\n","     53    10       0.0088      0.00879     8.75e-06        0.057       0.0725       0.0509       0.0692         0.06       0.0638       0.0874       0.0756        0.209      0.00218\n","     53    11      0.00823      0.00821     1.89e-05       0.0546       0.0701       0.0471       0.0696       0.0584       0.0586       0.0888       0.0737        0.315      0.00328\n","     53    12      0.00786      0.00786     2.46e-06       0.0513       0.0686        0.039       0.0759       0.0574       0.0503       0.0951       0.0727        0.102      0.00106\n","     53    13      0.00811      0.00804     6.91e-05       0.0519       0.0694       0.0395       0.0765        0.058       0.0502        0.097       0.0736        0.616      0.00642\n","     53    14      0.00852      0.00849     3.49e-05       0.0538       0.0713       0.0423       0.0768       0.0595        0.054        0.097       0.0755        0.435      0.00453\n","     53    15       0.0073      0.00721     8.04e-05       0.0487       0.0657       0.0375       0.0712       0.0543       0.0491       0.0901       0.0696        0.662      0.00689\n","     53    16      0.00821      0.00813     7.41e-05       0.0525       0.0698       0.0433        0.071       0.0571       0.0558       0.0916       0.0737        0.636      0.00662\n","     53    17      0.00784      0.00783      3.2e-06       0.0519       0.0685       0.0411       0.0734       0.0572       0.0526       0.0924       0.0725        0.111      0.00116\n","     53    18      0.00789      0.00783     5.53e-05       0.0519       0.0685       0.0424       0.0709       0.0567       0.0542       0.0904       0.0723        0.544      0.00566\n","     53    19         0.01         0.01     1.39e-06       0.0584       0.0774       0.0465       0.0821       0.0643       0.0599        0.104       0.0819       0.0705     0.000734\n","     53    20      0.00804      0.00804     1.11e-06        0.052       0.0694       0.0419        0.072        0.057       0.0539       0.0928       0.0734       0.0652      0.00068\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     53     1      0.00888      0.00888     2.73e-06       0.0551       0.0729       0.0438       0.0775       0.0607       0.0569       0.0973       0.0771        0.103      0.00107\n","     53     2      0.00882      0.00882     1.03e-06       0.0541       0.0727       0.0428       0.0768       0.0598       0.0558       0.0981       0.0769       0.0564     0.000588\n","     53     3      0.00818      0.00818     1.48e-06       0.0525         0.07       0.0416       0.0742       0.0579       0.0536       0.0946       0.0741       0.0733     0.000764\n","     53     4      0.00862      0.00862      1.3e-06       0.0535       0.0718       0.0424       0.0756        0.059       0.0548       0.0974       0.0761       0.0761     0.000792\n","     53     5      0.00806      0.00806     1.47e-06       0.0524       0.0694       0.0424       0.0726       0.0575       0.0552       0.0915       0.0733       0.0641     0.000667\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              53  217.265    0.005      0.00841     2.88e-05      0.00844       0.0539       0.0709       0.0436       0.0744        0.059       0.0557       0.0943        0.075        0.342      0.00356\n","! Validation         53  217.265    0.005      0.00851      1.6e-06      0.00851       0.0535       0.0714       0.0426       0.0753        0.059       0.0552       0.0958       0.0755       0.0745     0.000776\n","Wall time: 217.26600102499992\n","! Best model       53    0.009\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     54     1      0.00731      0.00731     1.17e-06       0.0502       0.0662       0.0409       0.0688       0.0549       0.0526       0.0872       0.0699        0.075     0.000781\n","     54     2      0.00891      0.00891     6.91e-06       0.0548        0.073       0.0426       0.0792       0.0609       0.0547          0.1       0.0774         0.19      0.00198\n","     54     3      0.00783      0.00782     1.01e-05       0.0532       0.0684       0.0457        0.068       0.0569       0.0577       0.0859       0.0718        0.235      0.00245\n","     54     4      0.00841      0.00838      2.8e-05       0.0539       0.0708       0.0433        0.075       0.0592       0.0546       0.0953        0.075        0.379      0.00394\n","     54     5      0.00803      0.00796     6.79e-05       0.0526        0.069       0.0409       0.0759       0.0584       0.0518       0.0944       0.0731        0.609      0.00635\n","     54     6      0.00706      0.00703     2.24e-05         0.05       0.0649       0.0407       0.0687       0.0547       0.0511       0.0861       0.0686        0.346      0.00361\n","     54     7       0.0103       0.0103     1.51e-05       0.0603       0.0785       0.0476       0.0856       0.0666       0.0609        0.105       0.0831        0.273      0.00285\n","     54     8       0.0118       0.0116     0.000183        0.064       0.0835       0.0528       0.0865       0.0696       0.0682        0.108        0.088        0.999       0.0104\n","     54     9      0.00915      0.00915     2.93e-06       0.0574        0.074       0.0509       0.0703       0.0606       0.0644       0.0901       0.0773       0.0947     0.000987\n","     54    10      0.00837      0.00834     2.75e-05       0.0528       0.0707       0.0415       0.0754       0.0584       0.0536       0.0961       0.0748        0.387      0.00403\n","     54    11      0.00874      0.00873     4.12e-06       0.0552       0.0723       0.0461       0.0735       0.0598       0.0589       0.0935       0.0762         0.15      0.00156\n","     54    12      0.00959      0.00959     3.62e-06       0.0589       0.0758       0.0493       0.0781       0.0637       0.0617       0.0979       0.0798        0.137      0.00143\n","     54    13      0.00895      0.00894     1.24e-06       0.0555       0.0732       0.0439       0.0785       0.0612        0.056       0.0989       0.0775       0.0627     0.000653\n","     54    14      0.00793      0.00785     7.34e-05       0.0513       0.0686       0.0404       0.0731       0.0568       0.0526       0.0926       0.0726        0.632      0.00658\n","     54    15      0.00773      0.00772     6.95e-06       0.0515        0.068       0.0418       0.0708       0.0563       0.0527       0.0911       0.0719        0.195      0.00203\n","     54    16       0.0084      0.00834     6.14e-05       0.0528       0.0706       0.0423       0.0738        0.058       0.0548       0.0947       0.0747        0.576        0.006\n","     54    17      0.00932      0.00925     7.26e-05        0.055       0.0744        0.042       0.0808       0.0614       0.0547        0.103       0.0789        0.628      0.00655\n","     54    18      0.00791       0.0079     7.82e-06       0.0522       0.0688       0.0416       0.0735       0.0576       0.0533       0.0922       0.0728        0.196      0.00204\n","     54    19      0.00916      0.00906     9.82e-05       0.0559       0.0736       0.0473       0.0733       0.0603       0.0604       0.0947       0.0776        0.736      0.00766\n","     54    20       0.0084      0.00835     5.29e-05       0.0547       0.0707       0.0465       0.0711       0.0588       0.0583       0.0905       0.0744        0.536      0.00558\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     54     1      0.00879      0.00878     2.64e-06       0.0548       0.0725       0.0435       0.0772       0.0604       0.0564        0.097       0.0767        0.101      0.00105\n","     54     2      0.00874      0.00874     1.14e-06       0.0538       0.0723       0.0425       0.0765       0.0595       0.0554       0.0977       0.0766       0.0585     0.000609\n","     54     3      0.00812      0.00812     1.43e-06       0.0523       0.0697       0.0414        0.074       0.0577       0.0533       0.0943       0.0738       0.0699     0.000728\n","     54     4      0.00854      0.00854     1.22e-06       0.0532       0.0715       0.0422       0.0753       0.0587       0.0544        0.097       0.0757       0.0732     0.000763\n","     54     5      0.00798      0.00798     1.46e-06       0.0522       0.0691       0.0421       0.0724       0.0573       0.0549       0.0911        0.073        0.062     0.000646\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              54  221.425    0.005      0.00863     3.73e-05      0.00867       0.0546       0.0719       0.0444        0.075       0.0597       0.0568       0.0951       0.0759        0.372      0.00387\n","! Validation         54  221.425    0.005      0.00843     1.58e-06      0.00843       0.0533        0.071       0.0424       0.0751       0.0587       0.0549       0.0955       0.0752        0.073      0.00076\n","Wall time: 221.42555265399983\n","! Best model       54    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     55     1       0.0065       0.0065     1.47e-06       0.0476       0.0624       0.0381       0.0665       0.0523       0.0479       0.0841        0.066       0.0834     0.000869\n","     55     2      0.00764      0.00759     4.63e-05       0.0508       0.0674       0.0404       0.0716        0.056       0.0511       0.0917       0.0714        0.503      0.00524\n","     55     3      0.00911      0.00908     3.33e-05       0.0563       0.0737       0.0462       0.0765       0.0613       0.0587        0.097       0.0779         0.42      0.00438\n","     55     4      0.00794      0.00792     1.93e-05       0.0517       0.0689       0.0412       0.0728        0.057       0.0527       0.0932       0.0729        0.324      0.00338\n","     55     5      0.00769      0.00769     1.72e-06       0.0514       0.0679       0.0408       0.0725       0.0566       0.0513       0.0924       0.0719       0.0844     0.000879\n","     55     6      0.00806      0.00805     5.21e-06       0.0514       0.0694       0.0401       0.0739        0.057       0.0533       0.0937       0.0735        0.162      0.00169\n","     55     7      0.00879      0.00879     3.64e-06       0.0552       0.0725       0.0443        0.077       0.0606       0.0569       0.0965       0.0767        0.124      0.00129\n","     55     8      0.00995      0.00995     7.27e-06       0.0593       0.0772       0.0497       0.0785       0.0641       0.0626          0.1       0.0813        0.195      0.00203\n","     55     9       0.0132       0.0131     0.000114       0.0694       0.0886       0.0611       0.0859       0.0735       0.0767        0.109       0.0927        0.794      0.00827\n","     55    10       0.0123       0.0123     1.29e-05       0.0679       0.0859       0.0606       0.0825       0.0715       0.0754        0.104       0.0896        0.248      0.00259\n","     55    11       0.0081      0.00805      4.7e-05       0.0535       0.0694       0.0436       0.0732       0.0584       0.0552       0.0915       0.0733        0.506      0.00527\n","     55    12      0.00753       0.0075     3.05e-05       0.0508        0.067       0.0414       0.0695       0.0554       0.0529       0.0888       0.0708        0.404      0.00421\n","     55    13      0.00972      0.00971     5.69e-06        0.058       0.0762       0.0486       0.0767       0.0626       0.0626       0.0979       0.0803        0.157      0.00163\n","     55    14       0.0113       0.0112     5.27e-05       0.0626        0.082       0.0519       0.0839       0.0679       0.0665        0.106       0.0865        0.536      0.00559\n","     55    15      0.00855      0.00855     2.17e-06        0.053       0.0715       0.0421       0.0748       0.0585       0.0567       0.0944       0.0756       0.0982      0.00102\n","     55    16      0.00785      0.00784      2.7e-06       0.0527       0.0685       0.0418       0.0743       0.0581       0.0534       0.0916       0.0725       0.0934     0.000972\n","     55    17         0.01      0.00999     6.71e-06       0.0596       0.0773       0.0486       0.0815       0.0651        0.061        0.103       0.0817        0.172      0.00179\n","     55    18      0.00971       0.0097     6.54e-06       0.0595       0.0762       0.0526       0.0733       0.0629       0.0664       0.0928       0.0796        0.173       0.0018\n","     55    19      0.00874      0.00874     9.67e-07       0.0547       0.0723       0.0448       0.0745       0.0597       0.0564       0.0965       0.0765       0.0488     0.000509\n","     55    20      0.00785      0.00779     5.71e-05       0.0516       0.0683       0.0409        0.073        0.057       0.0526       0.0919       0.0723        0.551      0.00574\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     55     1      0.00871      0.00871     2.81e-06       0.0545       0.0722       0.0433        0.077       0.0601       0.0561       0.0967       0.0764        0.104      0.00109\n","     55     2      0.00867      0.00867     1.01e-06       0.0536       0.0721       0.0423       0.0763       0.0593       0.0551       0.0974       0.0763        0.055     0.000573\n","     55     3      0.00806      0.00806     1.44e-06        0.052       0.0694       0.0412       0.0737       0.0575        0.053       0.0941       0.0735       0.0737     0.000768\n","     55     4      0.00847      0.00847     1.31e-06        0.053       0.0712       0.0419       0.0751       0.0585       0.0541       0.0968       0.0754       0.0757     0.000788\n","     55     5      0.00792      0.00791      1.4e-06        0.052       0.0688       0.0419       0.0721        0.057       0.0546       0.0908       0.0727       0.0614      0.00064\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              55  225.486    0.005      0.00901     2.29e-05      0.00903       0.0558       0.0734       0.0459       0.0756       0.0608        0.059        0.096       0.0775        0.284      0.00296\n","! Validation         55  225.486    0.005      0.00836     1.59e-06      0.00837        0.053       0.0708       0.0421       0.0748       0.0585       0.0546       0.0952       0.0749        0.074     0.000771\n","Wall time: 225.4867199209998\n","! Best model       55    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     56     1      0.00833      0.00828     4.27e-05       0.0533       0.0704       0.0441       0.0716       0.0579       0.0571       0.0914       0.0742        0.479      0.00498\n","     56     2      0.00896      0.00895     9.11e-06        0.055       0.0732       0.0438       0.0774       0.0606       0.0568       0.0981       0.0774        0.218      0.00227\n","     56     3      0.00805      0.00803     2.06e-05       0.0534       0.0693       0.0439       0.0723       0.0581       0.0553       0.0911       0.0732        0.331      0.00345\n","     56     4      0.00808      0.00807     1.06e-05       0.0535       0.0695       0.0428       0.0749       0.0589       0.0537       0.0934       0.0736        0.231       0.0024\n","     56     5      0.00739      0.00739     5.86e-07         0.05       0.0665       0.0394        0.071       0.0552       0.0504       0.0905       0.0704       0.0443     0.000462\n","     56     6      0.00788      0.00788     3.21e-06       0.0524       0.0687       0.0421       0.0731       0.0576       0.0531       0.0923       0.0727        0.128      0.00133\n","     56     7      0.00913      0.00912     9.64e-06       0.0564       0.0739       0.0461       0.0771       0.0616        0.059        0.097        0.078        0.228      0.00237\n","     56     8      0.00855      0.00854     5.96e-06       0.0542       0.0715       0.0441       0.0744       0.0593       0.0556       0.0956       0.0756        0.179      0.00187\n","     56     9      0.00772      0.00772     5.23e-07       0.0511        0.068       0.0406        0.072       0.0563       0.0519       0.0921        0.072       0.0424     0.000441\n","     56    10       0.0125       0.0125     1.04e-06       0.0654       0.0864       0.0527       0.0907       0.0717        0.068        0.115       0.0914       0.0672       0.0007\n","     56    11       0.0113       0.0112      6.1e-05       0.0655        0.082       0.0588       0.0789       0.0689       0.0715       0.0997       0.0856        0.575      0.00599\n","     56    12       0.0107       0.0107      4.2e-06       0.0623       0.0799       0.0535       0.0798       0.0667       0.0681       0.0995       0.0838        0.124       0.0013\n","     56    13      0.00839      0.00835     4.25e-05       0.0529       0.0707       0.0411       0.0765       0.0588       0.0532       0.0966       0.0749        0.478      0.00498\n","     56    14      0.00802      0.00799     2.72e-05       0.0525       0.0692       0.0408       0.0757       0.0583       0.0521       0.0945       0.0733        0.382      0.00398\n","     56    15      0.00744      0.00743     4.55e-06       0.0501       0.0667       0.0394       0.0716       0.0555       0.0502       0.0912       0.0707        0.154       0.0016\n","     56    16      0.00696      0.00694     2.37e-05       0.0481       0.0644       0.0377        0.069       0.0533       0.0476        0.089       0.0683        0.354      0.00369\n","     56    17      0.00904      0.00902     1.14e-05       0.0558       0.0735       0.0443       0.0786       0.0615       0.0578       0.0976       0.0777        0.244      0.00254\n","     56    18      0.00747      0.00747     2.42e-06       0.0508       0.0669       0.0415       0.0693       0.0554       0.0523       0.0891       0.0707        0.102      0.00106\n","     56    19      0.00707      0.00706      3.2e-06       0.0497        0.065       0.0414       0.0664       0.0539       0.0522        0.085       0.0686        0.107      0.00112\n","     56    20      0.00758      0.00758     3.24e-06       0.0505       0.0674       0.0392        0.073       0.0561       0.0509       0.0918       0.0714        0.113      0.00118\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     56     1      0.00864      0.00863      2.7e-06       0.0543       0.0719        0.043       0.0767       0.0599       0.0558       0.0963       0.0761        0.102      0.00106\n","     56     2      0.00861      0.00861     1.09e-06       0.0534       0.0718       0.0421        0.076       0.0591       0.0549       0.0971        0.076       0.0563     0.000587\n","     56     3        0.008        0.008      1.4e-06       0.0518       0.0692        0.041       0.0736       0.0573       0.0527       0.0939       0.0733       0.0701      0.00073\n","     56     4      0.00841       0.0084     1.26e-06       0.0528       0.0709       0.0417       0.0749       0.0583       0.0537       0.0965       0.0751       0.0742     0.000773\n","     56     5      0.00785      0.00785     1.38e-06       0.0517       0.0685       0.0417       0.0719       0.0568       0.0543       0.0905       0.0724       0.0604     0.000629\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              56  229.509    0.005      0.00851     1.44e-05      0.00853       0.0541       0.0714       0.0439       0.0747       0.0593       0.0562       0.0947       0.0754        0.229      0.00239\n","! Validation         56  229.509    0.005       0.0083     1.57e-06       0.0083       0.0528       0.0705       0.0419       0.0746       0.0583       0.0543       0.0949       0.0746       0.0725     0.000756\n","Wall time: 229.51012924399993\n","! Best model       56    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     57     1      0.00818      0.00817     1.99e-06       0.0536       0.0699       0.0436       0.0736       0.0586       0.0551       0.0927       0.0739       0.0842     0.000877\n","     57     2      0.00766      0.00765     5.38e-06       0.0514       0.0677       0.0418       0.0705       0.0562       0.0533       0.0898       0.0715        0.139      0.00145\n","     57     3      0.00665      0.00665     3.05e-06       0.0477       0.0631       0.0379       0.0672       0.0526       0.0481       0.0855       0.0668        0.122      0.00127\n","     57     4      0.00739      0.00739     3.01e-06       0.0497       0.0665       0.0396         0.07       0.0548       0.0512       0.0897       0.0704        0.103      0.00107\n","     57     5      0.00781      0.00781     5.01e-06       0.0521       0.0683       0.0414       0.0734       0.0574       0.0526       0.0921       0.0723        0.158      0.00165\n","     57     6      0.00935      0.00927     8.61e-05       0.0574       0.0745       0.0451       0.0819       0.0635       0.0565        0.101       0.0789        0.686      0.00715\n","     57     7      0.00877      0.00876     5.99e-06       0.0541       0.0724       0.0441        0.074        0.059       0.0574       0.0955       0.0765        0.159      0.00166\n","     57     8      0.00832      0.00828     3.72e-05       0.0529       0.0704       0.0423       0.0741       0.0582       0.0554       0.0935       0.0744         0.45      0.00469\n","     57     9      0.00743      0.00742     6.45e-06       0.0501       0.0667         0.04       0.0703       0.0551        0.052        0.089       0.0705        0.161      0.00167\n","     57    10      0.00756      0.00755     5.38e-06       0.0513       0.0672       0.0417       0.0706       0.0562       0.0531        0.089       0.0711        0.157      0.00164\n","     57    11      0.00731       0.0073     1.05e-05       0.0501       0.0661       0.0399       0.0706       0.0552       0.0512       0.0887       0.0699        0.234      0.00244\n","     57    12      0.00739      0.00738      5.7e-06       0.0492       0.0665       0.0374       0.0728       0.0551       0.0476       0.0933       0.0705        0.173       0.0018\n","     57    13      0.00706      0.00706     6.63e-07       0.0494        0.065       0.0399       0.0684       0.0541       0.0514       0.0859       0.0687       0.0525     0.000547\n","     57    14      0.00764      0.00763     1.52e-05       0.0516       0.0676       0.0408       0.0734       0.0571       0.0512        0.092       0.0716        0.287      0.00299\n","     57    15      0.00801      0.00801      2.9e-06       0.0525       0.0692       0.0436       0.0704        0.057       0.0553       0.0909       0.0731        0.103      0.00107\n","     57    16      0.00818      0.00818     6.57e-06       0.0531         0.07       0.0448       0.0697       0.0572       0.0567       0.0908       0.0738        0.189      0.00197\n","     57    17      0.00948      0.00946     2.47e-05       0.0565       0.0752       0.0445       0.0804       0.0625       0.0579        0.101       0.0796        0.367      0.00382\n","     57    18      0.00811      0.00811     1.73e-06       0.0527       0.0697       0.0415       0.0752       0.0583       0.0531       0.0944       0.0738       0.0793     0.000826\n","     57    19       0.0099      0.00987     2.19e-05       0.0578       0.0769       0.0469       0.0795       0.0632       0.0609        0.102       0.0812        0.347      0.00361\n","     57    20       0.0103       0.0102     3.22e-05       0.0596       0.0783       0.0491       0.0807       0.0649       0.0619        0.103       0.0827         0.42      0.00437\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     57     1      0.00856      0.00856     2.68e-06        0.054       0.0716       0.0428       0.0764       0.0596       0.0555        0.096       0.0757        0.101      0.00105\n","     57     2      0.00854      0.00854      1.2e-06       0.0532       0.0715       0.0419       0.0759       0.0589       0.0545       0.0969       0.0757       0.0604     0.000629\n","     57     3      0.00795      0.00794     1.43e-06       0.0516        0.069       0.0408       0.0733       0.0571       0.0524       0.0937        0.073       0.0694     0.000723\n","     57     4      0.00833      0.00833     1.26e-06       0.0525       0.0706       0.0415       0.0747       0.0581       0.0534       0.0962       0.0748       0.0734     0.000765\n","     57     5      0.00778      0.00778     1.42e-06       0.0515       0.0682       0.0414       0.0717       0.0565        0.054       0.0902       0.0721       0.0607     0.000633\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              57  233.543    0.005      0.00811     1.41e-05      0.00812       0.0526       0.0697       0.0423       0.0733       0.0578       0.0542       0.0932       0.0737        0.224      0.00233\n","! Validation         57  233.543    0.005      0.00823      1.6e-06      0.00823       0.0526       0.0702       0.0417       0.0744        0.058        0.054       0.0946       0.0743        0.073      0.00076\n","Wall time: 233.54392095599997\n","! Best model       57    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     58     1      0.00785      0.00785     3.74e-06       0.0526       0.0685       0.0414        0.075       0.0582       0.0521       0.0931       0.0726        0.138      0.00144\n","     58     2      0.00659      0.00658     1.26e-05        0.047       0.0627       0.0371        0.067        0.052       0.0474       0.0856       0.0665        0.258      0.00269\n","     58     3      0.00885      0.00881      4.2e-05       0.0544       0.0726       0.0437       0.0759       0.0598       0.0563       0.0973       0.0768        0.473      0.00493\n","     58     4       0.0084      0.00838     1.76e-05        0.054       0.0708       0.0447       0.0727       0.0587       0.0568       0.0927       0.0748        0.304      0.00316\n","     58     5      0.00868      0.00867     1.04e-05       0.0544        0.072       0.0434       0.0765       0.0599       0.0551       0.0974       0.0763        0.219      0.00228\n","     58     6      0.00771      0.00769     1.78e-05       0.0516       0.0679       0.0414        0.072       0.0567       0.0533       0.0902       0.0717        0.305      0.00318\n","     58     7      0.00666      0.00663     2.52e-05       0.0486        0.063       0.0398       0.0663       0.0531       0.0493       0.0839       0.0666        0.369      0.00384\n","     58     8       0.0076       0.0076     2.88e-06       0.0511       0.0674       0.0414       0.0705        0.056       0.0538       0.0887       0.0712         0.11      0.00115\n","     58     9      0.00787      0.00787     7.71e-06        0.052       0.0686       0.0408       0.0742       0.0575       0.0521       0.0933       0.0727        0.201      0.00209\n","     58    10       0.0101       0.0101     3.84e-05       0.0594       0.0776       0.0484       0.0812       0.0648       0.0619        0.102        0.082        0.437      0.00456\n","     58    11      0.00868      0.00868     5.13e-06       0.0548       0.0721       0.0475       0.0694       0.0584       0.0601       0.0914       0.0758        0.158      0.00165\n","     58    12      0.00748      0.00747     7.03e-06       0.0513       0.0669       0.0408       0.0724       0.0566       0.0518       0.0897       0.0708        0.177      0.00184\n","     58    13      0.00837      0.00836     5.55e-06       0.0531       0.0707       0.0409       0.0775       0.0592       0.0529       0.0971        0.075        0.166      0.00173\n","     58    14      0.00717      0.00715     1.87e-05       0.0499       0.0654       0.0399       0.0698       0.0549       0.0508       0.0876       0.0692        0.312      0.00325\n","     58    15      0.00868      0.00867     1.23e-05       0.0543        0.072       0.0444        0.074       0.0592       0.0578       0.0943        0.076        0.257      0.00268\n","     58    16      0.00782      0.00782     3.23e-06       0.0514       0.0684       0.0408       0.0725       0.0566        0.052       0.0929       0.0725         0.12      0.00125\n","     58    17       0.0078      0.00777     3.03e-05       0.0515       0.0682       0.0402        0.074       0.0571       0.0508       0.0937       0.0723        0.406      0.00423\n","     58    18      0.00769      0.00769     4.81e-07        0.051       0.0679       0.0409       0.0713       0.0561       0.0523       0.0913       0.0718       0.0445     0.000464\n","     58    19      0.00701        0.007     9.74e-06       0.0485       0.0647       0.0389       0.0677       0.0533       0.0495       0.0875       0.0685        0.218      0.00227\n","     58    20      0.00752      0.00752     9.91e-07       0.0504       0.0671       0.0395       0.0721       0.0558       0.0512       0.0909       0.0711       0.0648     0.000675\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     58     1       0.0085      0.00849     2.66e-06       0.0538       0.0713       0.0426       0.0762       0.0594       0.0552       0.0957       0.0755        0.101      0.00106\n","     58     2      0.00848      0.00847     1.12e-06        0.053       0.0712       0.0417       0.0757       0.0587       0.0542       0.0966       0.0754       0.0574     0.000598\n","     58     3      0.00789      0.00789     1.42e-06       0.0515       0.0687       0.0406       0.0732       0.0569       0.0521       0.0935       0.0728       0.0707     0.000736\n","     58     4      0.00827      0.00826     1.28e-06       0.0523       0.0703       0.0412       0.0744       0.0578       0.0531       0.0959       0.0745       0.0746     0.000777\n","     58     5      0.00772      0.00771     1.38e-06       0.0513       0.0679       0.0412       0.0714       0.0563       0.0537       0.0899       0.0718       0.0618     0.000644\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              58  237.574    0.005      0.00791     1.36e-05      0.00793       0.0521       0.0688       0.0418       0.0726       0.0572       0.0535       0.0921       0.0728        0.237      0.00247\n","! Validation         58  237.574    0.005      0.00817     1.57e-06      0.00817       0.0524       0.0699       0.0415       0.0742       0.0578       0.0537       0.0944        0.074       0.0732     0.000762\n","Wall time: 237.57469818699997\n","! Best model       58    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     59     1      0.00664      0.00664     3.22e-06       0.0472        0.063       0.0378       0.0661       0.0519       0.0481       0.0854       0.0668        0.121      0.00127\n","     59     2      0.00713      0.00713     9.06e-07       0.0491       0.0653        0.038       0.0713       0.0546       0.0481       0.0904       0.0692       0.0582     0.000606\n","     59     3      0.00893      0.00892     3.19e-06        0.056       0.0731       0.0459        0.076        0.061       0.0587       0.0956       0.0771        0.114      0.00118\n","     59     4      0.00872       0.0087     1.78e-05       0.0553       0.0722       0.0467       0.0724       0.0596       0.0595       0.0924        0.076        0.307       0.0032\n","     59     5      0.00832      0.00832     2.49e-06       0.0522       0.0706       0.0404       0.0759       0.0582       0.0529       0.0966       0.0748          0.1      0.00104\n","     59     6      0.00756      0.00754     1.27e-05       0.0515       0.0672       0.0414       0.0716       0.0565       0.0525       0.0896       0.0711        0.261      0.00272\n","     59     7      0.00741       0.0074     1.06e-05       0.0514       0.0666       0.0424       0.0693       0.0558       0.0535        0.087       0.0702        0.241      0.00251\n","     59     8      0.00881       0.0088     1.27e-05       0.0542       0.0726       0.0412       0.0801       0.0607       0.0531        0.101       0.0769        0.263      0.00274\n","     59     9      0.00736      0.00731     4.47e-05       0.0497       0.0662       0.0399       0.0694       0.0547       0.0515       0.0885         0.07        0.496      0.00517\n","     59    10      0.00795      0.00789     5.89e-05       0.0524       0.0687       0.0426       0.0722       0.0574       0.0549       0.0902       0.0725        0.561      0.00585\n","     59    11      0.00667      0.00667     3.73e-06       0.0481       0.0632       0.0385       0.0673       0.0529       0.0487        0.085       0.0669        0.136      0.00142\n","     59    12      0.00867      0.00865     1.97e-05       0.0539        0.072       0.0433       0.0751       0.0592       0.0555       0.0968       0.0762        0.318      0.00331\n","     59    13      0.00779      0.00777     1.82e-05       0.0508       0.0682       0.0404       0.0714       0.0559       0.0514       0.0931       0.0723        0.299      0.00312\n","     59    14      0.00766      0.00766     1.33e-06       0.0515       0.0677       0.0402        0.074       0.0571       0.0512       0.0923       0.0717       0.0736     0.000767\n","     59    15      0.00695      0.00695     1.66e-06       0.0484       0.0645       0.0386       0.0679       0.0533       0.0488       0.0879       0.0683       0.0785     0.000818\n","     59    16      0.00771      0.00771     3.54e-06       0.0514       0.0679       0.0401       0.0738        0.057       0.0521       0.0917       0.0719        0.121      0.00126\n","     59    17      0.00709      0.00709     5.32e-06         0.05       0.0651       0.0409       0.0682       0.0546       0.0513       0.0863       0.0688        0.163       0.0017\n","     59    18      0.00825      0.00824     1.51e-05       0.0536       0.0702       0.0427       0.0755       0.0591       0.0541       0.0945       0.0743        0.269      0.00281\n","     59    19        0.008      0.00798     1.71e-05       0.0521       0.0691       0.0417       0.0729       0.0573       0.0544       0.0917       0.0731        0.293      0.00305\n","     59    20      0.00964      0.00962     1.78e-05       0.0589       0.0759       0.0501       0.0765       0.0633       0.0635        0.096       0.0798        0.311      0.00323\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     59     1      0.00842      0.00842     2.61e-06       0.0535        0.071       0.0423       0.0759       0.0591       0.0548       0.0954       0.0751       0.0997      0.00104\n","     59     2       0.0084       0.0084     1.26e-06       0.0528       0.0709       0.0414       0.0754       0.0584       0.0539       0.0963       0.0751       0.0608     0.000634\n","     59     3      0.00784      0.00784     1.44e-06       0.0513       0.0685       0.0404        0.073       0.0567       0.0518       0.0933       0.0726       0.0698     0.000727\n","     59     4      0.00819      0.00819     1.24e-06       0.0521         0.07        0.041       0.0742       0.0576       0.0528       0.0956       0.0742       0.0726     0.000756\n","     59     5      0.00764      0.00764     1.38e-06       0.0511       0.0676        0.041       0.0712       0.0561       0.0533       0.0896       0.0715       0.0606     0.000632\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              59  241.603    0.005      0.00785     1.35e-05      0.00786       0.0519       0.0685       0.0417       0.0724        0.057       0.0533       0.0917       0.0725        0.229      0.00239\n","! Validation         59  241.603    0.005       0.0081     1.59e-06       0.0081       0.0521       0.0696       0.0412       0.0739       0.0576       0.0533       0.0941       0.0737       0.0727     0.000757\n","Wall time: 241.6041189189998\n","! Best model       59    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     60     1       0.0109       0.0109     1.41e-05       0.0632       0.0808       0.0533        0.083       0.0682       0.0678        0.102       0.0848        0.271      0.00282\n","     60     2       0.0122       0.0122     3.27e-05       0.0658       0.0854       0.0539       0.0894       0.0717        0.068        0.112       0.0902        0.423      0.00441\n","     60     3      0.00981      0.00981     6.83e-06       0.0586       0.0766       0.0476       0.0806       0.0641       0.0602        0.102        0.081        0.185      0.00192\n","     60     4      0.00815      0.00814     6.24e-06       0.0522       0.0698       0.0416       0.0736       0.0576        0.054       0.0937       0.0739         0.18      0.00188\n","     60     5      0.00722      0.00717     5.27e-05       0.0493       0.0655       0.0408       0.0663       0.0535       0.0522       0.0862       0.0692        0.537      0.00559\n","     60     6      0.00859      0.00858      9.2e-06       0.0557       0.0717       0.0479       0.0714       0.0597       0.0607       0.0896       0.0752        0.207      0.00216\n","     60     7      0.00801      0.00797     3.82e-05       0.0517       0.0691       0.0409       0.0731        0.057       0.0533       0.0929       0.0731        0.458      0.00477\n","     60     8      0.00772      0.00766      5.8e-05       0.0514       0.0677       0.0415       0.0712       0.0564       0.0519       0.0915       0.0717         0.56      0.00583\n","     60     9      0.00901      0.00899      1.5e-05       0.0568       0.0734       0.0489       0.0727       0.0608       0.0617       0.0923        0.077        0.276      0.00288\n","     60    10      0.00804        0.008     4.56e-05       0.0534       0.0692       0.0434       0.0732       0.0583       0.0542       0.0921       0.0732          0.5      0.00521\n","     60    11      0.00859      0.00857     2.27e-05       0.0546       0.0716        0.042       0.0797       0.0609       0.0528       0.0991       0.0759        0.346      0.00361\n","     60    12      0.00701      0.00701     3.42e-06       0.0493       0.0648       0.0387       0.0705       0.0546       0.0489       0.0883       0.0686         0.11      0.00114\n","     60    13      0.00764      0.00764     4.77e-06       0.0511       0.0676       0.0402       0.0729       0.0565       0.0511       0.0922       0.0716        0.138      0.00143\n","     60    14      0.00821      0.00816      4.5e-05       0.0536       0.0699       0.0434        0.074       0.0587       0.0539        0.094        0.074        0.492      0.00513\n","     60    15       0.0112       0.0111     1.03e-05       0.0629       0.0817       0.0541       0.0806       0.0673       0.0683        0.103       0.0858        0.212      0.00221\n","     60    16       0.0122       0.0122     3.28e-06       0.0653       0.0855       0.0535       0.0887       0.0711       0.0692        0.111       0.0902        0.121      0.00126\n","     60    17      0.00998      0.00997     5.18e-06        0.059       0.0773       0.0487       0.0795       0.0641       0.0612        0.102       0.0816        0.162      0.00169\n","     60    18      0.00858      0.00858     1.56e-06       0.0538       0.0717       0.0445       0.0725       0.0585       0.0576       0.0937       0.0756       0.0832     0.000867\n","     60    19      0.00865      0.00865     2.21e-06       0.0537       0.0719       0.0418       0.0776       0.0597       0.0549       0.0975       0.0762          0.1      0.00104\n","     60    20        0.011       0.0109     5.22e-05       0.0626       0.0808        0.055       0.0776       0.0663       0.0704       0.0984       0.0844        0.531      0.00553\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     60     1      0.00836      0.00835     2.75e-06       0.0533       0.0707       0.0421       0.0757       0.0589       0.0545       0.0952       0.0748        0.103      0.00107\n","     60     2      0.00834      0.00833     1.08e-06       0.0526       0.0706       0.0412       0.0752       0.0582       0.0536        0.096       0.0748       0.0554     0.000577\n","     60     3      0.00779      0.00778     1.41e-06       0.0511       0.0683       0.0402       0.0728       0.0565       0.0516       0.0931       0.0723       0.0707     0.000736\n","     60     4      0.00814      0.00813      1.3e-06       0.0519       0.0698       0.0408       0.0741       0.0574       0.0524       0.0954       0.0739       0.0753     0.000784\n","     60     5      0.00758      0.00758     1.32e-06       0.0508       0.0674       0.0408        0.071       0.0559        0.053       0.0894       0.0712       0.0583     0.000607\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              60  245.627    0.005      0.00911     2.15e-05      0.00914       0.0562       0.0739       0.0461       0.0764       0.0612        0.059        0.097        0.078        0.295      0.00307\n","! Validation         60  245.627    0.005      0.00804     1.57e-06      0.00804       0.0519       0.0694        0.041       0.0738       0.0574        0.053       0.0938       0.0734       0.0724     0.000755\n","Wall time: 245.6282340219998\n","! Best model       60    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     61     1       0.0133       0.0133     3.15e-06         0.07       0.0891       0.0624       0.0853       0.0738       0.0774        0.109        0.093        0.129      0.00134\n","     61     2      0.00929      0.00928     5.27e-06       0.0568       0.0745       0.0464       0.0774       0.0619       0.0596       0.0978       0.0787        0.151      0.00158\n","     61     3      0.00683      0.00679     3.58e-05        0.048       0.0638       0.0375       0.0692       0.0533       0.0478       0.0874       0.0676        0.442      0.00461\n","     61     4      0.00761      0.00761     1.35e-06       0.0514       0.0675       0.0407       0.0728       0.0567       0.0511       0.0919       0.0715       0.0604     0.000629\n","     61     5      0.00796      0.00788     7.25e-05       0.0528       0.0687       0.0446       0.0693        0.057       0.0553       0.0897       0.0725        0.629      0.00655\n","     61     6      0.00675      0.00675     2.66e-07       0.0486       0.0636       0.0402       0.0653       0.0528       0.0508       0.0834       0.0671       0.0342     0.000356\n","     61     7      0.00727      0.00724     2.71e-05       0.0497       0.0658       0.0393       0.0705       0.0549       0.0506       0.0888       0.0697        0.378      0.00393\n","     61     8       0.0077       0.0077     2.66e-06       0.0522       0.0679       0.0426       0.0713       0.0569       0.0541       0.0893       0.0717        0.108      0.00113\n","     61     9      0.00905      0.00904     1.74e-05       0.0567       0.0735       0.0479       0.0742       0.0611       0.0602       0.0948       0.0775        0.287      0.00299\n","     61    10      0.00775      0.00774     1.22e-05       0.0516        0.068       0.0406       0.0735        0.057       0.0514       0.0928       0.0721        0.254      0.00265\n","     61    11      0.00749      0.00748     1.18e-05       0.0502       0.0669        0.041       0.0686       0.0548        0.053       0.0883       0.0707         0.25       0.0026\n","     61    12      0.00979      0.00974     4.39e-05       0.0578       0.0764       0.0465       0.0803       0.0634       0.0599        0.102       0.0807        0.487      0.00508\n","     61    13      0.00976      0.00975     1.27e-05       0.0595       0.0764       0.0507       0.0769       0.0638       0.0631       0.0977       0.0804        0.261      0.00271\n","     61    14      0.00946      0.00944     1.98e-05       0.0569       0.0752        0.047       0.0767       0.0619       0.0599       0.0989       0.0794        0.326       0.0034\n","     61    15      0.00794      0.00794     3.37e-06        0.052       0.0689       0.0399       0.0763       0.0581       0.0514       0.0947        0.073        0.104      0.00109\n","     61    16      0.00886      0.00885     6.99e-06       0.0562       0.0728       0.0481       0.0724       0.0602        0.061        0.092       0.0765        0.189      0.00197\n","     61    17       0.0104       0.0104     3.11e-05       0.0612       0.0788        0.053       0.0775       0.0652       0.0667       0.0987       0.0827        0.404      0.00421\n","     61    18      0.00889      0.00888     9.36e-06       0.0552       0.0729        0.044       0.0777       0.0608       0.0559       0.0985       0.0772        0.216      0.00225\n","     61    19      0.00858      0.00857     5.66e-06       0.0541       0.0716       0.0415       0.0793       0.0604        0.053       0.0988       0.0759        0.153      0.00159\n","     61    20       0.0072      0.00719      1.5e-05       0.0495       0.0656       0.0403       0.0681       0.0542       0.0517       0.0869       0.0693        0.284      0.00296\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     61     1      0.00829      0.00829     2.67e-06       0.0531       0.0704       0.0419       0.0755       0.0587       0.0542       0.0949       0.0745        0.101      0.00105\n","     61     2      0.00827      0.00827     1.09e-06       0.0523       0.0704        0.041        0.075        0.058       0.0533       0.0957       0.0745       0.0561     0.000584\n","     61     3      0.00773      0.00773     1.39e-06       0.0509        0.068         0.04       0.0726       0.0563       0.0513       0.0928       0.0721       0.0699     0.000728\n","     61     4      0.00807      0.00807     1.35e-06       0.0516       0.0695       0.0405       0.0739       0.0572       0.0521       0.0952       0.0737       0.0764     0.000795\n","     61     5      0.00752      0.00752     1.31e-06       0.0506       0.0671       0.0405       0.0708       0.0557       0.0528       0.0891       0.0709       0.0592     0.000616\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              61  249.645    0.005      0.00858     1.69e-05      0.00859       0.0545       0.0716       0.0447       0.0741       0.0594       0.0571       0.0942       0.0757        0.257      0.00268\n","! Validation         61  249.645    0.005      0.00798     1.56e-06      0.00798       0.0517       0.0691       0.0408       0.0735       0.0572       0.0528       0.0936       0.0732       0.0726     0.000756\n","Wall time: 249.64528836\n","! Best model       61    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     62     1      0.00707      0.00707     1.87e-06       0.0499       0.0651       0.0393       0.0711       0.0552       0.0493       0.0885       0.0689       0.0775     0.000808\n","     62     2      0.00769      0.00766     2.28e-05        0.051       0.0677        0.041       0.0709        0.056        0.053       0.0902       0.0716        0.351      0.00366\n","     62     3      0.00726      0.00724     2.02e-05       0.0495       0.0658       0.0391       0.0705       0.0548       0.0507       0.0886       0.0697        0.323      0.00336\n","     62     4       0.0066       0.0066     3.07e-06       0.0468       0.0628       0.0353       0.0699       0.0526       0.0451       0.0883       0.0667        0.104      0.00108\n","     62     5      0.00843      0.00842     1.17e-05       0.0536        0.071        0.043       0.0749       0.0589       0.0549       0.0952       0.0751        0.251      0.00262\n","     62     6      0.00723      0.00723     2.94e-06       0.0496       0.0658       0.0388        0.071       0.0549       0.0492       0.0902       0.0697        0.104      0.00109\n","     62     7      0.00705      0.00704     4.54e-06       0.0502       0.0649       0.0408       0.0691       0.0549       0.0508       0.0865       0.0687        0.148      0.00155\n","     62     8      0.00726      0.00726     7.43e-07       0.0497       0.0659         0.04       0.0692       0.0546       0.0512       0.0883       0.0698       0.0498     0.000519\n","     62     9      0.00806      0.00803     2.79e-05       0.0532       0.0693       0.0447       0.0702       0.0574       0.0558       0.0905       0.0731        0.388      0.00404\n","     62    10      0.00982      0.00982     1.83e-06       0.0587       0.0767       0.0485       0.0792       0.0638       0.0621       0.0996       0.0808       0.0873     0.000909\n","     62    11      0.00705        0.007     5.15e-05       0.0487       0.0647       0.0371       0.0718       0.0545       0.0465       0.0908       0.0687         0.53      0.00552\n","     62    12      0.00825      0.00822     3.15e-05       0.0522       0.0701       0.0411       0.0746       0.0578       0.0534       0.0952       0.0743        0.411      0.00428\n","     62    13      0.00813      0.00809     4.03e-05       0.0523       0.0696       0.0432       0.0703       0.0568        0.056       0.0908       0.0734        0.468      0.00487\n","     62    14      0.00782      0.00774     8.23e-05       0.0502       0.0681       0.0391       0.0726       0.0558       0.0499       0.0944       0.0722        0.672        0.007\n","     62    15      0.00785      0.00784     3.25e-06       0.0511       0.0685       0.0401       0.0732       0.0566       0.0527       0.0924       0.0725        0.112      0.00117\n","     62    16      0.00719      0.00717     1.99e-05       0.0493       0.0655       0.0385        0.071       0.0547       0.0491       0.0897       0.0694        0.327      0.00341\n","     62    17      0.00795      0.00795     1.92e-06        0.052        0.069       0.0425       0.0712       0.0568       0.0546       0.0912       0.0729       0.0912      0.00095\n","     62    18      0.00892      0.00891     4.82e-06       0.0556        0.073        0.047        0.073         0.06       0.0596       0.0944        0.077        0.144       0.0015\n","     62    19      0.00685      0.00684     7.39e-06       0.0487        0.064       0.0391       0.0679       0.0535       0.0491       0.0864       0.0677        0.189      0.00197\n","     62    20      0.00735      0.00735     6.34e-07       0.0512       0.0663       0.0421       0.0694       0.0558       0.0532       0.0869         0.07       0.0469     0.000488\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     62     1      0.00823      0.00822     2.69e-06       0.0529       0.0702       0.0417       0.0752       0.0584       0.0539       0.0946       0.0743        0.101      0.00105\n","     62     2      0.00822      0.00822     1.12e-06       0.0521       0.0701       0.0408       0.0748       0.0578       0.0531       0.0955       0.0743       0.0565     0.000589\n","     62     3      0.00769      0.00769     1.39e-06       0.0507       0.0678       0.0398       0.0725       0.0562       0.0511       0.0926       0.0719       0.0693     0.000722\n","     62     4      0.00801      0.00801     1.31e-06       0.0514       0.0693       0.0403       0.0737        0.057       0.0519       0.0949       0.0734       0.0751     0.000782\n","     62     5      0.00747      0.00747      1.3e-06       0.0504       0.0669       0.0404       0.0706       0.0555       0.0525       0.0889       0.0707       0.0592     0.000616\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              62  253.665    0.005      0.00767     1.71e-05      0.00769       0.0512       0.0678        0.041       0.0715       0.0563       0.0525        0.091       0.0717        0.244      0.00254\n","! Validation         62  253.665    0.005      0.00792     1.56e-06      0.00792       0.0515       0.0689       0.0406       0.0733        0.057       0.0525       0.0933       0.0729       0.0722     0.000752\n","Wall time: 253.666195625\n","! Best model       62    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     63     1      0.00846      0.00844     1.16e-05       0.0532       0.0711       0.0427       0.0742       0.0584       0.0558       0.0945       0.0752        0.246      0.00256\n","     63     2       0.0076      0.00757     2.82e-05       0.0513       0.0673       0.0399       0.0741        0.057       0.0498       0.0929       0.0714        0.392      0.00409\n","     63     3      0.00935      0.00934     1.09e-05       0.0558       0.0748       0.0427        0.082       0.0623       0.0553        0.103       0.0793        0.237      0.00247\n","     63     4      0.00825      0.00825     3.49e-06       0.0542       0.0703       0.0464       0.0698       0.0581       0.0592       0.0883       0.0738        0.116      0.00121\n","     63     5      0.00979      0.00978     9.89e-06       0.0592       0.0765       0.0509       0.0756       0.0633       0.0645       0.0962       0.0803        0.215      0.00224\n","     63     6      0.00781       0.0078     5.51e-06       0.0526       0.0683       0.0429       0.0718       0.0574       0.0545       0.0898       0.0722        0.162      0.00169\n","     63     7      0.00766      0.00765     3.87e-06       0.0507       0.0677       0.0401       0.0717       0.0559       0.0525       0.0908       0.0716        0.112      0.00117\n","     63     8       0.0098       0.0098     3.26e-06        0.059       0.0766       0.0499       0.0773       0.0636       0.0627       0.0986       0.0807        0.125       0.0013\n","     63     9       0.0124       0.0124     1.57e-05       0.0681       0.0862       0.0614       0.0814       0.0714       0.0755        0.104       0.0899         0.28      0.00292\n","     63    10      0.00843      0.00842     8.57e-06       0.0543        0.071       0.0445       0.0738       0.0591       0.0559       0.0943       0.0751        0.215      0.00224\n","     63    11      0.00706      0.00706     3.85e-06       0.0487        0.065       0.0384       0.0692       0.0538       0.0494       0.0883       0.0688         0.13      0.00136\n","     63    12      0.00928      0.00928     4.54e-06       0.0572       0.0745       0.0464       0.0788       0.0626       0.0589       0.0986       0.0787        0.146      0.00152\n","     63    13       0.0108       0.0108     2.35e-05       0.0635       0.0802       0.0551       0.0802       0.0677       0.0682          0.1       0.0841        0.356      0.00371\n","     63    14       0.0106       0.0106     1.16e-06       0.0621       0.0797       0.0544       0.0776        0.066       0.0677       0.0995       0.0836       0.0672       0.0007\n","     63    15      0.00765      0.00765      4.6e-06       0.0515       0.0677       0.0427       0.0693        0.056       0.0543       0.0885       0.0714         0.15      0.00157\n","     63    16      0.00753      0.00752     1.35e-05       0.0506       0.0671       0.0397       0.0725       0.0561       0.0508       0.0914       0.0711        0.266      0.00277\n","     63    17       0.0103       0.0102     6.22e-05       0.0604       0.0781       0.0521       0.0771       0.0646       0.0656       0.0984        0.082        0.583      0.00607\n","     63    18       0.0127       0.0127     8.26e-06       0.0681       0.0871       0.0576       0.0891       0.0733       0.0723        0.111       0.0916        0.202      0.00211\n","     63    19       0.0106       0.0106     1.77e-05       0.0605       0.0796       0.0517       0.0782        0.065       0.0655        0.102       0.0838        0.311      0.00324\n","     63    20      0.00684      0.00682     1.74e-05       0.0484       0.0639        0.039        0.067        0.053       0.0486       0.0867       0.0677        0.304      0.00317\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     63     1      0.00817      0.00817     2.62e-06       0.0526       0.0699       0.0415        0.075       0.0582       0.0537       0.0944        0.074       0.0999      0.00104\n","     63     2      0.00816      0.00816     1.22e-06       0.0519       0.0699       0.0406       0.0746       0.0576       0.0528       0.0953        0.074        0.058     0.000604\n","     63     3      0.00764      0.00764      1.4e-06       0.0505       0.0676       0.0396       0.0723        0.056       0.0508       0.0925       0.0717       0.0688     0.000717\n","     63     4      0.00796      0.00796      1.3e-06       0.0513        0.069       0.0401       0.0735       0.0568       0.0516       0.0947       0.0731       0.0741     0.000772\n","     63     5      0.00741      0.00741     1.29e-06       0.0502       0.0666       0.0402       0.0704       0.0553       0.0522       0.0886       0.0704       0.0582     0.000606\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              63  257.693    0.005      0.00913     1.29e-05      0.00914       0.0565       0.0739       0.0469       0.0755       0.0612       0.0598       0.0961        0.078        0.231      0.00241\n","! Validation         63  257.693    0.005      0.00787     1.57e-06      0.00787       0.0513       0.0686       0.0404       0.0732       0.0568       0.0522       0.0931       0.0727       0.0718     0.000748\n","Wall time: 257.6936186859998\n","! Best model       63    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     64     1      0.00853      0.00853     1.39e-06       0.0536       0.0714       0.0432       0.0743       0.0587       0.0562       0.0949       0.0755       0.0707     0.000736\n","     64     2      0.00956       0.0095     6.39e-05       0.0591       0.0754       0.0502        0.077       0.0636       0.0618        0.097       0.0794        0.593      0.00618\n","     64     3      0.00859      0.00859     2.25e-06       0.0542       0.0717       0.0444       0.0738       0.0591        0.057       0.0944       0.0757         0.11      0.00115\n","     64     4      0.00728      0.00728      2.9e-06       0.0499        0.066       0.0394       0.0708       0.0551       0.0496       0.0903       0.0699        0.112      0.00117\n","     64     5      0.00803      0.00802     8.38e-06       0.0519       0.0693       0.0421       0.0715       0.0568       0.0539       0.0927       0.0733        0.206      0.00214\n","     64     6      0.00771       0.0077     8.27e-06       0.0504       0.0679       0.0395       0.0721       0.0558       0.0507       0.0932        0.072        0.199      0.00208\n","     64     7      0.00754      0.00754     1.03e-06       0.0503       0.0672       0.0395        0.072       0.0557       0.0518       0.0904       0.0711        0.059     0.000614\n","     64     8      0.00771       0.0077     1.21e-05        0.051       0.0679       0.0416       0.0699       0.0557       0.0538       0.0897       0.0717        0.221      0.00231\n","     64     9      0.00713      0.00713     1.82e-06       0.0488       0.0653       0.0395       0.0673       0.0534        0.051       0.0872       0.0691       0.0834     0.000869\n","     64    10      0.00809      0.00808     2.42e-06       0.0524       0.0696       0.0414       0.0745        0.058       0.0534       0.0938       0.0736        0.111      0.00116\n","     64    11      0.00656      0.00656     2.52e-06       0.0474       0.0627       0.0382        0.066       0.0521       0.0482       0.0845       0.0663        0.109      0.00113\n","     64    12      0.00808      0.00807     4.77e-06       0.0518       0.0695       0.0393       0.0768       0.0581       0.0498       0.0977       0.0737        0.151      0.00157\n","     64    13      0.00837      0.00837     9.66e-07       0.0526       0.0708       0.0419       0.0738       0.0579       0.0532       0.0968        0.075       0.0701      0.00073\n","     64    14      0.00766      0.00761     5.03e-05       0.0519       0.0675       0.0437       0.0684       0.0561       0.0551       0.0871       0.0711        0.524      0.00546\n","     64    15      0.00728      0.00728      1.1e-06       0.0496        0.066       0.0409        0.067        0.054       0.0524       0.0871       0.0697       0.0703     0.000732\n","     64    16      0.00788      0.00784     3.55e-05       0.0523       0.0685       0.0411       0.0745       0.0578       0.0529       0.0922       0.0725        0.441       0.0046\n","     64    17      0.00791      0.00788     2.64e-05       0.0518       0.0687        0.041       0.0735       0.0572       0.0533       0.0921       0.0727        0.376      0.00392\n","     64    18      0.00773      0.00773     3.22e-06       0.0497        0.068       0.0376        0.074       0.0558       0.0502        0.094       0.0721        0.118      0.00123\n","     64    19      0.00843      0.00839     3.31e-05       0.0532       0.0709       0.0396       0.0805         0.06       0.0507       0.0996       0.0752        0.411      0.00428\n","     64    20      0.00715      0.00714     8.42e-06       0.0495       0.0654       0.0398       0.0688       0.0543       0.0509       0.0875       0.0692         0.21      0.00219\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     64     1      0.00811       0.0081     2.66e-06       0.0524       0.0696       0.0412       0.0748        0.058       0.0534       0.0941       0.0737       0.0994      0.00104\n","     64     2      0.00811      0.00811     1.19e-06       0.0518       0.0697       0.0404       0.0744       0.0574       0.0526        0.095       0.0738       0.0568     0.000592\n","     64     3       0.0076       0.0076      1.4e-06       0.0504       0.0674       0.0395       0.0722       0.0558       0.0506       0.0923       0.0715       0.0688     0.000717\n","     64     4       0.0079       0.0079     1.31e-06        0.051       0.0688       0.0399       0.0733       0.0566       0.0513       0.0944       0.0729       0.0753     0.000784\n","     64     5      0.00737      0.00737     1.23e-06       0.0501       0.0664         0.04       0.0702       0.0551        0.052       0.0885       0.0702       0.0559     0.000582\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              64  261.713    0.005      0.00785     1.35e-05      0.00786       0.0516       0.0685       0.0412       0.0723       0.0568       0.0529       0.0922       0.0725        0.212      0.00221\n","! Validation         64  261.713    0.005      0.00782     1.56e-06      0.00782       0.0511       0.0684       0.0402        0.073       0.0566        0.052       0.0929       0.0724       0.0712     0.000742\n","Wall time: 261.7135286849998\n","! Best model       64    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     65     1      0.00775      0.00775      3.6e-07       0.0508       0.0681       0.0396       0.0732       0.0564       0.0515       0.0928       0.0721        0.032     0.000334\n","     65     2      0.00784      0.00783     5.02e-06       0.0512       0.0685        0.041       0.0716       0.0563       0.0523       0.0927       0.0725        0.161      0.00168\n","     65     3      0.00855      0.00853     1.63e-05       0.0551       0.0715        0.047       0.0712       0.0591       0.0593        0.091       0.0752        0.281      0.00293\n","     65     4      0.00766      0.00766     1.93e-06       0.0511       0.0677       0.0413       0.0706        0.056       0.0518       0.0916       0.0717       0.0895     0.000932\n","     65     5       0.0105       0.0105     7.42e-06       0.0626       0.0793       0.0578       0.0721        0.065       0.0725       0.0913       0.0819        0.198      0.00207\n","     65     6      0.00963      0.00963     6.62e-07       0.0598       0.0759       0.0561       0.0673       0.0617       0.0696       0.0873       0.0784       0.0408     0.000425\n","     65     7      0.00793      0.00793     3.13e-07       0.0517       0.0689       0.0397       0.0759       0.0578         0.05       0.0962       0.0731         0.04     0.000417\n","     65     8      0.00763      0.00761     2.03e-05       0.0523       0.0675       0.0445       0.0679       0.0562       0.0555       0.0866        0.071        0.332      0.00346\n","     65     9      0.00998      0.00998     5.65e-07       0.0586       0.0773        0.047       0.0817       0.0644       0.0602        0.103       0.0817       0.0482     0.000503\n","     65    10      0.00963      0.00951     0.000125       0.0562       0.0754        0.044       0.0805       0.0622       0.0565        0.103       0.0799        0.828      0.00862\n","     65    11      0.00829      0.00829      3.5e-06        0.055       0.0704       0.0461       0.0726       0.0594       0.0581       0.0902       0.0741        0.117      0.00121\n","     65    12      0.00812      0.00799     0.000132       0.0527       0.0692       0.0427       0.0727       0.0577       0.0542       0.0921       0.0731        0.852      0.00888\n","     65    13      0.00832       0.0083     1.23e-05       0.0546       0.0705       0.0476       0.0685       0.0581       0.0594       0.0886        0.074        0.251      0.00261\n","     65    14      0.00814      0.00809     5.52e-05       0.0532       0.0696       0.0428        0.074       0.0584       0.0552       0.0917       0.0735        0.547      0.00569\n","     65    15      0.00812      0.00806     5.62e-05       0.0529       0.0695       0.0438       0.0709       0.0574       0.0565       0.0899       0.0732        0.546      0.00569\n","     65    16      0.00849      0.00848     1.29e-05       0.0559       0.0712       0.0489       0.0699       0.0594       0.0611       0.0881       0.0746        0.252      0.00263\n","     65    17      0.00799       0.0079     8.89e-05       0.0525       0.0688       0.0426       0.0723       0.0574       0.0535        0.092       0.0727        0.697      0.00726\n","     65    18      0.00743      0.00742     5.54e-06       0.0502       0.0667       0.0403       0.0698       0.0551       0.0519       0.0891       0.0705        0.145      0.00151\n","     65    19      0.00818      0.00816     2.75e-05       0.0524       0.0699       0.0405       0.0763       0.0584       0.0514       0.0967       0.0741        0.371      0.00386\n","     65    20      0.00826      0.00825     1.36e-05       0.0534       0.0703       0.0426       0.0748       0.0587       0.0535       0.0953       0.0744        0.258      0.00269\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     65     1      0.00807      0.00806     2.67e-06       0.0522       0.0695       0.0411       0.0746       0.0578       0.0531        0.094       0.0736        0.101      0.00105\n","     65     2      0.00806      0.00806      1.1e-06       0.0516       0.0694       0.0403       0.0743       0.0573       0.0523       0.0948       0.0736       0.0549     0.000572\n","     65     3      0.00756      0.00756     1.33e-06       0.0502       0.0672       0.0393        0.072       0.0557       0.0504       0.0921       0.0713       0.0688     0.000717\n","     65     4      0.00786      0.00785     1.37e-06       0.0509       0.0686       0.0397       0.0732       0.0565       0.0511       0.0943       0.0727       0.0774     0.000807\n","     65     5      0.00731      0.00731     1.22e-06       0.0499       0.0662       0.0398         0.07       0.0549       0.0517       0.0882         0.07        0.057     0.000594\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              65  265.749    0.005      0.00839     2.93e-05      0.00842       0.0541       0.0709       0.0448       0.0727       0.0587        0.057       0.0926       0.0748        0.304      0.00317\n","! Validation         65  265.749    0.005      0.00777     1.54e-06      0.00777        0.051       0.0682         0.04       0.0728       0.0564       0.0517       0.0927       0.0722       0.0718     0.000748\n","Wall time: 265.7493934209999\n","! Best model       65    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     66     1      0.00826      0.00826     4.08e-06       0.0552       0.0703       0.0472       0.0712       0.0592       0.0595        0.088       0.0738        0.145      0.00151\n","     66     2      0.00693      0.00692     1.21e-06       0.0485       0.0644       0.0388       0.0679       0.0534       0.0488       0.0876       0.0682       0.0709     0.000739\n","     66     3      0.00833       0.0083     3.09e-05       0.0545       0.0705       0.0447        0.074       0.0594       0.0564       0.0925       0.0744         0.41      0.00427\n","     66     4      0.00674      0.00673      1.7e-06       0.0479       0.0635        0.038       0.0678       0.0529       0.0493       0.0851       0.0672       0.0834     0.000869\n","     66     5      0.00758      0.00755     2.99e-05       0.0509       0.0672         0.04       0.0727       0.0564        0.051       0.0913       0.0712        0.395      0.00412\n","     66     6      0.00767      0.00766     3.47e-06       0.0519       0.0677       0.0405       0.0745       0.0575       0.0519       0.0915       0.0717        0.105      0.00109\n","     66     7      0.00795      0.00794     1.44e-05       0.0516       0.0689       0.0402       0.0744       0.0573       0.0513       0.0948       0.0731        0.279      0.00291\n","     66     8      0.00763      0.00761     1.23e-05       0.0517       0.0675       0.0432       0.0688        0.056       0.0556       0.0865       0.0711        0.248      0.00258\n","     66     9      0.00801        0.008     1.21e-05       0.0533       0.0692       0.0454        0.069       0.0572       0.0576        0.088       0.0728        0.257      0.00267\n","     66    10       0.0068       0.0068     4.44e-06       0.0484       0.0638        0.038        0.069       0.0535       0.0481       0.0871       0.0676        0.126      0.00131\n","     66    11      0.00823      0.00815     7.75e-05        0.053       0.0699       0.0414       0.0761       0.0588       0.0522       0.0959        0.074        0.644      0.00671\n","     66    12      0.00744      0.00744      1.4e-06       0.0509       0.0667       0.0409        0.071       0.0559       0.0524       0.0887       0.0705       0.0775     0.000808\n","     66    13      0.00889      0.00883     5.97e-05       0.0564       0.0727       0.0471       0.0752       0.0611       0.0598       0.0933       0.0766         0.57      0.00593\n","     66    14      0.00687      0.00686      1.4e-05       0.0482       0.0641       0.0381       0.0685       0.0533       0.0488       0.0869       0.0678        0.277      0.00288\n","     66    15      0.00921      0.00918     3.03e-05       0.0558       0.0741       0.0458       0.0758       0.0608       0.0589       0.0977       0.0783        0.408      0.00425\n","     66    16      0.00685      0.00685     1.99e-06       0.0482        0.064       0.0381       0.0684       0.0532       0.0485       0.0871       0.0678       0.0943     0.000983\n","     66    17       0.0078      0.00779     1.25e-05       0.0516       0.0683        0.041        0.073        0.057       0.0526        0.092       0.0723        0.258      0.00269\n","     66    18      0.00723      0.00722     6.13e-06       0.0475       0.0657       0.0366       0.0691       0.0529       0.0473       0.0921       0.0697        0.176      0.00184\n","     66    19        0.008      0.00796     3.61e-05        0.051        0.069       0.0401       0.0729       0.0565       0.0509       0.0955       0.0732        0.443      0.00461\n","     66    20      0.00774      0.00772     1.31e-05       0.0513        0.068       0.0426       0.0689       0.0557       0.0545       0.0891       0.0718        0.263      0.00274\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     66     1      0.00801      0.00801     2.67e-06        0.052       0.0692       0.0409       0.0744       0.0576       0.0529       0.0937       0.0733       0.0997      0.00104\n","     66     2      0.00801        0.008     1.14e-06       0.0514       0.0692       0.0401       0.0741       0.0571       0.0521       0.0946       0.0733       0.0558     0.000581\n","     66     3      0.00752      0.00752     1.38e-06       0.0501       0.0671       0.0392       0.0719       0.0555       0.0502        0.092       0.0711       0.0699     0.000728\n","     66     4       0.0078       0.0078     1.35e-06       0.0507       0.0683       0.0396        0.073       0.0563       0.0508        0.094       0.0724       0.0763     0.000794\n","     66     5      0.00726      0.00726      1.2e-06       0.0497       0.0659       0.0396       0.0698       0.0547       0.0514        0.088       0.0697       0.0567     0.000591\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              66  269.790    0.005      0.00769     1.84e-05      0.00771       0.0514       0.0678       0.0414       0.0714       0.0564       0.0529       0.0906       0.0718        0.266      0.00278\n","! Validation         66  269.790    0.005      0.00772     1.55e-06      0.00772       0.0508        0.068       0.0398       0.0726       0.0562       0.0515       0.0925        0.072       0.0717     0.000747\n","Wall time: 269.790730684\n","! Best model       66    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     67     1      0.00779      0.00773      5.8e-05       0.0519        0.068       0.0427       0.0703       0.0565       0.0544       0.0892       0.0718        0.558      0.00582\n","     67     2      0.00728      0.00728     1.55e-06       0.0487        0.066       0.0375       0.0711       0.0543       0.0484       0.0915         0.07       0.0795     0.000828\n","     67     3      0.00713      0.00712     1.34e-05       0.0501       0.0653       0.0412        0.068       0.0546        0.051        0.087        0.069        0.265      0.00276\n","     67     4       0.0078      0.00778     2.22e-05       0.0513       0.0682       0.0395        0.075       0.0572       0.0506        0.094       0.0723        0.345      0.00359\n","     67     5       0.0093      0.00925     5.11e-05       0.0564       0.0744       0.0451        0.079       0.0621        0.057        0.101       0.0788        0.529      0.00551\n","     67     6      0.00734      0.00733     7.45e-06       0.0496       0.0662       0.0383       0.0722       0.0553       0.0496       0.0908       0.0702        0.199      0.00207\n","     67     7      0.00921      0.00914     6.39e-05       0.0551        0.074       0.0429       0.0796       0.0612       0.0552        0.102       0.0784        0.583      0.00608\n","     67     8       0.0114       0.0114     2.83e-06       0.0639       0.0825       0.0571       0.0774       0.0672       0.0722          0.1       0.0861        0.115       0.0012\n","     67     9       0.0133       0.0132     4.63e-05       0.0708        0.089       0.0655       0.0814       0.0734       0.0809        0.103       0.0921        0.497      0.00518\n","     67    10        0.011       0.0109     6.58e-05        0.063       0.0809       0.0528       0.0833        0.068       0.0659        0.105       0.0853        0.601      0.00626\n","     67    11      0.00619      0.00611     8.38e-05       0.0456       0.0605       0.0354        0.066       0.0507       0.0443       0.0839       0.0641        0.678      0.00706\n","     67    12      0.00907      0.00898      9.5e-05       0.0557       0.0733        0.044       0.0789       0.0615       0.0559       0.0994       0.0776        0.722      0.00752\n","     67    13       0.0117       0.0117     9.56e-05       0.0646       0.0835       0.0531       0.0874       0.0703       0.0666         0.11       0.0882        0.723      0.00753\n","     67    14      0.00853      0.00853     6.86e-06       0.0563       0.0714       0.0517       0.0655       0.0586        0.065       0.0829       0.0739         0.17      0.00177\n","     67    15      0.00763      0.00761     1.89e-05       0.0512       0.0675       0.0396       0.0743       0.0569       0.0507       0.0924       0.0715        0.308      0.00321\n","     67    16         0.01         0.01      9.6e-07       0.0616       0.0775       0.0571       0.0706       0.0639       0.0708       0.0895       0.0801       0.0666     0.000694\n","     67    17       0.0111       0.0111     2.42e-06       0.0654       0.0816       0.0597       0.0767       0.0682       0.0737       0.0955       0.0846        0.108      0.00113\n","     67    18      0.00898      0.00896     1.98e-05       0.0549       0.0732       0.0423       0.0802       0.0612       0.0546        0.101       0.0776        0.328      0.00342\n","     67    19      0.00783      0.00782     4.09e-06        0.052       0.0684       0.0431       0.0698       0.0565       0.0553       0.0891       0.0722        0.136      0.00141\n","     67    20       0.0105       0.0105      1.7e-05       0.0595       0.0792       0.0454       0.0877       0.0666       0.0573        0.111        0.084        0.299      0.00311\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     67     1      0.00796      0.00796     2.66e-06       0.0519        0.069       0.0407       0.0742       0.0574       0.0526       0.0935       0.0731       0.0995      0.00104\n","     67     2      0.00796      0.00796     1.19e-06       0.0513        0.069       0.0399        0.074       0.0569       0.0519       0.0944       0.0731       0.0573     0.000597\n","     67     3      0.00747      0.00747      1.4e-06       0.0499       0.0669        0.039       0.0717       0.0553         0.05       0.0918       0.0709       0.0695     0.000724\n","     67     4      0.00775      0.00775     1.38e-06       0.0505       0.0681       0.0393       0.0728       0.0561       0.0505       0.0939       0.0722       0.0774     0.000807\n","     67     5      0.00722      0.00721     1.21e-06       0.0495       0.0657       0.0394       0.0697       0.0545       0.0512       0.0878       0.0695       0.0562     0.000585\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              67  273.825    0.005      0.00912     3.38e-05      0.00916       0.0564       0.0739       0.0467       0.0757       0.0612       0.0597       0.0961       0.0779        0.366      0.00381\n","! Validation         67  273.825    0.005      0.00767     1.57e-06      0.00767       0.0506       0.0678       0.0397       0.0725       0.0561       0.0512       0.0923       0.0718        0.072      0.00075\n","Wall time: 273.82549994199985\n","! Best model       67    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     68     1       0.0106       0.0105     1.83e-05       0.0607       0.0794       0.0495       0.0831       0.0663       0.0627        0.105       0.0839        0.311      0.00324\n","     68     2      0.00867      0.00866     8.31e-06       0.0554        0.072       0.0456        0.075       0.0603       0.0581       0.0938        0.076        0.193      0.00201\n","     68     3      0.00794      0.00792     2.45e-05       0.0509       0.0688       0.0394        0.074       0.0567       0.0507       0.0953        0.073         0.35      0.00365\n","     68     4       0.0135       0.0135     3.49e-05       0.0717       0.0899       0.0681       0.0788       0.0734       0.0838        0.101       0.0924        0.435      0.00453\n","     68     5       0.0153       0.0153     2.57e-06       0.0766       0.0958       0.0674        0.095       0.0812       0.0822        0.118          0.1        0.106       0.0011\n","     68     6       0.0101         0.01     3.18e-05       0.0594       0.0775        0.048       0.0824       0.0652       0.0597        0.104        0.082        0.415      0.00432\n","     68     7      0.00617      0.00615     1.73e-05       0.0461       0.0607       0.0364       0.0655        0.051        0.046       0.0825       0.0643        0.302      0.00315\n","     68     8      0.00953      0.00944     8.93e-05       0.0563       0.0752       0.0453       0.0781       0.0617       0.0571        0.102       0.0796        0.699      0.00728\n","     68     9      0.00941       0.0094     1.73e-05       0.0574        0.075       0.0492       0.0737       0.0615       0.0624       0.0953       0.0789        0.304      0.00317\n","     68    10      0.00729      0.00727     1.79e-05       0.0496        0.066       0.0404        0.068       0.0542       0.0521       0.0873       0.0697        0.305      0.00318\n","     68    11      0.00773      0.00773     1.51e-06       0.0524        0.068       0.0428       0.0716       0.0572       0.0541       0.0896       0.0719        0.085     0.000885\n","     68    12      0.00839      0.00839     1.41e-06       0.0538       0.0709       0.0442       0.0731       0.0586       0.0566        0.093       0.0748        0.073     0.000761\n","     68    13      0.00781      0.00781     2.46e-06       0.0522       0.0684        0.041       0.0745       0.0577       0.0519       0.0928       0.0724        0.104      0.00108\n","     68    14      0.00794      0.00794     5.59e-06       0.0531       0.0689       0.0436       0.0722       0.0579       0.0556       0.0898       0.0727        0.148      0.00155\n","     68    15       0.0099      0.00988      1.8e-05       0.0583       0.0769       0.0466       0.0816       0.0641       0.0594        0.103       0.0814        0.314      0.00327\n","     68    16       0.0079       0.0079     3.07e-06       0.0519       0.0687       0.0411       0.0735       0.0573       0.0518       0.0939       0.0728        0.122      0.00127\n","     68    17      0.00732       0.0073     1.89e-05       0.0495       0.0661        0.038       0.0725       0.0552       0.0488       0.0913       0.0701        0.303      0.00316\n","     68    18      0.00769      0.00769     5.72e-07       0.0507       0.0678       0.0403       0.0714       0.0559       0.0517        0.092       0.0719       0.0457     0.000476\n","     68    19      0.00747      0.00746      8.7e-06       0.0507       0.0668       0.0416       0.0687       0.0552       0.0536       0.0875       0.0705        0.176      0.00183\n","     68    20      0.00642      0.00641     1.78e-05       0.0469       0.0619       0.0371       0.0667       0.0519       0.0471       0.0841       0.0656        0.307      0.00319\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     68     1      0.00791      0.00791      2.7e-06       0.0517       0.0688       0.0405       0.0741       0.0573       0.0524       0.0934       0.0729        0.101      0.00105\n","     68     2      0.00792      0.00792     1.17e-06       0.0511       0.0688       0.0398       0.0738       0.0568       0.0517       0.0942        0.073       0.0563     0.000587\n","     68     3      0.00744      0.00744      1.4e-06       0.0498       0.0667       0.0388       0.0716       0.0552       0.0498       0.0917       0.0707       0.0696     0.000725\n","     68     4      0.00771      0.00771      1.4e-06       0.0503       0.0679       0.0391       0.0727       0.0559       0.0503       0.0937        0.072       0.0779     0.000812\n","     68     5      0.00718      0.00718     1.15e-06       0.0493       0.0655       0.0392       0.0696       0.0544        0.051       0.0877       0.0693       0.0551     0.000574\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              68  277.935    0.005      0.00884      1.7e-05      0.00885       0.0552       0.0727       0.0453        0.075       0.0601       0.0581       0.0955       0.0768        0.255      0.00266\n","! Validation         68  277.935    0.005      0.00763     1.56e-06      0.00763       0.0504       0.0676       0.0395       0.0724       0.0559        0.051       0.0922       0.0716        0.072      0.00075\n","Wall time: 277.93588387299997\n","! Best model       68    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     69     1      0.00689      0.00688     6.15e-06       0.0488       0.0642       0.0396       0.0672       0.0534       0.0508       0.0848       0.0678        0.175      0.00183\n","     69     2      0.00686      0.00686     5.68e-06       0.0478       0.0641       0.0389       0.0657       0.0523       0.0497       0.0858       0.0678        0.171      0.00178\n","     69     3      0.00689      0.00689     4.94e-06        0.048       0.0642       0.0374       0.0693       0.0533       0.0476       0.0886       0.0681        0.158      0.00165\n","     69     4      0.00804      0.00803     8.19e-06       0.0519       0.0693       0.0397       0.0762        0.058       0.0506       0.0964       0.0735        0.207      0.00216\n","     69     5      0.00708      0.00707     4.18e-06       0.0491       0.0651       0.0388       0.0696       0.0542       0.0503       0.0874       0.0688        0.131      0.00136\n","     69     6      0.00808      0.00806     1.59e-05       0.0533       0.0695       0.0436       0.0727       0.0581       0.0563       0.0902       0.0733        0.286      0.00298\n","     69     7      0.00799      0.00799     3.16e-06       0.0524       0.0692       0.0415       0.0742       0.0578       0.0533       0.0931       0.0732        0.117      0.00122\n","     69     8      0.00609      0.00607     1.96e-05       0.0457       0.0603        0.036       0.0651       0.0506        0.045       0.0827       0.0639        0.327       0.0034\n","     69     9      0.00748      0.00748     3.75e-07       0.0511       0.0669       0.0405       0.0724       0.0565       0.0508       0.0909       0.0709       0.0355      0.00037\n","     69    10      0.00758      0.00756     1.64e-05        0.051       0.0673       0.0399       0.0732       0.0566       0.0499       0.0927       0.0713        0.298      0.00311\n","     69    11      0.00759      0.00759     5.43e-06       0.0508       0.0674       0.0402       0.0721       0.0561        0.051       0.0918       0.0714        0.156      0.00162\n","     69    12      0.00642      0.00641      1.1e-05       0.0469        0.062       0.0375       0.0656       0.0516       0.0471       0.0841       0.0656        0.243      0.00253\n","     69    13      0.00836      0.00835     4.73e-06       0.0548       0.0707       0.0463       0.0719       0.0591       0.0579        0.091       0.0745         0.15      0.00157\n","     69    14       0.0096      0.00959     7.56e-06       0.0587       0.0758       0.0512       0.0736       0.0624       0.0646       0.0942       0.0794        0.195      0.00203\n","     69    15       0.0073      0.00728     1.88e-05       0.0506        0.066       0.0423       0.0671       0.0547       0.0534       0.0858       0.0696        0.318      0.00331\n","     69    16      0.00725      0.00722     3.19e-05       0.0502       0.0657       0.0411       0.0686       0.0548       0.0519        0.087       0.0695        0.414      0.00432\n","     69    17       0.0112       0.0112      4.4e-05       0.0632       0.0817       0.0523       0.0851       0.0687        0.066        0.106       0.0862        0.487      0.00508\n","     69    18       0.0139       0.0139      2.1e-05       0.0714       0.0911       0.0601       0.0939        0.077       0.0756        0.116       0.0959        0.328      0.00341\n","     69    19      0.00968      0.00968     1.24e-06       0.0583       0.0761       0.0488       0.0772        0.063        0.063       0.0972       0.0801       0.0695     0.000724\n","     69    20      0.00743      0.00743     1.17e-06       0.0501       0.0667       0.0396       0.0712       0.0554       0.0501       0.0912       0.0707       0.0709     0.000739\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     69     1      0.00788      0.00787     2.64e-06       0.0515       0.0686       0.0403       0.0739       0.0571       0.0522       0.0932       0.0727        0.099      0.00103\n","     69     2      0.00788      0.00788     1.14e-06        0.051       0.0687       0.0396       0.0737       0.0567       0.0515       0.0941       0.0728       0.0545     0.000568\n","     69     3       0.0074       0.0074     1.42e-06       0.0496       0.0666       0.0387       0.0715       0.0551       0.0496       0.0915       0.0705       0.0709     0.000739\n","     69     4      0.00767      0.00767     1.46e-06       0.0502       0.0677        0.039       0.0725       0.0558       0.0501       0.0935       0.0718       0.0784     0.000817\n","     69     5      0.00714      0.00713     1.16e-06       0.0492       0.0653       0.0391       0.0694       0.0542       0.0507       0.0875       0.0691        0.054     0.000563\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              69  282.082    0.005      0.00807     1.16e-05      0.00809       0.0527       0.0695       0.0428       0.0726       0.0577       0.0548       0.0922       0.0735        0.217      0.00226\n","! Validation         69  282.082    0.005      0.00759     1.57e-06      0.00759       0.0503       0.0674       0.0393       0.0722       0.0558       0.0508        0.092       0.0714       0.0714     0.000743\n","Wall time: 282.0832084949998\n","! Best model       69    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     70     1       0.0103       0.0103     3.79e-05        0.062       0.0785       0.0567       0.0726       0.0647       0.0704       0.0925       0.0814        0.454      0.00473\n","     70     2      0.00951      0.00951     2.77e-06       0.0585       0.0754       0.0507       0.0741       0.0624       0.0641       0.0941       0.0791        0.112      0.00116\n","     70     3      0.00706      0.00704     1.88e-05       0.0489       0.0649       0.0408       0.0652        0.053       0.0527       0.0842       0.0684        0.311      0.00324\n","     70     4      0.00875      0.00875     2.09e-06       0.0562       0.0724       0.0489       0.0708       0.0598       0.0616       0.0901       0.0759       0.0914     0.000952\n","     70     5       0.0077       0.0077     1.46e-06       0.0504       0.0679       0.0391       0.0731       0.0561       0.0504       0.0935        0.072       0.0693     0.000722\n","     70     6      0.00722      0.00719      3.4e-05       0.0504       0.0656       0.0415       0.0682       0.0548       0.0521       0.0865       0.0693        0.422      0.00439\n","     70     7      0.00968      0.00968     1.72e-06       0.0575       0.0761       0.0457        0.081       0.0634        0.058        0.103       0.0806        0.093     0.000968\n","     70     8      0.00877      0.00869     8.18e-05       0.0539       0.0721       0.0426       0.0765       0.0595       0.0543       0.0985       0.0764         0.67      0.00698\n","     70     9       0.0067       0.0067     7.95e-07       0.0479       0.0633       0.0376       0.0685       0.0531       0.0477       0.0864       0.0671       0.0572     0.000596\n","     70    10      0.00927      0.00918     9.47e-05       0.0554       0.0741       0.0441       0.0781       0.0611       0.0566          0.1       0.0785        0.721      0.00752\n","     70    11       0.0078      0.00779     6.97e-06       0.0523       0.0683       0.0431       0.0705       0.0568       0.0546       0.0895       0.0721        0.181      0.00188\n","     70    12      0.00631      0.00628     3.16e-05       0.0472       0.0613        0.038       0.0657       0.0518        0.048       0.0816       0.0648         0.41      0.00427\n","     70    13      0.00787      0.00785     2.61e-05       0.0521       0.0685       0.0421       0.0722       0.0572       0.0537       0.0913       0.0725        0.374       0.0039\n","     70    14      0.00806      0.00806     8.99e-07       0.0534       0.0695       0.0445       0.0714       0.0579       0.0563       0.0901       0.0732       0.0553     0.000576\n","     70    15      0.00687      0.00686      1.3e-05       0.0487       0.0641       0.0391       0.0681       0.0536       0.0497       0.0858       0.0678        0.254      0.00264\n","     70    16      0.00889      0.00888     6.19e-06       0.0568       0.0729       0.0505       0.0695         0.06       0.0637       0.0885       0.0761        0.183       0.0019\n","     70    17      0.00856      0.00856     8.44e-07       0.0541       0.0716       0.0439       0.0747       0.0593       0.0562       0.0951       0.0757       0.0584     0.000608\n","     70    18      0.00794      0.00794     2.88e-06       0.0524       0.0689       0.0413       0.0746        0.058       0.0519       0.0942        0.073       0.0842     0.000877\n","     70    19       0.0069      0.00689     8.63e-06       0.0488       0.0642        0.039       0.0684       0.0537       0.0492       0.0868        0.068        0.202       0.0021\n","     70    20      0.00771       0.0077     7.85e-06       0.0514       0.0679       0.0398       0.0747       0.0572         0.05        0.094        0.072        0.199      0.00207\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     70     1      0.00783      0.00783     2.56e-06       0.0514       0.0685       0.0402       0.0737        0.057        0.052        0.093       0.0725       0.0967      0.00101\n","     70     2      0.00784      0.00784     1.22e-06       0.0509       0.0685       0.0395       0.0736       0.0565       0.0513       0.0939       0.0726       0.0577     0.000601\n","     70     3      0.00737      0.00737      1.4e-06       0.0495       0.0664       0.0385       0.0714        0.055       0.0494       0.0914       0.0704       0.0694     0.000723\n","     70     4      0.00763      0.00762     1.41e-06         0.05       0.0676       0.0389       0.0724       0.0556       0.0499       0.0933       0.0716       0.0766     0.000798\n","     70     5      0.00709      0.00709     1.19e-06        0.049       0.0651       0.0389       0.0693       0.0541       0.0505       0.0873       0.0689       0.0557      0.00058\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              70  286.115    0.005      0.00808     1.91e-05       0.0081       0.0529       0.0695       0.0434       0.0719       0.0577       0.0554       0.0915       0.0734         0.25       0.0026\n","! Validation         70  286.115    0.005      0.00755     1.56e-06      0.00755       0.0502       0.0672       0.0392       0.0721       0.0556       0.0506       0.0918       0.0712       0.0712     0.000742\n","Wall time: 286.11565662199996\n","! Best model       70    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     71     1       0.0081      0.00809     9.69e-06        0.052       0.0696       0.0405       0.0751       0.0578       0.0527       0.0947       0.0737        0.215      0.00224\n","     71     2      0.00859      0.00859     5.58e-07       0.0553       0.0717       0.0468       0.0724       0.0596       0.0589       0.0921       0.0755       0.0432      0.00045\n","     71     3      0.00752      0.00751     5.03e-07        0.051       0.0671       0.0404       0.0723       0.0563        0.051       0.0911        0.071       0.0475     0.000494\n","     71     4       0.0086      0.00859      7.8e-06        0.055       0.0717       0.0433       0.0782       0.0608       0.0541       0.0979        0.076        0.196      0.00204\n","     71     5      0.00712       0.0071     1.66e-05       0.0485       0.0652       0.0386       0.0681       0.0534         0.05        0.088        0.069        0.299      0.00311\n","     71     6      0.00918      0.00915     3.54e-05       0.0558        0.074       0.0446       0.0781       0.0614       0.0567       0.0999       0.0783        0.437      0.00455\n","     71     7      0.00637      0.00634     2.71e-05       0.0459       0.0616       0.0361       0.0655       0.0508       0.0458       0.0848       0.0653        0.386      0.00402\n","     71     8      0.00741      0.00734     6.48e-05       0.0508       0.0663       0.0407        0.071       0.0559       0.0518       0.0885       0.0701        0.595       0.0062\n","     71     9      0.00742       0.0074     2.39e-05       0.0488       0.0666        0.037       0.0723       0.0546       0.0486       0.0925       0.0706        0.354      0.00369\n","     71    10      0.00672      0.00664     7.65e-05        0.049        0.063       0.0405       0.0661       0.0533       0.0505       0.0826       0.0665        0.644      0.00671\n","     71    11      0.00687      0.00687     5.46e-07       0.0482       0.0641       0.0375       0.0696       0.0536       0.0477       0.0883        0.068       0.0506     0.000527\n","     71    12      0.00787      0.00776     0.000105       0.0518       0.0682       0.0416       0.0722       0.0569       0.0533       0.0908       0.0721        0.757      0.00789\n","     71    13      0.00771      0.00767     3.67e-05       0.0502       0.0678       0.0393       0.0721       0.0557       0.0503       0.0933       0.0718        0.445      0.00464\n","     71    14      0.00712      0.00702     9.48e-05       0.0489       0.0648        0.039       0.0687       0.0538       0.0502        0.087       0.0686        0.715      0.00745\n","     71    15      0.00724       0.0072     4.67e-05       0.0495       0.0656       0.0382       0.0722       0.0552        0.048       0.0912       0.0696        0.505      0.00526\n","     71    16      0.00726      0.00725     1.59e-05       0.0488       0.0659       0.0385       0.0696        0.054       0.0488       0.0908       0.0698        0.291      0.00303\n","     71    17       0.0081      0.00807     2.42e-05       0.0538       0.0695       0.0457       0.0702       0.0579       0.0576       0.0887       0.0731        0.358      0.00373\n","     71    18      0.00723      0.00723     3.77e-06       0.0505       0.0658       0.0419       0.0678       0.0548       0.0532       0.0856       0.0694        0.125      0.00131\n","     71    19      0.00703      0.00701     1.22e-05       0.0482       0.0648       0.0378       0.0689       0.0534       0.0483        0.089       0.0687        0.247      0.00257\n","     71    20      0.00659      0.00658     1.41e-05        0.048       0.0628       0.0393       0.0656       0.0524       0.0502       0.0823       0.0662        0.253      0.00264\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     71     1      0.00779      0.00779     2.68e-06       0.0512       0.0683       0.0401       0.0735       0.0568       0.0518       0.0928       0.0723       0.0998      0.00104\n","     71     2       0.0078      0.00779     1.27e-06       0.0507       0.0683       0.0393       0.0735       0.0564       0.0511       0.0937       0.0724       0.0584     0.000608\n","     71     3      0.00734      0.00734     1.43e-06       0.0494       0.0663       0.0384       0.0714       0.0549       0.0492       0.0913       0.0702         0.07     0.000729\n","     71     4      0.00758      0.00758     1.43e-06       0.0499       0.0674       0.0387       0.0722       0.0555       0.0497       0.0931       0.0714       0.0771     0.000803\n","     71     5      0.00705      0.00705     1.15e-06       0.0489       0.0649       0.0387       0.0691       0.0539       0.0503       0.0871       0.0687       0.0552     0.000575\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              71  290.147    0.005      0.00747     3.08e-05       0.0075       0.0505       0.0669       0.0404       0.0708       0.0556       0.0515       0.0901       0.0708        0.348      0.00363\n","! Validation         71  290.147    0.005      0.00751     1.59e-06      0.00751         0.05        0.067       0.0391       0.0719       0.0555       0.0504       0.0916        0.071       0.0721     0.000751\n","Wall time: 290.1475253409999\n","! Best model       71    0.008\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     72     1      0.00672      0.00671     8.55e-06       0.0474       0.0634       0.0374       0.0674       0.0524       0.0485       0.0857       0.0671        0.198      0.00206\n","     72     2      0.00656      0.00656      1.9e-06       0.0467       0.0627       0.0369       0.0662       0.0516       0.0475       0.0853       0.0664       0.0873     0.000909\n","     72     3       0.0067      0.00669      6.3e-06        0.048       0.0633       0.0368       0.0704       0.0536       0.0459       0.0883       0.0671        0.161      0.00168\n","     72     4      0.00734      0.00734     1.75e-06       0.0497       0.0663       0.0394       0.0702       0.0548       0.0513       0.0889       0.0701       0.0902      0.00094\n","     72     5      0.00672      0.00672     3.24e-06       0.0481       0.0634       0.0376       0.0689       0.0533       0.0476       0.0868       0.0672        0.115       0.0012\n","     72     6      0.00834      0.00834     8.85e-07       0.0534       0.0707       0.0418       0.0766       0.0592       0.0534       0.0963       0.0749       0.0639     0.000665\n","     72     7      0.00713      0.00711     1.28e-05       0.0498       0.0653       0.0399       0.0698       0.0548       0.0504       0.0878       0.0691         0.25       0.0026\n","     72     8      0.00686      0.00685     1.16e-05        0.049        0.064       0.0401       0.0667       0.0534       0.0501       0.0853       0.0677         0.22      0.00229\n","     72     9      0.00734      0.00734     5.04e-06       0.0483       0.0663       0.0363       0.0725       0.0544       0.0472       0.0934       0.0703        0.155      0.00161\n","     72    10      0.00758      0.00758     8.93e-06       0.0509       0.0673       0.0402       0.0723       0.0563       0.0517       0.0909       0.0713        0.214      0.00223\n","     72    11      0.00724      0.00724     6.25e-06       0.0491       0.0658       0.0384       0.0706       0.0545        0.049       0.0905       0.0697        0.181      0.00189\n","     72    12      0.00701      0.00698     2.66e-05       0.0477       0.0647        0.037       0.0692       0.0531       0.0469       0.0902       0.0686        0.381      0.00397\n","     72    13      0.00621      0.00621     3.23e-07       0.0465        0.061       0.0367       0.0659       0.0513       0.0462        0.083       0.0646       0.0377     0.000393\n","     72    14      0.00681       0.0068     1.63e-05       0.0478       0.0638       0.0371       0.0693       0.0532       0.0474       0.0878       0.0676        0.296      0.00308\n","     72    15      0.00813      0.00813     9.34e-06       0.0525       0.0697       0.0409       0.0758       0.0584        0.052       0.0958       0.0739        0.222      0.00232\n","     72    16      0.00793       0.0079     3.69e-05        0.052       0.0688        0.042       0.0719        0.057       0.0538       0.0916       0.0727        0.447      0.00465\n","     72    17      0.00722      0.00722     2.54e-06       0.0501       0.0657       0.0396        0.071       0.0553       0.0512       0.0879       0.0695          0.1      0.00104\n","     72    18      0.00638      0.00637     5.07e-06       0.0465       0.0617       0.0363       0.0669       0.0516       0.0456       0.0853       0.0655        0.162      0.00168\n","     72    19      0.00665      0.00664     2.11e-06       0.0478       0.0631       0.0384       0.0666       0.0525       0.0484       0.0851       0.0668       0.0816      0.00085\n","     72    20       0.0073       0.0073     7.57e-07       0.0495       0.0661       0.0386       0.0711       0.0549       0.0499       0.0902         0.07        0.059     0.000614\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     72     1      0.00774      0.00774     2.66e-06        0.051       0.0681       0.0399       0.0733       0.0566       0.0516       0.0925       0.0721       0.0977      0.00102\n","     72     2      0.00774      0.00774     1.28e-06       0.0506       0.0681       0.0392       0.0733       0.0563       0.0508       0.0935       0.0721       0.0601     0.000626\n","     72     3       0.0073       0.0073     1.44e-06       0.0492       0.0661       0.0382       0.0712       0.0547        0.049       0.0911       0.0701       0.0697     0.000726\n","     72     4      0.00754      0.00754     1.39e-06       0.0498       0.0672       0.0386        0.072       0.0553       0.0495       0.0929       0.0712       0.0752     0.000783\n","     72     5      0.00701      0.00701     1.18e-06       0.0487       0.0648       0.0386        0.069       0.0538       0.0501       0.0869       0.0685       0.0547      0.00057\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              72  294.181    0.005       0.0071     8.36e-06      0.00711        0.049       0.0652       0.0386         0.07       0.0543       0.0493       0.0889       0.0691        0.176      0.00183\n","! Validation         72  294.181    0.005      0.00746     1.59e-06      0.00747       0.0499       0.0668       0.0389       0.0718       0.0553       0.0502       0.0914       0.0708       0.0715     0.000744\n","Wall time: 294.1822671609998\n","! Best model       72    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     73     1      0.00704      0.00704     3.73e-06       0.0485       0.0649       0.0381       0.0692       0.0537       0.0481       0.0895       0.0688        0.128      0.00133\n","     73     2      0.00678      0.00678     1.35e-06       0.0482       0.0637       0.0389       0.0669       0.0529       0.0498       0.0849       0.0674       0.0783     0.000816\n","     73     3      0.00835      0.00834     1.47e-05        0.053       0.0706       0.0417       0.0758       0.0587        0.053       0.0967       0.0749        0.275      0.00286\n","     73     4      0.00877      0.00877     8.34e-06       0.0548       0.0724       0.0438        0.077       0.0604       0.0549       0.0985       0.0767        0.193      0.00201\n","     73     5      0.00765      0.00765     2.17e-06       0.0507       0.0676       0.0413       0.0696       0.0555       0.0538       0.0891       0.0714       0.0953     0.000993\n","     73     6      0.00787      0.00786     7.74e-06       0.0518       0.0686       0.0414       0.0727       0.0571       0.0528       0.0925       0.0726        0.187      0.00195\n","     73     7      0.00924      0.00924     1.09e-06       0.0577       0.0744       0.0473       0.0785       0.0629       0.0593       0.0978       0.0785       0.0625     0.000651\n","     73     8      0.00993      0.00993     2.96e-06       0.0596       0.0771       0.0506       0.0775       0.0641       0.0638       0.0984       0.0811        0.119      0.00124\n","     73     9       0.0086      0.00857     3.59e-05       0.0536       0.0716       0.0429        0.075       0.0589       0.0553       0.0963       0.0758        0.443      0.00461\n","     73    10      0.00662      0.00662     5.59e-07       0.0472        0.063       0.0369       0.0678       0.0523       0.0474        0.086       0.0667       0.0441      0.00046\n","     73    11      0.00911      0.00908     3.17e-05       0.0569       0.0737       0.0456       0.0795       0.0626        0.056          0.1       0.0781        0.417      0.00434\n","     73    12      0.00886      0.00886      1.1e-06        0.057       0.0728       0.0469       0.0772       0.0621       0.0588       0.0948       0.0768       0.0729     0.000759\n","     73    13      0.00697      0.00695     1.67e-05       0.0486       0.0645       0.0403       0.0654       0.0528       0.0513        0.085       0.0681        0.282      0.00294\n","     73    14      0.00741      0.00741     8.35e-07       0.0503       0.0666        0.039       0.0727       0.0559       0.0495       0.0917       0.0706       0.0598     0.000623\n","     73    15      0.00913      0.00906     7.17e-05       0.0568       0.0736       0.0477       0.0749       0.0613       0.0605       0.0946       0.0775        0.629      0.00655\n","     73    16      0.00956      0.00955     1.44e-06       0.0585       0.0756       0.0482       0.0793       0.0637       0.0602       0.0995       0.0799       0.0705     0.000734\n","     73    17      0.00706      0.00706     8.07e-07       0.0486        0.065       0.0387       0.0685       0.0536       0.0493       0.0884       0.0689       0.0512     0.000533\n","     73    18      0.00765      0.00763     2.04e-05       0.0514       0.0676       0.0389       0.0762       0.0576       0.0497       0.0936       0.0717        0.326      0.00339\n","     73    19      0.00976      0.00974        2e-05       0.0589       0.0764       0.0488        0.079       0.0639       0.0613       0.0999       0.0806        0.326      0.00339\n","     73    20       0.0109       0.0109     3.81e-05       0.0629       0.0807       0.0543       0.0801       0.0672       0.0676        0.102       0.0847        0.453      0.00472\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     73     1       0.0077      0.00769     2.62e-06       0.0509       0.0679       0.0397       0.0732       0.0565       0.0514       0.0924       0.0719       0.0972      0.00101\n","     73     2       0.0077       0.0077     1.21e-06       0.0504       0.0679        0.039       0.0732       0.0561       0.0506       0.0933       0.0719       0.0567     0.000591\n","     73     3      0.00726      0.00726     1.41e-06       0.0491       0.0659       0.0381       0.0711       0.0546       0.0488        0.091       0.0699       0.0703     0.000732\n","     73     4       0.0075      0.00749     1.43e-06       0.0496        0.067       0.0384       0.0719       0.0552       0.0493       0.0927        0.071       0.0771     0.000804\n","     73     5      0.00696      0.00696     1.15e-06       0.0486       0.0646       0.0384       0.0688       0.0536       0.0499       0.0868       0.0683       0.0536     0.000558\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              73  298.205    0.005      0.00835     1.41e-05      0.00836       0.0538       0.0707       0.0436       0.0741       0.0589       0.0554       0.0941       0.0748        0.216      0.00225\n","! Validation         73  298.205    0.005      0.00742     1.56e-06      0.00742       0.0497       0.0666       0.0387       0.0716       0.0552         0.05       0.0913       0.0706        0.071      0.00074\n","Wall time: 298.20596618499985\n","! Best model       73    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     74     1      0.00741       0.0074     9.25e-06       0.0499       0.0666       0.0393        0.071       0.0552       0.0508       0.0901       0.0705        0.205      0.00213\n","     74     2      0.00726      0.00726     7.05e-06       0.0501       0.0659       0.0387       0.0728       0.0558       0.0491       0.0906       0.0698        0.185      0.00192\n","     74     3      0.00816      0.00816     9.46e-07       0.0528       0.0699       0.0433       0.0717       0.0575       0.0552       0.0925       0.0739       0.0596     0.000621\n","     74     4      0.00744      0.00742     1.07e-05         0.05       0.0667       0.0381        0.074        0.056       0.0483       0.0931       0.0707        0.238      0.00248\n","     74     5      0.00749      0.00742     6.22e-05       0.0497       0.0667       0.0383       0.0726       0.0554       0.0495       0.0918       0.0707        0.582      0.00606\n","     74     6       0.0075       0.0075     3.02e-06       0.0499        0.067       0.0389       0.0718       0.0554       0.0501       0.0919        0.071        0.104      0.00109\n","     74     7       0.0085      0.00835     0.000154       0.0542       0.0707        0.044       0.0746       0.0593       0.0552       0.0943       0.0748        0.918      0.00956\n","     74     8      0.00799      0.00796        3e-05       0.0516        0.069       0.0422       0.0705       0.0564       0.0552       0.0906       0.0729        0.403       0.0042\n","     74     9      0.00723      0.00722      8.3e-06       0.0499       0.0658       0.0396       0.0706       0.0551       0.0502        0.089       0.0696        0.201      0.00209\n","     74    10      0.00804        0.008     4.54e-05       0.0525       0.0692       0.0428       0.0719       0.0573       0.0554       0.0907        0.073        0.495      0.00515\n","     74    11      0.00777      0.00776     4.92e-06       0.0521       0.0682       0.0434       0.0696       0.0565       0.0546       0.0893        0.072        0.144       0.0015\n","     74    12       0.0073      0.00727     2.85e-05       0.0498        0.066       0.0393       0.0708       0.0551       0.0499       0.0899       0.0699        0.394       0.0041\n","     74    13      0.00712      0.00711     2.25e-06       0.0496       0.0653       0.0399       0.0689       0.0544       0.0507       0.0873        0.069       0.0975      0.00102\n","     74    14      0.00797      0.00791     6.82e-05       0.0515       0.0688       0.0387       0.0772        0.058       0.0498       0.0961       0.0729        0.611      0.00637\n","     74    15        0.009      0.00899     1.18e-05       0.0563       0.0734       0.0459       0.0771       0.0615       0.0574       0.0977       0.0776        0.248      0.00259\n","     74    16      0.00701      0.00695     5.81e-05       0.0495       0.0645       0.0411       0.0664       0.0537       0.0519       0.0842       0.0681        0.564      0.00587\n","     74    17      0.00646      0.00644     1.75e-05       0.0461       0.0621       0.0362       0.0659       0.0511        0.047       0.0845       0.0658        0.304      0.00316\n","     74    18      0.00693      0.00693     1.92e-06       0.0475       0.0644       0.0368       0.0688       0.0528       0.0484       0.0881       0.0682       0.0908     0.000946\n","     74    19      0.00728      0.00726     2.39e-05       0.0497       0.0659       0.0393       0.0706       0.0549       0.0508       0.0887       0.0698        0.349      0.00364\n","     74    20      0.00676      0.00672     4.58e-05       0.0488       0.0634        0.039       0.0684       0.0537       0.0488       0.0855       0.0671        0.494      0.00514\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     74     1      0.00765      0.00765     2.57e-06       0.0507       0.0677       0.0396        0.073       0.0563       0.0512       0.0922       0.0717       0.0957     0.000997\n","     74     2      0.00766      0.00766     1.32e-06       0.0502       0.0677       0.0388        0.073       0.0559       0.0504       0.0931       0.0717       0.0601     0.000626\n","     74     3      0.00722      0.00722     1.43e-06        0.049       0.0657       0.0379        0.071       0.0545       0.0486       0.0908       0.0697       0.0692     0.000721\n","     74     4      0.00745      0.00745     1.45e-06       0.0494       0.0668       0.0383       0.0718        0.055        0.049       0.0925       0.0708       0.0767     0.000799\n","     74     5      0.00692      0.00692     1.13e-06       0.0484       0.0644       0.0383       0.0687       0.0535       0.0496       0.0866       0.0681       0.0542     0.000565\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              74  302.256    0.005       0.0075     2.97e-05      0.00753       0.0506        0.067       0.0402       0.0713       0.0558       0.0515       0.0904       0.0709        0.334      0.00348\n","! Validation         74  302.256    0.005      0.00738     1.58e-06      0.00738       0.0496       0.0665       0.0386       0.0715        0.055       0.0498       0.0911       0.0704       0.0712     0.000741\n","Wall time: 302.25683701899993\n","! Best model       74    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     75     1      0.00702      0.00702     5.55e-07       0.0504       0.0648       0.0411        0.069       0.0551       0.0512       0.0857       0.0685        0.048       0.0005\n","     75     2      0.00718      0.00715     3.45e-05       0.0488       0.0654       0.0394       0.0676       0.0535       0.0507       0.0876       0.0692        0.433      0.00451\n","     75     3      0.00799      0.00799     2.68e-06       0.0509       0.0691       0.0398        0.073       0.0564       0.0526       0.0939       0.0732        0.112      0.00117\n","     75     4      0.00865      0.00863     1.09e-05       0.0559       0.0719       0.0486       0.0705       0.0595        0.061       0.0898       0.0754        0.237      0.00247\n","     75     5      0.00633      0.00633     3.99e-07       0.0472       0.0616       0.0391       0.0634       0.0513        0.049        0.081        0.065       0.0314     0.000328\n","     75     6      0.00789      0.00787     2.22e-05       0.0518       0.0686       0.0427         0.07       0.0564        0.056       0.0886       0.0723        0.331      0.00344\n","     75     7      0.00874      0.00872      1.2e-05       0.0569       0.0723       0.0489       0.0729       0.0609       0.0611       0.0905       0.0758        0.246      0.00256\n","     75     8      0.00912       0.0091     1.81e-05       0.0557       0.0738       0.0454       0.0764       0.0609        0.057       0.0992       0.0781        0.314      0.00328\n","     75     9      0.00733       0.0073     2.92e-05       0.0503       0.0661       0.0392       0.0727       0.0559       0.0493       0.0908       0.0701        0.396      0.00412\n","     75    10      0.00717      0.00715     1.95e-05       0.0491       0.0654       0.0391       0.0691       0.0541       0.0494       0.0892       0.0693        0.322      0.00336\n","     75    11      0.00923      0.00923     1.42e-06        0.056       0.0743       0.0455        0.077       0.0612        0.058       0.0992       0.0786       0.0664     0.000692\n","     75    12      0.00911       0.0091     7.62e-06       0.0568       0.0738        0.048       0.0744       0.0612        0.061       0.0944       0.0777        0.188      0.00196\n","     75    13      0.00713      0.00712     8.51e-06       0.0491       0.0653       0.0399       0.0675       0.0537       0.0513       0.0867        0.069         0.21      0.00219\n","     75    14      0.00799      0.00797     1.53e-05       0.0525       0.0691       0.0411       0.0752       0.0581       0.0524        0.094       0.0732        0.281      0.00293\n","     75    15      0.00914      0.00914      3.1e-07       0.0578        0.074       0.0494       0.0745       0.0619       0.0616        0.094       0.0778       0.0365      0.00038\n","     75    16      0.00914      0.00913     2.67e-06       0.0576       0.0739       0.0493       0.0743       0.0618       0.0608       0.0949       0.0778        0.101      0.00105\n","     75    17       0.0073      0.00728     1.49e-05         0.05        0.066       0.0393       0.0715       0.0554       0.0495       0.0904         0.07        0.284      0.00296\n","     75    18      0.00695      0.00694     1.73e-05       0.0485       0.0644       0.0386       0.0685       0.0535       0.0497       0.0867       0.0682        0.275      0.00287\n","     75    19      0.00963      0.00961     1.48e-05       0.0568       0.0758       0.0429       0.0846       0.0637       0.0552        0.106       0.0804        0.279      0.00291\n","     75    20      0.00804      0.00804     3.58e-06       0.0526       0.0694       0.0426       0.0727       0.0577       0.0545       0.0921       0.0733         0.12      0.00125\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     75     1      0.00762      0.00762     2.63e-06       0.0506       0.0675       0.0395       0.0729       0.0562        0.051        0.092       0.0715       0.0976      0.00102\n","     75     2      0.00762      0.00762     1.18e-06       0.0501       0.0675       0.0387       0.0729       0.0558       0.0502       0.0929       0.0716       0.0562     0.000586\n","     75     3      0.00719      0.00719      1.4e-06       0.0488       0.0656       0.0378       0.0709       0.0544       0.0484       0.0907       0.0695       0.0695     0.000724\n","     75     4      0.00742      0.00741     1.45e-06       0.0493       0.0666       0.0381       0.0717       0.0549       0.0488       0.0924       0.0706       0.0779     0.000812\n","     75     5      0.00688      0.00688     1.14e-06       0.0483       0.0642       0.0381       0.0686       0.0533       0.0494       0.0864       0.0679       0.0539     0.000562\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              75  306.308    0.005      0.00804     1.18e-05      0.00805       0.0527       0.0694        0.043       0.0722       0.0576       0.0548       0.0919       0.0733        0.216      0.00225\n","! Validation         75  306.308    0.005      0.00734     1.56e-06      0.00735       0.0494       0.0663       0.0384       0.0714       0.0549       0.0496       0.0909       0.0703        0.071      0.00074\n","Wall time: 306.3090275259999\n","! Best model       75    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     76     1      0.00663      0.00663     1.99e-06       0.0474        0.063       0.0376       0.0669       0.0523       0.0485       0.0848       0.0667       0.0871     0.000907\n","     76     2      0.00704      0.00704     2.41e-06       0.0488       0.0649       0.0377        0.071       0.0544       0.0477       0.0899       0.0688        0.103      0.00107\n","     76     3      0.00863      0.00863     7.71e-07       0.0552       0.0719       0.0462       0.0733       0.0598        0.057       0.0949       0.0759       0.0502     0.000523\n","     76     4       0.0101       0.0101     2.19e-05       0.0598       0.0777       0.0481       0.0832       0.0656       0.0605        0.104       0.0822        0.335      0.00349\n","     76     5      0.00828      0.00826     1.57e-05       0.0537       0.0703       0.0433       0.0744       0.0588       0.0551       0.0936       0.0744        0.287      0.00298\n","     76     6      0.00652       0.0065     1.32e-05       0.0469       0.0624       0.0357       0.0695       0.0526       0.0453        0.087       0.0662        0.264      0.00275\n","     76     7      0.00762      0.00761     3.81e-06       0.0509       0.0675         0.04       0.0726       0.0563       0.0508       0.0923       0.0715        0.138      0.00144\n","     76     8      0.00682      0.00681      3.2e-06       0.0479       0.0639       0.0377       0.0682        0.053       0.0482       0.0871       0.0676        0.127      0.00132\n","     76     9      0.00596      0.00595     4.63e-06       0.0449       0.0597       0.0329       0.0687       0.0508       0.0419       0.0847       0.0633        0.154       0.0016\n","     76    10       0.0072       0.0072     5.23e-06       0.0493       0.0656       0.0382       0.0716       0.0549       0.0491         0.09       0.0695        0.156      0.00162\n","     76    11      0.00718      0.00717     8.67e-06       0.0493       0.0655       0.0398       0.0683        0.054       0.0511       0.0875       0.0693        0.206      0.00215\n","     76    12      0.00696      0.00696     2.41e-06       0.0494       0.0645       0.0389       0.0704       0.0546       0.0488        0.088       0.0684          0.1      0.00104\n","     76    13      0.00703      0.00702     1.01e-05       0.0493       0.0648       0.0391       0.0697       0.0544       0.0489       0.0884       0.0687        0.231      0.00241\n","     76    14      0.00743      0.00743     2.28e-06       0.0504       0.0667       0.0407       0.0699       0.0553       0.0514       0.0897       0.0706       0.0971      0.00101\n","     76    15      0.00625      0.00624      1.2e-05       0.0449       0.0611       0.0345       0.0656       0.0501       0.0451       0.0845       0.0648        0.246      0.00256\n","     76    16      0.00788      0.00787     2.01e-06        0.051       0.0686       0.0397       0.0737       0.0567        0.051       0.0945       0.0728       0.0688     0.000716\n","     76    17       0.0076      0.00756     4.63e-05       0.0506       0.0672       0.0406       0.0706       0.0556       0.0521       0.0902       0.0712        0.501      0.00522\n","     76    18        0.007      0.00697     2.52e-05       0.0489       0.0646       0.0391       0.0685       0.0538       0.0495       0.0873       0.0684        0.367      0.00382\n","     76    19      0.00723      0.00721     1.36e-05       0.0487       0.0657       0.0386       0.0688       0.0537       0.0505       0.0886       0.0696        0.259       0.0027\n","     76    20      0.00692      0.00691     7.44e-06       0.0486       0.0643        0.039       0.0677       0.0533       0.0496       0.0866       0.0681        0.192        0.002\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     76     1      0.00758      0.00758     2.61e-06       0.0505       0.0673       0.0393       0.0727        0.056       0.0508       0.0919       0.0713        0.097      0.00101\n","     76     2      0.00759      0.00759     1.22e-06         0.05       0.0674       0.0386       0.0728       0.0557       0.0501       0.0928       0.0714       0.0566      0.00059\n","     76     3      0.00716      0.00716     1.43e-06       0.0487       0.0654       0.0376       0.0708       0.0542       0.0482       0.0906       0.0694       0.0708     0.000738\n","     76     4      0.00738      0.00737     1.44e-06       0.0492       0.0664        0.038       0.0715       0.0548       0.0486       0.0922       0.0704       0.0772     0.000805\n","     76     5      0.00684      0.00684     1.12e-06       0.0481        0.064        0.038       0.0684       0.0532       0.0492       0.0863       0.0677       0.0525     0.000547\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              76  310.356    0.005       0.0073     1.01e-05      0.00731       0.0498       0.0661       0.0394       0.0706        0.055       0.0503       0.0898         0.07        0.198      0.00207\n","! Validation         76  310.356    0.005      0.00731     1.57e-06      0.00731       0.0493       0.0661       0.0383       0.0713       0.0548       0.0494       0.0908       0.0701       0.0708     0.000738\n","Wall time: 310.35695990499994\n","! Best model       76    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     77     1       0.0063       0.0063      1.4e-06       0.0463       0.0614       0.0354       0.0681       0.0517       0.0443        0.086       0.0651       0.0717     0.000747\n","     77     2      0.00776      0.00776     3.37e-06        0.051       0.0681       0.0401       0.0728       0.0565       0.0511       0.0934       0.0722        0.125       0.0013\n","     77     3      0.00645      0.00644      5.4e-06       0.0468       0.0621       0.0384       0.0637       0.0511       0.0486       0.0827       0.0656        0.155      0.00161\n","     77     4      0.00615      0.00615     7.67e-07        0.045       0.0607       0.0349       0.0653       0.0501       0.0443       0.0844       0.0643       0.0627     0.000653\n","     77     5      0.00755      0.00753        2e-05         0.05       0.0671       0.0403       0.0693       0.0548       0.0516       0.0905        0.071        0.323      0.00336\n","     77     6      0.00751      0.00749     1.23e-05       0.0502        0.067        0.038       0.0744       0.0562       0.0488       0.0932        0.071        0.256      0.00267\n","     77     7      0.00633      0.00632     1.06e-05       0.0461       0.0615       0.0358       0.0667       0.0513       0.0456       0.0847       0.0652        0.236      0.00246\n","     77     8      0.00695      0.00694     3.35e-06       0.0489       0.0645        0.038       0.0705       0.0543       0.0485       0.0882       0.0683        0.134       0.0014\n","     77     9      0.00783      0.00782     5.55e-06       0.0521       0.0684        0.041       0.0742       0.0576       0.0518       0.0932       0.0725        0.161      0.00167\n","     77    10      0.00828      0.00827     5.41e-06       0.0527       0.0704       0.0404       0.0773       0.0589       0.0512        0.098       0.0746        0.166      0.00173\n","     77    11      0.00766      0.00765     4.12e-06       0.0512       0.0677       0.0418         0.07       0.0559       0.0538       0.0891       0.0715        0.136      0.00141\n","     77    12      0.00705      0.00705     4.89e-06       0.0496       0.0649       0.0394         0.07       0.0547       0.0498       0.0877       0.0688        0.146      0.00152\n","     77    13      0.00631      0.00629     2.07e-05       0.0461       0.0613        0.036       0.0664       0.0512       0.0468       0.0831        0.065         0.32      0.00333\n","     77    14      0.00626      0.00625     1.37e-05       0.0465       0.0611        0.037       0.0656       0.0513        0.047       0.0825       0.0647        0.248      0.00259\n","     77    15      0.00747      0.00746     9.75e-06       0.0502       0.0668       0.0404       0.0697       0.0551       0.0522       0.0892       0.0707        0.208      0.00216\n","     77    16      0.00728      0.00728     2.11e-06       0.0507        0.066       0.0419       0.0684       0.0551       0.0527       0.0867       0.0697       0.0912      0.00095\n","     77    17      0.00642      0.00642     6.52e-06       0.0479        0.062       0.0383        0.067       0.0527       0.0481       0.0831       0.0656        0.173      0.00181\n","     77    18      0.00613      0.00613     1.59e-06       0.0461       0.0606       0.0375       0.0632       0.0504       0.0467       0.0815       0.0641       0.0883      0.00092\n","     77    19      0.00807      0.00806     3.23e-06       0.0528       0.0695       0.0431       0.0722       0.0577       0.0554       0.0914       0.0734        0.117      0.00122\n","     77    20       0.0072       0.0072     3.12e-06       0.0491       0.0656       0.0365       0.0745       0.0555       0.0462        0.093       0.0696        0.127      0.00132\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     77     1      0.00754      0.00753      2.6e-06       0.0503       0.0671       0.0392       0.0725       0.0559       0.0506       0.0916       0.0711       0.0976      0.00102\n","     77     2      0.00754      0.00754     1.23e-06       0.0499       0.0672       0.0384       0.0727       0.0556       0.0498       0.0926       0.0712       0.0567     0.000591\n","     77     3      0.00713      0.00712      1.4e-06       0.0486       0.0653       0.0375       0.0707       0.0541        0.048       0.0905       0.0692       0.0694     0.000723\n","     77     4      0.00734      0.00734     1.49e-06        0.049       0.0663       0.0379       0.0714       0.0546       0.0485       0.0921       0.0703       0.0779     0.000812\n","     77     5      0.00681       0.0068     1.11e-06        0.048       0.0638       0.0379       0.0683       0.0531        0.049       0.0861       0.0676        0.053     0.000552\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              77  314.403    0.005      0.00704      6.9e-06      0.00705        0.049       0.0649       0.0387       0.0695       0.0541       0.0493       0.0882       0.0688        0.167      0.00174\n","! Validation         77  314.403    0.005      0.00727     1.57e-06      0.00727       0.0492       0.0659       0.0382       0.0711       0.0546       0.0492       0.0906       0.0699       0.0709     0.000739\n","Wall time: 314.40389001099993\n","! Best model       77    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     78     1      0.00657      0.00656     5.22e-06       0.0474       0.0627       0.0378       0.0667       0.0523       0.0475       0.0852       0.0664        0.122      0.00127\n","     78     2       0.0083      0.00829     1.41e-05       0.0541       0.0704       0.0436       0.0753       0.0594       0.0541        0.095       0.0746        0.273      0.00285\n","     78     3      0.00854      0.00853     1.37e-05       0.0546       0.0714        0.044       0.0759       0.0599       0.0557       0.0954       0.0756        0.275      0.00286\n","     78     4      0.00743      0.00743     3.48e-07        0.051       0.0667       0.0424       0.0683       0.0554        0.054       0.0867       0.0703       0.0379     0.000395\n","     78     5      0.00683      0.00682     9.57e-06       0.0479       0.0639       0.0374       0.0691       0.0532       0.0477       0.0878       0.0677        0.224      0.00233\n","     78     6      0.00856      0.00856     5.45e-07       0.0552       0.0716       0.0475       0.0708       0.0591       0.0595       0.0911       0.0753       0.0375     0.000391\n","     78     7      0.00764      0.00764     2.32e-06       0.0517       0.0676       0.0417       0.0716       0.0567       0.0532       0.0898       0.0715        0.107      0.00112\n","     78     8      0.00803      0.00803     5.24e-07       0.0524       0.0693       0.0423       0.0724       0.0574       0.0543       0.0923       0.0733       0.0457     0.000476\n","     78     9      0.00683      0.00683     1.41e-06       0.0487       0.0639       0.0403       0.0656       0.0529       0.0512       0.0838       0.0675       0.0533     0.000555\n","     78    10      0.00668      0.00668     7.53e-07       0.0478       0.0632       0.0373       0.0686        0.053       0.0473       0.0866        0.067       0.0586      0.00061\n","     78    11      0.00616      0.00616     3.66e-06       0.0448       0.0607       0.0341       0.0661       0.0501       0.0428       0.0859       0.0644        0.133      0.00139\n","     78    12      0.00773      0.00772     2.45e-06       0.0507        0.068       0.0389       0.0742       0.0566         0.05       0.0942       0.0721       0.0844     0.000879\n","     78    13      0.00804        0.008     3.88e-05       0.0522       0.0692       0.0421       0.0724       0.0573       0.0544       0.0919       0.0731        0.456      0.00475\n","     78    14      0.00716      0.00715     9.54e-06       0.0494       0.0654       0.0397       0.0689       0.0543       0.0502       0.0884       0.0693         0.22      0.00229\n","     78    15      0.00699      0.00691     7.42e-05       0.0482       0.0643       0.0369       0.0706       0.0538       0.0463       0.0902       0.0682        0.637      0.00664\n","     78    16      0.00887      0.00885     1.14e-05       0.0552       0.0728       0.0484        0.069       0.0587       0.0621       0.0904       0.0763        0.224      0.00233\n","     78    17      0.00751      0.00748     3.75e-05       0.0507       0.0669       0.0406        0.071       0.0558       0.0524        0.089       0.0707        0.448      0.00467\n","     78    18      0.00663      0.00661     1.56e-05       0.0479       0.0629       0.0385       0.0666       0.0526       0.0484       0.0848       0.0666        0.282      0.00294\n","     78    19      0.00718      0.00714     3.98e-05       0.0509       0.0654       0.0418       0.0693       0.0555       0.0526       0.0854        0.069        0.464      0.00483\n","     78    20       0.0092      0.00916     4.78e-05       0.0556        0.074       0.0435       0.0798       0.0616       0.0551        0.102       0.0785        0.506      0.00527\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     78     1      0.00749      0.00749     2.66e-06       0.0502        0.067       0.0391       0.0724       0.0557       0.0504       0.0915       0.0709       0.0974      0.00101\n","     78     2      0.00749      0.00749     1.22e-06       0.0497        0.067       0.0383       0.0726       0.0554       0.0496       0.0924        0.071       0.0564     0.000588\n","     78     3      0.00709      0.00709     1.45e-06       0.0484       0.0651       0.0373       0.0706        0.054       0.0478       0.0903       0.0691       0.0705     0.000734\n","     78     4       0.0073       0.0073     1.49e-06       0.0489       0.0661       0.0377       0.0712       0.0545       0.0483       0.0919       0.0701       0.0784     0.000817\n","     78     5      0.00677      0.00677     1.07e-06       0.0479       0.0637       0.0377       0.0682       0.0529       0.0488       0.0859       0.0674        0.052     0.000541\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              78  318.430    0.005      0.00753     1.65e-05      0.00754       0.0508       0.0671       0.0409       0.0706       0.0558       0.0521       0.0899        0.071        0.234      0.00244\n","! Validation         78  318.430    0.005      0.00723     1.58e-06      0.00723        0.049       0.0658        0.038        0.071       0.0545        0.049       0.0904       0.0697       0.0709     0.000739\n","Wall time: 318.43105850999996\n","! Best model       78    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     79     1      0.00878      0.00871     6.51e-05       0.0543       0.0722       0.0447       0.0737       0.0592       0.0565       0.0963       0.0764        0.597      0.00622\n","     79     2      0.00735      0.00733      1.9e-05       0.0498       0.0662       0.0389       0.0717       0.0553       0.0492       0.0913       0.0702         0.32      0.00334\n","     79     3      0.00768      0.00764     4.22e-05       0.0503       0.0676       0.0381       0.0748       0.0564       0.0489       0.0945       0.0717        0.475      0.00495\n","     79     4      0.00821      0.00821     2.48e-06       0.0543       0.0701       0.0455       0.0719       0.0587       0.0572       0.0906       0.0739       0.0961        0.001\n","     79     5      0.00746      0.00744     2.09e-05       0.0517       0.0667       0.0427       0.0696       0.0562       0.0538        0.087       0.0704        0.325      0.00339\n","     79     6      0.00751       0.0075      9.8e-06       0.0504        0.067       0.0393       0.0725       0.0559       0.0495       0.0925        0.071        0.227      0.00237\n","     79     7      0.00709      0.00705     3.61e-05       0.0495        0.065       0.0407        0.067       0.0538       0.0526       0.0845       0.0685        0.435      0.00453\n","     79     8      0.00719      0.00716     3.26e-05       0.0484       0.0654       0.0373       0.0706        0.054       0.0477       0.0911       0.0694        0.418      0.00435\n","     79     9      0.00734      0.00729     5.21e-05       0.0488       0.0661       0.0372       0.0718       0.0545       0.0487       0.0913         0.07        0.533      0.00555\n","     79    10      0.00673      0.00667     6.26e-05       0.0476       0.0632       0.0371       0.0686       0.0529        0.047        0.087        0.067        0.584      0.00608\n","     79    11      0.00767      0.00763     3.73e-05       0.0508       0.0676       0.0397       0.0732       0.0564       0.0511       0.0921       0.0716        0.447      0.00466\n","     79    12      0.00855      0.00852     2.35e-05       0.0561       0.0714         0.05       0.0684       0.0592       0.0621       0.0872       0.0746        0.357      0.00372\n","     79    13      0.00774      0.00774     1.44e-06       0.0531       0.0681       0.0467       0.0661       0.0564       0.0582       0.0843       0.0713       0.0652      0.00068\n","     79    14       0.0068      0.00678     2.08e-05        0.048       0.0637       0.0367       0.0707       0.0537       0.0467       0.0883       0.0675         0.33      0.00344\n","     79    15      0.00853      0.00853     4.37e-07       0.0552       0.0714       0.0468        0.072       0.0594       0.0589       0.0915       0.0752       0.0396     0.000413\n","     79    16      0.00801      0.00797     4.12e-05       0.0518       0.0691       0.0419       0.0716       0.0567       0.0528       0.0935       0.0731        0.474      0.00493\n","     79    17      0.00683      0.00683     1.09e-06       0.0472       0.0639       0.0362       0.0692       0.0527       0.0471       0.0885       0.0678       0.0754     0.000785\n","     79    18      0.00841      0.00835      6.4e-05       0.0544       0.0707       0.0436        0.076       0.0598       0.0553       0.0942       0.0747        0.589      0.00614\n","     79    19      0.00913      0.00913     2.32e-06       0.0569       0.0739       0.0475       0.0758       0.0616       0.0592       0.0969        0.078        0.101      0.00105\n","     79    20      0.00718      0.00718     6.23e-06       0.0501       0.0655       0.0411       0.0681       0.0546       0.0519       0.0866       0.0693        0.169      0.00176\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     79     1      0.00745      0.00745     2.59e-06         0.05       0.0668       0.0389       0.0722       0.0556       0.0502       0.0913       0.0708       0.0956     0.000996\n","     79     2      0.00745      0.00745     1.23e-06       0.0496       0.0668       0.0382       0.0724       0.0553       0.0494       0.0922       0.0708        0.057     0.000594\n","     79     3      0.00706      0.00706     1.41e-06       0.0483        0.065       0.0372       0.0705       0.0539       0.0476       0.0902       0.0689       0.0704     0.000733\n","     79     4      0.00727      0.00727     1.54e-06       0.0488       0.0659       0.0376       0.0712       0.0544       0.0481       0.0918       0.0699       0.0784     0.000817\n","     79     5      0.00673      0.00673     1.07e-06       0.0477       0.0635       0.0376       0.0681       0.0528       0.0486       0.0858       0.0672        0.052     0.000541\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              79  322.476    0.005      0.00768     2.71e-05      0.00771       0.0514       0.0678       0.0416       0.0712       0.0564       0.0529       0.0905       0.0717        0.333      0.00347\n","! Validation         79  322.476    0.005      0.00719     1.57e-06      0.00719       0.0489       0.0656       0.0379       0.0709       0.0544       0.0488       0.0903       0.0695       0.0707     0.000736\n","Wall time: 322.47630192299994\n","! Best model       79    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     80     1      0.00652       0.0065     2.01e-05       0.0471       0.0624       0.0368       0.0676       0.0522       0.0463       0.0859       0.0661        0.328      0.00342\n","     80     2      0.00777      0.00776     8.84e-06       0.0517       0.0681       0.0404       0.0741       0.0573       0.0512       0.0932       0.0722        0.196      0.00204\n","     80     3      0.00846      0.00844     1.91e-05       0.0543       0.0711        0.045       0.0729        0.059       0.0568       0.0932        0.075        0.314      0.00327\n","     80     4      0.00672      0.00672     2.86e-07       0.0486       0.0634       0.0392       0.0674       0.0533       0.0495       0.0846       0.0671       0.0316      0.00033\n","     80     5      0.00668      0.00668     2.26e-06       0.0479       0.0632       0.0384       0.0669       0.0527       0.0482       0.0856       0.0669        0.105       0.0011\n","     80     6      0.00577      0.00577     2.23e-06       0.0438       0.0588       0.0336       0.0642       0.0489       0.0434       0.0812       0.0623        0.105       0.0011\n","     80     7      0.00646      0.00646     2.42e-06       0.0473       0.0622       0.0371       0.0677       0.0524       0.0468        0.085       0.0659        0.106       0.0011\n","     80     8      0.00664      0.00664     9.77e-07       0.0467        0.063       0.0361       0.0679        0.052       0.0473       0.0863       0.0668       0.0674     0.000702\n","     80     9      0.00669      0.00669     1.41e-06       0.0472       0.0633       0.0355       0.0706        0.053        0.046       0.0882       0.0671       0.0564     0.000588\n","     80    10      0.00662      0.00662     1.15e-06       0.0472       0.0629       0.0359       0.0699       0.0529       0.0455        0.088       0.0667       0.0771     0.000804\n","     80    11      0.00619      0.00619     4.17e-06       0.0451       0.0609       0.0335       0.0681       0.0508        0.042       0.0871       0.0645        0.138      0.00144\n","     80    12      0.00711      0.00711     3.63e-06       0.0485       0.0652       0.0375       0.0705        0.054       0.0486       0.0897       0.0691        0.116       0.0012\n","     80    13      0.00643      0.00642     3.82e-06       0.0466        0.062       0.0367       0.0664       0.0516       0.0474       0.0839       0.0657        0.109      0.00113\n","     80    14      0.00745      0.00745     3.46e-07       0.0481       0.0668       0.0361       0.0721       0.0541        0.048       0.0936       0.0708       0.0338     0.000352\n","     80    15      0.00778      0.00778     2.06e-06       0.0508       0.0682        0.038       0.0763       0.0572       0.0495       0.0952       0.0724         0.09     0.000938\n","     80    16      0.00705      0.00704     6.34e-06       0.0496       0.0649       0.0388       0.0712        0.055       0.0486        0.089       0.0688        0.166      0.00173\n","     80    17      0.00639      0.00638     1.06e-05       0.0472       0.0618        0.039       0.0634       0.0512       0.0499       0.0805       0.0652        0.236      0.00245\n","     80    18      0.00754      0.00753     7.64e-06       0.0517       0.0671       0.0436       0.0677       0.0557       0.0556       0.0857       0.0706        0.194      0.00202\n","     80    19      0.00781       0.0078     4.46e-06       0.0498       0.0683       0.0365       0.0764       0.0564       0.0467       0.0982       0.0725        0.153       0.0016\n","     80    20      0.00887      0.00883     4.04e-05        0.055       0.0727       0.0433       0.0784       0.0609        0.055        0.099        0.077        0.466      0.00485\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     80     1      0.00741      0.00741     2.63e-06       0.0499       0.0666       0.0388        0.072       0.0554         0.05       0.0911       0.0706       0.0969      0.00101\n","     80     2      0.00742      0.00742     1.17e-06       0.0495       0.0666        0.038       0.0723       0.0552       0.0492        0.092       0.0706       0.0554     0.000577\n","     80     3      0.00703      0.00703     1.43e-06       0.0482       0.0649       0.0371       0.0704       0.0538       0.0475       0.0901       0.0688       0.0713     0.000743\n","     80     4      0.00723      0.00723     1.54e-06       0.0486       0.0658       0.0375        0.071       0.0542       0.0479       0.0916       0.0698       0.0792     0.000825\n","     80     5       0.0067       0.0067     1.05e-06       0.0476       0.0633       0.0374        0.068       0.0527       0.0484       0.0857       0.0671       0.0506     0.000527\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              80  326.491    0.005      0.00704     7.11e-06      0.00705       0.0487       0.0649       0.0381         0.07        0.054       0.0488       0.0888       0.0688        0.154      0.00161\n","! Validation         80  326.491    0.005      0.00716     1.57e-06      0.00716       0.0488       0.0654       0.0378       0.0707       0.0543       0.0486       0.0901       0.0694       0.0707     0.000736\n","Wall time: 326.492147941\n","! Best model       80    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     81     1      0.00722      0.00722     6.35e-07       0.0503       0.0657       0.0407       0.0695       0.0551       0.0517       0.0873       0.0695       0.0574     0.000598\n","     81     2      0.00644      0.00644     3.82e-06       0.0476       0.0621       0.0377       0.0674       0.0526       0.0474       0.0841       0.0657         0.13      0.00136\n","     81     3      0.00631       0.0063     2.06e-06       0.0468       0.0614        0.038       0.0644       0.0512       0.0481       0.0818        0.065       0.0793     0.000826\n","     81     4      0.00631      0.00631     3.84e-06       0.0474       0.0614       0.0393       0.0636       0.0515       0.0494       0.0802       0.0648        0.123      0.00128\n","     81     5      0.00669      0.00668     4.79e-06       0.0474       0.0632       0.0356       0.0711       0.0533       0.0448       0.0893       0.0671        0.147      0.00153\n","     81     6      0.00699      0.00699     4.52e-07       0.0497       0.0647       0.0405       0.0681       0.0543       0.0503       0.0866       0.0684       0.0496     0.000517\n","     81     7      0.00729      0.00728     1.08e-05       0.0492        0.066       0.0389       0.0699       0.0544       0.0502       0.0896       0.0699         0.24       0.0025\n","     81     8      0.00634      0.00633     9.19e-06       0.0466       0.0615       0.0354        0.069       0.0522       0.0452       0.0853       0.0653         0.22      0.00229\n","     81     9      0.00588      0.00586     1.53e-05       0.0445       0.0592       0.0351       0.0633       0.0492       0.0444       0.0811       0.0628        0.286      0.00298\n","     81    10      0.00803      0.00803     3.87e-06       0.0515       0.0693       0.0403       0.0739       0.0571       0.0524       0.0945       0.0734        0.144       0.0015\n","     81    11      0.00837      0.00837      1.6e-06       0.0535       0.0708       0.0435       0.0736       0.0585       0.0562       0.0933       0.0747       0.0805     0.000838\n","     81    12      0.00666      0.00666     1.75e-06       0.0473       0.0631       0.0377       0.0665       0.0521       0.0478       0.0859       0.0669       0.0762     0.000793\n","     81    13      0.00668      0.00668      3.1e-06       0.0481       0.0632       0.0396       0.0651       0.0523       0.0498       0.0838       0.0668        0.116      0.00121\n","     81    14      0.00732       0.0073     1.84e-05       0.0512       0.0661       0.0412       0.0711       0.0561       0.0512       0.0888         0.07        0.312      0.00326\n","     81    15      0.00871      0.00871     1.21e-06       0.0549       0.0722       0.0428       0.0791       0.0609       0.0544       0.0985       0.0765       0.0746     0.000777\n","     81    16      0.00933      0.00925     7.64e-05       0.0571       0.0744       0.0461        0.079       0.0626       0.0575          0.1       0.0787        0.644      0.00671\n","     81    17      0.00835      0.00834     5.86e-06       0.0543       0.0707       0.0442       0.0745       0.0593        0.056       0.0933       0.0747        0.168      0.00175\n","     81    18      0.00715      0.00712     2.67e-05       0.0492       0.0653       0.0392       0.0693       0.0542         0.05       0.0883       0.0691        0.378      0.00393\n","     81    19      0.00811      0.00811     2.64e-06       0.0528       0.0697       0.0426       0.0732       0.0579       0.0544       0.0929       0.0737        0.105       0.0011\n","     81    20      0.00875      0.00872     2.23e-05       0.0558       0.0723        0.046       0.0754       0.0607        0.058       0.0945       0.0763        0.338      0.00352\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     81     1      0.00738      0.00738     2.63e-06       0.0497       0.0664       0.0387       0.0719       0.0553       0.0498        0.091       0.0704       0.0965      0.00101\n","     81     2      0.00737      0.00737     1.15e-06       0.0493       0.0664       0.0379       0.0722        0.055        0.049       0.0918       0.0704       0.0529     0.000551\n","     81     3        0.007        0.007     1.37e-06       0.0481       0.0647        0.037       0.0703       0.0536       0.0473       0.0899       0.0686       0.0703     0.000732\n","     81     4      0.00719      0.00719     1.51e-06       0.0485       0.0656       0.0373       0.0709       0.0541       0.0477       0.0915       0.0696       0.0791     0.000824\n","     81     5      0.00667      0.00667     9.95e-07       0.0475       0.0632       0.0373       0.0678       0.0526       0.0482       0.0856       0.0669       0.0495     0.000516\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              81  330.522    0.005      0.00733     1.07e-05      0.00735       0.0503       0.0663       0.0402       0.0704       0.0553       0.0511       0.0891       0.0701        0.188      0.00196\n","! Validation         81  330.522    0.005      0.00712     1.53e-06      0.00712       0.0486       0.0653       0.0376       0.0706       0.0541       0.0484         0.09       0.0692       0.0697     0.000726\n","Wall time: 330.5227252779998\n","! Best model       81    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     82     1      0.00938      0.00936     2.03e-05       0.0578       0.0749       0.0483       0.0768       0.0626       0.0606       0.0973       0.0789        0.331      0.00344\n","     82     2      0.00878      0.00871     7.34e-05       0.0531       0.0722       0.0413       0.0766       0.0589       0.0535       0.0996       0.0765        0.636      0.00663\n","     82     3      0.00619      0.00619     3.01e-06       0.0461       0.0609       0.0353       0.0675       0.0514       0.0447       0.0843       0.0645        0.107      0.00111\n","     82     4      0.00697      0.00695     2.29e-05       0.0485       0.0645       0.0378       0.0699       0.0539       0.0482       0.0885       0.0683        0.349      0.00363\n","     82     5      0.00712      0.00712     8.05e-07       0.0492       0.0653       0.0403       0.0669       0.0536       0.0523       0.0856       0.0689       0.0627     0.000653\n","     82     6      0.00661      0.00661     1.42e-06       0.0477       0.0629       0.0373       0.0686       0.0529        0.047       0.0864       0.0667       0.0771     0.000804\n","     82     7      0.00629      0.00629     8.63e-07       0.0455       0.0614       0.0355       0.0657       0.0506       0.0462       0.0838        0.065       0.0596     0.000621\n","     82     8      0.00698      0.00698     2.84e-06       0.0484       0.0646       0.0387       0.0677       0.0532         0.05       0.0868       0.0684       0.0916     0.000954\n","     82     9      0.00798      0.00798     3.69e-07       0.0508       0.0691       0.0397        0.073       0.0563       0.0507       0.0958       0.0733       0.0398     0.000415\n","     82    10      0.00646      0.00644     2.34e-05       0.0465       0.0621       0.0351       0.0692       0.0522       0.0446       0.0871       0.0658        0.349      0.00363\n","     82    11      0.00648      0.00648        2e-06       0.0464       0.0623       0.0343       0.0706       0.0524       0.0434       0.0887        0.066       0.0953     0.000993\n","     82    12      0.00788      0.00782     5.34e-05       0.0512       0.0684       0.0397       0.0742        0.057       0.0506       0.0945       0.0725        0.535      0.00557\n","     82    13      0.00683      0.00683     2.96e-06       0.0495       0.0639       0.0412       0.0661       0.0537       0.0519       0.0829       0.0674        0.123      0.00128\n","     82    14      0.00678      0.00676     2.21e-05       0.0487       0.0636       0.0379       0.0701        0.054       0.0471       0.0877       0.0674        0.343      0.00357\n","     82    15      0.00608      0.00608     2.66e-07       0.0452       0.0603        0.035       0.0656       0.0503       0.0446       0.0832       0.0639       0.0285     0.000297\n","     82    16      0.00678      0.00677     1.35e-05       0.0486       0.0636        0.039       0.0678       0.0534       0.0496        0.085       0.0673         0.26      0.00271\n","     82    17      0.00622      0.00621     7.47e-06       0.0463        0.061       0.0367       0.0654       0.0511       0.0471        0.082       0.0645        0.174      0.00181\n","     82    18      0.00655      0.00654     1.28e-05       0.0479       0.0626       0.0388        0.066       0.0524       0.0486       0.0838       0.0662        0.263      0.00274\n","     82    19      0.00802      0.00801     1.43e-05        0.052       0.0692        0.042       0.0722       0.0571       0.0534       0.0932       0.0733        0.252      0.00263\n","     82    20      0.00872       0.0087     1.36e-05       0.0543       0.0722        0.042       0.0791       0.0605       0.0527          0.1       0.0765        0.272      0.00283\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     82     1      0.00734      0.00734     2.52e-06       0.0496       0.0663       0.0385       0.0718       0.0552       0.0497       0.0908       0.0702       0.0949     0.000989\n","     82     2      0.00734      0.00734     1.16e-06       0.0492       0.0663       0.0378       0.0721       0.0549       0.0489       0.0917       0.0703       0.0545     0.000568\n","     82     3      0.00697      0.00697     1.38e-06        0.048       0.0646       0.0368       0.0702       0.0535       0.0471       0.0898       0.0685       0.0696     0.000725\n","     82     4      0.00716      0.00716     1.54e-06       0.0484       0.0654       0.0372       0.0708        0.054       0.0475       0.0913       0.0694       0.0794     0.000827\n","     82     5      0.00664      0.00664     9.96e-07       0.0473        0.063       0.0372       0.0677       0.0524        0.048       0.0854       0.0667       0.0503     0.000524\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              82  334.564    0.005      0.00714     1.46e-05      0.00716       0.0492       0.0654       0.0388         0.07       0.0544       0.0495        0.089       0.0693        0.222      0.00232\n","! Validation         82  334.564    0.005      0.00709     1.52e-06      0.00709       0.0485       0.0651       0.0375       0.0705        0.054       0.0482       0.0898        0.069       0.0697     0.000727\n","Wall time: 334.56496590999996\n","! Best model       82    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     83     1        0.007        0.007     3.95e-07       0.0479       0.0647       0.0376       0.0684        0.053       0.0479       0.0893       0.0686       0.0434     0.000452\n","     83     2      0.00695      0.00694     8.94e-06       0.0491       0.0645        0.038       0.0714       0.0547       0.0473       0.0894       0.0684        0.215      0.00224\n","     83     3      0.00829      0.00826     3.34e-05       0.0552       0.0703       0.0465       0.0726       0.0595       0.0582       0.0898        0.074        0.424      0.00442\n","     83     4       0.0102       0.0102     4.69e-06       0.0626       0.0781       0.0585       0.0708       0.0647       0.0725       0.0882       0.0803        0.135       0.0014\n","     83     5        0.008        0.008     1.19e-06       0.0535       0.0692       0.0462        0.068       0.0571       0.0584       0.0869       0.0726       0.0613     0.000639\n","     83     6      0.00795      0.00789     6.11e-05       0.0518       0.0687       0.0405       0.0745       0.0575        0.052       0.0936       0.0728        0.579      0.00604\n","     83     7      0.00698      0.00698      5.1e-06       0.0487       0.0646       0.0376        0.071       0.0543       0.0482       0.0888       0.0685         0.15      0.00157\n","     83     8      0.00801      0.00799     1.48e-05       0.0523       0.0692       0.0403       0.0764       0.0583       0.0503       0.0964       0.0733        0.283      0.00294\n","     83     9      0.00788      0.00788     5.38e-06       0.0518       0.0687        0.042       0.0714       0.0567       0.0534       0.0918       0.0726        0.149      0.00155\n","     83    10      0.00759      0.00758      5.7e-06       0.0511       0.0674       0.0413       0.0707        0.056       0.0519       0.0907       0.0713        0.172       0.0018\n","     83    11      0.00705      0.00703     2.66e-05       0.0496       0.0649       0.0392       0.0704       0.0548       0.0486       0.0888       0.0687        0.377      0.00392\n","     83    12      0.00976      0.00976     1.28e-06       0.0593       0.0764       0.0514       0.0751       0.0632       0.0639       0.0967       0.0803       0.0762     0.000793\n","     83    13       0.0103       0.0103     1.49e-06       0.0608       0.0786       0.0515       0.0795       0.0655       0.0647        0.101       0.0828       0.0709     0.000739\n","     83    14      0.00871      0.00867     3.41e-05       0.0538       0.0721       0.0417        0.078       0.0599       0.0544       0.0983       0.0763        0.429      0.00447\n","     83    15      0.00696      0.00695     9.78e-06       0.0481       0.0645       0.0379       0.0685       0.0532       0.0478       0.0889       0.0683        0.231      0.00241\n","     83    16      0.00674      0.00674     1.49e-06       0.0485       0.0635       0.0388       0.0679       0.0533       0.0484       0.0861       0.0673       0.0803     0.000836\n","     83    17      0.00804      0.00804      1.1e-06        0.053       0.0694        0.043        0.073        0.058       0.0548       0.0919       0.0733       0.0605     0.000631\n","     83    18      0.00711      0.00706     4.89e-05       0.0509        0.065       0.0442       0.0642       0.0542        0.055       0.0813       0.0682        0.513      0.00534\n","     83    19      0.00651      0.00649     1.84e-05       0.0468       0.0623       0.0362        0.068       0.0521       0.0466       0.0855        0.066        0.311      0.00324\n","     83    20      0.00788      0.00785     3.63e-05       0.0525       0.0685        0.042       0.0737       0.0578       0.0536       0.0913       0.0725        0.445      0.00463\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     83     1      0.00731      0.00731     2.54e-06       0.0495       0.0661       0.0384       0.0717       0.0551       0.0495       0.0907       0.0701       0.0943     0.000983\n","     83     2      0.00731      0.00731     1.22e-06       0.0491       0.0661       0.0377       0.0719       0.0548       0.0487       0.0915       0.0701       0.0562     0.000586\n","     83     3      0.00694      0.00694      1.4e-06       0.0478       0.0644       0.0367       0.0701       0.0534        0.047       0.0897       0.0683       0.0704     0.000733\n","     83     4      0.00713      0.00713     1.55e-06       0.0483       0.0653        0.037       0.0707       0.0539       0.0473       0.0912       0.0693       0.0783     0.000816\n","     83     5      0.00661       0.0066     1.03e-06       0.0472       0.0629        0.037       0.0676       0.0523       0.0478       0.0853       0.0666       0.0513     0.000534\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              83  338.725    0.005      0.00788      1.6e-05       0.0079       0.0524       0.0687       0.0427       0.0717       0.0572       0.0543       0.0908       0.0726         0.24       0.0025\n","! Validation         83  338.725    0.005      0.00706     1.55e-06      0.00706       0.0484        0.065       0.0374       0.0704       0.0539       0.0481       0.0897       0.0689       0.0701      0.00073\n","Wall time: 338.7254808429998\n","! Best model       83    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     84     1      0.00744      0.00742     2.57e-05       0.0502       0.0666       0.0407       0.0693        0.055       0.0517       0.0892       0.0705         0.37      0.00385\n","     84     2      0.00705      0.00705     1.55e-06       0.0482       0.0649       0.0375       0.0696       0.0535       0.0482       0.0895       0.0688       0.0785     0.000818\n","     84     3      0.00681      0.00679     2.51e-05       0.0486       0.0637       0.0397       0.0662        0.053       0.0503       0.0845       0.0674        0.366      0.00382\n","     84     4      0.00716      0.00716     2.46e-07       0.0495       0.0655       0.0392       0.0702       0.0547       0.0498       0.0889       0.0693       0.0301     0.000313\n","     84     5      0.00854      0.00849     5.75e-05       0.0527       0.0713       0.0395       0.0791       0.0593       0.0512          0.1       0.0756        0.551      0.00574\n","     84     6      0.00757      0.00755     1.85e-05       0.0522       0.0672        0.044       0.0685       0.0563       0.0555        0.086       0.0708        0.314      0.00327\n","     84     7      0.00706      0.00702     3.89e-05       0.0486       0.0648       0.0378       0.0702        0.054       0.0482       0.0892       0.0687        0.456      0.00475\n","     84     8       0.0061      0.00609     7.85e-06       0.0448       0.0604       0.0346       0.0651       0.0498       0.0444       0.0836        0.064        0.185      0.00193\n","     84     9      0.00694       0.0069     3.54e-05       0.0495       0.0643       0.0419       0.0646       0.0533       0.0521       0.0834       0.0678         0.44      0.00458\n","     84    10      0.00739      0.00739      1.7e-06       0.0504       0.0665       0.0394       0.0725       0.0559       0.0499       0.0911       0.0705       0.0826     0.000861\n","     84    11      0.00724      0.00718     6.03e-05       0.0496       0.0656       0.0388       0.0711        0.055       0.0492       0.0898       0.0695        0.575      0.00599\n","     84    12       0.0057      0.00569      4.5e-06       0.0445       0.0584       0.0354       0.0626        0.049       0.0447       0.0788       0.0618        0.135       0.0014\n","     84    13      0.00812      0.00807     4.51e-05       0.0525       0.0695       0.0414       0.0749       0.0581       0.0519       0.0954       0.0737        0.485      0.00505\n","     84    14      0.00854      0.00854     5.56e-06       0.0561       0.0715       0.0471        0.074       0.0606       0.0586        0.092       0.0753        0.161      0.00168\n","     84    15      0.00738      0.00737     2.47e-06       0.0517       0.0664       0.0428       0.0694       0.0561       0.0539       0.0862         0.07        0.106      0.00111\n","     84    16      0.00654      0.00651     3.08e-05       0.0467       0.0624       0.0361        0.068       0.0521       0.0467       0.0856       0.0661        0.404       0.0042\n","     84    17      0.00592      0.00592     2.81e-06       0.0448       0.0595       0.0347        0.065       0.0499       0.0435       0.0827       0.0631        0.115       0.0012\n","     84    18      0.00741      0.00738     2.86e-05       0.0498       0.0665        0.039       0.0713       0.0552       0.0501       0.0907       0.0704        0.386      0.00402\n","     84    19       0.0074      0.00739      1.3e-05       0.0495       0.0665       0.0399       0.0686       0.0542        0.051       0.0898       0.0704        0.263      0.00274\n","     84    20      0.00719      0.00719     3.51e-06       0.0491       0.0656       0.0384       0.0703       0.0544       0.0492       0.0898       0.0695        0.133      0.00139\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     84     1      0.00728      0.00728     2.55e-06       0.0494        0.066       0.0383       0.0716       0.0549       0.0493       0.0905       0.0699        0.094      0.00098\n","     84     2      0.00727      0.00727     1.17e-06        0.049        0.066       0.0375       0.0718       0.0547       0.0485       0.0913       0.0699       0.0553     0.000576\n","     84     3      0.00691      0.00691     1.35e-06       0.0477       0.0643       0.0366         0.07       0.0533       0.0468       0.0896       0.0682       0.0691      0.00072\n","     84     4       0.0071       0.0071     1.53e-06       0.0482       0.0652       0.0369       0.0706       0.0538       0.0472       0.0911       0.0691       0.0794     0.000827\n","     84     5      0.00657      0.00657      9.9e-07       0.0471       0.0627       0.0369       0.0675       0.0522       0.0477       0.0852       0.0664       0.0486     0.000507\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              84  342.789    0.005      0.00715     2.05e-05      0.00718       0.0494       0.0654       0.0394       0.0695       0.0545       0.0501       0.0884       0.0693        0.282      0.00294\n","! Validation         84  342.789    0.005      0.00703     1.52e-06      0.00703       0.0483       0.0648       0.0372       0.0703       0.0538       0.0479       0.0896       0.0687       0.0693     0.000722\n","Wall time: 342.7901261789998\n","! Best model       84    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     85     1      0.00635      0.00635     1.05e-06       0.0462       0.0617       0.0356       0.0675       0.0515        0.046       0.0847       0.0654       0.0705     0.000734\n","     85     2      0.00691      0.00689     1.61e-05       0.0481       0.0642       0.0381       0.0683       0.0532       0.0489       0.0872        0.068        0.279      0.00291\n","     85     3      0.00841       0.0084     1.57e-05       0.0533       0.0709       0.0423       0.0754       0.0588       0.0543       0.0958       0.0751        0.284      0.00295\n","     85     4      0.00663      0.00656     6.79e-05       0.0473       0.0627       0.0369       0.0679       0.0524       0.0469        0.086       0.0664         0.61      0.00635\n","     85     5      0.00581      0.00581     3.83e-06       0.0438        0.059       0.0336       0.0641       0.0488       0.0424       0.0827       0.0625        0.139      0.00145\n","     85     6      0.00663      0.00662     1.29e-05       0.0468       0.0629        0.036       0.0682       0.0521       0.0452       0.0882       0.0667        0.262      0.00273\n","     85     7      0.00723      0.00723     5.14e-06       0.0494       0.0658       0.0376       0.0729       0.0553       0.0471       0.0924       0.0697        0.166      0.00173\n","     85     8      0.00652      0.00652     2.27e-06       0.0469       0.0625       0.0364       0.0677       0.0521       0.0464        0.086       0.0662        0.103      0.00107\n","     85     9      0.00592      0.00591     1.33e-05       0.0445       0.0595       0.0351       0.0634       0.0492       0.0455       0.0805        0.063        0.267      0.00278\n","     85    10      0.00635      0.00633     1.48e-05       0.0469       0.0616       0.0372       0.0663       0.0517       0.0473        0.083       0.0652        0.266      0.00278\n","     85    11      0.00729      0.00728      1.4e-05       0.0499        0.066         0.04       0.0696       0.0548       0.0506       0.0892       0.0699        0.274      0.00286\n","     85    12      0.00601        0.006      1.2e-05       0.0454       0.0599       0.0354       0.0653       0.0504       0.0444       0.0826       0.0635        0.242      0.00252\n","     85    13      0.00677      0.00677     6.51e-07       0.0471       0.0636       0.0367        0.068       0.0523       0.0473       0.0876       0.0675       0.0439     0.000458\n","     85    14      0.00676      0.00674     1.24e-05       0.0473       0.0635       0.0366       0.0687       0.0526       0.0478       0.0868       0.0673        0.255      0.00266\n","     85    15      0.00669      0.00667      1.4e-05       0.0482       0.0632       0.0386       0.0675        0.053       0.0485       0.0853       0.0669        0.261      0.00272\n","     85    16      0.00645      0.00643     2.12e-05       0.0469        0.062       0.0364       0.0678       0.0521        0.046       0.0855       0.0658        0.338      0.00352\n","     85    17      0.00684      0.00683     1.53e-05       0.0481       0.0639       0.0379       0.0686       0.0533       0.0482       0.0872       0.0677        0.284      0.00296\n","     85    18      0.00705      0.00705     5.75e-07       0.0488       0.0649       0.0373       0.0717       0.0545       0.0477         0.09       0.0688       0.0482     0.000503\n","     85    19      0.00666      0.00664     2.68e-05       0.0476        0.063       0.0372       0.0684       0.0528        0.048       0.0855       0.0667        0.377      0.00393\n","     85    20      0.00709      0.00706     2.86e-05       0.0491        0.065       0.0374       0.0725       0.0549       0.0477       0.0901       0.0689        0.386      0.00402\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     85     1      0.00724      0.00724     2.47e-06       0.0493       0.0658       0.0382       0.0715       0.0548       0.0492       0.0904       0.0698       0.0929     0.000967\n","     85     2      0.00724      0.00724     1.14e-06       0.0488       0.0658       0.0374       0.0717       0.0546       0.0484       0.0912       0.0698        0.054     0.000563\n","     85     3      0.00688      0.00688      1.4e-06       0.0476       0.0642       0.0365         0.07       0.0532       0.0466       0.0894        0.068       0.0704     0.000733\n","     85     4      0.00707      0.00706     1.52e-06        0.048        0.065       0.0368       0.0705       0.0536        0.047       0.0909        0.069       0.0787      0.00082\n","     85     5      0.00655      0.00655        1e-06        0.047       0.0626       0.0368       0.0674       0.0521       0.0475       0.0851       0.0663       0.0497     0.000518\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              85  346.834    0.005       0.0067     1.49e-05      0.00672       0.0476       0.0633       0.0371       0.0685       0.0528       0.0474       0.0869       0.0671        0.248      0.00258\n","! Validation         85  346.834    0.005      0.00699     1.51e-06        0.007       0.0481       0.0647       0.0371       0.0702       0.0537       0.0477       0.0894       0.0686       0.0691      0.00072\n","Wall time: 346.83502122999994\n","! Best model       85    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     86     1      0.00612      0.00608     3.47e-05       0.0454       0.0603       0.0348       0.0668       0.0508       0.0436       0.0844        0.064        0.435      0.00453\n","     86     2      0.00653      0.00646     6.92e-05       0.0461       0.0622       0.0346        0.069       0.0518        0.044       0.0879        0.066        0.614       0.0064\n","     86     3      0.00747      0.00746     1.66e-05       0.0501       0.0668       0.0398       0.0706       0.0552       0.0513       0.0901       0.0707          0.3      0.00313\n","     86     4      0.00699      0.00695      4.8e-05       0.0488       0.0645       0.0389       0.0685       0.0537       0.0493       0.0872       0.0683        0.508       0.0053\n","     86     5       0.0071       0.0071     1.87e-06       0.0495       0.0652       0.0385       0.0715        0.055       0.0488       0.0894       0.0691       0.0762     0.000793\n","     86     6      0.00738      0.00734     3.67e-05       0.0488       0.0663       0.0374       0.0716       0.0545       0.0482       0.0924       0.0703        0.446      0.00465\n","     86     7      0.00677      0.00676     7.74e-06       0.0476       0.0636       0.0375        0.068       0.0527       0.0485       0.0862       0.0674        0.193      0.00201\n","     86     8      0.00567      0.00564     3.07e-05       0.0439       0.0581        0.033       0.0656       0.0493       0.0413        0.082       0.0616        0.405      0.00422\n","     86     9      0.00662      0.00659     2.83e-05       0.0478       0.0628       0.0379       0.0675       0.0527        0.048        0.085       0.0665        0.386      0.00402\n","     86    10      0.00637      0.00632     5.75e-05       0.0462       0.0615        0.036       0.0667       0.0513       0.0462       0.0841       0.0651        0.562      0.00585\n","     86    11        0.006      0.00599     4.52e-06       0.0458       0.0599       0.0359       0.0656       0.0507       0.0452       0.0817       0.0635        0.142      0.00148\n","     86    12      0.00668      0.00665     2.69e-05       0.0479       0.0631       0.0385       0.0665       0.0525       0.0491       0.0844       0.0667        0.381      0.00397\n","     86    13      0.00633      0.00629     3.35e-05       0.0463       0.0614       0.0364       0.0659       0.0512        0.046       0.0841        0.065        0.428      0.00446\n","     86    14      0.00652      0.00652     1.38e-06       0.0475       0.0625       0.0385       0.0655        0.052        0.048       0.0842       0.0661       0.0713     0.000743\n","     86    15      0.00687      0.00685     2.44e-05        0.048        0.064       0.0365        0.071       0.0537       0.0469       0.0888       0.0679        0.363      0.00378\n","     86    16       0.0077      0.00769     1.07e-06       0.0513       0.0679       0.0399       0.0741        0.057       0.0509       0.0929       0.0719       0.0703     0.000732\n","     86    17      0.00823      0.00822     9.62e-06       0.0528       0.0701       0.0416       0.0752       0.0584       0.0538       0.0947       0.0742         0.22      0.00229\n","     86    18      0.00841      0.00841      1.9e-06       0.0549       0.0709       0.0489       0.0668       0.0579       0.0611       0.0874       0.0742       0.0926     0.000964\n","     86    19      0.00777      0.00776     8.75e-07       0.0516       0.0682       0.0418       0.0711       0.0564       0.0541         0.09        0.072       0.0525     0.000547\n","     86    20      0.00764      0.00762     2.62e-05       0.0508       0.0675       0.0402       0.0721       0.0562       0.0502        0.093       0.0716        0.365       0.0038\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     86     1      0.00721      0.00721     2.55e-06       0.0491       0.0657        0.038       0.0713       0.0547        0.049       0.0902       0.0696       0.0943     0.000983\n","     86     2      0.00721      0.00721      1.1e-06       0.0487       0.0657       0.0373       0.0716       0.0544       0.0482       0.0911       0.0696       0.0519      0.00054\n","     86     3      0.00685      0.00685     1.37e-06       0.0475        0.064       0.0363       0.0699       0.0531       0.0465       0.0894       0.0679       0.0707     0.000736\n","     86     4      0.00703      0.00703     1.53e-06       0.0479       0.0649       0.0367       0.0704       0.0535       0.0468       0.0908       0.0688       0.0786     0.000819\n","     86     5      0.00652      0.00652      9.5e-07       0.0469       0.0625       0.0367       0.0673        0.052       0.0473        0.085       0.0662       0.0479     0.000498\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              86  350.875    0.005      0.00694     2.31e-05      0.00696       0.0485       0.0644       0.0383        0.069       0.0536       0.0489       0.0876       0.0682        0.306      0.00318\n","! Validation         86  350.875    0.005      0.00696      1.5e-06      0.00697        0.048       0.0646        0.037       0.0701       0.0535       0.0476       0.0893       0.0684       0.0687     0.000715\n","Wall time: 350.87537264699995\n","! Best model       86    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     87     1      0.00604      0.00604     6.23e-07       0.0455       0.0601       0.0366       0.0634         0.05       0.0469       0.0803       0.0636       0.0541     0.000564\n","     87     2      0.00769      0.00769     5.61e-06         0.05       0.0678        0.038       0.0739       0.0559       0.0481       0.0958        0.072        0.171      0.00178\n","     87     3      0.00742      0.00741     1.42e-05       0.0498       0.0666       0.0399       0.0698       0.0548       0.0513       0.0897       0.0705        0.279       0.0029\n","     87     4      0.00907      0.00907     3.56e-06       0.0571       0.0737       0.0511       0.0693       0.0602        0.065       0.0885       0.0768        0.131      0.00137\n","     87     5      0.00731      0.00726     5.13e-05        0.051       0.0659       0.0423       0.0684       0.0554       0.0525       0.0867       0.0696        0.529      0.00551\n","     87     6      0.00672      0.00672     4.66e-06       0.0482       0.0634       0.0377       0.0692       0.0535       0.0469       0.0875       0.0672        0.146      0.00152\n","     87     7      0.00676      0.00667     8.22e-05       0.0483       0.0632       0.0374       0.0701       0.0538       0.0466       0.0874        0.067        0.664      0.00692\n","     87     8      0.00744      0.00742     1.81e-05       0.0513       0.0667       0.0402       0.0735       0.0568       0.0513       0.0898       0.0706        0.302      0.00314\n","     87     9      0.00756      0.00752     4.02e-05       0.0503       0.0671       0.0385       0.0738       0.0562       0.0489       0.0934       0.0711        0.471      0.00491\n","     87    10      0.00725      0.00724      1.1e-05       0.0498       0.0658       0.0393       0.0708        0.055         0.05       0.0894       0.0697        0.236      0.00245\n","     87    11      0.00792      0.00792     1.61e-06       0.0526       0.0689       0.0447       0.0685       0.0566       0.0567       0.0883       0.0725       0.0854     0.000889\n","     87    12      0.00741       0.0074     1.56e-05       0.0503       0.0665       0.0387       0.0735       0.0561       0.0489       0.0921       0.0705        0.293      0.00305\n","     87    13      0.00882      0.00881     9.78e-06       0.0542       0.0726         0.04       0.0827       0.0613       0.0507        0.103        0.077        0.227      0.00236\n","     87    14      0.00703        0.007      2.7e-05       0.0482       0.0647       0.0385       0.0676        0.053       0.0495       0.0876       0.0686        0.376      0.00391\n","     87    15      0.00739      0.00737     1.58e-05        0.051       0.0664       0.0408       0.0713       0.0561       0.0509       0.0898       0.0703        0.271      0.00282\n","     87    16      0.00603        0.006     3.09e-05       0.0449       0.0599        0.034       0.0665       0.0503       0.0432       0.0839       0.0635        0.407      0.00424\n","     87    17      0.00679      0.00678      1.4e-05       0.0482       0.0637       0.0393       0.0659       0.0526       0.0511       0.0833       0.0672        0.271      0.00282\n","     87    18      0.00749      0.00749     3.03e-06         0.05        0.067       0.0395       0.0711       0.0553       0.0518       0.0899       0.0709          0.1      0.00104\n","     87    19      0.00654       0.0065      4.4e-05       0.0463       0.0624       0.0361       0.0666       0.0513       0.0471       0.0851       0.0661        0.483      0.00503\n","     87    20      0.00623      0.00623     1.16e-06       0.0458       0.0611       0.0358       0.0658       0.0508       0.0459       0.0836       0.0647       0.0658     0.000686\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     87     1      0.00717      0.00717      2.5e-06        0.049       0.0655       0.0379       0.0712       0.0546       0.0488       0.0901       0.0694       0.0931     0.000969\n","     87     2      0.00718      0.00717      1.2e-06       0.0486       0.0655       0.0372       0.0715       0.0543        0.048       0.0909       0.0695       0.0551     0.000574\n","     87     3      0.00683      0.00683     1.41e-06       0.0474       0.0639       0.0362       0.0698        0.053       0.0463       0.0893       0.0678        0.071      0.00074\n","     87     4        0.007        0.007     1.56e-06       0.0478       0.0647       0.0366       0.0702       0.0534       0.0466       0.0907       0.0686       0.0789     0.000822\n","     87     5      0.00648      0.00648     9.63e-07       0.0467       0.0623       0.0365       0.0672       0.0518       0.0471       0.0848        0.066       0.0483     0.000504\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              87  354.923    0.005      0.00723     1.97e-05      0.00725       0.0496       0.0658       0.0394       0.0701       0.0547       0.0504       0.0889       0.0696        0.278       0.0029\n","! Validation         87  354.923    0.005      0.00693     1.53e-06      0.00693       0.0479       0.0644       0.0369         0.07       0.0534       0.0474       0.0892       0.0683       0.0693     0.000722\n","Wall time: 354.923964115\n","! Best model       87    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     88     1      0.00728      0.00724     3.98e-05       0.0496       0.0658       0.0389        0.071        0.055       0.0499       0.0896       0.0697        0.468      0.00487\n","     88     2      0.00762      0.00762     1.21e-06       0.0511       0.0675        0.041       0.0713       0.0561       0.0515       0.0916       0.0715       0.0688     0.000716\n","     88     3       0.0064      0.00639     8.99e-06       0.0474       0.0618       0.0381       0.0659        0.052       0.0485       0.0823       0.0654        0.214      0.00223\n","     88     4      0.00578      0.00577     1.28e-05        0.044       0.0588       0.0346       0.0628       0.0487       0.0441       0.0804       0.0623        0.263      0.00274\n","     88     5      0.00811      0.00811     4.15e-06       0.0537       0.0697       0.0452       0.0706       0.0579       0.0575       0.0891       0.0733        0.131      0.00136\n","     88     6      0.00816      0.00811     4.43e-05       0.0532       0.0697        0.044       0.0715       0.0578       0.0559       0.0912       0.0736        0.492      0.00512\n","     88     7      0.00663      0.00662     2.68e-06       0.0469        0.063       0.0348       0.0712        0.053       0.0437       0.0899       0.0668        0.107      0.00112\n","     88     8      0.00615      0.00613     2.76e-05       0.0456       0.0606       0.0367       0.0634         0.05       0.0464       0.0818       0.0641        0.386      0.00402\n","     88     9      0.00813      0.00813     1.29e-06       0.0527       0.0698       0.0407       0.0768       0.0587       0.0517       0.0962       0.0739        0.068     0.000708\n","     88    10      0.00857      0.00857     1.22e-06       0.0554       0.0716       0.0439       0.0784       0.0612       0.0538        0.098       0.0759       0.0771     0.000804\n","     88    11      0.00849      0.00847     1.33e-05       0.0556       0.0712       0.0476       0.0717       0.0597       0.0595       0.0902       0.0748        0.247      0.00257\n","     88    12      0.00655      0.00655     1.83e-06       0.0471       0.0626       0.0388       0.0637       0.0513       0.0501       0.0821       0.0661        0.085     0.000885\n","     88    13      0.00681       0.0068     8.49e-06       0.0476       0.0638       0.0358       0.0713       0.0535       0.0458       0.0895       0.0677        0.215      0.00224\n","     88    14      0.00701      0.00701     1.56e-06       0.0491       0.0648       0.0382       0.0711       0.0546       0.0483        0.089       0.0687        0.076     0.000791\n","     88    15      0.00923      0.00923     7.83e-07       0.0568       0.0743       0.0467       0.0772       0.0619       0.0583       0.0989       0.0786       0.0492     0.000513\n","     88    16      0.00967      0.00964     2.82e-05       0.0571        0.076       0.0462       0.0788       0.0625       0.0595        0.101       0.0803        0.385      0.00401\n","     88    17      0.00702      0.00697     4.75e-05       0.0499       0.0646       0.0422       0.0653       0.0538       0.0529       0.0832       0.0681        0.508      0.00529\n","     88    18      0.00604      0.00604     6.19e-06       0.0454       0.0601       0.0351       0.0658       0.0505       0.0444       0.0831       0.0637        0.178      0.00185\n","     88    19      0.00683      0.00681     2.55e-05        0.048       0.0638       0.0388       0.0665       0.0527       0.0503       0.0847       0.0675        0.373      0.00388\n","     88    20      0.00863      0.00863     1.94e-06       0.0558       0.0719       0.0475       0.0724         0.06       0.0597       0.0914       0.0756       0.0859     0.000895\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     88     1      0.00714      0.00714     2.46e-06       0.0489       0.0654       0.0378       0.0711       0.0544       0.0486       0.0899       0.0693       0.0922      0.00096\n","     88     2      0.00714      0.00714     1.13e-06       0.0485       0.0654        0.037       0.0714       0.0542       0.0479       0.0908       0.0693       0.0545     0.000568\n","     88     3      0.00679      0.00679     1.33e-06       0.0473       0.0638        0.036       0.0697       0.0529       0.0461       0.0891       0.0676       0.0681     0.000709\n","     88     4      0.00697      0.00697     1.56e-06       0.0477       0.0646       0.0364       0.0702       0.0533       0.0464       0.0906       0.0685       0.0793     0.000826\n","     88     5      0.00645      0.00645     9.52e-07       0.0466       0.0621       0.0364       0.0671       0.0517       0.0469       0.0847       0.0658       0.0481     0.000502\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              88  358.964    0.005      0.00744      1.4e-05      0.00746       0.0506       0.0667       0.0407       0.0703       0.0555       0.0519       0.0894       0.0706        0.224      0.00233\n","! Validation         88  358.964    0.005       0.0069     1.49e-06       0.0069       0.0478       0.0643       0.0367       0.0699       0.0533       0.0472       0.0891       0.0681       0.0684     0.000713\n","Wall time: 358.96472193599993\n","! Best model       88    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     89     1      0.00878      0.00878     8.88e-07       0.0561       0.0725       0.0477       0.0728       0.0603       0.0592       0.0936       0.0764       0.0613     0.000639\n","     89     2      0.00654      0.00654     4.15e-06       0.0469       0.0626       0.0369       0.0667       0.0518       0.0466        0.086       0.0663        0.148      0.00154\n","     89     3       0.0068       0.0068     8.32e-07       0.0479       0.0638       0.0377       0.0683        0.053       0.0483       0.0869       0.0676       0.0477     0.000496\n","     89     4      0.00537      0.00536     2.76e-06        0.043       0.0567       0.0336       0.0617       0.0477       0.0421        0.078       0.0601        0.102      0.00107\n","     89     5      0.00615      0.00614     4.82e-06       0.0461       0.0606       0.0359       0.0664       0.0511       0.0452       0.0833       0.0643        0.146      0.00153\n","     89     6      0.00642      0.00642     3.51e-06       0.0461        0.062       0.0353       0.0676       0.0514       0.0452       0.0863       0.0657        0.131      0.00136\n","     89     7      0.00692      0.00691     9.89e-06       0.0488       0.0643       0.0383         0.07       0.0541       0.0483       0.0879       0.0681        0.207      0.00215\n","     89     8      0.00758      0.00757     9.03e-07       0.0499       0.0673       0.0402       0.0693       0.0547       0.0519       0.0907       0.0713       0.0617     0.000643\n","     89     9      0.00656      0.00656      1.6e-06        0.048       0.0626       0.0392       0.0655       0.0524       0.0496       0.0827       0.0662        0.084     0.000875\n","     89    10      0.00732      0.00731     1.87e-05       0.0497       0.0661       0.0392       0.0706       0.0549       0.0498       0.0903       0.0701        0.321      0.00334\n","     89    11      0.00686      0.00686     1.37e-06       0.0483       0.0641       0.0368       0.0713        0.054       0.0468       0.0891       0.0679       0.0807      0.00084\n","     89    12       0.0082      0.00818     2.65e-05       0.0526         0.07       0.0425       0.0727       0.0576       0.0537       0.0944       0.0741        0.376      0.00392\n","     89    13      0.00715      0.00715     5.88e-07       0.0505       0.0654        0.042       0.0673       0.0547       0.0526       0.0855        0.069       0.0467     0.000486\n","     89    14        0.007        0.007     2.62e-06       0.0483       0.0647       0.0362       0.0725       0.0544       0.0466       0.0907       0.0686        0.101      0.00106\n","     89    15      0.00674      0.00672      1.8e-05       0.0479       0.0634       0.0379       0.0678       0.0529        0.049       0.0853       0.0671        0.302      0.00315\n","     89    16      0.00718      0.00717     3.38e-06       0.0494       0.0655       0.0398       0.0687       0.0542       0.0511       0.0876       0.0693        0.125       0.0013\n","     89    17      0.00713       0.0071     2.41e-05        0.048       0.0652       0.0351        0.074       0.0545       0.0445       0.0937       0.0691        0.362      0.00377\n","     89    18      0.00669      0.00669     6.54e-07       0.0476       0.0633       0.0365       0.0696       0.0531       0.0461       0.0881       0.0671       0.0508     0.000529\n","     89    19      0.00597      0.00596     4.18e-06       0.0445       0.0597        0.034       0.0655       0.0497       0.0436       0.0831       0.0634        0.114      0.00119\n","     89    20      0.00637      0.00637     5.41e-06       0.0463       0.0617       0.0358       0.0673       0.0516        0.046       0.0849       0.0654        0.155      0.00162\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     89     1      0.00711      0.00711     2.43e-06       0.0488       0.0652       0.0377        0.071       0.0543       0.0485       0.0898       0.0691       0.0911     0.000949\n","     89     2      0.00711      0.00711     1.11e-06       0.0484       0.0652       0.0369       0.0713       0.0541       0.0477       0.0906       0.0692        0.053     0.000552\n","     89     3      0.00677      0.00677     1.38e-06       0.0472       0.0636       0.0359       0.0697       0.0528        0.046        0.089       0.0675       0.0704     0.000733\n","     89     4      0.00694      0.00694     1.55e-06       0.0476       0.0644       0.0363       0.0701       0.0532       0.0463       0.0904       0.0684       0.0786     0.000819\n","     89     5      0.00642      0.00642     9.42e-07       0.0465        0.062       0.0363        0.067       0.0516       0.0468       0.0846       0.0657       0.0479     0.000499\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              89  363.005    0.005      0.00688     6.74e-06      0.00689       0.0483       0.0642        0.038       0.0688       0.0534       0.0485       0.0875        0.068        0.151      0.00157\n","! Validation         89  363.005    0.005      0.00687     1.48e-06      0.00687       0.0477       0.0641       0.0366       0.0698       0.0532       0.0471       0.0889        0.068       0.0682     0.000711\n","Wall time: 363.0059814939998\n","! Best model       89    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     90     1      0.00706      0.00705     6.99e-06       0.0477        0.065       0.0363       0.0705       0.0534       0.0475       0.0903       0.0689         0.19      0.00198\n","     90     2      0.00669      0.00669      1.1e-06       0.0483       0.0633       0.0374       0.0702       0.0538       0.0472       0.0869       0.0671       0.0666     0.000694\n","     90     3       0.0072       0.0072      4.6e-06       0.0491       0.0656       0.0387       0.0699       0.0543        0.049       0.0901       0.0696        0.108      0.00113\n","     90     4      0.00706      0.00706     2.08e-06       0.0487        0.065         0.04       0.0662       0.0531       0.0511       0.0863       0.0687        0.105      0.00109\n","     90     5       0.0066      0.00659     1.25e-05       0.0459       0.0628       0.0355       0.0668       0.0511       0.0462       0.0869       0.0666         0.26       0.0027\n","     90     6      0.00604      0.00604      1.4e-06       0.0457       0.0601       0.0346       0.0679       0.0513       0.0436       0.0839       0.0638       0.0619     0.000645\n","     90     7      0.00629      0.00628     1.69e-06       0.0462       0.0613       0.0364       0.0659       0.0512       0.0463       0.0836        0.065        0.093     0.000968\n","     90     8      0.00655      0.00655     5.19e-07       0.0477       0.0626       0.0367       0.0696       0.0532       0.0461       0.0866       0.0664        0.051     0.000531\n","     90     9      0.00603      0.00603     2.11e-06       0.0453       0.0601       0.0348       0.0664       0.0506        0.044       0.0834       0.0637        0.102      0.00106\n","     90    10      0.00628      0.00628     4.35e-06       0.0458       0.0613       0.0352       0.0671       0.0512       0.0448       0.0852        0.065        0.143      0.00149\n","     90    11      0.00754      0.00754     4.02e-06       0.0498       0.0672       0.0378       0.0738       0.0558       0.0484        0.094       0.0712        0.142      0.00148\n","     90    12      0.00751      0.00747     3.81e-05        0.051       0.0669       0.0419       0.0693       0.0556       0.0522       0.0892       0.0707        0.454      0.00473\n","     90    13      0.00748      0.00747      2.5e-06       0.0511       0.0669       0.0422       0.0689       0.0556       0.0535       0.0878       0.0706       0.0787      0.00082\n","     90    14      0.00677      0.00676     6.56e-06       0.0478       0.0636       0.0366         0.07       0.0533       0.0464       0.0885       0.0674        0.182      0.00189\n","     90    15       0.0069      0.00688     1.88e-05       0.0475       0.0642       0.0371       0.0682       0.0526       0.0482       0.0878        0.068         0.32      0.00333\n","     90    16      0.00574      0.00573        1e-06       0.0448       0.0586       0.0354       0.0634       0.0494        0.044       0.0802       0.0621       0.0615     0.000641\n","     90    17      0.00711      0.00706     4.89e-05        0.049        0.065       0.0389       0.0691        0.054       0.0501       0.0875       0.0688        0.512      0.00533\n","     90    18      0.00607      0.00606     1.08e-06       0.0445       0.0602       0.0338       0.0659       0.0498       0.0431       0.0847       0.0639       0.0668     0.000696\n","     90    19      0.00695      0.00691     3.59e-05       0.0491       0.0643       0.0391       0.0692       0.0542        0.049       0.0872       0.0681        0.444      0.00463\n","     90    20      0.00606      0.00605     4.83e-06       0.0447       0.0602       0.0336       0.0668       0.0502       0.0436        0.084       0.0638        0.156      0.00162\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     90     1      0.00708      0.00708     2.45e-06       0.0487       0.0651       0.0376       0.0709       0.0542       0.0483       0.0896        0.069       0.0912      0.00095\n","     90     2      0.00708      0.00707     1.09e-06       0.0483       0.0651       0.0368       0.0712        0.054       0.0475       0.0905        0.069       0.0528      0.00055\n","     90     3      0.00674      0.00674     1.34e-06       0.0471       0.0635       0.0358       0.0696       0.0527       0.0458       0.0889       0.0674       0.0705     0.000734\n","     90     4      0.00691      0.00691     1.54e-06       0.0475       0.0643       0.0362       0.0699       0.0531       0.0461       0.0903       0.0682       0.0784     0.000817\n","     90     5       0.0064       0.0064     9.56e-07       0.0464       0.0619       0.0361       0.0669       0.0515       0.0466       0.0845       0.0656       0.0473     0.000492\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              90  367.047    0.005      0.00669     9.95e-06       0.0067       0.0475       0.0633       0.0371       0.0683       0.0527       0.0473       0.0868        0.067         0.18      0.00187\n","! Validation         90  367.047    0.005      0.00684     1.48e-06      0.00684       0.0476        0.064       0.0365       0.0697       0.0531       0.0469       0.0888       0.0678        0.068     0.000709\n","Wall time: 367.0477038309998\n","! Best model       90    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     91     1      0.00793       0.0079     3.22e-05       0.0525       0.0688        0.044       0.0693       0.0567       0.0564       0.0884       0.0724        0.409      0.00426\n","     91     2      0.00643      0.00643     3.33e-06       0.0464        0.062       0.0356       0.0681       0.0518       0.0446        0.087       0.0658        0.122      0.00127\n","     91     3      0.00932      0.00928     3.85e-05        0.057       0.0745       0.0444       0.0823       0.0633       0.0553        0.103        0.079        0.452       0.0047\n","     91     4       0.0088      0.00879     1.22e-05        0.056       0.0725       0.0467       0.0747       0.0607       0.0592       0.0936       0.0764        0.249      0.00259\n","     91     5      0.00752      0.00752     2.33e-06       0.0525       0.0671       0.0463       0.0648       0.0556       0.0574       0.0831       0.0703       0.0912      0.00095\n","     91     6      0.00619      0.00619     1.08e-06       0.0457       0.0609       0.0353       0.0666       0.0509       0.0451       0.0839       0.0645       0.0754     0.000785\n","     91     7      0.00626      0.00624     1.47e-05       0.0462       0.0611       0.0366       0.0654        0.051       0.0464       0.0831       0.0647        0.267      0.00278\n","     91     8      0.00617      0.00617     6.54e-07       0.0455       0.0608       0.0335       0.0695       0.0515       0.0426       0.0863       0.0645       0.0389     0.000405\n","     91     9      0.00652      0.00651     1.01e-05       0.0468       0.0624        0.037       0.0663       0.0517       0.0469       0.0854       0.0661        0.231       0.0024\n","     91    10      0.00646      0.00645     5.24e-06       0.0465       0.0621       0.0357       0.0681       0.0519       0.0449       0.0869       0.0659        0.169      0.00176\n","     91    11      0.00759      0.00759     7.42e-06       0.0498       0.0674       0.0371       0.0752       0.0561       0.0479       0.0951       0.0715          0.2      0.00208\n","     91    12      0.00641      0.00641     2.95e-06       0.0465       0.0619       0.0368       0.0659       0.0513        0.046       0.0853       0.0656        0.111      0.00115\n","     91    13      0.00765      0.00765     2.31e-06        0.052       0.0677       0.0444       0.0673       0.0558        0.056       0.0864       0.0712        0.108      0.00113\n","     91    14      0.00623      0.00622     6.97e-06       0.0464        0.061        0.037       0.0652       0.0511       0.0473       0.0818       0.0646        0.186      0.00194\n","     91    15      0.00606      0.00606     1.29e-06        0.045       0.0602       0.0341       0.0667       0.0504       0.0432       0.0845       0.0639       0.0717     0.000747\n","     91    16       0.0068      0.00677     2.39e-05       0.0478       0.0637        0.038       0.0673       0.0527        0.048       0.0869       0.0675        0.358      0.00373\n","     91    17      0.00779      0.00777     1.61e-05       0.0504       0.0682       0.0387       0.0737       0.0562        0.051       0.0936       0.0723        0.295      0.00307\n","     91    18      0.00638      0.00637      1.5e-05       0.0444       0.0617       0.0335       0.0662       0.0498       0.0435       0.0875       0.0655        0.285      0.00297\n","     91    19      0.00674      0.00673     1.01e-05       0.0476       0.0635       0.0357       0.0714       0.0535       0.0449       0.0897       0.0673         0.22      0.00229\n","     91    20       0.0086      0.00854      5.7e-05       0.0552       0.0715       0.0457       0.0744         0.06       0.0576       0.0933       0.0754        0.558      0.00581\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     91     1      0.00705      0.00705     2.51e-06       0.0486        0.065       0.0374       0.0708       0.0541       0.0482       0.0895       0.0689       0.0927     0.000965\n","     91     2      0.00706      0.00706      1.1e-06       0.0482        0.065       0.0367       0.0711       0.0539       0.0474       0.0904       0.0689       0.0523     0.000545\n","     91     3      0.00672      0.00672     1.36e-06        0.047       0.0634       0.0357       0.0695       0.0526       0.0457       0.0888       0.0672       0.0704     0.000733\n","     91     4      0.00688      0.00688      1.5e-06       0.0473       0.0642       0.0361       0.0698        0.053        0.046       0.0901       0.0681        0.079     0.000823\n","     91     5      0.00637      0.00637      9.2e-07       0.0463       0.0617        0.036       0.0668       0.0514       0.0464       0.0844       0.0654        0.046     0.000479\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              91  371.080    0.005      0.00708     1.32e-05      0.00709        0.049       0.0651       0.0388       0.0694       0.0541       0.0495       0.0884       0.0689        0.225      0.00234\n","! Validation         91  371.080    0.005      0.00681     1.48e-06      0.00682       0.0475       0.0639       0.0364       0.0696        0.053       0.0468       0.0887       0.0677       0.0681     0.000709\n","Wall time: 371.0804419719998\n","! Best model       91    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     92     1      0.00935      0.00935     4.07e-06       0.0587       0.0748       0.0529       0.0702       0.0616       0.0652        0.091       0.0781        0.135      0.00141\n","     92     2      0.00671      0.00666     4.63e-05       0.0478       0.0631       0.0387        0.066       0.0523       0.0485       0.0852       0.0668        0.502      0.00523\n","     92     3      0.00622      0.00621      1.1e-05       0.0466        0.061        0.038       0.0638       0.0509       0.0475       0.0815       0.0645        0.229      0.00239\n","     92     4      0.00851      0.00847      4.2e-05       0.0549       0.0712       0.0454        0.074       0.0597       0.0573       0.0929       0.0751        0.479      0.00499\n","     92     5      0.00964      0.00949     0.000145       0.0579       0.0754       0.0469       0.0799       0.0634       0.0588        0.101       0.0797        0.891      0.00928\n","     92     6      0.00868      0.00862     5.95e-05       0.0544       0.0718       0.0438       0.0755       0.0596       0.0547       0.0975       0.0761        0.571      0.00595\n","     92     7      0.00644      0.00634       0.0001       0.0466       0.0616       0.0373       0.0652       0.0513       0.0475       0.0828       0.0652         0.74      0.00771\n","     92     8      0.00691       0.0069     2.95e-06       0.0482       0.0643       0.0376       0.0694       0.0535       0.0471       0.0893       0.0682        0.124      0.00129\n","     92     9      0.00649      0.00647     1.77e-05       0.0479       0.0622       0.0401       0.0636       0.0518       0.0506       0.0805       0.0656        0.308       0.0032\n","     92    10      0.00748      0.00746     1.99e-05       0.0509       0.0668       0.0434        0.066       0.0547       0.0549       0.0858       0.0703        0.329      0.00343\n","     92    11      0.00629      0.00628     1.05e-05       0.0458       0.0613       0.0365       0.0642       0.0504       0.0461       0.0838        0.065        0.232      0.00241\n","     92    12      0.00657      0.00656     7.89e-06       0.0482       0.0627       0.0388        0.067       0.0529       0.0491       0.0834       0.0663        0.182       0.0019\n","     92    13       0.0069       0.0069     6.21e-07       0.0477       0.0643       0.0371       0.0688        0.053       0.0483       0.0879       0.0681        0.051     0.000531\n","     92    14      0.00724      0.00724     3.86e-06       0.0496       0.0658       0.0389        0.071        0.055       0.0494       0.0901       0.0697        0.143      0.00149\n","     92    15      0.00587      0.00587     1.11e-06       0.0446       0.0593       0.0337       0.0664       0.0501       0.0428        0.083       0.0629       0.0639     0.000665\n","     92    16      0.00742      0.00742     1.44e-06       0.0508       0.0667       0.0393       0.0737       0.0565       0.0492       0.0921       0.0707       0.0701      0.00073\n","     92    17      0.00683      0.00682     1.66e-05       0.0478       0.0639       0.0355       0.0725        0.054       0.0453       0.0902       0.0677        0.298       0.0031\n","     92    18      0.00781      0.00781     3.04e-06       0.0527       0.0684       0.0432       0.0716       0.0574       0.0548       0.0896       0.0722        0.109      0.00114\n","     92    19      0.00669      0.00667     1.58e-05       0.0475       0.0632       0.0371       0.0683       0.0527       0.0467       0.0873        0.067        0.286      0.00297\n","     92    20      0.00708      0.00708     4.87e-06       0.0498       0.0651       0.0397       0.0702       0.0549       0.0497       0.0881       0.0689        0.155      0.00162\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     92     1      0.00703      0.00703     2.47e-06       0.0485       0.0649       0.0373       0.0707        0.054       0.0481       0.0894       0.0688       0.0918     0.000956\n","     92     2      0.00703      0.00703     1.08e-06       0.0481       0.0648       0.0366        0.071       0.0538       0.0473       0.0903       0.0688       0.0517     0.000538\n","     92     3      0.00669      0.00669     1.36e-06       0.0469       0.0633       0.0356       0.0695       0.0525       0.0455       0.0887       0.0671       0.0704     0.000733\n","     92     4      0.00685      0.00685     1.58e-06       0.0473        0.064        0.036       0.0698       0.0529       0.0458         0.09       0.0679       0.0807      0.00084\n","     92     5      0.00634      0.00634     9.29e-07       0.0462       0.0616       0.0359       0.0667       0.0513       0.0463       0.0843       0.0653       0.0475     0.000494\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              92  375.119    0.005      0.00723     2.57e-05      0.00726       0.0499       0.0658       0.0402       0.0694       0.0548       0.0509       0.0883       0.0696        0.295      0.00307\n","! Validation         92  375.119    0.005      0.00679     1.48e-06      0.00679       0.0474       0.0637       0.0363       0.0695       0.0529       0.0466       0.0886       0.0676       0.0684     0.000712\n","Wall time: 375.12033861199984\n","! Best model       92    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     93     1      0.00661       0.0066      1.2e-05       0.0469       0.0628       0.0361       0.0684       0.0523       0.0464       0.0869       0.0666        0.245      0.00255\n","     93     2      0.00794      0.00794     7.62e-07        0.055       0.0689       0.0498       0.0654       0.0576        0.061       0.0825       0.0718       0.0516     0.000537\n","     93     3      0.00818      0.00817      5.4e-06       0.0536       0.0699       0.0445       0.0718       0.0581        0.057       0.0904       0.0737         0.17      0.00177\n","     93     4      0.00754      0.00754     3.72e-06       0.0504       0.0672       0.0389       0.0732       0.0561       0.0489       0.0936       0.0712        0.141      0.00146\n","     93     5      0.00645      0.00642     2.84e-05       0.0459        0.062        0.034       0.0697       0.0519       0.0436       0.0879       0.0658        0.389      0.00405\n","     93     6      0.00696      0.00693     3.22e-05       0.0482       0.0644       0.0367        0.071       0.0539       0.0465         0.09       0.0683        0.418      0.00435\n","     93     7      0.00754      0.00744     9.44e-05       0.0509       0.0667       0.0421       0.0685       0.0553       0.0538        0.087       0.0704        0.719      0.00749\n","     93     8      0.00716      0.00713     2.03e-05       0.0487       0.0653       0.0396       0.0669       0.0533       0.0504       0.0879       0.0692        0.325      0.00338\n","     93     9      0.00676      0.00673     2.72e-05       0.0478       0.0635       0.0363        0.071       0.0536       0.0461       0.0886       0.0673        0.374      0.00389\n","     93    10      0.00718      0.00714     4.88e-05       0.0491       0.0654       0.0407       0.0658       0.0533       0.0527       0.0852        0.069        0.514      0.00535\n","     93    11      0.00746      0.00746     1.44e-06       0.0506       0.0668       0.0394       0.0729       0.0562       0.0498       0.0918       0.0708       0.0678     0.000706\n","     93    12      0.00747      0.00733     0.000134       0.0499       0.0662       0.0386       0.0724       0.0555        0.048       0.0925       0.0702        0.857      0.00893\n","     93    13      0.00574      0.00574     7.12e-06       0.0443       0.0586       0.0344       0.0641       0.0493       0.0438       0.0804       0.0621        0.189      0.00197\n","     93    14      0.00761      0.00755     6.17e-05       0.0505       0.0672       0.0394       0.0728       0.0561       0.0504       0.0921       0.0712        0.581      0.00605\n","     93    15      0.00726      0.00726     2.79e-06       0.0491       0.0659       0.0385       0.0704       0.0545       0.0493       0.0904       0.0698        0.119      0.00124\n","     93    16      0.00685      0.00684     8.46e-06       0.0486        0.064         0.04       0.0657       0.0528       0.0505       0.0847       0.0676        0.208      0.00216\n","     93    17      0.00544      0.00544     5.34e-06        0.043        0.057       0.0344       0.0602       0.0473       0.0443       0.0764       0.0603        0.129      0.00134\n","     93    18      0.00741      0.00741     5.32e-06       0.0515       0.0666       0.0411       0.0723       0.0567       0.0512       0.0897       0.0705        0.157      0.00164\n","     93    19       0.0104       0.0103     2.02e-05       0.0597       0.0787       0.0484       0.0823       0.0654       0.0615        0.105       0.0832        0.329      0.00343\n","     93    20      0.00948      0.00936     0.000112        0.058       0.0749       0.0481       0.0779        0.063         0.06        0.098        0.079        0.783      0.00816\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     93     1        0.007        0.007     2.46e-06       0.0484       0.0647       0.0372       0.0706       0.0539       0.0479       0.0893       0.0686       0.0943     0.000983\n","     93     2        0.007        0.007     9.56e-07        0.048       0.0647       0.0365        0.071       0.0537       0.0471       0.0902       0.0686       0.0463     0.000482\n","     93     3      0.00667      0.00667     1.36e-06       0.0468       0.0632       0.0355       0.0694       0.0524       0.0454       0.0886        0.067       0.0716     0.000746\n","     93     4      0.00683      0.00682     1.62e-06       0.0471       0.0639       0.0359       0.0697       0.0528       0.0457       0.0899       0.0678       0.0823     0.000858\n","     93     5      0.00633      0.00632     8.94e-07       0.0461       0.0615       0.0358       0.0666       0.0512       0.0461       0.0843       0.0652       0.0463     0.000482\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              93  379.155    0.005      0.00734     3.16e-05      0.00737       0.0501       0.0663       0.0401       0.0701       0.0551       0.0511       0.0892       0.0701        0.338      0.00352\n","! Validation         93  379.155    0.005      0.00676     1.46e-06      0.00676       0.0473       0.0636       0.0362       0.0695       0.0528       0.0465       0.0885       0.0675       0.0682      0.00071\n","Wall time: 379.1553809059999\n","! Best model       93    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     94     1        0.007      0.00698     2.27e-05       0.0488       0.0646       0.0389       0.0685       0.0537       0.0496       0.0872       0.0684        0.346       0.0036\n","     94     2      0.00692      0.00688     4.13e-05       0.0478       0.0642       0.0364       0.0706       0.0535       0.0471        0.089        0.068        0.475      0.00495\n","     94     3      0.00743      0.00738     4.38e-05       0.0509       0.0665        0.041       0.0708       0.0559       0.0512       0.0895       0.0704        0.489       0.0051\n","     94     4      0.00824      0.00823     2.52e-06       0.0556       0.0702       0.0482       0.0705       0.0593       0.0591       0.0883       0.0737       0.0877     0.000913\n","     94     5      0.00853      0.00847      5.9e-05       0.0548       0.0712       0.0449       0.0746       0.0598       0.0564        0.094       0.0752        0.565      0.00588\n","     94     6      0.00649      0.00649     4.53e-07        0.046       0.0623       0.0359        0.066        0.051        0.046       0.0861       0.0661       0.0445     0.000464\n","     94     7      0.00658      0.00656     1.31e-05       0.0471       0.0627       0.0365       0.0683       0.0524       0.0473       0.0855       0.0664         0.26      0.00271\n","     94     8      0.00833      0.00832     5.31e-06       0.0539       0.0706       0.0426       0.0764       0.0595       0.0529       0.0967       0.0748        0.164      0.00171\n","     94     9      0.00884      0.00883     1.27e-05       0.0558       0.0727       0.0462       0.0749       0.0606       0.0582       0.0953       0.0768        0.248      0.00259\n","     94    10      0.00694      0.00694      7.2e-06       0.0485       0.0644       0.0394       0.0668       0.0531       0.0508       0.0854       0.0681        0.196      0.00204\n","     94    11      0.00647      0.00647     1.24e-06        0.047       0.0622       0.0354         0.07       0.0527       0.0447       0.0873        0.066       0.0539     0.000562\n","     94    12      0.00758      0.00757     1.71e-05       0.0511       0.0673       0.0434       0.0667        0.055       0.0554       0.0863       0.0708        0.283      0.00294\n","     94    13      0.00766      0.00765     7.42e-06       0.0518       0.0677       0.0417       0.0721       0.0569       0.0527       0.0904       0.0716        0.168      0.00175\n","     94    14       0.0057      0.00569     7.17e-06       0.0433       0.0584        0.033        0.064       0.0485       0.0419       0.0819       0.0619        0.183      0.00191\n","     94    15      0.00668      0.00668     1.02e-06       0.0484       0.0632       0.0394       0.0665       0.0529       0.0495       0.0842       0.0669       0.0701      0.00073\n","     94    16      0.00795      0.00795     8.28e-07       0.0521        0.069       0.0423       0.0717        0.057       0.0537       0.0922        0.073       0.0557      0.00058\n","     94    17      0.00754      0.00751     2.92e-05       0.0494        0.067       0.0374       0.0734       0.0554       0.0483       0.0938       0.0711        0.398      0.00415\n","     94    18      0.00615      0.00614     1.23e-06       0.0456       0.0606       0.0348       0.0672        0.051       0.0438       0.0849       0.0643       0.0631     0.000657\n","     94    19      0.00587      0.00586     9.73e-06       0.0451       0.0592        0.036       0.0633       0.0496       0.0451       0.0803       0.0627        0.227      0.00237\n","     94    20      0.00618      0.00618     1.44e-06       0.0456       0.0608       0.0354       0.0659       0.0507       0.0454       0.0834       0.0644       0.0814     0.000848\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     94     1      0.00698      0.00698     2.41e-06       0.0483       0.0646       0.0371       0.0705       0.0538       0.0478       0.0892       0.0685       0.0909     0.000947\n","     94     2      0.00698      0.00698     1.06e-06       0.0479       0.0646       0.0364       0.0709       0.0536        0.047       0.0901       0.0685       0.0513     0.000534\n","     94     3      0.00665      0.00665     1.35e-06       0.0467       0.0631       0.0354       0.0693       0.0524       0.0453       0.0885       0.0669       0.0695     0.000724\n","     94     4       0.0068       0.0068     1.56e-06        0.047       0.0638       0.0358       0.0696       0.0527       0.0456       0.0898       0.0677       0.0793     0.000826\n","     94     5       0.0063       0.0063     9.24e-07        0.046       0.0614       0.0357       0.0665       0.0511        0.046       0.0842       0.0651        0.046     0.000479\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              94  383.178    0.005      0.00714     1.42e-05      0.00715       0.0494       0.0654       0.0394       0.0694       0.0544       0.0502       0.0882       0.0692        0.223      0.00232\n","! Validation         94  383.178    0.005      0.00674     1.46e-06      0.00674       0.0472       0.0635       0.0361       0.0694       0.0527       0.0463       0.0884       0.0673       0.0674     0.000702\n","Wall time: 383.17898488499986\n","! Best model       94    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     95     1       0.0064       0.0064     7.19e-07       0.0464       0.0619       0.0357       0.0678       0.0517       0.0456       0.0857       0.0656        0.051     0.000531\n","     95     2      0.00669      0.00668     1.16e-05       0.0487       0.0632       0.0392       0.0676       0.0534       0.0491       0.0848       0.0669        0.217      0.00226\n","     95     3      0.00605      0.00605     4.54e-07       0.0458       0.0602       0.0372       0.0628         0.05       0.0467       0.0806       0.0636       0.0461      0.00048\n","     95     4      0.00691      0.00689     2.14e-05       0.0475       0.0642       0.0363       0.0699       0.0531       0.0473       0.0888       0.0681        0.343      0.00358\n","     95     5      0.00608      0.00604     3.42e-05       0.0451       0.0601       0.0348       0.0657       0.0503       0.0439       0.0836       0.0638        0.431      0.00449\n","     95     6      0.00739      0.00739     1.39e-06        0.049       0.0665       0.0363       0.0745       0.0554       0.0468       0.0943       0.0705       0.0744     0.000775\n","     95     7      0.00721      0.00705     0.000165       0.0497       0.0649       0.0407       0.0677       0.0542       0.0515       0.0857       0.0686        0.952      0.00992\n","     95     8      0.00657      0.00656     6.26e-06       0.0477       0.0627       0.0373       0.0685       0.0529       0.0473       0.0855       0.0664        0.183      0.00191\n","     95     9      0.00672      0.00662     9.26e-05       0.0467        0.063       0.0352       0.0696       0.0524       0.0445        0.089       0.0668        0.714      0.00744\n","     95    10      0.00832       0.0083     2.11e-05       0.0546       0.0705       0.0481       0.0678       0.0579       0.0618       0.0853       0.0735        0.317       0.0033\n","     95    11      0.00795      0.00794     8.92e-06       0.0528       0.0689        0.042       0.0744       0.0582       0.0529        0.093        0.073        0.219      0.00228\n","     95    12      0.00583       0.0058     2.97e-05       0.0443       0.0589       0.0334       0.0662       0.0498       0.0421        0.083       0.0625        0.394       0.0041\n","     95    13      0.00613      0.00611     2.06e-05       0.0452       0.0605       0.0365       0.0628       0.0496       0.0468       0.0812        0.064        0.331      0.00345\n","     95    14      0.00782      0.00776     5.77e-05       0.0505       0.0681       0.0378       0.0758       0.0568       0.0488       0.0958       0.0723        0.561      0.00585\n","     95    15      0.00695       0.0069     5.57e-05       0.0487       0.0643        0.038       0.0699        0.054       0.0485       0.0877       0.0681        0.553      0.00576\n","     95    16      0.00765      0.00756     8.73e-05       0.0507       0.0673         0.04       0.0721       0.0561       0.0513       0.0912       0.0713        0.692       0.0072\n","     95    17      0.00608      0.00607     7.57e-06       0.0447       0.0603       0.0345       0.0653       0.0499       0.0445       0.0832       0.0639          0.2      0.00208\n","     95    18      0.00704      0.00695     8.67e-05        0.049       0.0645       0.0397       0.0678       0.0537       0.0505        0.086       0.0682        0.687      0.00715\n","     95    19      0.00706      0.00704     2.56e-05       0.0473       0.0649       0.0361       0.0696       0.0528       0.0466       0.0911       0.0688        0.371      0.00386\n","     95    20      0.00626       0.0062     5.97e-05       0.0457       0.0609       0.0353       0.0666       0.0509       0.0452        0.084       0.0646         0.57      0.00594\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     95     1      0.00696      0.00695     2.48e-06       0.0482       0.0645       0.0371       0.0704       0.0537       0.0477       0.0891       0.0684       0.0931     0.000969\n","     95     2      0.00696      0.00695     9.76e-07       0.0478       0.0645       0.0363       0.0708       0.0536       0.0469       0.0899       0.0684       0.0484     0.000505\n","     95     3      0.00662      0.00662     1.31e-06       0.0466        0.063       0.0353       0.0693       0.0523       0.0451       0.0884       0.0668       0.0694     0.000723\n","     95     4      0.00678      0.00678     1.57e-06       0.0469       0.0637       0.0357       0.0695       0.0526       0.0454       0.0897       0.0676       0.0805     0.000838\n","     95     5      0.00628      0.00628     9.17e-07       0.0459       0.0613       0.0356       0.0665        0.051       0.0458       0.0841        0.065       0.0466     0.000485\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              95  387.204    0.005      0.00682     3.97e-05      0.00685        0.048       0.0639       0.0377       0.0686       0.0532       0.0483       0.0871       0.0677        0.395      0.00412\n","! Validation         95  387.204    0.005      0.00672     1.45e-06      0.00672       0.0471       0.0634        0.036       0.0693       0.0526       0.0462       0.0883       0.0672       0.0676     0.000704\n","Wall time: 387.20470428\n","! Best model       95    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     96     1      0.00695      0.00688     7.22e-05       0.0489       0.0642       0.0385       0.0695        0.054        0.048        0.088        0.068        0.628      0.00654\n","     96     2      0.00869      0.00865     4.51e-05       0.0536       0.0719       0.0432       0.0744       0.0588       0.0547       0.0977       0.0762        0.496      0.00517\n","     96     3      0.00839      0.00839     3.68e-06       0.0552       0.0709       0.0454       0.0748       0.0601       0.0567       0.0928       0.0748         0.14      0.00146\n","     96     4      0.00668      0.00668     5.68e-07       0.0473       0.0632       0.0374       0.0673       0.0523       0.0474       0.0866        0.067       0.0488     0.000509\n","     96     5      0.00642      0.00642     1.74e-06       0.0456        0.062       0.0354       0.0661       0.0508        0.046       0.0854       0.0657       0.0871     0.000907\n","     96     6      0.00746      0.00745     7.07e-06       0.0512       0.0668        0.042       0.0695       0.0557       0.0542       0.0867       0.0704        0.188      0.00196\n","     96     7       0.0082      0.00818     2.17e-05       0.0525         0.07        0.042       0.0734       0.0577       0.0538       0.0944       0.0741        0.344      0.00358\n","     96     8      0.00762      0.00762     1.61e-06       0.0508       0.0675       0.0415       0.0692       0.0554       0.0526       0.0903       0.0714       0.0881     0.000918\n","     96     9      0.00662      0.00662     2.05e-06       0.0474       0.0629       0.0386        0.065       0.0518       0.0494       0.0837       0.0665       0.0959     0.000999\n","     96    10      0.00628      0.00628     3.49e-06       0.0449       0.0613       0.0333       0.0682       0.0508       0.0428       0.0872        0.065        0.128      0.00133\n","     96    11      0.00683      0.00683     6.05e-06       0.0483       0.0639       0.0382       0.0684       0.0533       0.0489       0.0864       0.0677        0.168      0.00175\n","     96    12      0.00544      0.00544     5.65e-06        0.043        0.057       0.0334       0.0622       0.0478       0.0422       0.0787       0.0605        0.152      0.00158\n","     96    13      0.00599      0.00598      9.8e-06       0.0451       0.0598       0.0353       0.0648       0.0501       0.0441       0.0828       0.0634        0.211       0.0022\n","     96    14      0.00587      0.00586     1.52e-05       0.0435       0.0592       0.0321       0.0663       0.0492       0.0407       0.0849       0.0628        0.288        0.003\n","     96    15      0.00606      0.00605        1e-05       0.0452       0.0602       0.0342       0.0672       0.0507       0.0433       0.0844       0.0638        0.221       0.0023\n","     96    16      0.00526      0.00521     4.94e-05       0.0418       0.0559        0.033       0.0594       0.0462        0.042       0.0764       0.0592         0.52      0.00541\n","     96    17      0.00639      0.00639     1.55e-06       0.0466       0.0618       0.0371       0.0657       0.0514       0.0477       0.0832       0.0654        0.084     0.000875\n","     96    18      0.00582      0.00575     6.25e-05       0.0444       0.0587       0.0339       0.0654       0.0496       0.0428       0.0817       0.0622        0.585       0.0061\n","     96    19      0.00573      0.00572     1.19e-05       0.0439       0.0585       0.0337       0.0644        0.049       0.0437       0.0803        0.062        0.252      0.00263\n","     96    20       0.0064      0.00636     3.93e-05       0.0463       0.0617       0.0346       0.0697       0.0522       0.0436       0.0873       0.0655        0.457      0.00476\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     96     1      0.00693      0.00693     2.51e-06       0.0481       0.0644        0.037       0.0703       0.0536       0.0476        0.089       0.0683       0.0935     0.000974\n","     96     2      0.00693      0.00693     1.01e-06       0.0477       0.0644       0.0362       0.0707       0.0535       0.0468       0.0898       0.0683       0.0481     0.000502\n","     96     3       0.0066       0.0066     1.38e-06       0.0465       0.0629       0.0352       0.0692       0.0522        0.045       0.0883       0.0667        0.072      0.00075\n","     96     4      0.00676      0.00675     1.56e-06       0.0469       0.0636       0.0356       0.0694       0.0525       0.0453       0.0896       0.0674       0.0801     0.000834\n","     96     5      0.00626      0.00626     8.72e-07       0.0458       0.0612       0.0355       0.0664        0.051       0.0457        0.084       0.0649       0.0465     0.000484\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              96  391.272    0.005      0.00664     1.85e-05      0.00666       0.0473        0.063       0.0371       0.0676       0.0523       0.0475       0.0861       0.0668        0.259       0.0027\n","! Validation         96  391.272    0.005      0.00669     1.47e-06       0.0067        0.047       0.0633       0.0359       0.0692       0.0526       0.0461       0.0882       0.0671        0.068     0.000709\n","Wall time: 391.27328334899994\n","! Best model       96    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     97     1      0.00652       0.0065     1.67e-05       0.0463       0.0624       0.0354       0.0682       0.0518       0.0454       0.0869       0.0661        0.299      0.00311\n","     97     2       0.0075      0.00748     2.33e-05       0.0509       0.0669       0.0415       0.0697       0.0556       0.0527       0.0887       0.0707        0.346      0.00361\n","     97     3      0.00628      0.00627     5.47e-06       0.0464       0.0613       0.0372       0.0649       0.0511       0.0471       0.0826       0.0649        0.169      0.00176\n","     97     4      0.00636      0.00635     1.19e-05       0.0449       0.0617       0.0339       0.0668       0.0504       0.0434       0.0874       0.0654        0.254      0.00264\n","     97     5       0.0065      0.00647     3.19e-05       0.0466       0.0622       0.0365        0.067       0.0517       0.0473       0.0846       0.0659        0.413       0.0043\n","     97     6      0.00587      0.00585     1.91e-05        0.045       0.0592       0.0351        0.065         0.05       0.0438       0.0817       0.0627        0.314      0.00327\n","     97     7      0.00649      0.00639     9.48e-05        0.045       0.0619       0.0342       0.0666       0.0504       0.0443       0.0869       0.0656        0.718      0.00748\n","     97     8      0.00609      0.00608     1.17e-05       0.0453       0.0603       0.0354       0.0652       0.0503       0.0453       0.0825       0.0639        0.252      0.00262\n","     97     9      0.00645      0.00633     0.000123       0.0461       0.0616       0.0366       0.0653       0.0509       0.0466       0.0838       0.0652        0.822      0.00857\n","     97    10      0.00687      0.00686     2.26e-06       0.0479       0.0641       0.0357       0.0723        0.054        0.046         0.09        0.068       0.0918     0.000956\n","     97    11      0.00775      0.00772     3.63e-05       0.0522        0.068       0.0431       0.0705       0.0568       0.0549       0.0885       0.0717        0.439      0.00457\n","     97    12      0.00757      0.00749     7.87e-05        0.052        0.067       0.0437       0.0687       0.0562        0.055       0.0861       0.0705        0.656      0.00683\n","     97    13      0.00708      0.00707     1.49e-05       0.0483        0.065       0.0388       0.0673        0.053       0.0506        0.087       0.0688        0.281      0.00292\n","     97    14      0.00552      0.00551     1.32e-05       0.0426       0.0574        0.033       0.0617       0.0474       0.0419       0.0799       0.0609        0.265      0.00276\n","     97    15      0.00745      0.00745     1.12e-06       0.0484       0.0668       0.0362       0.0727       0.0544       0.0475       0.0941       0.0708       0.0613     0.000639\n","     97    16       0.0078       0.0078     7.66e-06       0.0525       0.0683       0.0416       0.0742       0.0579       0.0516       0.0931       0.0724        0.199      0.00207\n","     97    17      0.00846      0.00845     8.18e-06       0.0547       0.0711       0.0463       0.0714       0.0589       0.0584       0.0914       0.0749        0.193      0.00201\n","     97    18      0.00746      0.00745     1.54e-05       0.0498       0.0668       0.0387       0.0722       0.0554       0.0501       0.0914       0.0707        0.289      0.00301\n","     97    19      0.00597      0.00597     1.37e-06       0.0456       0.0598       0.0365       0.0638       0.0501       0.0463       0.0802       0.0632       0.0822     0.000857\n","     97    20      0.00631      0.00631     2.78e-06       0.0461       0.0614       0.0367       0.0649       0.0508       0.0466       0.0835       0.0651        0.114      0.00119\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     97     1      0.00691      0.00691     2.46e-06        0.048       0.0643       0.0369       0.0702       0.0535       0.0474       0.0889       0.0682       0.0928     0.000966\n","     97     2      0.00691       0.0069     1.02e-06       0.0476       0.0643       0.0361       0.0707       0.0534       0.0466       0.0897       0.0682       0.0489      0.00051\n","     97     3      0.00658      0.00658     1.32e-06       0.0465       0.0628       0.0351       0.0692       0.0521       0.0449       0.0882       0.0666       0.0706     0.000735\n","     97     4      0.00674      0.00673     1.62e-06       0.0468       0.0635       0.0355       0.0694       0.0524       0.0452       0.0895       0.0673       0.0816      0.00085\n","     97     5      0.00624      0.00624     8.95e-07       0.0457       0.0611       0.0354       0.0663       0.0509       0.0456       0.0839       0.0648       0.0456     0.000475\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              97  395.448    0.005      0.00679      2.6e-05      0.00682       0.0478       0.0637       0.0378       0.0679       0.0529       0.0484       0.0866       0.0675        0.313      0.00326\n","! Validation         97  395.448    0.005      0.00667     1.46e-06      0.00667       0.0469       0.0632       0.0358       0.0692       0.0525        0.046       0.0881        0.067       0.0679     0.000707\n","Wall time: 395.4489410219999\n","! Best model       97    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     98     1        0.007      0.00697     2.86e-05        0.049       0.0646       0.0384       0.0701       0.0542       0.0484       0.0885       0.0684        0.389      0.00405\n","     98     2        0.007      0.00699     4.11e-06       0.0492       0.0647       0.0379       0.0719       0.0549        0.048       0.0891       0.0686        0.112      0.00117\n","     98     3      0.00582      0.00581     1.43e-05       0.0432        0.059       0.0319       0.0657       0.0488       0.0402       0.0848       0.0625        0.277      0.00288\n","     98     4      0.00726      0.00726      4.7e-07       0.0483       0.0659       0.0356       0.0736       0.0546       0.0461       0.0937       0.0699       0.0436     0.000454\n","     98     5      0.00698      0.00696     2.36e-05       0.0475       0.0645        0.036       0.0705       0.0532       0.0461       0.0908       0.0685         0.36      0.00375\n","     98     6      0.00578      0.00578     3.74e-06       0.0442       0.0588       0.0327       0.0671       0.0499       0.0415       0.0833       0.0624        0.125       0.0013\n","     98     7      0.00624      0.00623     1.62e-05       0.0455        0.061       0.0348       0.0668       0.0508       0.0448       0.0847       0.0647        0.296      0.00309\n","     98     8      0.00577      0.00577     2.89e-06       0.0443       0.0588       0.0349        0.063        0.049       0.0445         0.08       0.0622        0.113      0.00118\n","     98     9      0.00686      0.00686      2.7e-06       0.0484       0.0641       0.0386       0.0681       0.0534       0.0487       0.0869       0.0678        0.109      0.00114\n","     98    10      0.00717      0.00713     3.73e-05       0.0491       0.0653       0.0387         0.07       0.0543       0.0499       0.0885       0.0692         0.45      0.00469\n","     98    11      0.00718      0.00717      6.7e-06       0.0491       0.0655       0.0375       0.0724        0.055       0.0481       0.0909       0.0695        0.186      0.00194\n","     98    12      0.00634      0.00634     6.14e-06       0.0467       0.0616       0.0376       0.0649       0.0512       0.0473       0.0831       0.0652        0.179      0.00187\n","     98    13      0.00581      0.00581     4.14e-06       0.0445        0.059       0.0338       0.0658       0.0498       0.0428       0.0823       0.0625        0.137      0.00142\n","     98    14       0.0062       0.0062     5.11e-06       0.0452       0.0609       0.0353        0.065       0.0501       0.0445       0.0847       0.0646        0.159      0.00165\n","     98    15      0.00583      0.00583     9.37e-07        0.044        0.059        0.034       0.0639        0.049       0.0429       0.0823       0.0626       0.0686     0.000714\n","     98    16      0.00496      0.00496     1.55e-06       0.0416       0.0545       0.0332       0.0584       0.0458       0.0418       0.0736       0.0577        0.076     0.000791\n","     98    17      0.00721      0.00721        3e-07       0.0484       0.0657       0.0369       0.0713       0.0541       0.0477       0.0916       0.0697       0.0348     0.000362\n","     98    18      0.00642       0.0064     1.92e-05       0.0471       0.0619       0.0365       0.0682       0.0523       0.0454       0.0858       0.0656        0.323      0.00337\n","     98    19       0.0063       0.0063     1.13e-06       0.0463       0.0614       0.0367       0.0654        0.051       0.0474       0.0826        0.065        0.067     0.000698\n","     98    20      0.00573      0.00572     1.01e-05       0.0434       0.0585       0.0334       0.0633       0.0484       0.0438       0.0802        0.062        0.233      0.00243\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     98     1      0.00688      0.00688     2.41e-06       0.0479       0.0642       0.0368       0.0701       0.0535       0.0473       0.0888        0.068       0.0903     0.000941\n","     98     2      0.00688      0.00688     1.07e-06       0.0476       0.0642        0.036       0.0706       0.0533       0.0465       0.0896        0.068       0.0514     0.000535\n","     98     3      0.00656      0.00656     1.38e-06       0.0464       0.0627        0.035       0.0691        0.052       0.0448       0.0881       0.0664       0.0714     0.000744\n","     98     4      0.00671      0.00671     1.55e-06       0.0467       0.0634       0.0354       0.0693       0.0523        0.045       0.0894       0.0672        0.079     0.000823\n","     98     5      0.00622      0.00622     8.56e-07       0.0456        0.061       0.0353       0.0663       0.0508       0.0455       0.0839       0.0647       0.0451      0.00047\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              98  399.476    0.005      0.00638     9.46e-06      0.00639       0.0462       0.0618       0.0357       0.0673       0.0515       0.0456       0.0855       0.0655        0.187      0.00195\n","! Validation         98  399.476    0.005      0.00665     1.45e-06      0.00665       0.0468       0.0631       0.0357       0.0691       0.0524       0.0458        0.088       0.0669       0.0674     0.000703\n","Wall time: 399.4772256649999\n","! Best model       98    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     99     1      0.00634      0.00633     1.31e-05        0.046       0.0615       0.0356       0.0669       0.0512       0.0453       0.0852       0.0653        0.257      0.00268\n","     99     2      0.00599      0.00598     1.09e-05       0.0442       0.0598       0.0349       0.0628       0.0489       0.0454       0.0813       0.0633        0.218      0.00227\n","     99     3      0.00641       0.0064     5.35e-06        0.046       0.0619       0.0342       0.0697        0.052       0.0442        0.087       0.0656        0.165      0.00172\n","     99     4      0.00864      0.00859     4.82e-05       0.0546       0.0717       0.0449       0.0741       0.0595       0.0567       0.0948       0.0758        0.514      0.00536\n","     99     5      0.00911      0.00911     2.65e-06       0.0566       0.0738       0.0459       0.0779       0.0619       0.0574       0.0988       0.0781        0.104      0.00109\n","     99     6      0.00862      0.00862     6.47e-07       0.0564       0.0718       0.0462       0.0769       0.0615       0.0563       0.0955       0.0759       0.0463     0.000482\n","     99     7      0.00806      0.00803     3.14e-05       0.0535       0.0693       0.0444       0.0718       0.0581       0.0569       0.0891        0.073        0.413       0.0043\n","     99     8      0.00605      0.00605      2.7e-06       0.0445       0.0602       0.0349       0.0636       0.0493       0.0442       0.0834       0.0638        0.114      0.00118\n","     99     9       0.0066       0.0066     2.57e-06       0.0468       0.0629       0.0354       0.0696       0.0525       0.0452       0.0881       0.0667        0.101      0.00105\n","     99    10        0.007      0.00699     6.28e-06       0.0484       0.0647       0.0392       0.0669        0.053       0.0508        0.086       0.0684        0.183       0.0019\n","     99    11      0.00712       0.0071     1.73e-05       0.0498       0.0652       0.0421       0.0651       0.0536       0.0537       0.0836       0.0686        0.299      0.00312\n","     99    12      0.00568      0.00568      4.3e-06       0.0435       0.0583       0.0338       0.0628       0.0483       0.0429       0.0807       0.0618        0.146      0.00152\n","     99    13      0.00585      0.00585     9.55e-07        0.044       0.0592       0.0324       0.0672       0.0498       0.0413       0.0842       0.0628       0.0568     0.000592\n","     99    14      0.00694      0.00694     1.02e-06       0.0467       0.0645       0.0354       0.0692       0.0523       0.0467         0.09       0.0684       0.0645     0.000671\n","     99    15       0.0064      0.00638     2.02e-05       0.0462       0.0618       0.0355       0.0675       0.0515       0.0449       0.0862       0.0656        0.323      0.00337\n","     99    16      0.00619      0.00618     6.52e-06       0.0452       0.0608       0.0332       0.0694       0.0513       0.0422       0.0868       0.0645        0.188      0.00196\n","     99    17      0.00657      0.00654     2.56e-05       0.0477       0.0626       0.0384       0.0664       0.0524       0.0482       0.0842       0.0662        0.371      0.00386\n","     99    18      0.00755      0.00755      8.7e-07       0.0526       0.0672       0.0443       0.0693       0.0568       0.0553       0.0862       0.0708        0.051     0.000531\n","     99    19      0.00822      0.00821     2.54e-06        0.053       0.0701       0.0427       0.0736       0.0581        0.055       0.0933       0.0741       0.0959     0.000999\n","     99    20      0.00629      0.00628     2.38e-06       0.0456       0.0613       0.0348       0.0672        0.051       0.0445       0.0856        0.065          0.1      0.00104\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","     99     1      0.00686      0.00685     2.43e-06       0.0478        0.064       0.0367         0.07       0.0534       0.0472       0.0886       0.0679        0.092     0.000958\n","     99     2      0.00686      0.00686     1.02e-06       0.0475       0.0641       0.0359       0.0706       0.0532       0.0464       0.0895       0.0679       0.0491     0.000512\n","     99     3      0.00654      0.00654     1.36e-06       0.0463       0.0626       0.0349       0.0691        0.052       0.0446       0.0881       0.0664       0.0715     0.000745\n","     99     4      0.00669      0.00669     1.59e-06       0.0466       0.0633       0.0353       0.0693       0.0523       0.0449       0.0893       0.0671       0.0808     0.000841\n","     99     5      0.00619      0.00619     8.61e-07       0.0455       0.0609       0.0352       0.0662       0.0507       0.0453       0.0837       0.0645       0.0462     0.000481\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train              99  403.507    0.005      0.00697     1.03e-05      0.00698       0.0486       0.0646       0.0384       0.0689       0.0536       0.0492       0.0876       0.0684         0.19      0.00198\n","! Validation         99  403.507    0.005      0.00663     1.45e-06      0.00663       0.0467        0.063       0.0356        0.069       0.0523       0.0457       0.0879       0.0668       0.0679     0.000707\n","Wall time: 403.50787386799993\n","! Best model       99    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    100     1      0.00575      0.00575     2.84e-06       0.0442       0.0587       0.0339       0.0647       0.0493        0.042       0.0824       0.0622        0.093     0.000968\n","    100     2      0.00585      0.00584     5.68e-06       0.0445       0.0591       0.0345       0.0645       0.0495       0.0444       0.0809       0.0627        0.162      0.00169\n","    100     3      0.00614      0.00613     6.32e-06        0.045       0.0606        0.034       0.0668       0.0504       0.0442       0.0843       0.0642        0.173      0.00181\n","    100     4      0.00541      0.00539     1.36e-05       0.0431       0.0568       0.0341        0.061       0.0476       0.0427       0.0777       0.0602        0.266      0.00278\n","    100     5      0.00612      0.00611     6.86e-06       0.0445       0.0605       0.0334       0.0666         0.05       0.0422        0.086       0.0641        0.187      0.00194\n","    100     6      0.00636      0.00636     5.51e-07       0.0444       0.0617       0.0333       0.0666         0.05        0.043       0.0879       0.0654       0.0436     0.000454\n","    100     7      0.00645      0.00645     1.14e-06       0.0466       0.0621       0.0369       0.0659       0.0514       0.0476        0.084       0.0658       0.0652      0.00068\n","    100     8      0.00663      0.00662     1.19e-05       0.0483       0.0629       0.0403       0.0643       0.0523       0.0512       0.0815       0.0664        0.245      0.00256\n","    100     9      0.00644      0.00644     1.35e-06       0.0475       0.0621       0.0384       0.0657       0.0521       0.0481       0.0832       0.0657       0.0793     0.000826\n","    100    10      0.00577      0.00577     1.45e-06       0.0442       0.0588       0.0333       0.0661       0.0497       0.0423       0.0824       0.0623       0.0723     0.000753\n","    100    11      0.00703      0.00702     2.85e-06       0.0488       0.0648       0.0394       0.0676       0.0535       0.0501       0.0872       0.0686        0.108      0.00112\n","    100    12      0.00658      0.00657     1.02e-06        0.047       0.0627       0.0363       0.0685       0.0524       0.0455       0.0876       0.0665       0.0643     0.000669\n","    100    13      0.00745      0.00743     2.51e-05       0.0494       0.0667       0.0372       0.0738       0.0555       0.0477       0.0938       0.0707        0.353      0.00368\n","    100    14      0.00614      0.00614     7.29e-07       0.0455       0.0606       0.0343       0.0679       0.0511       0.0434       0.0852       0.0643       0.0547      0.00057\n","    100    15      0.00553      0.00552     1.52e-05       0.0437       0.0575       0.0341       0.0628       0.0484       0.0435       0.0783       0.0609        0.278       0.0029\n","    100    16      0.00693      0.00693     1.71e-06       0.0483       0.0644       0.0383       0.0682       0.0533       0.0498       0.0865       0.0682       0.0916     0.000954\n","    100    17      0.00652      0.00651     9.14e-06       0.0486       0.0624       0.0388        0.068       0.0534       0.0485       0.0836        0.066        0.217      0.00226\n","    100    18      0.00654      0.00654     2.05e-06       0.0468       0.0626       0.0351       0.0703       0.0527        0.045       0.0877       0.0663       0.0727     0.000757\n","    100    19      0.00617      0.00616      5.7e-06       0.0449       0.0607        0.035       0.0647       0.0499       0.0446       0.0842       0.0644        0.173       0.0018\n","    100    20      0.00662      0.00661     3.62e-06       0.0464       0.0629       0.0338       0.0716       0.0527       0.0437       0.0898       0.0667        0.128      0.00133\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    100     1      0.00683      0.00683     2.45e-06       0.0477       0.0639       0.0366       0.0699       0.0533        0.047       0.0885       0.0678       0.0921     0.000959\n","    100     2      0.00683      0.00683     9.96e-07       0.0474       0.0639       0.0358       0.0705       0.0532       0.0462       0.0894       0.0678       0.0498     0.000519\n","    100     3      0.00652      0.00652     1.36e-06       0.0462       0.0625       0.0348        0.069       0.0519       0.0445        0.088       0.0663       0.0706     0.000735\n","    100     4      0.00666      0.00666     1.58e-06       0.0465       0.0631       0.0352       0.0692       0.0522       0.0448       0.0892        0.067       0.0803     0.000836\n","    100     5      0.00617      0.00617     8.64e-07       0.0454       0.0608       0.0351       0.0661       0.0506       0.0452       0.0836       0.0644        0.045     0.000469\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train             100  407.557    0.005      0.00631     5.94e-06      0.00632       0.0461       0.0615       0.0357       0.0668       0.0512       0.0456       0.0848       0.0652        0.146      0.00152\n","! Validation        100  407.557    0.005       0.0066     1.45e-06       0.0066       0.0466       0.0629       0.0355       0.0689       0.0522       0.0456       0.0878       0.0667       0.0676     0.000704\n","Wall time: 407.5582995059999\n","! Best model      100    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    101     1      0.00603      0.00603     4.29e-07       0.0451       0.0601       0.0343       0.0666       0.0505       0.0431       0.0844       0.0637       0.0367     0.000382\n","    101     2      0.00635      0.00634      7.6e-06       0.0456       0.0616       0.0332       0.0703       0.0517       0.0424       0.0882       0.0653        0.189      0.00197\n","    101     3      0.00515      0.00514     2.76e-06       0.0422       0.0555       0.0319       0.0626       0.0473         0.04       0.0777       0.0588       0.0758     0.000789\n","    101     4      0.00612      0.00611     9.78e-06       0.0457       0.0605       0.0345       0.0683       0.0514       0.0436       0.0846       0.0641        0.227      0.00236\n","    101     5      0.00725      0.00725     9.83e-07       0.0505       0.0659       0.0427       0.0661       0.0544       0.0537       0.0851       0.0694       0.0672       0.0007\n","    101     6      0.00793      0.00793     2.15e-06        0.053       0.0689       0.0451       0.0689        0.057       0.0568       0.0882       0.0725       0.0982      0.00102\n","    101     7      0.00667      0.00665     2.51e-05       0.0476       0.0631       0.0367       0.0694       0.0531        0.046       0.0878       0.0669        0.367      0.00382\n","    101     8       0.0059       0.0059     6.22e-07       0.0436       0.0594       0.0334       0.0641       0.0487        0.043        0.083        0.063       0.0566      0.00059\n","    101     9      0.00676      0.00673      3.2e-05       0.0483       0.0635        0.038        0.069       0.0535        0.048       0.0864       0.0672        0.416      0.00434\n","    101    10      0.00729      0.00728     5.15e-06       0.0501        0.066       0.0413       0.0677       0.0545       0.0529       0.0864       0.0697        0.164      0.00171\n","    101    11      0.00568      0.00568     1.43e-06       0.0441       0.0583        0.034       0.0644       0.0492       0.0431       0.0805       0.0618       0.0814     0.000848\n","    101    12      0.00625      0.00625     1.12e-06       0.0457       0.0612       0.0342       0.0686       0.0514        0.043       0.0868       0.0649       0.0672       0.0007\n","    101    13      0.00578      0.00577      9.9e-06       0.0442       0.0588       0.0344       0.0638       0.0491       0.0443       0.0803       0.0623         0.23       0.0024\n","    101    14      0.00732      0.00731     6.12e-07       0.0492       0.0662       0.0377       0.0722       0.0549        0.049       0.0913       0.0701       0.0391     0.000407\n","    101    15      0.00592      0.00591     1.27e-05       0.0453       0.0595       0.0342       0.0676       0.0509       0.0426       0.0835       0.0631        0.255      0.00265\n","    101    16      0.00608      0.00608      9.8e-07       0.0452       0.0603        0.035       0.0654       0.0502       0.0452       0.0826       0.0639        0.066     0.000688\n","    101    17      0.00592      0.00592     3.75e-06       0.0439       0.0595       0.0328       0.0663       0.0495       0.0419       0.0844       0.0631         0.13      0.00135\n","    101    18      0.00645      0.00645     9.47e-07       0.0462       0.0621       0.0343       0.0702       0.0522       0.0441       0.0877       0.0659       0.0639     0.000665\n","    101    19      0.00609      0.00609     3.03e-06       0.0448       0.0604       0.0356       0.0631       0.0494       0.0456       0.0823        0.064       0.0955     0.000995\n","    101    20      0.00717      0.00715     1.69e-05        0.049       0.0654        0.039        0.069        0.054       0.0496        0.089       0.0693        0.289      0.00301\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    101     1       0.0068       0.0068     2.43e-06       0.0476       0.0638       0.0365       0.0698       0.0532       0.0469       0.0884       0.0677       0.0922      0.00096\n","    101     2      0.00681      0.00681     1.04e-06       0.0473       0.0638       0.0357       0.0704       0.0531       0.0461       0.0893       0.0677       0.0483     0.000504\n","    101     3       0.0065       0.0065     1.26e-06       0.0461       0.0624       0.0347       0.0689       0.0518       0.0444       0.0879       0.0662        0.069     0.000719\n","    101     4      0.00664      0.00664     1.58e-06       0.0464        0.063       0.0351       0.0691       0.0521       0.0446       0.0891       0.0669       0.0802     0.000835\n","    101     5      0.00615      0.00615      8.6e-07       0.0454       0.0607        0.035        0.066       0.0505        0.045       0.0836       0.0643       0.0454     0.000473\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train             101  411.610    0.005       0.0064      6.9e-06      0.00641       0.0465       0.0619       0.0361       0.0672       0.0516       0.0461       0.0851       0.0656        0.151      0.00157\n","! Validation        101  411.610    0.005      0.00658     1.43e-06      0.00658       0.0466       0.0628       0.0354       0.0689       0.0521       0.0454       0.0877       0.0666        0.067     0.000698\n","Wall time: 411.61081608999984\n","! Best model      101    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    102     1      0.00582      0.00581     2.36e-06       0.0443        0.059       0.0331       0.0669         0.05       0.0423       0.0828       0.0626        0.101      0.00105\n","    102     2      0.00694      0.00693      9.5e-06        0.049       0.0644       0.0396       0.0678       0.0537       0.0508       0.0853       0.0681        0.228      0.00237\n","    102     3      0.00761      0.00761      2.1e-06       0.0513       0.0675       0.0412       0.0714       0.0563       0.0521       0.0908       0.0714        0.099      0.00103\n","    102     4      0.00801        0.008     1.91e-06       0.0524       0.0692       0.0413       0.0748        0.058       0.0526        0.094       0.0733       0.0777      0.00081\n","    102     5      0.00773      0.00773     7.94e-06       0.0522        0.068       0.0415       0.0735       0.0575       0.0518       0.0922        0.072        0.199      0.00207\n","    102     6      0.00646      0.00645     1.01e-05       0.0472       0.0621       0.0393       0.0629       0.0511       0.0497       0.0815       0.0656        0.219      0.00228\n","    102     7      0.00671      0.00671     4.48e-06       0.0483       0.0634       0.0386       0.0678       0.0532       0.0491        0.085       0.0671         0.15      0.00156\n","    102     8      0.00587      0.00585     2.13e-05       0.0445       0.0591       0.0349       0.0639       0.0494       0.0438       0.0816       0.0627        0.336       0.0035\n","    102     9      0.00834      0.00834     1.73e-06       0.0546       0.0707       0.0453       0.0732       0.0592       0.0568       0.0923       0.0746       0.0746     0.000777\n","    102    10      0.00848      0.00847     8.27e-06       0.0545       0.0712       0.0428       0.0781       0.0604       0.0538        0.097       0.0754        0.207      0.00215\n","    102    11      0.00785      0.00783     2.52e-05       0.0523       0.0684        0.042       0.0731       0.0575       0.0535       0.0913       0.0724        0.364      0.00379\n","    102    12      0.00638      0.00633     4.21e-05        0.047       0.0616       0.0387       0.0635       0.0511       0.0485       0.0817       0.0651        0.476      0.00496\n","    102    13      0.00572      0.00569     2.45e-05       0.0443       0.0584       0.0354       0.0621       0.0487       0.0449       0.0787       0.0618        0.364      0.00379\n","    102    14      0.00665      0.00659     5.82e-05       0.0475       0.0628       0.0364       0.0696        0.053       0.0466       0.0866       0.0666        0.559      0.00582\n","    102    15      0.00742      0.00737     4.63e-05       0.0513       0.0664       0.0422       0.0694       0.0558       0.0524        0.088       0.0702          0.5      0.00521\n","    102    16       0.0079       0.0079     9.43e-07       0.0518       0.0688       0.0406       0.0742       0.0574       0.0513       0.0945       0.0729       0.0594     0.000618\n","    102    17      0.00653      0.00635     0.000183       0.0467       0.0617       0.0377       0.0645       0.0511       0.0476       0.0829       0.0653            1       0.0105\n","    102    18      0.00594      0.00593     1.39e-05       0.0436       0.0596       0.0331       0.0647       0.0489       0.0422       0.0841       0.0632        0.267      0.00278\n","    102    19      0.00757      0.00746     0.000109         0.05       0.0668       0.0383       0.0733       0.0558       0.0486       0.0931       0.0709        0.775      0.00807\n","    102    20       0.0092       0.0092     2.19e-06       0.0575       0.0742       0.0498        0.073       0.0614       0.0633       0.0922       0.0777       0.0977      0.00102\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    102     1      0.00678      0.00678     2.39e-06       0.0475       0.0637       0.0364       0.0697       0.0531       0.0468       0.0883       0.0675       0.0894     0.000931\n","    102     2      0.00678      0.00678     1.02e-06       0.0472       0.0637       0.0356       0.0703        0.053        0.046       0.0891       0.0675       0.0492     0.000513\n","    102     3      0.00648      0.00648     1.32e-06        0.046       0.0623       0.0346       0.0688       0.0517       0.0442       0.0878        0.066         0.07     0.000729\n","    102     4      0.00662      0.00662     1.55e-06       0.0463       0.0629        0.035        0.069        0.052       0.0445        0.089       0.0668       0.0796     0.000829\n","    102     5      0.00612      0.00612     8.78e-07       0.0452       0.0605       0.0349        0.066       0.0504       0.0449       0.0835       0.0642        0.046     0.000479\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train             102  415.652    0.005      0.00713     2.88e-05      0.00716       0.0495       0.0653       0.0396       0.0694       0.0545       0.0503       0.0879       0.0691        0.308      0.00321\n","! Validation        102  415.652    0.005      0.00655     1.43e-06      0.00656       0.0465       0.0626       0.0353       0.0688        0.052       0.0453       0.0876       0.0664       0.0668     0.000696\n","Wall time: 415.6524084869998\n","! Best model      102    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    103     1      0.00817      0.00816      5.2e-06       0.0546       0.0699        0.047       0.0699       0.0584       0.0583       0.0886       0.0735        0.143      0.00149\n","    103     2      0.00655      0.00655     3.77e-06       0.0468       0.0626       0.0348       0.0708       0.0528       0.0441       0.0887       0.0664        0.135      0.00141\n","    103     3      0.00691      0.00689     2.49e-05       0.0481       0.0642       0.0367       0.0707       0.0537       0.0468       0.0894       0.0681        0.364       0.0038\n","    103     4      0.00642      0.00641     4.87e-06       0.0465       0.0619        0.036       0.0674       0.0517       0.0457       0.0856       0.0657        0.138      0.00144\n","    103     5      0.00543      0.00536      6.9e-05       0.0435       0.0567       0.0347       0.0612       0.0479       0.0434       0.0766         0.06        0.609      0.00635\n","    103     6      0.00592      0.00592     5.53e-06       0.0443       0.0595       0.0335       0.0659       0.0497       0.0427       0.0835       0.0631        0.161      0.00167\n","    103     7      0.00753      0.00747     5.42e-05       0.0504       0.0669        0.038       0.0753       0.0566       0.0479        0.094       0.0709        0.544      0.00567\n","    103     8      0.00775      0.00775     3.11e-07       0.0519       0.0681       0.0413        0.073       0.0571       0.0522        0.092       0.0721       0.0332     0.000346\n","    103     9      0.00814      0.00812      1.8e-05       0.0551       0.0697       0.0506        0.064       0.0573       0.0622       0.0827       0.0724        0.315      0.00328\n","    103    10      0.00692      0.00691     8.83e-06       0.0482       0.0643        0.039       0.0667       0.0528       0.0501       0.0859        0.068        0.219      0.00228\n","    103    11       0.0068      0.00679     1.03e-05       0.0484       0.0637       0.0364       0.0723       0.0543       0.0456       0.0896       0.0676        0.229      0.00239\n","    103    12      0.00605      0.00605      2.9e-06       0.0457       0.0602       0.0369       0.0635       0.0502       0.0475       0.0797       0.0636        0.111      0.00115\n","    103    13      0.00811      0.00809     2.17e-05        0.052       0.0696        0.039       0.0782       0.0586       0.0489       0.0987       0.0738        0.338      0.00352\n","    103    14       0.0067       0.0067     7.74e-06       0.0468       0.0633       0.0363       0.0679       0.0521       0.0469       0.0874       0.0671        0.189      0.00197\n","    103    15      0.00626      0.00626     7.34e-06       0.0471       0.0612       0.0381       0.0651       0.0516       0.0471       0.0825       0.0648        0.188      0.00195\n","    103    16      0.00687      0.00687     4.07e-06       0.0486       0.0641       0.0376       0.0706       0.0541       0.0468       0.0891        0.068        0.143      0.00149\n","    103    17       0.0086       0.0086     5.12e-07       0.0574       0.0717        0.052       0.0682       0.0601       0.0636       0.0858       0.0747       0.0445     0.000464\n","    103    18      0.00919      0.00918     4.67e-06       0.0587       0.0741        0.053       0.0701       0.0615       0.0654        0.089       0.0772        0.145      0.00151\n","    103    19      0.00954      0.00953     1.13e-05       0.0573       0.0755       0.0442       0.0835       0.0639        0.056        0.104         0.08        0.239      0.00249\n","    103    20       0.0073      0.00729      1.1e-05       0.0501       0.0661       0.0389       0.0725       0.0557       0.0483       0.0918         0.07        0.244      0.00255\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    103     1      0.00676      0.00675     2.33e-06       0.0474       0.0636       0.0363       0.0696        0.053       0.0466       0.0882       0.0674       0.0896     0.000934\n","    103     2      0.00676      0.00675     9.35e-07       0.0471       0.0636       0.0356       0.0702       0.0529       0.0459        0.089       0.0674       0.0469     0.000488\n","    103     3      0.00646      0.00646     1.34e-06       0.0459       0.0622       0.0345       0.0687       0.0516       0.0441       0.0877       0.0659       0.0704     0.000733\n","    103     4       0.0066       0.0066     1.57e-06       0.0462       0.0628       0.0349        0.069       0.0519       0.0444       0.0889       0.0666       0.0807      0.00084\n","    103     5      0.00611      0.00611     7.89e-07       0.0452       0.0605       0.0348       0.0659       0.0504       0.0448       0.0834       0.0641       0.0441      0.00046\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train             103  419.668    0.005      0.00724     1.38e-05      0.00726       0.0501       0.0658       0.0402       0.0698        0.055       0.0509       0.0885       0.0697        0.227      0.00236\n","! Validation        103  419.668    0.005      0.00653      1.4e-06      0.00654       0.0464       0.0625       0.0352       0.0687        0.052       0.0452       0.0875       0.0663       0.0663     0.000691\n","Wall time: 419.66828930299994\n","! Best model      103    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    104     1      0.00525      0.00525     3.08e-06       0.0423        0.056       0.0315        0.064       0.0478       0.0398       0.0791       0.0594        0.121      0.00126\n","    104     2      0.00726      0.00724        2e-05       0.0485       0.0658       0.0357       0.0742       0.0549       0.0459       0.0938       0.0698        0.328      0.00341\n","    104     3      0.00773      0.00772     1.04e-05       0.0537        0.068       0.0475       0.0662       0.0568       0.0593       0.0826        0.071        0.225      0.00234\n","    104     4      0.00919      0.00919     2.18e-06       0.0574       0.0742       0.0497       0.0727       0.0612       0.0632       0.0923       0.0777       0.0754     0.000785\n","    104     5      0.00781      0.00779     1.64e-05       0.0516       0.0683       0.0417       0.0714       0.0566       0.0528       0.0918       0.0723        0.286      0.00298\n","    104     6      0.00558      0.00556     1.89e-05       0.0429       0.0577       0.0323        0.064       0.0482       0.0408       0.0816       0.0612        0.318      0.00331\n","    104     7      0.00562      0.00562     7.57e-06       0.0433        0.058       0.0336       0.0626       0.0481       0.0423       0.0807       0.0615        0.203      0.00211\n","    104     8      0.00679      0.00677     1.92e-05       0.0479       0.0636       0.0384        0.067       0.0527       0.0481       0.0867       0.0674        0.324      0.00338\n","    104     9      0.00732      0.00732     2.75e-07       0.0507       0.0662       0.0419       0.0681        0.055       0.0529       0.0868       0.0699       0.0348     0.000362\n","    104    10      0.00636      0.00636     4.48e-06       0.0475       0.0617       0.0377       0.0669       0.0523       0.0469       0.0838       0.0653        0.146      0.00152\n","    104    11      0.00675      0.00674      6.2e-06       0.0486       0.0635        0.038       0.0697       0.0539       0.0488       0.0857       0.0673        0.163       0.0017\n","    104    12      0.00731       0.0073     3.61e-06       0.0513       0.0661       0.0442       0.0655       0.0549       0.0555       0.0833       0.0694        0.138      0.00143\n","    104    13      0.00742      0.00742     1.79e-06       0.0501       0.0666       0.0393       0.0718       0.0555         0.05       0.0913       0.0706        0.076     0.000791\n","    104    14      0.00666      0.00663     3.45e-05       0.0469        0.063       0.0352       0.0701       0.0527       0.0447       0.0889       0.0668        0.432       0.0045\n","    104    15      0.00603      0.00603     3.67e-06       0.0441       0.0601       0.0347        0.063       0.0489       0.0451       0.0822       0.0636         0.11      0.00114\n","    104    16      0.00607      0.00605     2.22e-05       0.0455       0.0602       0.0362       0.0641       0.0501        0.046       0.0815       0.0637        0.338      0.00352\n","    104    17      0.00773      0.00773     1.12e-06       0.0526        0.068       0.0435       0.0708       0.0572       0.0548       0.0887       0.0718       0.0633     0.000659\n","    104    18      0.00724      0.00724     2.46e-07       0.0513       0.0658       0.0429        0.068       0.0555       0.0534       0.0854       0.0694       0.0293     0.000305\n","    104    19      0.00716      0.00716     1.29e-06        0.048       0.0654        0.035       0.0742       0.0546       0.0445       0.0943       0.0694       0.0672       0.0007\n","    104    20      0.00778      0.00778     8.35e-07       0.0528       0.0682       0.0459       0.0667       0.0563       0.0574       0.0858       0.0716       0.0621     0.000647\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    104     1      0.00674      0.00673     2.34e-06       0.0473       0.0635       0.0362       0.0696       0.0529       0.0465       0.0881       0.0673       0.0905     0.000943\n","    104     2      0.00674      0.00674      9.6e-07        0.047       0.0635       0.0355       0.0702       0.0528       0.0458       0.0889       0.0673        0.046     0.000479\n","    104     3      0.00644      0.00644     1.35e-06       0.0459       0.0621       0.0345       0.0687       0.0516       0.0441       0.0877       0.0659       0.0717     0.000747\n","    104     4      0.00658      0.00658     1.62e-06       0.0462       0.0628       0.0348       0.0689       0.0518       0.0443       0.0888       0.0666       0.0823     0.000858\n","    104     5      0.00609      0.00609     8.35e-07       0.0451       0.0604       0.0347       0.0659       0.0503       0.0446       0.0834        0.064       0.0458     0.000477\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train             104  423.679    0.005      0.00694      8.9e-06      0.00695       0.0489       0.0645       0.0393       0.0681       0.0537         0.05       0.0864       0.0682        0.177      0.00184\n","! Validation        104  423.679    0.005      0.00652     1.42e-06      0.00652       0.0463       0.0625       0.0351       0.0686       0.0519       0.0451       0.0874       0.0662       0.0673     0.000701\n","Wall time: 423.6799101239999\n","! Best model      104    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    105     1      0.00718      0.00718     1.29e-06         0.05       0.0655       0.0417       0.0667       0.0542       0.0533       0.0849       0.0691       0.0783     0.000816\n","    105     2      0.00697      0.00696     1.28e-05       0.0475       0.0645       0.0344       0.0738       0.0541       0.0445       0.0924       0.0684        0.264      0.00275\n","    105     3      0.00696      0.00696     3.46e-07       0.0485       0.0646       0.0386       0.0683       0.0534       0.0499       0.0868       0.0683       0.0297     0.000309\n","    105     4       0.0063      0.00627     2.88e-05        0.047       0.0613       0.0379       0.0652       0.0516        0.047       0.0827       0.0648        0.396      0.00412\n","    105     5      0.00617      0.00617     1.47e-06       0.0458       0.0608       0.0359       0.0654       0.0507       0.0464       0.0823       0.0644       0.0764     0.000795\n","    105     6      0.00607      0.00606     1.53e-05       0.0449       0.0602       0.0344       0.0657       0.0501       0.0431       0.0846       0.0638        0.284      0.00296\n","    105     7      0.00634      0.00634     7.03e-07       0.0461       0.0616        0.035       0.0685       0.0517       0.0444       0.0863       0.0653       0.0518     0.000539\n","    105     8        0.007        0.007      4.2e-06       0.0496       0.0647       0.0413       0.0661       0.0537        0.052       0.0845       0.0683        0.132      0.00138\n","    105     9      0.00608      0.00607     2.58e-06       0.0443       0.0603        0.032       0.0689       0.0505        0.041       0.0869       0.0639        0.115       0.0012\n","    105    10      0.00681       0.0068      8.1e-06       0.0502       0.0638       0.0421       0.0663       0.0542       0.0526       0.0817       0.0672        0.205      0.00213\n","    105    11      0.00831       0.0083      1.9e-05       0.0547       0.0705       0.0458       0.0726       0.0592       0.0577       0.0908       0.0742        0.314      0.00327\n","    105    12      0.00849      0.00847     2.39e-05       0.0545       0.0712       0.0435       0.0765         0.06       0.0537       0.0971       0.0754         0.36      0.00375\n","    105    13      0.00645      0.00639     5.12e-05       0.0458       0.0619       0.0357       0.0658       0.0508       0.0457       0.0855       0.0656        0.526      0.00547\n","    105    14      0.00654      0.00648     5.97e-05       0.0461       0.0623       0.0343       0.0697        0.052       0.0434       0.0887       0.0661        0.569      0.00593\n","    105    15      0.00771      0.00766     5.05e-05        0.051       0.0677       0.0419       0.0692       0.0556       0.0539       0.0891       0.0715        0.522      0.00544\n","    105    16      0.00828      0.00828     3.79e-07       0.0552       0.0704       0.0499        0.066       0.0579       0.0619       0.0848       0.0734       0.0312     0.000326\n","    105    17      0.00778      0.00778      2.8e-06       0.0512       0.0682       0.0397       0.0741       0.0569       0.0503       0.0944       0.0723        0.117      0.00122\n","    105    18      0.00668      0.00668     1.05e-06       0.0476       0.0632       0.0375       0.0677       0.0526       0.0468       0.0872        0.067        0.068     0.000708\n","    105    19      0.00578      0.00576     2.71e-05       0.0437       0.0587        0.034       0.0631       0.0486       0.0434        0.081       0.0622        0.379      0.00395\n","    105    20      0.00799      0.00796     2.69e-05       0.0522        0.069       0.0401       0.0764       0.0582       0.0507       0.0957       0.0732        0.371      0.00387\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    105     1      0.00672      0.00672      2.3e-06       0.0473       0.0634       0.0362       0.0695       0.0528       0.0464        0.088       0.0672       0.0889     0.000926\n","    105     2      0.00672      0.00672     1.02e-06        0.047       0.0634       0.0354       0.0701       0.0528       0.0457       0.0889       0.0673       0.0487     0.000508\n","    105     3      0.00643      0.00643     1.36e-06       0.0458        0.062       0.0344       0.0687       0.0516        0.044       0.0876       0.0658       0.0704     0.000733\n","    105     4      0.00657      0.00657     1.58e-06       0.0461       0.0627       0.0347       0.0689       0.0518       0.0442       0.0888       0.0665       0.0801     0.000834\n","    105     5      0.00607      0.00607     8.45e-07        0.045       0.0603       0.0346       0.0658       0.0502       0.0445       0.0833       0.0639        0.045     0.000469\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train             105  427.699    0.005      0.00698     1.69e-05      0.00699       0.0488       0.0646       0.0388       0.0688       0.0538       0.0494       0.0875       0.0684        0.244      0.00255\n","! Validation        105  427.699    0.005       0.0065     1.42e-06       0.0065       0.0462       0.0624       0.0351       0.0686       0.0518        0.045       0.0873       0.0662       0.0666     0.000694\n","Wall time: 427.70027167599983\n","! Best model      105    0.007\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    106     1      0.00682      0.00669     0.000136       0.0479       0.0633        0.038       0.0677       0.0529       0.0482       0.0858        0.067        0.862      0.00897\n","    106     2       0.0073      0.00725     5.02e-05       0.0513       0.0659       0.0442       0.0656       0.0549        0.055       0.0835       0.0692        0.525      0.00546\n","    106     3      0.00627      0.00624     2.72e-05       0.0462       0.0611       0.0353       0.0679       0.0516       0.0457       0.0838       0.0648        0.374       0.0039\n","    106     4       0.0065      0.00635     0.000153       0.0462       0.0616       0.0373       0.0642       0.0507       0.0472       0.0834       0.0653        0.918      0.00957\n","    106     5      0.00607      0.00607     4.88e-07       0.0445       0.0603       0.0337       0.0661       0.0499       0.0433       0.0845       0.0639       0.0424     0.000441\n","    106     6      0.00717      0.00704     0.000124       0.0482       0.0649       0.0364       0.0717        0.054       0.0473       0.0904       0.0689        0.822      0.00856\n","    106     7      0.00715      0.00714     3.13e-06       0.0495       0.0654       0.0398       0.0688       0.0543       0.0501       0.0884       0.0692         0.11      0.00115\n","    106     8      0.00639      0.00638     1.27e-05       0.0473       0.0618       0.0384       0.0651       0.0517       0.0482       0.0825       0.0654        0.254      0.00265\n","    106     9      0.00552      0.00552     6.21e-07        0.043       0.0575       0.0325       0.0639       0.0482       0.0405       0.0814       0.0609       0.0525     0.000547\n","    106    10      0.00648      0.00647     5.86e-06       0.0472       0.0623       0.0384       0.0647       0.0516       0.0484       0.0833       0.0659        0.158      0.00164\n","    106    11      0.00502      0.00502     3.63e-07       0.0409       0.0548       0.0309        0.061        0.046       0.0394       0.0768       0.0581       0.0346      0.00036\n","    106    12      0.00711      0.00708     3.21e-05       0.0499       0.0651       0.0402       0.0691       0.0547       0.0509       0.0868       0.0688        0.419      0.00437\n","    106    13      0.00655      0.00655     2.38e-06       0.0474       0.0626       0.0396        0.063       0.0513       0.0499       0.0823       0.0661        0.114      0.00118\n","    106    14      0.00771      0.00764     7.43e-05       0.0511       0.0676       0.0399       0.0735       0.0567       0.0501       0.0933       0.0717        0.638      0.00665\n","    106    15      0.00643      0.00642      3.9e-06       0.0461        0.062       0.0342       0.0699       0.0521       0.0433       0.0883       0.0658        0.137      0.00143\n","    106    16      0.00655      0.00653      2.7e-05       0.0469       0.0625       0.0368       0.0673        0.052        0.047       0.0855       0.0662        0.382      0.00398\n","    106    17      0.00648      0.00648     7.36e-06       0.0452       0.0623       0.0327       0.0702       0.0514       0.0418       0.0902        0.066        0.199      0.00208\n","    106    18      0.00638      0.00637     9.37e-06       0.0459       0.0618       0.0356       0.0665        0.051       0.0463       0.0847       0.0655        0.197      0.00205\n","    106    19      0.00584      0.00583     9.76e-07       0.0448       0.0591        0.035       0.0642       0.0496       0.0443        0.081       0.0626       0.0674     0.000702\n","    106    20      0.00664      0.00664     7.46e-07       0.0474       0.0631        0.036         0.07        0.053       0.0456       0.0882       0.0669        0.051     0.000531\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    106     1       0.0067       0.0067     2.27e-06       0.0472       0.0633       0.0361       0.0694       0.0528       0.0464       0.0879       0.0671       0.0896     0.000933\n","    106     2       0.0067       0.0067     9.73e-07       0.0469       0.0633       0.0354       0.0701       0.0527       0.0456       0.0888       0.0672       0.0469     0.000488\n","    106     3      0.00642      0.00642     1.37e-06       0.0458        0.062       0.0344       0.0687       0.0515       0.0439       0.0876       0.0657       0.0721     0.000751\n","    106     4      0.00655      0.00655     1.57e-06       0.0461       0.0626       0.0347       0.0688       0.0518       0.0441       0.0887       0.0664       0.0801     0.000834\n","    106     5      0.00606      0.00606     7.73e-07       0.0449       0.0602       0.0345       0.0657       0.0501       0.0444       0.0832       0.0638       0.0436     0.000454\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train             106  431.739    0.005      0.00649     3.36e-05      0.00652       0.0468       0.0623       0.0367        0.067       0.0519       0.0468       0.0853        0.066        0.318      0.00331\n","! Validation        106  431.739    0.005      0.00649     1.39e-06      0.00649       0.0462       0.0623        0.035       0.0686       0.0518       0.0449       0.0873       0.0661       0.0664     0.000692\n","Wall time: 431.7396727319999\n","! Best model      106    0.006\n","\n","training\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    107     1      0.00574      0.00574     4.82e-06       0.0441       0.0586       0.0346       0.0633       0.0489       0.0438       0.0804       0.0621        0.152      0.00158\n","    107     2       0.0058       0.0058     3.96e-06        0.045       0.0589       0.0358       0.0634       0.0496       0.0452       0.0795       0.0624        0.133      0.00138\n","    107     3      0.00648      0.00646      1.4e-05       0.0458       0.0622       0.0354       0.0668       0.0511       0.0461       0.0858       0.0659        0.271      0.00282\n","    107     4      0.00623      0.00622      3.4e-06       0.0451        0.061       0.0346       0.0662       0.0504       0.0443       0.0852       0.0647         0.12      0.00125\n","    107     5      0.00598      0.00597     9.91e-06       0.0433       0.0598       0.0323       0.0652       0.0488       0.0417        0.085       0.0634        0.225      0.00235\n","    107     6      0.00616      0.00615     1.67e-05       0.0453       0.0607       0.0347       0.0666       0.0507       0.0442       0.0844       0.0643        0.297      0.00309\n","    107     7      0.00618      0.00617     3.11e-06        0.046       0.0608       0.0351       0.0677       0.0514       0.0441       0.0848       0.0645        0.124       0.0013\n","    107     8      0.00695      0.00689     5.54e-05       0.0483       0.0642       0.0378       0.0692       0.0535       0.0486       0.0875        0.068         0.55      0.00573\n","    107     9      0.00591      0.00591     3.96e-06       0.0445       0.0595       0.0333        0.067       0.0501       0.0425       0.0836       0.0631        0.116       0.0012\n","    107    10      0.00608      0.00606     2.81e-05       0.0451       0.0602       0.0335       0.0683       0.0509       0.0426       0.0851       0.0639        0.386      0.00402\n","    107    11      0.00608      0.00605     3.26e-05       0.0455       0.0602       0.0353       0.0659       0.0506       0.0455        0.082       0.0637        0.421      0.00438\n","    107    12      0.00637      0.00636     2.53e-06       0.0459       0.0617       0.0347       0.0683       0.0515       0.0444       0.0865       0.0655        0.107      0.00111\n","    107    13      0.00611      0.00607     4.26e-05       0.0447       0.0603       0.0336       0.0669       0.0503       0.0423       0.0856       0.0639        0.479      0.00499\n","    107    14      0.00578      0.00574      4.2e-05       0.0442       0.0586       0.0334       0.0657       0.0495       0.0428       0.0815       0.0622         0.48        0.005\n","    107    15      0.00585      0.00583     1.36e-05       0.0446       0.0591       0.0355       0.0627       0.0491       0.0449       0.0803       0.0626         0.27      0.00281\n","    107    16      0.00618      0.00617     5.73e-06       0.0451       0.0608       0.0342        0.067       0.0506       0.0443       0.0846       0.0644        0.169      0.00176\n","    107    17      0.00589      0.00587      1.6e-05       0.0442       0.0593       0.0345       0.0635        0.049       0.0436       0.0821       0.0628        0.296      0.00308\n","    107    18      0.00591      0.00591     2.91e-06       0.0438       0.0595       0.0332        0.065       0.0491       0.0426       0.0836       0.0631        0.104      0.00108\n","    107    19      0.00694      0.00691     2.76e-05       0.0485       0.0643       0.0385       0.0686       0.0535        0.049       0.0872       0.0681        0.388      0.00404\n","    107    20      0.00711      0.00711     6.12e-07       0.0506       0.0652       0.0411       0.0695       0.0553       0.0515       0.0864       0.0689       0.0445     0.000464\n","\n","validation\n","# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","    107     1      0.00668      0.00668     2.26e-06       0.0471       0.0632        0.036       0.0694       0.0527       0.0462       0.0878        0.067       0.0877     0.000913\n","    107     2      0.00668      0.00668     9.68e-07       0.0468       0.0632       0.0353         0.07       0.0526       0.0454       0.0887       0.0671       0.0468     0.000487\n","    107     3       0.0064       0.0064     1.33e-06       0.0457       0.0619       0.0343       0.0686       0.0514       0.0438       0.0875       0.0656       0.0703     0.000732\n","    107     4      0.00653      0.00653     1.57e-06        0.046       0.0625       0.0346       0.0687       0.0517        0.044       0.0886       0.0663       0.0794     0.000827\n","    107     5      0.00604      0.00604     8.26e-07       0.0449       0.0601       0.0344       0.0657       0.0501       0.0443       0.0832       0.0637       0.0449     0.000468\n","\n","\n","  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      O_f_mae  psavg_f_mae     H_f_rmse     O_f_rmse psavg_f_rmse        e_mae      e/N_mae\n","! Train             107  435.781    0.005      0.00617     1.65e-05      0.00619       0.0455       0.0608       0.0351       0.0663       0.0507       0.0448       0.0841       0.0644        0.257      0.00267\n","! Validation        107  435.781    0.005      0.00647     1.39e-06      0.00647       0.0461       0.0622       0.0349       0.0685       0.0517       0.0448       0.0872        0.066       0.0658     0.000686\n","Wall time: 435.781681125\n","! Best model      107    0.006\n","! Stop training: Early stopping: validation_loss has not reduced for 50 epochs\n","Wall time: 435.81221616999983\n","Cumulative wall time: 435.81221616999983\n"]}],"source":["!rm -rf ./results\n","!nequip-train /content/nequip/configs/water-example.yaml"]},{"cell_type":"markdown","metadata":{"id":"kJitSZgLYNNF"},"source":["### Deploy the model"]},{"cell_type":"markdown","metadata":{"id":"Lo_kIpYV00as"},"source":["<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/deploy.png?raw=true\" width=\"60%\">"]},{"cell_type":"markdown","metadata":{"id":"7VoeGtlA02KQ"},"source":["We now convert the model to a potential file. This makes it independent of NequIP and we can use it any downstream application, such as LAMMPS. "]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eLlagzVhrVGz","outputId":"c5a72d02-fa1b-4ae2-c4f4-457169ed1885","executionInfo":{"status":"ok","timestamp":1668712046780,"user_tz":-60,"elapsed":46,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["total 5.3M\n","-rw------- 1 root root 227K Nov 17 19:00 config.yaml\n","-rw-r--r-- 1 root root  489 Nov 17 19:00 metrics_initialization.csv\n","-rw-r--r-- 1 root root 517K Nov 17 19:07 metrics_batch_train.csv\n","-rw-r--r-- 1 root root 131K Nov 17 19:07 metrics_batch_val.csv\n","-rw-r--r-- 1 root root  45K Nov 17 19:07 metrics_epoch.csv\n","-rw------- 1 root root 657K Nov 17 19:07 best_model.pth\n","-rw-r--r-- 1 root root 594K Nov 17 19:07 log\n","-rw------- 1 root root 2.5M Nov 17 19:07 trainer.pth\n","-rw------- 1 root root 657K Nov 17 19:07 last_model.pth\n"]}],"source":["! ls -lrth results/water/example-run-water"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PJmFAbBzez3P","outputId":"396666b0-0e1f-40b8-9bbb-727eada33a0f","executionInfo":{"status":"ok","timestamp":1668712049346,"user_tz":-60,"elapsed":2569,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["usage: nequip-deploy [-h] [--verbose VERBOSE] {info,build} ...\n","\n","Create and view information about deployed NequIP potentials.\n","\n","optional arguments:\n","  -h, --help         show this help message and exit\n","  --verbose VERBOSE  log level\n","\n","commands:\n","  {info,build}\n","    info             Get information from a deployed model file\n","    build            Build a deployment model\n"]}],"source":["! nequip-deploy -h"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y3NJJgtDIDNc","outputId":"3cc8449b-0947-46ac-8f7d-a69140a37616","executionInfo":{"status":"ok","timestamp":1668712055933,"user_tz":-60,"elapsed":6602,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:root:Loading best_model from training session...\n","INFO:root:Compiled & optimized model.\n"]}],"source":["!nequip-deploy build --train-dir /content/results/water/example-run-water water-deploy.pth"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qN_Uk4ak62Ez","executionInfo":{"status":"ok","timestamp":1668712058065,"user_tz":-60,"elapsed":2135,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}},"outputId":"333ff5df-872d-4694-a6fb-ad934cd8ddb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["usage: nequip-deploy build [-h] [--model MODEL] [--train-dir TRAIN_DIR]\n","                           out_file\n","nequip-deploy build: error: argument --train-dir: expected one argument\n"]}],"source":["! nequip-deploy build --model water-example-with-stress.yaml --train -dir /content/results/water/example-run-water deployed-stress.pth"]},{"cell_type":"markdown","metadata":{"id":"UXpcE3oP0LyD"},"source":["## Evaluate Test Error on all remaining frames"]},{"cell_type":"markdown","metadata":{"id":"4wRKKCZ2PRl3"},"source":["Before running inference, we'd like to know how well the model is doing on a hold-out test set. We run the nequip-evaluate command to compute the test error on all data that we didn't use for training or validation. "]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mB54WSrN0PaS","outputId":"4507912d-e9c7-4ec5-e9f4-397ed8eb9ad9","executionInfo":{"status":"ok","timestamp":1668712140044,"user_tz":-60,"elapsed":81982,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","WARNING: please note that models running on CUDA are usually nondeterministc and that this manifests in the final test errors; for a _more_ deterministic result, please use `--device cpu`\n","Loading model... \n","loaded model from training session\n","Loading original dataset...\n","Loaded dataset specified in config.yaml.\n","Using origial training dataset (10000 frames) minus training (100 frames) and validation frames (50 frames), yielding a test set size of 9850 frames.\n","Starting...\n","  0% 0/9850 [00:00<?, ?it/s]\n","\u001b[A\n","  1% 50/9850 [00:00<01:46, 91.89it/s]\n","  1% 100/9850 [00:01<02:40, 60.74it/s]\n","  2% 150/9850 [00:03<03:47, 42.60it/s]\n","  2% 200/9850 [00:04<04:17, 37.51it/s]\n","  3% 250/9850 [00:05<03:07, 51.24it/s]\n","  3% 300/9850 [00:05<02:25, 65.77it/s]\n","  4% 350/9850 [00:05<01:58, 80.02it/s]\n","  4% 400/9850 [00:06<01:41, 93.32it/s]\n","  5% 450/9850 [00:06<01:29, 105.29it/s]\n","  5% 500/9850 [00:06<01:21, 115.14it/s]\n","  6% 550/9850 [00:07<01:15, 122.57it/s]\n","  6% 600/9850 [00:07<01:12, 128.37it/s]\n","  7% 650/9850 [00:07<01:09, 132.60it/s]\n","  7% 700/9850 [00:08<01:07, 135.58it/s]\n","  8% 750/9850 [00:08<01:05, 138.14it/s]\n","  8% 800/9850 [00:08<01:04, 140.06it/s]\n","  9% 850/9850 [00:09<01:03, 141.70it/s]\n","  9% 900/9850 [00:09<01:03, 141.96it/s]\n"," 10% 950/9850 [00:09<01:01, 143.58it/s]\n"," 10% 1000/9850 [00:10<01:01, 144.04it/s]\n"," 11% 1050/9850 [00:10<01:01, 144.24it/s]\n"," 11% 1100/9850 [00:10<01:00, 145.09it/s]\n"," 12% 1150/9850 [00:11<00:59, 145.89it/s]\n"," 12% 1200/9850 [00:11<00:58, 146.65it/s]\n"," 13% 1250/9850 [00:11<00:58, 147.09it/s]\n"," 13% 1300/9850 [00:12<00:57, 147.84it/s]\n"," 14% 1350/9850 [00:12<00:57, 148.01it/s]\n"," 14% 1400/9850 [00:12<00:57, 147.74it/s]\n"," 15% 1450/9850 [00:13<00:56, 147.46it/s]\n"," 15% 1500/9850 [00:13<00:56, 147.67it/s]\n"," 16% 1550/9850 [00:13<00:56, 148.01it/s]\n"," 16% 1600/9850 [00:14<00:55, 147.87it/s]\n"," 17% 1650/9850 [00:14<00:55, 148.15it/s]\n"," 17% 1700/9850 [00:14<00:55, 148.12it/s]\n"," 18% 1750/9850 [00:15<00:54, 147.52it/s]\n"," 18% 1800/9850 [00:15<00:54, 146.95it/s]\n"," 19% 1850/9850 [00:16<00:54, 146.80it/s]\n"," 19% 1900/9850 [00:16<00:53, 147.30it/s]\n"," 20% 1950/9850 [00:16<00:53, 147.13it/s]\n"," 20% 2000/9850 [00:17<00:53, 147.08it/s]\n"," 21% 2050/9850 [00:17<00:53, 146.88it/s]\n"," 21% 2100/9850 [00:17<00:52, 146.80it/s]\n"," 22% 2150/9850 [00:18<00:52, 147.02it/s]\n"," 22% 2200/9850 [00:18<00:52, 147.05it/s]\n"," 23% 2250/9850 [00:18<00:51, 147.16it/s]\n"," 23% 2300/9850 [00:19<00:51, 147.22it/s]\n"," 24% 2350/9850 [00:19<00:50, 147.38it/s]\n"," 24% 2400/9850 [00:19<00:50, 147.30it/s]\n"," 25% 2450/9850 [00:20<00:50, 147.57it/s]\n"," 25% 2500/9850 [00:20<00:49, 147.90it/s]\n"," 26% 2550/9850 [00:20<00:49, 147.39it/s]\n"," 26% 2600/9850 [00:21<00:49, 146.85it/s]\n"," 27% 2650/9850 [00:21<00:48, 147.16it/s]\n"," 27% 2700/9850 [00:21<00:48, 147.09it/s]\n"," 28% 2750/9850 [00:22<00:48, 147.31it/s]\n"," 28% 2800/9850 [00:22<00:47, 147.73it/s]\n"," 29% 2850/9850 [00:22<00:47, 147.50it/s]\n"," 29% 2900/9850 [00:23<00:47, 146.96it/s]\n"," 30% 2950/9850 [00:23<00:46, 147.24it/s]\n"," 30% 3000/9850 [00:23<00:46, 147.66it/s]\n"," 31% 3050/9850 [00:24<00:45, 147.91it/s]\n"," 31% 3100/9850 [00:24<00:45, 148.78it/s]\n"," 32% 3150/9850 [00:24<00:44, 148.90it/s]\n"," 32% 3200/9850 [00:25<00:44, 148.82it/s]\n"," 33% 3250/9850 [00:25<00:44, 149.15it/s]\n"," 34% 3300/9850 [00:25<00:44, 148.58it/s]\n"," 34% 3350/9850 [00:26<00:43, 148.16it/s]\n"," 35% 3400/9850 [00:26<00:43, 148.24it/s]\n"," 35% 3450/9850 [00:26<00:43, 148.22it/s]\n"," 36% 3500/9850 [00:27<00:42, 148.36it/s]\n"," 36% 3550/9850 [00:27<00:42, 147.96it/s]\n"," 37% 3600/9850 [00:27<00:42, 147.72it/s]\n"," 37% 3650/9850 [00:28<00:42, 147.32it/s]\n"," 38% 3700/9850 [00:28<00:41, 146.92it/s]\n"," 38% 3750/9850 [00:28<00:41, 146.59it/s]\n"," 39% 3800/9850 [00:29<00:41, 146.41it/s]\n"," 39% 3850/9850 [00:29<00:41, 146.33it/s]\n"," 40% 3900/9850 [00:29<00:40, 145.72it/s]\n"," 40% 3950/9850 [00:30<00:40, 145.60it/s]\n"," 41% 4000/9850 [00:30<00:40, 145.28it/s]\n"," 41% 4050/9850 [00:30<00:39, 145.16it/s]\n"," 42% 4100/9850 [00:31<00:39, 145.15it/s]\n"," 42% 4150/9850 [00:31<00:39, 144.70it/s]\n"," 43% 4200/9850 [00:31<00:39, 144.67it/s]\n"," 43% 4250/9850 [00:32<00:38, 144.40it/s]\n"," 44% 4300/9850 [00:32<00:38, 144.88it/s]\n"," 44% 4350/9850 [00:33<00:37, 145.26it/s]\n"," 45% 4400/9850 [00:33<00:37, 145.29it/s]\n"," 45% 4450/9850 [00:33<00:37, 144.99it/s]\n"," 46% 4500/9850 [00:34<00:36, 145.33it/s]\n"," 46% 4550/9850 [00:34<00:36, 145.11it/s]\n"," 47% 4600/9850 [00:34<00:36, 145.21it/s]\n"," 47% 4650/9850 [00:35<00:35, 145.10it/s]\n"," 48% 4700/9850 [00:35<00:35, 145.17it/s]\n"," 48% 4750/9850 [00:35<00:35, 145.07it/s]\n"," 49% 4800/9850 [00:36<00:34, 144.67it/s]\n"," 49% 4850/9850 [00:36<00:34, 144.58it/s]\n"," 50% 4900/9850 [00:36<00:34, 144.70it/s]\n"," 50% 4950/9850 [00:37<00:33, 144.77it/s]\n"," 51% 5000/9850 [00:37<00:33, 144.88it/s]\n"," 51% 5050/9850 [00:37<00:33, 144.76it/s]\n"," 52% 5100/9850 [00:38<00:32, 144.90it/s]\n"," 52% 5150/9850 [00:38<00:32, 144.83it/s]\n"," 53% 5200/9850 [00:38<00:32, 144.96it/s]\n"," 53% 5250/9850 [00:39<00:31, 145.12it/s]\n"," 54% 5300/9850 [00:39<00:31, 144.82it/s]\n"," 54% 5350/9850 [00:39<00:31, 144.82it/s]\n"," 55% 5400/9850 [00:40<00:30, 144.30it/s]\n"," 55% 5450/9850 [00:40<00:30, 144.13it/s]\n"," 56% 5500/9850 [00:40<00:30, 143.97it/s]\n"," 56% 5550/9850 [00:41<00:30, 143.26it/s]\n"," 57% 5600/9850 [00:41<00:29, 143.51it/s]\n"," 57% 5650/9850 [00:42<00:29, 143.69it/s]\n"," 58% 5700/9850 [00:42<00:28, 143.18it/s]\n"," 58% 5750/9850 [00:42<00:28, 142.97it/s]\n"," 59% 5800/9850 [00:43<00:28, 143.00it/s]\n"," 59% 5850/9850 [00:43<00:27, 143.13it/s]\n"," 60% 5900/9850 [00:43<00:27, 143.68it/s]\n"," 60% 5950/9850 [00:44<00:27, 143.76it/s]\n"," 61% 6000/9850 [00:44<00:26, 143.55it/s]\n"," 61% 6050/9850 [00:44<00:26, 143.41it/s]\n"," 62% 6100/9850 [00:45<00:26, 143.16it/s]\n"," 62% 6150/9850 [00:45<00:25, 143.32it/s]\n"," 63% 6200/9850 [00:45<00:25, 143.17it/s]\n"," 63% 6250/9850 [00:46<00:25, 143.01it/s]\n"," 64% 6300/9850 [00:46<00:24, 143.43it/s]\n"," 64% 6350/9850 [00:46<00:24, 143.46it/s]\n"," 65% 6400/9850 [00:47<00:23, 143.85it/s]\n"," 65% 6450/9850 [00:47<00:23, 144.02it/s]\n"," 66% 6500/9850 [00:47<00:23, 144.33it/s]\n"," 66% 6550/9850 [00:48<00:22, 144.37it/s]\n"," 67% 6600/9850 [00:48<00:22, 144.05it/s]\n"," 68% 6650/9850 [00:48<00:22, 143.21it/s]\n"," 68% 6700/9850 [00:49<00:21, 143.19it/s]\n"," 69% 6750/9850 [00:49<00:21, 142.52it/s]\n"," 69% 6800/9850 [00:50<00:21, 142.01it/s]\n"," 70% 6850/9850 [00:50<00:21, 141.77it/s]\n"," 70% 6900/9850 [00:50<00:20, 142.12it/s]\n"," 71% 6950/9850 [00:51<00:20, 142.53it/s]\n"," 71% 7000/9850 [00:51<00:19, 142.62it/s]\n"," 72% 7050/9850 [00:51<00:19, 142.62it/s]\n"," 72% 7100/9850 [00:52<00:19, 142.17it/s]\n"," 73% 7150/9850 [00:52<00:19, 140.63it/s]\n"," 73% 7200/9850 [00:52<00:18, 140.86it/s]\n"," 74% 7250/9850 [00:53<00:18, 140.83it/s]\n"," 74% 7300/9850 [00:53<00:18, 141.47it/s]\n"," 75% 7350/9850 [00:53<00:17, 141.72it/s]\n"," 75% 7400/9850 [00:54<00:17, 142.05it/s]\n"," 76% 7450/9850 [00:54<00:16, 142.66it/s]\n"," 76% 7500/9850 [00:54<00:16, 142.78it/s]\n"," 77% 7550/9850 [00:55<00:16, 142.99it/s]\n"," 77% 7600/9850 [00:55<00:15, 143.80it/s]\n"," 78% 7650/9850 [00:56<00:15, 144.08it/s]\n"," 78% 7700/9850 [00:56<00:14, 144.25it/s]\n"," 79% 7750/9850 [00:56<00:14, 144.43it/s]\n"," 79% 7800/9850 [00:57<00:14, 144.52it/s]\n"," 80% 7850/9850 [00:57<00:13, 144.76it/s]\n"," 80% 7900/9850 [00:57<00:13, 144.73it/s]\n"," 81% 7950/9850 [00:58<00:13, 144.74it/s]\n"," 81% 8000/9850 [00:58<00:12, 144.16it/s]\n"," 82% 8050/9850 [00:58<00:12, 143.85it/s]\n"," 82% 8100/9850 [00:59<00:12, 143.59it/s]\n"," 83% 8150/9850 [00:59<00:11, 143.76it/s]\n"," 83% 8200/9850 [00:59<00:11, 144.11it/s]\n"," 84% 8250/9850 [01:00<00:11, 144.03it/s]\n"," 84% 8300/9850 [01:00<00:10, 144.14it/s]\n"," 85% 8350/9850 [01:00<00:10, 144.43it/s]\n"," 85% 8400/9850 [01:01<00:10, 144.51it/s]\n"," 86% 8450/9850 [01:01<00:09, 144.45it/s]\n"," 86% 8500/9850 [01:01<00:09, 144.21it/s]\n"," 87% 8550/9850 [01:02<00:08, 144.56it/s]\n"," 87% 8600/9850 [01:02<00:08, 144.82it/s]\n"," 88% 8650/9850 [01:02<00:08, 145.09it/s]\n"," 88% 8700/9850 [01:03<00:07, 145.26it/s]\n"," 89% 8750/9850 [01:03<00:07, 145.00it/s]\n"," 89% 8800/9850 [01:03<00:07, 145.06it/s]\n"," 90% 8850/9850 [01:04<00:06, 144.69it/s]\n"," 90% 8900/9850 [01:04<00:06, 144.75it/s]\n"," 91% 8950/9850 [01:05<00:06, 144.64it/s]\n"," 91% 9000/9850 [01:05<00:05, 144.65it/s]\n"," 92% 9050/9850 [01:05<00:05, 144.44it/s]\n"," 92% 9100/9850 [01:06<00:05, 144.14it/s]\n"," 93% 9150/9850 [01:06<00:04, 143.98it/s]\n"," 93% 9200/9850 [01:06<00:04, 143.42it/s]\n"," 94% 9250/9850 [01:07<00:04, 143.76it/s]\n"," 94% 9300/9850 [01:07<00:03, 144.30it/s]\n"," 95% 9350/9850 [01:07<00:03, 144.55it/s]\n"," 95% 9400/9850 [01:08<00:03, 144.50it/s]\n"," 96% 9450/9850 [01:08<00:02, 143.97it/s]\n"," 96% 9500/9850 [01:08<00:02, 143.18it/s]\n"," 97% 9550/9850 [01:09<00:02, 142.79it/s]\n"," 97% 9600/9850 [01:09<00:01, 142.93it/s]\n"," 98% 9650/9850 [01:09<00:01, 142.84it/s]\n"," 98% 9700/9850 [01:10<00:01, 142.29it/s]\n"," 99% 9750/9850 [01:10<00:00, 141.91it/s]\n"," 99% 9800/9850 [01:10<00:00, 141.49it/s]\n","100% 9850/9850 [01:11<00:00, 138.12it/s]\n","\n","\n","--- Final result: ---\n","               f_mae =  0.046357           \n","              f_rmse =  0.062583           \n","             H_f_mae =  0.035020           \n","             O_f_mae =  0.069030           \n","         psavg_f_mae =  0.052025           \n","            H_f_rmse =  0.045198           \n","            O_f_rmse =  0.087545           \n","        psavg_f_rmse =  0.066372           \n","               e_mae =  0.059502           \n","             e/N_mae =  0.000620           \n","               f_mae =  0.046357           \n","              f_rmse =  0.062583           \n","             H_f_mae =  0.035020           \n","             O_f_mae =  0.069030           \n","         psavg_f_mae =  0.052025           \n","            H_f_rmse =  0.045198           \n","            O_f_rmse =  0.087545           \n","        psavg_f_rmse =  0.066372           \n","               e_mae =  0.059502           \n","             e/N_mae =  0.000620           \n"]}],"source":["!nequip-evaluate --train-dir results/water/example-run-water --batch-size 50"]},{"cell_type":"markdown","metadata":{"id":"HQHrMMnsPaJO"},"source":["Again, energy errors of < 1meV/atom (converted from kcal/mol to eV), and force errors of ~45 meV/A 🎉"]},{"cell_type":"markdown","metadata":{"id":"H4r5FBXaum9n"},"source":["# LAMMPS"]},{"cell_type":"markdown","metadata":{"id":"0qIYIYyr1B4O"},"source":["We are now in a position to run MD with our potential."]},{"cell_type":"markdown","metadata":{"id":"UirNBTlJ1BNZ"},"source":["<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/run.png?raw=true\" width=\"60%\">"]},{"cell_type":"markdown","metadata":{"id":"JQs0ijPhvAGb"},"source":["Set up a simple LAMMPS input file\n","\n","CAUTION: the reference data here are in eV for the energies and eV/A for the forces. The NequIP model will therefore also be predicting outputs in these units. We are therefore using `units metal` in LAMMPS (see [docs](https://docs.lammps.org/units.html)). Time units are also in`ps`)."]},{"cell_type":"markdown","metadata":{"id":"3AXv_y_JD1TJ"},"source":["### We now run MD in the NVT ensemble for 10 ps (20,000 steps)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"W090KfMsd2Do","executionInfo":{"status":"ok","timestamp":1668712140045,"user_tz":-60,"elapsed":5,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[],"source":["lammps_input_md = \"\"\"\n","units           metal\n","boundary        p p p\n","atom_style      atomic\n","thermo 1\n","newton off\n","read_data structure.data\n","\n","neighbor        1.0 bin\n","neigh_modify    every 10 delay 0 check no\n","\n","pair_style\tnequip\n","pair_coeff\t* * ../water-deploy.pth H O\n","mass            1 1.00794\n","mass            2 15.9994\n","\n","velocity        all create 300.0 23456789\n","timestep        0.0005\n","fix             1 all nvt temp 300.0 300.0 $(100.0*dt)\n","\n","#print log every X steps\n","thermo          100\n","thermo_style    custom step pe ke etotal temp press vol\n","\n","#print trajectory in xyz every X time units\n","dump              1 all xyz 20 water.xyz \n","dump_modify       1 element H O\n","\n","# dump            2 all custom 1 dump_frc.lamppstrj id type element fx fy fz\n","# dump_modify     2 element O H\n","# dump            3 all custom 1 dump.lammpstrj id type element x y z\n","# dump_modify     3 element O H\n","\n","run             20000\n","\"\"\"\n","\n","!mkdir lammps_run\n","with open(\"lammps_run/water_md.in\", \"w\") as f:\n","    f.write(lammps_input_md)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"Mu4kbiXOt1pI","executionInfo":{"status":"ok","timestamp":1668712140045,"user_tz":-60,"elapsed":4,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[],"source":["! cp /content/water-deploy.pth /content/lammps_run/."]},{"cell_type":"markdown","metadata":{"id":"AvWZCw1zvjRc"},"source":["We specify the initial water configuration by reading the extxyz file and writing to structure.data file easily parsed by lammps"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"eXeHb2ZPvIbU","executionInfo":{"status":"ok","timestamp":1668712140408,"user_tz":-60,"elapsed":366,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[],"source":["wat_pos_frc_trj = read(\"/content/nequip/datasets/AIMD_data/wat_pos_frc-10k.extxyz\")\n","write(\"/content/lammps_run/structure.data\", wat_pos_frc_trj, format=\"lammps-data\")"]},{"cell_type":"markdown","metadata":{"id":"QDuyueY11YBF"},"source":["### Run the LAMMPS command: "]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RG1LE98LukSO","outputId":"5c222314-92d9-4607-d2bb-d121a6073182","executionInfo":{"status":"ok","timestamp":1668712140757,"user_tz":-60,"elapsed":357,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","units           metal\n","boundary        p p p\n","atom_style      atomic\n","thermo 1\n","newton off\n","read_data structure.data\n","\n","neighbor        1.0 bin\n","neigh_modify    every 10 delay 0 check no\n","\n","pair_style\tnequip\n","pair_coeff\t* * ../water-deploy.pth H O\n","mass            1 1.00794\n","mass            2 15.9994\n","\n","velocity        all create 300.0 23456789\n","timestep        0.0005\n","fix             1 all nvt temp 300.0 300.0 $(100.0*dt)\n","\n","#print log every X steps\n","thermo          100\n","thermo_style    custom step pe ke etotal temp press vol\n","\n","#print trajectory in xyz every X time units\n","dump              1 all xyz 20 water.xyz \n","dump_modify       1 element H O\n","\n","# dump            2 all custom 1 dump_frc.lamppstrj id type element fx fy fz\n","# dump_modify     2 element O H\n","# dump            3 all custom 1 dump.lammpstrj id type element x y z\n","# dump_modify     3 element O H\n","\n","run             20000\n","/content/lammps_run/structure.data (written by ASE) \n","\n","96 \t atoms \n","2  atom types\n","0.0      9.8499999999999996  xlo xhi\n","0.0      9.8499999999999996  ylo yhi\n","0.0      9.8499999999999996  zlo zhi\n","\n","\n","Atoms \n","\n","     1   1      42.886169670000001   -0.055681660000000001      38.329161120000002\n","     2   1      34.202588720000001              -0.6185484      37.365568080000003\n","     3   1      30.080392589999999     -2.0124176500000002      36.480796079999998\n","     4   1      28.705791179999999     -2.6880392799999999      36.602098329999997\n","     5   1             36.24794267    -0.51634849000000005      34.492359610000001\n","     6   1      37.696472460000003   -0.041087279999999997      35.014073510000003\n","     7   1      27.760669979999999      7.4854206000000003      33.927691950000003\n","     8   1      28.816099959999999              6.49857774             34.21636084\n","     9   1      37.157637200000003      9.0188280800000005      31.926581209999998\n","    10   1      38.606381650000003      9.5820079600000003      32.343597279999997\n","    11   1      34.303195930000001      2.2195014400000002      45.988045190000001\n","    12   1      33.244413940000001              1.30253325      46.469842720000003\n","    13   1      38.728617479999997     -5.0541897699999998      26.074396839999999\n","    14   1      38.348392150000002     -6.2832846900000003      26.986725310000001\n","    15   1      32.864252090000001              3.20606327               30.897116\n","    16   1      31.290408809999999      3.0871834699999998             30.62739775\n","    17   1      33.751986969999997             -3.13832624      39.672760769999996\n","    18   1      34.664297990000001             -3.66438591      38.646602719999997\n","    19   1      42.717321439999999      5.1246883700000003      32.588340119999998\n","    20   1             41.56274552      5.5893544000000004      33.417490280000003\n","    21   1      32.428380089999997      9.1182520999999994             30.54776786\n","    22   1              32.6432407             10.77068308             30.48427787\n","    23   1      31.484867080000001      4.6777144699999997      37.395719499999998\n","    24   1      32.317188299999998     -6.2287496200000003      36.467186439999999\n","    25   1      26.662134099999999      3.1708123800000001      35.682014649999999\n","    26   1             26.52713675              1.60390403      35.488348299999998\n","    27   1      32.023823659999998      16.918208029999999      31.688356989999999\n","    28   1      31.400657970000001      7.0315610800000004      30.239455469999999\n","    29   1             33.52642531     -3.5594808100000002             34.26368308\n","    30   1      34.640485550000001             -3.26538336      35.497148240000001\n","    31   1      40.056437510000002    -0.30543864999999998      29.831207429999999\n","    32   1      39.478446419999997             -1.09483146      38.310114040000002\n","    33   1      39.704076149999999      1.9584631400000001      33.390237599999999\n","    34   1      38.333857080000001              2.69671781             42.92619457\n","    35   1      40.182045549999998     -7.2199289499999999      27.658039049999999\n","    36   1      39.320443179999998     -8.4564252700000004             28.13196589\n","    37   1             36.38769637      8.8117085900000003      38.354536240000002\n","    38   1      36.320563759999999      9.0063075799999996      36.752600139999998\n","    39   1      29.999158309999999     -5.5637817500000004      33.929505059999997\n","    40   1      30.772854550000002     -5.0385870199999996      35.199806719999998\n","    41   1      40.059251779999997      6.3305279499999996      28.257946189999998\n","    42   1      40.239836089999997      5.1745923999999999             29.29629568\n","    43   1             26.33209111      2.4393638599999998      33.565386850000003\n","    44   1      26.960697119999999      1.2711078899999999      32.592388440000001\n","    45   1      34.837269769999999    -0.47227084000000003      30.382436200000001\n","    46   1      35.396881370000003             -1.92684834      30.308183759999999\n","    47   1      32.121760719999997    -0.73334299000000003      36.510438299999997\n","    48   1      32.218084349999998      7.8454304099999996      35.667196779999998\n","    49   1             36.37809987     -4.3048878799999999              36.4539793\n","    50   1      35.811927560000001             -3.00139282      27.034893759999999\n","    51   1      29.645249140000001      1.0652123600000001      35.714365399999998\n","    52   1      30.379465499999998   -0.066814650000000003      34.988246869999998\n","    53   1             34.21493366             -1.65591205      33.887643709999999\n","    54   1      34.784243549999999     -1.0252141100000001      32.503483260000003\n","    55   1      40.464995450000004              1.14678254      31.307350320000001\n","    56   1      41.326246990000001     0.65508034999999998      32.455588290000001\n","    57   1             29.02108599      3.5038194900000001             39.90877029\n","    58   1      29.494542689999999      3.7276637300000002      41.376613800000001\n","    59   1      34.135966400000001     -6.7533422300000003      32.356841029999998\n","    60   1      34.954657009999998     -5.7704242399999996             31.45710669\n","    61   1      33.253235689999997      1.5268048299999999      44.056217189999998\n","    62   1      33.793166939999999     0.50146325999999997      43.059759010000001\n","    63   1      36.820540960000002      2.6214681999999998      40.683400659999997\n","    64   1      37.555270669999999              1.56498329             39.76489351\n","    65   2      43.209908720000001   -0.062845650000000003             47.25931559\n","    66   2      29.394058350000002     -2.3133019500000001      37.140788360000002\n","    67   2      36.741570840000001   -0.083871000000000001      35.259178339999998\n","    68   2             27.94247769      6.7622961500000001      34.564838440000003\n","    69   2      37.681265600000003      9.4216399800000001      32.647864349999999\n","    70   2      33.317129080000001      2.0951401700000001      45.872226509999997\n","    71   2      37.995135589999997      4.3611431200000004             26.55718199\n","    72   2      32.182467090000003      2.6611503399999998      30.457724819999999\n","    73   2      34.653801209999997     -3.4374573100000001      39.588924570000003\n","    74   2      42.292983370000002      5.9471069500000002      32.846099549999998\n","    75   2      32.960469009999997      9.9050313200000009      30.158730670000001\n","    76   2             31.42818866     -5.8338304000000001      36.673874359999999\n","    77   2             26.05637308      2.4973869199999998      35.348687040000002\n","    78   2      32.033492709999997      17.325228939999999      30.811601379999999\n","    79   2      33.825218239999998     -2.9520949600000002      35.022046070000002\n","    80   2      39.456998159999998    -0.30727594000000003      38.934782900000002\n","    81   2             29.48467089      2.8692561099999998      43.006186839999998\n","    82   2      39.286418410000003             -7.62061031      27.627114779999999\n","    83   2      35.879750280000003      8.6515870800000005      37.522173430000002\n","    84   2      30.358254389999999     -4.7607656800000004      34.335564570000003\n","    85   2      40.709895690000003      5.8331250199999998      28.755837589999999\n","    86   2      26.717908300000001      2.2415138300000002      32.657729799999998\n","    87   2      35.658925619999998    -0.99689035000000004      30.574953059999999\n","    88   2      31.585160200000001             -1.31218042      35.901110940000002\n","    89   2      35.548938669999998     -3.9056138900000001      26.821449000000001\n","    90   2      29.565661609999999     0.46817945999999999      34.967071199999999\n","    91   2             34.76151282    -0.95696802000000003      33.489136729999998\n","    92   2      40.485340669999999     0.40236209000000001             31.94254162\n","    93   2      29.672828979999998      4.0134825300000001               40.450578\n","    94   2      34.127228619999997     -5.8796882899999998      31.892542299999999\n","    95   2      33.116888420000002      1.2338084899999999      43.112771199999997\n","    96   2      37.199699350000003      2.5049007099999998      39.791712660000002\n"]}],"source":["! cat /content/lammps_run/water_md.in\n","! cat /content/lammps_run/structure.data"]},{"cell_type":"markdown","metadata":{"id":"plER-wpaFCCT"},"source":["## Run MD!"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gurLjNK5upvq","outputId":"bd88d22d-21e9-4d27-a7e1-469d6162349b","executionInfo":{"status":"ok","timestamp":1668712804350,"user_tz":-60,"elapsed":663596,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["LAMMPS (29 Sep 2021 - Update 2)\n","OMP_NUM_THREADS environment is not set. Defaulting to 1 thread. (src/comm.cpp:98)\n","  using 1 OpenMP thread(s) per MPI task\n","Reading data file ...\n","  orthogonal box = (0.0000000 0.0000000 0.0000000) to (9.8500000 9.8500000 9.8500000)\n","  1 by 1 by 1 MPI processor grid\n","  reading atoms ...\n","  96 atoms\n","  read_data CPU = 0.001 seconds\n","NEQUIP is using device cuda\n","NequIP Coeff: type 1 is element H\n","NequIP Coeff: type 2 is element O\n","Loading model from ../water-deploy.pth\n","Freezing TorchScript model...\n","Neighbor list info ...\n","  update every 10 steps, delay 0 steps, check no\n","  max neighbors/atom: 2000, page size: 100000\n","  master list distance cutoff = 5\n","  ghost atom cutoff = 5\n","  binsize = 2.5, bins = 4 4 4\n","  1 neighbor lists, perpetual/occasional/extra = 1 0 0\n","  (1) pair nequip, perpetual\n","      attributes: full, newton off\n","      pair build: full/bin/atomonly\n","      stencil: full/bin/3d\n","      bin: standard\n","Setting up Verlet run ...\n","  Unit style    : metal\n","  Current step  : 0\n","  Time step     : 0.0005\n","Per MPI rank memory allocation (min/avg/max) = 3.586 | 3.586 | 3.586 Mbytes\n","Step PotEng KinEng TotEng Temp Press Volume \n","       0   -14985.619    3.6839141   -14981.935          300    4117.3701    955.67162 \n","     100   -14985.152     3.612324    -14981.54    294.17005    4037.3566    955.67162 \n","     200   -14984.611    3.5536031   -14981.058    289.38811    3971.7265    955.67162 \n","     300   -14984.508    3.7110671   -14980.797     302.2112    4147.7179    955.67162 \n","     400   -14984.447    3.9257621   -14980.522    319.69492    4387.6744    955.67162 \n","     500   -14984.364    4.1368641   -14980.227    336.88604     4623.615    955.67162 \n","     600   -14983.877    3.9036365   -14979.973    317.89312    4362.9454    955.67162 \n","     700   -14983.338    4.0970836   -14979.241    333.64651    4579.1538    955.67162 \n","     800   -14983.479    4.3726452   -14979.107     356.0869    4887.1385    955.67162 \n","     900   -14983.754    3.7050978   -14980.049    301.72509    4141.0462    955.67162 \n","    1000   -14983.897    3.6030024   -14980.294    293.41095    4026.9382    955.67162 \n","    1100   -14984.224    3.6776204   -14980.546    299.48747    4110.3359    955.67162 \n","    1200   -14984.429    3.7831984   -14980.646    308.08522    4228.3363    955.67162 \n","    1300   -14984.685    3.6138912   -14981.071    294.29767    4039.1081    955.67162 \n","    1400   -14984.778    3.1407006   -14981.638    255.76334    3510.2411    955.67162 \n","    1500   -14985.055    3.5177092   -14981.537    286.46508    3931.6092    955.67162 \n","    1600   -14984.858    3.7259172   -14981.132    303.42053    4164.3154    955.67162 \n","    1700   -14984.539    3.4742883   -14981.065    282.92909    3883.0793    955.67162 \n","    1800    -14985.13    3.2131797   -14981.917    261.66568    3591.2482    955.67162 \n","    1900   -14985.279    3.1261023   -14982.153    254.57452    3493.9251    955.67162 \n","    2000   -14985.105    3.5462135   -14981.559    288.78633    3963.4674    955.67162 \n","    2100   -14984.312    3.2765919   -14981.036    266.82966    3662.1216    955.67162 \n","    2200   -14985.058    4.3769623   -14980.681    356.43846    4891.9635    955.67162 \n","    2300   -14984.551    3.6861064   -14980.865    300.17853    4119.8203    955.67162 \n","    2400    -14985.05    3.4404927   -14981.609    280.17695    3845.3073    955.67162 \n","    2500   -14984.776    3.1768726   -14981.599    258.70901    3550.6692    955.67162 \n","    2600   -14984.849    3.5970726   -14981.252    292.92805    4020.3107    955.67162 \n","    2700   -14984.456    3.4973393   -14980.959    284.80626    3908.8426    955.67162 \n","    2800   -14984.544    3.9722016   -14980.572    323.47672    4439.5779    955.67162 \n","    2900   -14984.508    3.9144584   -14980.593     318.7744    4375.0406    955.67162 \n","    3000   -14984.534    3.3283465   -14981.206     271.0443    3719.9657    955.67162 \n","    3100   -14984.602    2.9964865   -14981.605    244.01925    3349.0585    955.67162 \n","    3200   -14984.718    3.6737509   -14981.044    299.17236    4106.0111    955.67162 \n","    3300   -14984.778     4.107491   -14980.671    334.49403    4590.7858    955.67162 \n","    3400   -14984.731    4.0780811   -14980.653    332.09904    4557.9155    955.67162 \n","    3500   -14984.586    3.7604689   -14980.825    306.23425    4202.9324    955.67162 \n","    3600   -14984.553    3.2881342   -14981.265    267.76961    3675.0219    955.67162 \n","    3700   -14984.763     3.477299   -14981.285    283.17427    3886.4442    955.67162 \n","    3800   -14984.522    4.0002294   -14980.522    325.75917    4470.9036    955.67162 \n","    3900   -14984.785    4.3058628   -14980.479    350.64847    4812.4984    955.67162 \n","    4000   -14984.202    3.2663745   -14980.936    265.99761     3650.702    955.67162 \n","    4100   -14984.748    3.8593684   -14980.889    314.28814    4313.4687    955.67162 \n","    4200   -14984.486     3.962797   -14980.524    322.71086    4429.0668    955.67162 \n","    4300   -14984.459    4.6541762   -14979.805    379.01341     5201.795    955.67162 \n","    4400   -14983.649    3.6605157   -14979.989    298.09455    4091.2186    955.67162 \n","    4500   -14984.335    3.7143287   -14980.621    302.47681    4151.3633    955.67162 \n","    4600   -14984.321    3.5565305   -14980.765     289.6265    3974.9983    955.67162 \n","    4700   -14984.277    3.6328874   -14980.644    295.84464    4060.3395    955.67162 \n","    4800   -14984.116     3.536619    -14980.58    288.00501     3952.744    955.67162 \n","    4900   -14984.067      3.46779     -14980.6    282.39991    3875.8164    955.67162 \n","    5000   -14984.415    3.7825292   -14980.633    308.03073    4227.5884    955.67162 \n","    5100   -14984.566    3.4597203   -14981.107    281.74275    3866.7972    955.67162 \n","    5200   -14984.473    3.2240393   -14981.249    262.55004    3603.3856    955.67162 \n","    5300   -14984.331    3.7682545   -14980.563    306.86827    4211.6341    955.67162 \n","    5400   -14983.528    3.4062522   -14980.122    277.38856    3807.0379    955.67162 \n","    5500   -14983.453    3.6339775   -14979.819    295.93341    4061.5578    955.67162 \n","    5600    -14983.91    4.0985737   -14979.812    333.76785    4580.8193    955.67162 \n","    5700   -14983.989    3.4139926   -14980.575    278.01891    3815.6891    955.67162 \n","    5800    -14983.95    3.2971543   -14980.653    268.50416    3685.1033    955.67162 \n","    5900   -14983.851    3.5181097   -14980.332     286.4977    3932.0568    955.67162 \n","    6000   -14983.891    3.8737905   -14980.017    315.46261    4329.5877    955.67162 \n","    6100   -14984.548    4.0755889   -14980.472    331.89609    4555.1301    955.67162 \n","    6200   -14984.211    3.4520446   -14980.759    281.11768    3858.2184    955.67162 \n","    6300   -14983.976    3.3645639   -14980.611    273.99368    3760.4446    955.67162 \n","    6400   -14984.152    3.8288839   -14980.323    311.80563    4279.3973    955.67162 \n","    6500   -14984.061    3.7102239    -14980.35    302.14254    4146.7755    955.67162 \n","    6600    -14984.66    3.5345029   -14981.126    287.83268    3950.3789    955.67162 \n","    6700   -14984.645    3.3558336   -14981.289    273.28272     3750.687    955.67162 \n","    6800   -14984.591     3.668552   -14980.922    298.74899    4100.2005    955.67162 \n","    6900   -14984.059    3.4500296   -14980.609    280.95359    3855.9663    955.67162 \n","    7000   -14984.145    3.9232518   -14980.221    319.49049    4384.8687    955.67162 \n","    7100   -14983.592    3.5950264   -14979.997    292.76142    4018.0237    955.67162 \n","    7200   -14983.738    3.6558024   -14980.082    297.71071    4085.9506    955.67162 \n","    7300   -14983.522    2.9761686   -14980.546    242.36466      3326.35    955.67162 \n","    7400   -14984.009    3.4784852    -14980.53    283.27087      3887.77    955.67162 \n","    7500   -14983.463    3.3873412   -14980.076    275.84855    3785.9019    955.67162 \n","    7600    -14983.99    4.4480962   -14979.542    362.23126    4971.4672    955.67162 \n","    7700   -14983.761    4.1548785   -14979.606    338.35304     4643.749    955.67162 \n","    7800   -14983.569    3.5023228   -14980.067    285.21209    3914.4124    955.67162 \n","    7900     -14984.1    3.5258078   -14980.574     287.1246    3940.6607    955.67162 \n","    8000   -14984.181    3.4644994   -14980.716    282.13194    3872.1387    955.67162 \n","    8100   -14983.847     3.425918   -14980.421    278.99005    3829.0176    955.67162 \n","    8200   -14983.276    3.5679998   -14979.708     290.5605     3987.817    955.67162 \n","    8300    -14983.71    4.3019934   -14979.408    350.33336    4808.1737    955.67162 \n","    8400    -14983.79    3.7750133   -14980.015    307.41867    4219.1882    955.67162 \n","    8500   -14984.004    3.5256788   -14980.478    287.11409    3940.5165    955.67162 \n","    8600   -14984.098    3.5871102   -14980.511    292.11676    4009.1761    955.67162 \n","    8700   -14984.117     3.758519   -14980.359    306.07545    4200.7531    955.67162 \n","    8800   -14984.276    3.8838145   -14980.393    316.27891    4340.7911    955.67162 \n","    8900   -14984.431    3.4689211   -14980.962    282.49202    3877.0806    955.67162 \n","    9000   -14984.435    3.1695822   -14981.265    258.11532    3542.5209    955.67162 \n","    9100   -14984.769      3.53988   -14981.229    288.27056    3956.3887    955.67162 \n","    9200   -14984.793    3.7116492   -14981.081    302.25861    4148.3685    955.67162 \n","    9300   -14984.672    4.0937177   -14980.578     333.3724    4575.3918    955.67162 \n","    9400   -14983.942    3.8253302   -14980.117    311.51624    4275.4255    955.67162 \n","    9500    -14983.69    3.8483037   -14979.842    313.38708     4301.102    955.67162 \n","    9600   -14984.149    4.1192152    -14980.03    335.44879    4603.8894    955.67162 \n","    9700   -14984.782    3.8967015   -14980.886    317.32836    4355.1944    955.67162 \n","    9800   -14984.892    3.5873364   -14981.304    292.13518    4009.4289    955.67162 \n","    9900   -14984.663    3.2298993   -14981.433    263.02725    3609.9351    955.67162 \n","   10000   -14984.944    3.7592752   -14981.185    306.13704    4201.5983    955.67162 \n","   10100   -14984.396     3.931067   -14980.464    320.12692    4393.6034    955.67162 \n","   10200   -14984.202    3.9975193   -14980.205    325.53847    4467.8746    955.67162 \n","   10300    -14984.47    3.8658755   -14980.604    314.81804    4320.7413    955.67162 \n","   10400   -14984.578    3.2450926   -14981.333    264.26452    3626.9161    955.67162 \n","   10500   -14984.658    3.3900907   -14981.268    276.07245    3788.9748    955.67162 \n","   10600   -14984.618    3.4953637   -14981.123    284.64537    3906.6345    955.67162 \n","   10700   -14984.941    4.0090713   -14980.932    326.47921    4480.7858    955.67162 \n","   10800   -14984.702    3.7599402   -14980.942    306.19119    4202.3415    955.67162 \n","   10900   -14984.547    3.4362485   -14981.111    279.83132    3840.5637    955.67162 \n","   11000   -14984.585    3.5285763   -14981.056    287.35005    3943.7549    955.67162 \n","   11100   -14984.994    4.1217999   -14980.872    335.65928    4606.7783    955.67162 \n","   11200   -14984.458    3.6385142   -14980.819    296.30285    4066.6283    955.67162 \n","   11300   -14984.473     4.023416   -14980.449    327.64737    4496.8183    955.67162 \n","   11400   -14984.494     3.725143   -14980.769    303.35748      4163.45    955.67162 \n","   11500   -14984.652    3.3501262   -14981.302    272.81794    3744.3081    955.67162 \n","   11600   -14984.241    3.2684219   -14980.973    266.16434    3652.9903    955.67162 \n","   11700   -14984.084    3.5535533    -14980.53    289.38405    3971.6708    955.67162 \n","   11800   -14984.284    3.8922695   -14980.392    316.96744    4350.2409    955.67162 \n","   11900   -14984.331    3.6666571   -14980.664    298.59467    4098.0826    955.67162 \n","   12000   -14984.934    3.5772314   -14981.356    291.31228    3998.1349    955.67162 \n","   12100   -14985.116    3.5266536    -14981.59    287.19347     3941.606    955.67162 \n","   12200   -14984.967    3.4433802   -14981.523    280.41209    3848.5345    955.67162 \n","   12300   -14984.903    3.3982915   -14981.505    276.74029    3798.1406    955.67162 \n","   12400   -14984.548    3.6213453   -14980.927     294.9047    4047.4393    955.67162 \n","   12500   -14984.621    4.2844384   -14980.337    348.90377    4788.5531    955.67162 \n","   12600   -14984.237    3.6851675   -14980.552    300.10207    4118.7709    955.67162 \n","   12700     -14984.4    3.6368625   -14980.764    296.16835    4064.7823    955.67162 \n","   12800    -14984.52    3.4973799   -14981.022    284.80956    3908.8879    955.67162 \n","   12900   -14984.696    3.2396369   -14981.457    263.82023    3620.8184    955.67162 \n","   13000   -14984.997     3.299053   -14981.698    268.65878    3687.2255    955.67162 \n","   13100   -14984.814    3.6188462   -14981.196    294.70118    4044.6461    955.67162 \n","   13200   -14984.178    3.6424885   -14980.535     296.6265    4071.0702    955.67162 \n","   13300   -14984.447    3.9645642   -14980.483    322.85477    4431.0419    955.67162 \n","   13400   -14984.956    4.1257586    -14980.83    335.98166    4611.2028    955.67162 \n","   13500   -14984.926    3.3928188   -14981.533    276.29462     3792.024    955.67162 \n","   13600   -14984.691    3.0630297   -14981.628     249.4382    3423.4313    955.67162 \n","   13700   -14984.704    3.4372262   -14981.267    279.91094    3841.6564    955.67162 \n","   13800   -14984.605    3.8618042   -14980.744     314.4865     4316.191    955.67162 \n","   13900   -14984.407     3.773435   -14980.634    307.29014    4217.4241    955.67162 \n","   14000   -14984.873    3.8850131   -14980.988    316.37652    4342.1307    955.67162 \n","   14100   -14984.923    3.5089039   -14981.414    285.74802    3921.7678    955.67162 \n","   14200   -14985.082    2.9849743   -14982.097    243.08175    3336.1917    955.67162 \n","   14300   -14985.361    3.0920097   -14982.269    251.79819    3455.8211    955.67162 \n","   14400    -14985.16    3.5775753   -14981.583    291.34029    3998.5193    955.67162 \n","   14500   -14985.403    4.1056884   -14981.298    334.34724     4588.771    955.67162 \n","   14600   -14984.406    3.2728582   -14981.133    266.52561    3657.9486    955.67162 \n","   14700   -14984.657    3.8454793   -14980.812    313.15708    4297.9454    955.67162 \n","   14800   -14984.711    3.9275043   -14980.783    319.83679    4389.6215    955.67162 \n","   14900   -14984.263    3.0799917   -14981.183     250.8195    3442.3891    955.67162 \n","   15000   -14985.408    3.8775251   -14981.531    315.76674    4333.7617    955.67162 \n","   15100   -14985.256    3.6823172   -14981.574    299.86995    4115.5853    955.67162 \n","   15200   -14985.045    4.2202173   -14980.825    343.67391    4716.7757    955.67162 \n","   15300   -14984.421    3.8619987   -14980.559    314.50234    4316.4084    955.67162 \n","   15400   -14985.275    4.4283174   -14980.847    360.62057    4949.3611    955.67162 \n","   15500   -14984.497    3.0935647   -14981.404    251.92482    3457.5591    955.67162 \n","   15600   -14984.555    3.1954462   -14981.359    260.22155    3571.4281    955.67162 \n","   15700   -14984.995    3.8426339   -14981.152    312.92536    4294.7651    955.67162 \n","   15800   -14984.856    3.6886186   -14981.168    300.38311    4122.6281    955.67162 \n","   15900   -14985.006    3.2831138   -14981.723    267.36078    3669.4109    955.67162 \n","   16000    -14985.18    3.1636944   -14982.016    257.63584    3535.9404    955.67162 \n","   16100    -14985.26    3.6535205   -14981.606    297.52489    4083.4003    955.67162 \n","   16200   -14984.391    3.4733294   -14980.917    282.85101    3882.0076    955.67162 \n","   16300   -14984.856    4.0290471   -14980.827    328.10595     4503.112    955.67162 \n","   16400   -14984.674    3.7250707   -14980.949    303.35159    4163.3692    955.67162 \n","   16500   -14984.938    3.5512976   -14981.387    289.20035    3969.1496    955.67162 \n","   16600    -14985.05    3.1098099    -14981.94    253.24775    3475.7157    955.67162 \n","   16700    -14985.19    3.4879822   -14981.702    284.04426    3898.3845    955.67162 \n","   16800   -14985.293    4.2148462   -14981.078    343.23652    4710.7725    955.67162 \n","   16900   -14985.089    3.9121475   -14981.177    318.58621    4372.4578    955.67162 \n","   17000   -14984.912    3.4744971   -14981.438     282.9461    3883.3127    955.67162 \n","   17100   -14985.055    3.8989254   -14981.156    317.50947    4357.6799    955.67162 \n","   17200     -14984.9    4.1365359   -14980.764    336.85931    4623.2481    955.67162 \n","   17300   -14984.222    3.5821081    -14980.64    291.70941    4003.5854    955.67162 \n","   17400   -14984.497    4.0212782   -14980.476    327.47329    4494.4291    955.67162 \n","   17500   -14984.945    3.9874974   -14980.958    324.72234    4456.6736    955.67162 \n","   17600   -14985.405     3.912428   -14981.493    318.60906    4372.7713    955.67162 \n","   17700   -14985.224    3.5222804   -14981.701    286.83734    3936.7182    955.67162 \n","   17800    -14984.87    3.3011389   -14981.569    268.82865    3689.5568    955.67162 \n","   17900   -14984.663    3.8555475   -14980.808    313.97699    4309.1982    955.67162 \n","   18000   -14984.338    3.5823557   -14980.756    291.72958    4003.8621    955.67162 \n","   18100   -14984.818    3.7929416   -14981.025    308.87866    4239.2259    955.67162 \n","   18200   -14984.792    3.6545085   -14981.137    297.60535    4084.5046    955.67162 \n","   18300   -14984.994    4.0522548   -14980.942    329.99586    4529.0503    955.67162 \n","   18400   -14984.655    4.4135031   -14980.242    359.41416    4932.8038    955.67162 \n","   18500   -14983.652    3.7771038   -14979.875    307.58891    4221.5247    955.67162 \n","   18600    -14983.83     3.577896   -14980.252     291.3664    3998.8777    955.67162 \n","   18700   -14984.778    4.0194088   -14980.759    327.32105    4492.3397    955.67162 \n","   18800   -14984.897    3.8480818   -14981.049    313.36901     4300.854    955.67162 \n","   18900   -14984.701    3.7057984   -14980.995    301.78215    4141.8293    955.67162 \n","   19000   -14984.053    4.0634845   -14979.989    330.91036    4541.6014    955.67162 \n","   19100   -14984.279     4.008503   -14980.271    326.43293    4480.1506    955.67162 \n","   19200   -14984.741    3.7543584   -14980.987    305.73664    4196.1029    955.67162 \n","   19300   -14984.864    3.5195788   -14981.345    286.61734    3933.6988    955.67162 \n","   19400   -14985.299    3.7034369   -14981.595    301.58984      4139.19    955.67162 \n","   19500   -14985.022     3.484341   -14981.538    283.74774    3894.3148    955.67162 \n","   19600    -14984.61    3.9223821   -14980.688    319.41967    4383.8966    955.67162 \n","   19700   -14984.486    3.8480954   -14980.638    313.37012    4300.8692    955.67162 \n","   19800   -14984.431    3.3908344    -14981.04    276.13302    3789.8061    955.67162 \n","   19900   -14985.003    3.6527377    -14981.35    297.46114    4082.5254    955.67162 \n","   20000   -14984.908    3.3878032    -14981.52    275.88617    3786.4182    955.67162 \n","Loop time of 657.932 on 1 procs for 20000 steps with 96 atoms\n","\n","Performance: 1.313 ns/day, 18.276 hours/ns, 30.398 timesteps/s\n","99.8% CPU use with 1 MPI tasks x 1 OpenMP threads\n","\n","MPI task timing breakdown:\n","Section |  min time  |  avg time  |  max time  |%varavg| %total\n","---------------------------------------------------------------\n","Pair    | 656.56     | 656.56     | 656.56     |   0.0 | 99.79\n","Neigh   | 0.69735    | 0.69735    | 0.69735    |   0.0 |  0.11\n","Comm    | 0.15569    | 0.15569    | 0.15569    |   0.0 |  0.02\n","Output  | 0.13226    | 0.13226    | 0.13226    |   0.0 |  0.02\n","Modify  | 0.31541    | 0.31541    | 0.31541    |   0.0 |  0.05\n","Other   |            | 0.06983    |            |       |  0.01\n","\n","Nlocal:        96.0000 ave          96 max          96 min\n","Histogram: 1 0 0 0 0 0 0 0 0 0\n","Nghost:        688.000 ave         688 max         688 min\n","Histogram: 1 0 0 0 0 0 0 0 0 0\n","Neighs:         0.00000 ave           0 max           0 min\n","Histogram: 1 0 0 0 0 0 0 0 0 0\n","FullNghs:      5030.00 ave        5030 max        5030 min\n","Histogram: 1 0 0 0 0 0 0 0 0 0\n","\n","Total # of neighbors = 5030\n","Ave neighs/atom = 52.395833\n","Neighbor list builds = 2000\n","Dangerous builds not checked\n","Total wall time: 0:11:02\n","[a146eb1954fb:03087] *** Process received signal ***\n","[a146eb1954fb:03087] Signal: Segmentation fault (11)\n","[a146eb1954fb:03087] Signal code: Address not mapped (1)\n","[a146eb1954fb:03087] Failing at address: 0x7f7fe197c20d\n","[a146eb1954fb:03087] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f7fe482a980]\n","[a146eb1954fb:03087] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7f7fe4469775]\n","[a146eb1954fb:03087] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7f7fe4cd4e44]\n","[a146eb1954fb:03087] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7f7fe446a605]\n","[a146eb1954fb:03087] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7f7fe4cd2cb3]\n","[a146eb1954fb:03087] *** End of error message ***\n"]}],"source":["!cd /content/lammps_run/ && ../lammps/build/lmp -in water_md.in"]},{"cell_type":"markdown","metadata":{"id":"ETDEkUHc39u4"},"source":["### Visualize the trajectory with ase and ngl"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"YGEqNBu2fmKQ","executionInfo":{"status":"ok","timestamp":1668712804351,"user_tz":-60,"elapsed":5,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[],"source":["from ase.visualize import view"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"QYxvyvyK2ctM","executionInfo":{"status":"ok","timestamp":1668712804351,"user_tz":-60,"elapsed":4,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[],"source":["wat_traj = read(\"/content/lammps_run/water.xyz\", index=\"::10\")"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"1G_nFd7dgWhc","executionInfo":{"status":"ok","timestamp":1668712804925,"user_tz":-60,"elapsed":577,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[],"source":["for i in range(len(wat_traj)):\n","    wat_traj[i].cell = cell_vec_abc\n","    wat_traj[i].pbc = np.array([True, True, True])"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"cZLtkZ7bdszJ","executionInfo":{"status":"ok","timestamp":1668712804926,"user_tz":-60,"elapsed":56,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[],"source":["from google.colab import output\n","\n","output.enable_custom_widget_manager()"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":517,"referenced_widgets":["2496e29867a645a8bd36716d3f84207a","d0536f318cdc4cbe99568bb5e49acddc","937d4dab5bc44537899ded77688af525","2367c267599e4934b35f60d6527e0c02","f4fd7d83dd604ab58c49fe1918864ca8","740a307f2d2840158a4a595b05790e2f","f2c68e39f04241e8a2bb8fe7083affd8","d7899215c668451ebfb1eea2e15075d9","9cdc2060c25a4569892a9477578d4626","bf27d5ebba8f4dd1b943dde2816e17db","cdfb132b6041499c9a8bd7b085c5b83a","bcce3d2ae3f14555ba2bca117ba9861a","ddfc1304402e40ac9c37541f0d57a278","21734170ce974a9698ddc8ab35adeb3f","03f3727220834b2eb5d44e1420789043","5ef1e44123f84da18021f44cdc8af6d4","1565e6d619c2498fb64ddc72943dc5c0","482923ba20e84495a502bda7a8d8bf5b","f7f693793c704790aea2197d2501c8d8","ed3b8d237ad641079c75d0dba8064f85","b7363bd641eb46f781a4b7342dda3926","98c82b80dfd943bfbdd029f80f340b8a","dcf28aa26c7148e39cc898434e12d4f7","fa8be0046f18429ca2cc286dc3004839","17bc66541a4e474798efb323e30ab63b","bd3ac267a17a45d697071d1904c00f8b","173a62d58ae144c38b1a55228f0071b5","f9d40143745a49d38c8324d09ba9b1cb","1c47d1bc5c6940868dcca94f1c38a36c"]},"id":"-P5s2IbvfocL","outputId":"d70b2220-87a9-4f03-c2a0-b9b5b47255b0","executionInfo":{"status":"ok","timestamp":1668712804926,"user_tz":-60,"elapsed":49,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["HBox(children=(NGLWidget(max_frame=100), VBox(children=(Dropdown(description='Show', options=('All', 'H', 'O')…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2496e29867a645a8bd36716d3f84207a"}},"metadata":{"application/vnd.jupyter.widget-view+json":{"colab":{"custom_widget_manager":{"url":"https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/d2e234f7cc04bf79/manager.min.js"}}}}}],"source":["view(wat_traj, viewer=\"ngl\")"]},{"cell_type":"markdown","metadata":{"id":"1CMwyeFr4BAZ"},"source":["### Calculate radial distribution function with MDAnalysis"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QOo4fyaB8jAi","outputId":"9997dd8f-c559-4ee9-af4b-b2ba36957286","executionInfo":{"status":"ok","timestamp":1668712891689,"user_tz":-60,"elapsed":86767,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting MDAnalysis\n","  Downloading MDAnalysis-2.1.0.tar.gz (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 5.1 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (1.21.6)\n","Requirement already satisfied: networkx>=1.0 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (2.6.3)\n","Collecting gsd>=1.4.0\n","  Downloading gsd-2.6.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n","\u001b[K     |████████████████████████████████| 376 kB 72.7 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (3.2.2)\n","Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (1.2.0)\n","Collecting GridDataFormats>=0.4.0\n","  Downloading GridDataFormats-0.7.0-py2.py3-none-any.whl (2.1 MB)\n","\u001b[K     |████████████████████████████████| 2.1 MB 44.6 MB/s \n","\u001b[?25hCollecting mmtf-python>=1.0.0\n","  Downloading mmtf_python-1.1.3-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (1.7.3)\n","Requirement already satisfied: tqdm>=4.43.0 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (4.64.1)\n","Collecting biopython>=1.71\n","  Downloading biopython-1.79-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 23.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (21.3)\n","Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.7/dist-packages (from MDAnalysis) (3.1.0)\n","Collecting mrcfile\n","  Downloading mrcfile-1.4.3-py2.py3-none-any.whl (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 2.3 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GridDataFormats>=0.4.0->MDAnalysis) (1.15.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.1->MDAnalysis) (4.1.1)\n","Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from mmtf-python>=1.0.0->MDAnalysis) (1.0.4)\n","Building wheels for collected packages: MDAnalysis\n","  Building wheel for MDAnalysis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for MDAnalysis: filename=MDAnalysis-2.1.0-cp37-cp37m-linux_x86_64.whl size=4649734 sha256=fdc9aa1fd2fc56bd4bdfa4e72f008dd260c18261ad2fc33b20e96de5f364ff62\n","  Stored in directory: /root/.cache/pip/wheels/fa/dd/6b/9d51e7216a401b71949467a123e3b2dffba11256346f7f7bda\n","Successfully built MDAnalysis\n","Installing collected packages: mrcfile, mmtf-python, gsd, GridDataFormats, biopython, MDAnalysis\n","Successfully installed GridDataFormats-0.7.0 MDAnalysis-2.1.0 biopython-1.79 gsd-2.6.1 mmtf-python-1.1.3 mrcfile-1.4.3\n"]}],"source":["!pip3 install MDAnalysis"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"RV4F1NHf9WUo","executionInfo":{"status":"ok","timestamp":1668712892065,"user_tz":-60,"elapsed":387,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[],"source":["import MDAnalysis as mda\n","from MDAnalysis.analysis import rdf"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"LtqT9im4AC15","executionInfo":{"status":"ok","timestamp":1668712892066,"user_tz":-60,"elapsed":4,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[],"source":["reader = mda.coordinates.XYZ.XYZReader(\"/content/lammps_run/water.xyz\")\n","topology = mda.topology.XYZParser.XYZParser(\"/content/lammps_run/water.xyz\")\n","\n","u = mda.Universe(\"/content/lammps_run/water.xyz\")\n","\n","u.dimensions = [cell_vec_abc[0], cell_vec_abc[1], cell_vec_abc[2], 90.0, 90.0, 90.0]"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GkoMQR8ZA0Sc","outputId":"cf7ee752-023a-441d-b399-8abaa5ded9d8","executionInfo":{"status":"ok","timestamp":1668712894197,"user_tz":-60,"elapsed":2134,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/MDAnalysis/coordinates/base.py:892: UserWarning: Reader has no dt information, set to 1.0 ps\n","  warnings.warn(\"Reader has no dt information, set to 1.0 ps\")\n"]},{"output_type":"execute_result","data":{"text/plain":["<MDAnalysis.analysis.rdf.InterRDF at 0x7fbc72c74590>"]},"metadata":{},"execution_count":40}],"source":["O_at = u.select_atoms(\"name O\")\n","H_at = u.select_atoms(\"name H\")\n","\n","Ordf = rdf.InterRDF(\n","    O_at,\n","    O_at,\n","    nbins=75,  # default\n","    range=(0.00001, 4.9),  # distance in angstroms\n",")\n","Ordf.run()\n","\n","OHrdf = rdf.InterRDF(\n","    O_at,\n","    H_at,\n","    nbins=75,  # default\n","    range=(0.00001, 4.9),  # distance in angstroms\n",")\n","OHrdf.run()\n","\n","HHrdf = rdf.InterRDF(\n","    H_at,\n","    H_at,\n","    nbins=75,  # default\n","    range=(0.00001, 4.9),  # distance in angstroms\n",")\n","HHrdf.run()"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"Og07Lts5C1q_","outputId":"074d80fc-4e3e-44bd-b1e9-d1d0566ac1dc","executionInfo":{"status":"ok","timestamp":1668712894721,"user_tz":-60,"elapsed":528,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:417: DeprecationWarning: The `bins` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.bins` instead\n","  warnings.warn(wmsg, DeprecationWarning)\n","/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:425: DeprecationWarning: The `rdf` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.rdf` instead\n","  warnings.warn(wmsg, DeprecationWarning)\n"]},{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'Radial distribution')"]},"metadata":{},"execution_count":41},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(Ordf.bins, Ordf.rdf)\n","plt.xlabel(\"Radius (angstrom)\")\n","plt.ylabel(\"Radial distribution\")"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"w8dxSzw_L_3z","outputId":"a6b62ef5-62d0-4ba8-e90e-a75693b3a5c4","executionInfo":{"status":"ok","timestamp":1668712894722,"user_tz":-60,"elapsed":15,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:417: DeprecationWarning: The `bins` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.bins` instead\n","  warnings.warn(wmsg, DeprecationWarning)\n","/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:425: DeprecationWarning: The `rdf` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.rdf` instead\n","  warnings.warn(wmsg, DeprecationWarning)\n"]},{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'Radial distribution')"]},"metadata":{},"execution_count":42},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["plt.plot(OHrdf.bins, OHrdf.rdf)\n","plt.xlabel(\"Radius (angstrom)\")\n","plt.ylabel(\"Radial distribution\")"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"YeT-d_udMAE0","outputId":"2c294291-e5bd-464b-dbaa-4b7cfae757c4","executionInfo":{"status":"ok","timestamp":1668712895039,"user_tz":-60,"elapsed":323,"user":{"displayName":"Gabriele T.","userId":"11617781626997935386"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:417: DeprecationWarning: The `bins` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.bins` instead\n","  warnings.warn(wmsg, DeprecationWarning)\n","/usr/local/lib/python3.7/dist-packages/MDAnalysis/analysis/rdf.py:425: DeprecationWarning: The `rdf` attribute was deprecated in MDAnalysis 2.0.0 and will be removed in MDAnalysis 3.0.0. Please use `results.rdf` instead\n","  warnings.warn(wmsg, DeprecationWarning)\n"]},{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'Radial distribution')"]},"metadata":{},"execution_count":43},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["plt.plot(HHrdf.bins, HHrdf.rdf)\n","plt.xlabel(\"Radius (angstrom)\")\n","plt.ylabel(\"Radial distribution\")"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","interpreter":{"hash":"c9be9acec9edbd902b751bf46a8fbd7b71bbc5f0438c72d3ebaee4bffeb5e5e4"},"kernelspec":{"display_name":"Python 3.7.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"baaa03010f81450c97509f0242e8dcf0":{"model_module":"nglview-js-widgets","model_name":"ColormakerRegistryModel","model_module_version":"3.0.1","state":{"_dom_classes":[],"_model_module":"nglview-js-widgets","_model_module_version":"3.0.1","_model_name":"ColormakerRegistryModel","_msg_ar":[],"_msg_q":[],"_ready":false,"_view_count":null,"_view_module":"nglview-js-widgets","_view_module_version":"3.0.1","_view_name":"ColormakerRegistryView","layout":"IPY_MODEL_1cbdfcfd35104437bda558bdf9a2e49f"}},"2496e29867a645a8bd36716d3f84207a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d0536f318cdc4cbe99568bb5e49acddc","IPY_MODEL_937d4dab5bc44537899ded77688af525"],"layout":"IPY_MODEL_2367c267599e4934b35f60d6527e0c02"}},"d0536f318cdc4cbe99568bb5e49acddc":{"model_module":"nglview-js-widgets","model_name":"NGLModel","model_module_version":"3.0.1","state":{"_camera_orientation":[18.783115818947113,0,0,0,0,18.783115818947113,0,0,0,0,18.783115818947113,0,-4.905500038526952,-4.932000007480383,-4.892499856650829,1],"_camera_str":"orthographic","_dom_classes":[],"_gui_theme":null,"_ibtn_fullscreen":"IPY_MODEL_f7f693793c704790aea2197d2501c8d8","_igui":null,"_iplayer":"IPY_MODEL_ed3b8d237ad641079c75d0dba8064f85","_model_module":"nglview-js-widgets","_model_module_version":"3.0.1","_model_name":"NGLModel","_ngl_color_dict":{},"_ngl_coordinate_resource":{},"_ngl_full_stage_parameters":{"impostor":true,"quality":"medium","workerDefault":true,"sampleLevel":0,"backgroundColor":"white","rotateSpeed":2,"zoomSpeed":1.2,"panSpeed":1,"clipNear":0,"clipFar":100,"clipDist":0,"fogNear":50,"fogFar":100,"cameraFov":40,"cameraEyeSep":0.3,"cameraType":"orthographic","lightColor":14540253,"lightIntensity":1,"ambientColor":14540253,"ambientIntensity":0.2,"hoverTimeout":0,"tooltip":true,"mousePreset":"default"},"_ngl_msg_archive":[{"target":"Stage","type":"call_method","methodName":"loadFile","reconstruc_color_scheme":false,"args":[{"type":"blob","data":"CRYST1    9.850    9.850    9.850  90.00  90.00  90.00 P 1\nMODEL     1\nATOM      1    H MOL     1       3.486   9.794   8.779  1.00  0.00           H  \nATOM      2    H MOL     1       4.653   9.231   7.816  1.00  0.00           H  \nATOM      3    H MOL     1       0.530   7.838   6.931  1.00  0.00           H  \nATOM      4    H MOL     1       9.006   7.162   7.052  1.00  0.00           H  \nATOM      5    H MOL     1       6.698   9.334   4.942  1.00  0.00           H  \nATOM      6    H MOL     1       8.146   9.809   5.464  1.00  0.00           H  \nATOM      7    H MOL     1       8.061   7.485   4.378  1.00  0.00           H  \nATOM      8    H MOL     1       9.116   6.499   4.666  1.00  0.00           H  \nATOM      9    H MOL     1       7.608   9.019   2.377  1.00  0.00           H  \nATOM     10    H MOL     1       9.056   9.582   2.794  1.00  0.00           H  \nATOM     11    H MOL     1       4.753   2.220   6.588  1.00  0.00           H  \nATOM     12    H MOL     1       3.694   1.303   7.070  1.00  0.00           H  \nATOM     13    H MOL     1       9.179   4.796   6.374  1.00  0.00           H  \nATOM     14    H MOL     1       8.798   3.567   7.287  1.00  0.00           H  \nATOM     15    H MOL     1       3.314   3.206   1.347  1.00  0.00           H  \nATOM     16    H MOL     1       1.740   3.087   1.077  1.00  0.00           H  \nATOM     17    H MOL     1       4.202   6.712   0.273  1.00  0.00           H  \nATOM     18    H MOL     1       5.114   6.186   9.097  1.00  0.00           H  \nATOM     19    H MOL     1       3.317   5.125   3.038  1.00  0.00           H  \nATOM     20    H MOL     1       2.163   5.589   3.867  1.00  0.00           H  \nATOM     21    H MOL     1       2.878   9.118   0.998  1.00  0.00           H  \nATOM     22    H MOL     1       3.093   0.921   0.934  1.00  0.00           H  \nATOM     23    H MOL     1       1.935   4.678   7.846  1.00  0.00           H  \nATOM     24    H MOL     1       2.767   3.621   6.917  1.00  0.00           H  \nATOM     25    H MOL     1       6.962   3.171   6.132  1.00  0.00           H  \nATOM     26    H MOL     1       6.827   1.604   5.938  1.00  0.00           H  \nATOM     27    H MOL     1       2.474   7.068   2.138  1.00  0.00           H  \nATOM     28    H MOL     1       1.851   7.032   0.689  1.00  0.00           H  \nATOM     29    H MOL     1       3.976   6.291   4.714  1.00  0.00           H  \nATOM     30    H MOL     1       5.090   6.585   5.947  1.00  0.00           H  \nATOM     31    H MOL     1       0.656   9.545   0.281  1.00  0.00           H  \nATOM     32    H MOL     1       0.078   8.755   8.760  1.00  0.00           H  \nATOM     33    H MOL     1       0.304   1.958   3.840  1.00  0.00           H  \nATOM     34    H MOL     1       8.784   2.697   3.526  1.00  0.00           H  \nATOM     35    H MOL     1       0.782   2.630   7.958  1.00  0.00           H  \nATOM     36    H MOL     1       9.770   1.394   8.432  1.00  0.00           H  \nATOM     37    H MOL     1       6.838   8.812   8.805  1.00  0.00           H  \nATOM     38    H MOL     1       6.771   9.006   7.203  1.00  0.00           H  \nATOM     39    H MOL     1       0.449   4.286   4.380  1.00  0.00           H  \nATOM     40    H MOL     1       1.223   4.811   5.650  1.00  0.00           H  \nATOM     41    H MOL     1       0.659   6.331   8.558  1.00  0.00           H  \nATOM     42    H MOL     1       0.840   5.175   9.596  1.00  0.00           H  \nATOM     43    H MOL     1       6.632   2.439   4.015  1.00  0.00           H  \nATOM     44    H MOL     1       7.261   1.271   3.042  1.00  0.00           H  \nATOM     45    H MOL     1       5.287   9.378   0.832  1.00  0.00           H  \nATOM     46    H MOL     1       5.847   7.923   0.758  1.00  0.00           H  \nATOM     47    H MOL     1       2.572   9.117   6.960  1.00  0.00           H  \nATOM     48    H MOL     1       2.668   7.845   6.117  1.00  0.00           H  \nATOM     49    H MOL     1       6.828   5.545   6.904  1.00  0.00           H  \nATOM     50    H MOL     1       6.262   6.849   7.335  1.00  0.00           H  \nATOM     51    H MOL     1       0.095   1.065   6.164  1.00  0.00           H  \nATOM     52    H MOL     1       0.829   9.783   5.438  1.00  0.00           H  \nATOM     53    H MOL     1       4.665   8.194   4.338  1.00  0.00           H  \nATOM     54    H MOL     1       5.234   8.825   2.953  1.00  0.00           H  \nATOM     55    H MOL     1       1.065   1.147   1.757  1.00  0.00           H  \nATOM     56    H MOL     1       1.926   0.655   2.906  1.00  0.00           H  \nATOM     57    H MOL     1       9.321   3.504   0.509  1.00  0.00           H  \nATOM     58    H MOL     1       9.795   3.728   1.977  1.00  0.00           H  \nATOM     59    H MOL     1       4.586   3.097   2.807  1.00  0.00           H  \nATOM     60    H MOL     1       5.405   4.080   1.907  1.00  0.00           H  \nATOM     61    H MOL     1       3.703   1.527   4.656  1.00  0.00           H  \nATOM     62    H MOL     1       4.243   0.501   3.660  1.00  0.00           H  \nATOM     63    H MOL     1       7.271   2.621   1.283  1.00  0.00           H  \nATOM     64    H MOL     1       8.005   1.565   0.365  1.00  0.00           H  \nATOM     65    O MOL     1       3.810   9.787   7.859  1.00  0.00           O  \nATOM     66    O MOL     1       9.694   7.537   7.591  1.00  0.00           O  \nATOM     67    O MOL     1       7.192   9.766   5.709  1.00  0.00           O  \nATOM     68    O MOL     1       8.242   6.762   5.015  1.00  0.00           O  \nATOM     69    O MOL     1       8.131   9.422   3.098  1.00  0.00           O  \nATOM     70    O MOL     1       3.767   2.095   6.472  1.00  0.00           O  \nATOM     71    O MOL     1       8.445   4.361   6.857  1.00  0.00           O  \nATOM     72    O MOL     1       2.632   2.661   0.908  1.00  0.00           O  \nATOM     73    O MOL     1       5.104   6.413   0.189  1.00  0.00           O  \nATOM     74    O MOL     1       2.893   5.947   3.296  1.00  0.00           O  \nATOM     75    O MOL     1       3.410   0.055   0.609  1.00  0.00           O  \nATOM     76    O MOL     1       1.878   4.016   7.124  1.00  0.00           O  \nATOM     77    O MOL     1       6.356   2.497   5.799  1.00  0.00           O  \nATOM     78    O MOL     1       2.483   7.475   1.262  1.00  0.00           O  \nATOM     79    O MOL     1       4.275   6.898   5.472  1.00  0.00           O  \nATOM     80    O MOL     1       0.057   9.543   9.385  1.00  0.00           O  \nATOM     81    O MOL     1       9.785   2.869   3.606  1.00  0.00           O  \nATOM     82    O MOL     1       9.736   2.229   7.927  1.00  0.00           O  \nATOM     83    O MOL     1       6.330   8.652   7.972  1.00  0.00           O  \nATOM     84    O MOL     1       0.808   5.089   4.786  1.00  0.00           O  \nATOM     85    O MOL     1       1.310   5.833   9.056  1.00  0.00           O  \nATOM     86    O MOL     1       7.018   2.242   3.108  1.00  0.00           O  \nATOM     87    O MOL     1       6.109   8.853   1.025  1.00  0.00           O  \nATOM     88    O MOL     1       2.035   8.538   6.351  1.00  0.00           O  \nATOM     89    O MOL     1       5.999   5.944   7.121  1.00  0.00           O  \nATOM     90    O MOL     1       0.016   0.468   5.417  1.00  0.00           O  \nATOM     91    O MOL     1       5.212   8.893   3.939  1.00  0.00           O  \nATOM     92    O MOL     1       1.085   0.402   2.393  1.00  0.00           O  \nATOM     93    O MOL     1       0.123   4.013   1.051  1.00  0.00           O  \nATOM     94    O MOL     1       4.577   3.970   2.343  1.00  0.00           O  \nATOM     95    O MOL     1       3.567   1.234   3.713  1.00  0.00           O  \nATOM     96    O MOL     1       7.650   2.505   0.392  1.00  0.00           O  \nENDMDL\n","binary":false}],"kwargs":{"name":"nglview.adaptor.ASETrajectory","defaultRepresentation":false,"ext":"pdb"}},{"target":"Widget","type":"call_method","methodName":"setSize","reconstruc_color_scheme":false,"args":["500px","500px"],"kwargs":{}},{"component_index":0,"target":"compList","type":"call_method","methodName":"addRepresentation","reconstruc_color_scheme":false,"args":["unitcell"],"kwargs":{"sele":"all"}},{"component_index":0,"target":"compList","type":"call_method","methodName":"addRepresentation","reconstruc_color_scheme":false,"args":["spacefill"],"kwargs":{"sele":"all"}},{"target":"Stage","type":"call_method","methodName":"setParameters","reconstruc_color_scheme":false,"args":[],"kwargs":{"cameraType":"orthographic"}},{"target":"Widget","type":"call_method","methodName":"setParameters","reconstruc_color_scheme":false,"args":[{"clipDist":0}],"kwargs":{}},{"target":"Widget","type":"call_method","methodName":"updateRepresentationsByName","reconstruc_color_scheme":false,"args":["spacefill",0],"kwargs":{"radiusType":"covalent","radiusScale":0.5,"colorScheme":"element","colorScale":"rainbow"}},{"target":"Widget","type":"call_method","methodName":"updateRepresentationsByName","reconstruc_color_scheme":false,"args":["spacefill",0],"kwargs":{"radiusType":"covalent","radiusScale":0.5,"colorScheme":"element","colorScale":"rainbow"}},{"target":"Widget","type":"call_method","methodName":"updateRepresentationsByName","reconstruc_color_scheme":false,"args":["spacefill",0],"kwargs":{"radiusType":"covalent","radiusScale":0.49999999999999994,"colorScheme":"element","colorScale":"rainbow"}},{"target":"Widget","type":"call_method","methodName":"updateRepresentationsByName","reconstruc_color_scheme":false,"args":["spacefill",0],"kwargs":{"radiusType":"covalent","radiusScale":0.49999999999999994,"colorScheme":"element","colorScale":"rainbow"}}],"_ngl_original_stage_parameters":{"impostor":true,"quality":"medium","workerDefault":true,"sampleLevel":0,"backgroundColor":"white","rotateSpeed":2,"zoomSpeed":1.2,"panSpeed":1,"clipNear":0,"clipFar":100,"clipDist":10,"fogNear":50,"fogFar":100,"cameraFov":40,"cameraEyeSep":0.3,"cameraType":"perspective","lightColor":14540253,"lightIntensity":1,"ambientColor":14540253,"ambientIntensity":0.2,"hoverTimeout":0,"tooltip":true,"mousePreset":"default"},"_ngl_repr_dict":{"0":{"0":{"type":"unitcell","params":{"lazy":false,"visible":true,"quality":"medium","radiusSize":0.04924999783811635,"sphereDetail":1,"radialSegments":10,"disableImpostor":false,"radiusType":"vdw","radiusData":{},"radiusScale":1,"defaultAssembly":"","clipNear":0,"clipRadius":0,"clipCenter":{"x":0,"y":0,"z":0},"flatShaded":false,"opacity":1,"depthWrite":true,"side":"double","wireframe":false,"colorScheme":"element","colorScale":"","colorReverse":false,"colorValue":"orange","colorMode":"hcl","roughness":0.4,"metalness":0,"diffuse":16777215,"diffuseInterior":false,"useInteriorColor":true,"interiorColor":2236962,"interiorDarkening":0,"matrix":{"elements":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1]},"disablePicking":false,"sele":"all"}},"1":{"type":"unitcell","params":{"lazy":false,"visible":true,"quality":"medium","radiusSize":0.04924999783811635,"sphereDetail":1,"radialSegments":10,"disableImpostor":false,"radiusType":"vdw","radiusData":{},"radiusScale":1,"defaultAssembly":"","clipNear":0,"clipRadius":0,"clipCenter":{"x":0,"y":0,"z":0},"flatShaded":false,"opacity":1,"depthWrite":true,"side":"double","wireframe":false,"colorScheme":"element","colorScale":"","colorReverse":false,"colorValue":"orange","colorMode":"hcl","roughness":0.4,"metalness":0,"diffuse":16777215,"diffuseInterior":false,"useInteriorColor":true,"interiorColor":2236962,"interiorDarkening":0,"matrix":{"elements":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1]},"disablePicking":false,"sele":"all"}},"2":{"type":"spacefill","params":{"lazy":false,"visible":true,"quality":"medium","sphereDetail":1,"disableImpostor":false,"radiusType":"covalent","radiusData":{},"radiusSize":1,"radiusScale":0.5,"assembly":"default","defaultAssembly":"","clipNear":0,"clipRadius":0,"clipCenter":{"x":0,"y":0,"z":0},"flatShaded":false,"opacity":1,"depthWrite":true,"side":"double","wireframe":false,"colorScheme":"element","colorScale":"rainbow","colorReverse":false,"colorValue":9474192,"colorMode":"hcl","roughness":0.4,"metalness":0,"diffuse":16777215,"diffuseInterior":false,"useInteriorColor":true,"interiorColor":2236962,"interiorDarkening":0,"matrix":{"elements":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1]},"disablePicking":false,"sele":"all"}},"3":{"type":"spacefill","params":{"lazy":false,"visible":true,"quality":"medium","sphereDetail":1,"disableImpostor":false,"radiusType":"covalent","radiusData":{},"radiusSize":1,"radiusScale":0.5,"assembly":"default","defaultAssembly":"","clipNear":0,"clipRadius":0,"clipCenter":{"x":0,"y":0,"z":0},"flatShaded":false,"opacity":1,"depthWrite":true,"side":"double","wireframe":false,"colorScheme":"element","colorScale":"rainbow","colorReverse":false,"colorValue":9474192,"colorMode":"hcl","roughness":0.4,"metalness":0,"diffuse":16777215,"diffuseInterior":false,"useInteriorColor":true,"interiorColor":2236962,"interiorDarkening":0,"matrix":{"elements":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1]},"disablePicking":false,"sele":"all"}}},"1":{}},"_ngl_serialize":false,"_ngl_version":"2.0.0-dev.36","_ngl_view_id":["B68D02FB-96F1-4814-9A4E-723F555309C1"],"_player_dict":{},"_scene_position":{},"_scene_rotation":{},"_synced_model_ids":[],"_synced_repr_model_ids":[],"_view_count":null,"_view_height":"","_view_module":"nglview-js-widgets","_view_module_version":"3.0.1","_view_name":"NGLView","_view_width":"","background":"white","frame":0,"gui_style":null,"layout":"IPY_MODEL_482923ba20e84495a502bda7a8d8bf5b","max_frame":100,"n_components":2,"picked":{}}},"937d4dab5bc44537899ded77688af525":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_f4fd7d83dd604ab58c49fe1918864ca8","IPY_MODEL_740a307f2d2840158a4a595b05790e2f","IPY_MODEL_f2c68e39f04241e8a2bb8fe7083affd8","IPY_MODEL_d7899215c668451ebfb1eea2e15075d9"],"layout":"IPY_MODEL_9cdc2060c25a4569892a9477578d4626"}},"2367c267599e4934b35f60d6527e0c02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4fd7d83dd604ab58c49fe1918864ca8":{"model_module":"@jupyter-widgets/controls","model_name":"DropdownModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DropdownModel","_options_labels":["All","H","O"],"_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"DropdownView","description":"Show","description_tooltip":null,"disabled":false,"index":0,"layout":"IPY_MODEL_bf27d5ebba8f4dd1b943dde2816e17db","style":"IPY_MODEL_cdfb132b6041499c9a8bd7b085c5b83a"}},"740a307f2d2840158a4a595b05790e2f":{"model_module":"@jupyter-widgets/controls","model_name":"DropdownModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DropdownModel","_options_labels":[" ","picking","random","uniform","atomindex","residueindex","chainindex","modelindex","sstruc","element","resname","bfactor","hydrophobicity","value","volume","occupancy"],"_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"DropdownView","description":"Color scheme","description_tooltip":null,"disabled":false,"index":9,"layout":"IPY_MODEL_bcce3d2ae3f14555ba2bca117ba9861a","style":"IPY_MODEL_ddfc1304402e40ac9c37541f0d57a278"}},"f2c68e39f04241e8a2bb8fe7083affd8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatSliderModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FloatSliderView","continuous_update":true,"description":"Ball size","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_21734170ce974a9698ddc8ab35adeb3f","max":1.5,"min":0,"orientation":"horizontal","readout":true,"readout_format":".2f","step":0.01,"style":"IPY_MODEL_03f3727220834b2eb5d44e1420789043","value":0.49999999999999994}},"d7899215c668451ebfb1eea2e15075d9":{"model_module":"@jupyter-widgets/controls","model_name":"IntSliderModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"IntSliderView","continuous_update":true,"description":"","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_5ef1e44123f84da18021f44cdc8af6d4","max":100,"min":0,"orientation":"horizontal","readout":true,"readout_format":"d","step":1,"style":"IPY_MODEL_1565e6d619c2498fb64ddc72943dc5c0","value":0}},"9cdc2060c25a4569892a9477578d4626":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf27d5ebba8f4dd1b943dde2816e17db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdfb132b6041499c9a8bd7b085c5b83a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bcce3d2ae3f14555ba2bca117ba9861a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddfc1304402e40ac9c37541f0d57a278":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21734170ce974a9698ddc8ab35adeb3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03f3727220834b2eb5d44e1420789043":{"model_module":"@jupyter-widgets/controls","model_name":"SliderStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"5ef1e44123f84da18021f44cdc8af6d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1565e6d619c2498fb64ddc72943dc5c0":{"model_module":"@jupyter-widgets/controls","model_name":"SliderStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"482923ba20e84495a502bda7a8d8bf5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7f693793c704790aea2197d2501c8d8":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"","disabled":false,"icon":"compress","layout":"IPY_MODEL_f9d40143745a49d38c8324d09ba9b1cb","style":"IPY_MODEL_1c47d1bc5c6940868dcca94f1c38a36c","tooltip":""}},"ed3b8d237ad641079c75d0dba8064f85":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_17bc66541a4e474798efb323e30ab63b","IPY_MODEL_bd3ac267a17a45d697071d1904c00f8b"],"layout":"IPY_MODEL_173a62d58ae144c38b1a55228f0071b5"}},"b7363bd641eb46f781a4b7342dda3926":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98c82b80dfd943bfbdd029f80f340b8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dcf28aa26c7148e39cc898434e12d4f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa8be0046f18429ca2cc286dc3004839":{"model_module":"@jupyter-widgets/controls","model_name":"SliderStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"17bc66541a4e474798efb323e30ab63b":{"model_module":"@jupyter-widgets/controls","model_name":"PlayModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PlayModel","_playing":false,"_repeat":false,"_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PlayView","description":"","description_tooltip":null,"disabled":false,"interval":100,"layout":"IPY_MODEL_b7363bd641eb46f781a4b7342dda3926","max":100,"min":0,"show_repeat":true,"step":1,"style":"IPY_MODEL_98c82b80dfd943bfbdd029f80f340b8a","value":0}},"bd3ac267a17a45d697071d1904c00f8b":{"model_module":"@jupyter-widgets/controls","model_name":"IntSliderModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"IntSliderView","continuous_update":true,"description":"","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_dcf28aa26c7148e39cc898434e12d4f7","max":100,"min":0,"orientation":"horizontal","readout":true,"readout_format":"d","step":1,"style":"IPY_MODEL_fa8be0046f18429ca2cc286dc3004839","value":0}},"173a62d58ae144c38b1a55228f0071b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9d40143745a49d38c8324d09ba9b1cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"34px"}},"1c47d1bc5c6940868dcca94f1c38a36c":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}}}}},"nbformat":4,"nbformat_minor":0}